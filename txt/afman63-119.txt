BY ORDER OF THE                                                          AIF FORCE MANUAL 63-119
SECRETARY OF THE AIR FORCE                                                                20 JUNE 2008
                                                                          Certified Current 14 July 2010
                                                                                             Acquisition

                                                         CERTIFICATION OF SYSTEM READINESS
                                                        FOR DEDICATED OPERATIONAL TESTING



                 COMPLIANCE WITH THIS PUBLICATION IS MANDATORY

ACCESSIBILITY:         Publications and forms are available on the e-Publishing website at
                       www.e-publishing.af.mil for downloading or ordering.
RELEASABILITY: There are no releasability restrictions on this publication.


OPR: HQ USAF/TEP                                         Certified by: SAF/AQX (Mr. Blaise J. Durante)
Supersedes AFMAN 63-119, 22 Feb 95                                                          Pages: 74

This Air Force Manual (AFMAN) implements Air Force Policy Directive 63-1, Capability-Based Acqui-
sition System. It implements a process for certification of system readiness for dedicated operational test
and evaluation (OT&E) as required by Department of Defense Instruction (DODI) 5000.2, Operation of
the Defense Acquisition System. National Security Space (NSS) Acquisition Policy 03-01 must also be
consulted for space systems. Air Force Instruction (AFI) 99-103, Capabilities-Based Test and Evaluation,
requires certification for additional types of operational testing beyond those required by DODI 5000.2.
AFI 63-101, Operation of Capabilities Based Acquisition System, also requires use of this AFMAN and
certification process. Use this AFMAN with Air Force 10-, 13-, 14-, 16-, 21-, 33-, 63-, 91-, 99-, and other
series publications to ensure full understanding of all policies, terms, and concepts. This AFMAN applies
to all Air Force organizations involved in system acquisition or modification, including Air National
Guard and US Air Force Reserve Command units and members. Send proposed supplements or changes
to this AFMAN to SAF/AQXA, 1060 Air Force Pentagon, Washington DC 20330-1060; and HQ USAF/
TEP, 1530 Air Force Pentagon, Washington DC, 20330-1530. Ensure all records created as a result of pro-
cesses prescribed in this AFMAN are maintained according to AFMAN 37-123, Management of Records
(will convert to AFMAN 33-363), and disposed of according to Air Force Information Management Sys-
tem (AFRIMS) Records Disposition Schedule (RDS) located at https://afrims.amc.af.mil/
rds_series.cfm.

SUMMARY OF CHANGES
This document was substantially revised and must be completely reviewed. It reflects changes in the
DOD 5000-series documents, Chairman of the Joint Chiefs of Staff instructions (CJCSI), CJCS manuals
(CJCSM), and numerous AFIs and Air Force Policy Directives (AFPD). New templates were added for
Information Technology (IT) and National Security Systems (NSS), Test Plans That Are Integrated, and
Integrated Test Teams (ITT). Other templates were renamed, combined, or eliminated to make the certifi-
cation process more streamlined. All terminology was updated.
2                                                                                                   AFMAN63-119 20 JUNE 2008


Chapter 1— OVERVIEW OF THE CERTIFICATION PROCESS                                                                                              5
      1.1.    Overview. ...................................................................................................................    5
      1.2.    Applicability. .............................................................................................................     5
      1.3.    Delegation of Certification Official. ..........................................................................                 6
      1.4.    Responsibilities. .........................................................................................................      6
      1.5.    Links to Reference Documents. .................................................................................                  8

Chapter 2— THE CERTIFICATION PROCESS                                                                                                          9
      2.1.    Overview. ...................................................................................................................    9
      2.2.    Template Subject Matter. ...........................................................................................             9
Figure 2.1.   Matrix of Certification Templates. ...........................................................................                  10
      2.3.    Team Effort. ...............................................................................................................    11
      2.4.    Tailoring the Process. ................................................................................................         11
      2.5.    Continuous Process. ...................................................................................................         11
Figure 2.2.   Notional Timing of Certification Process Reviews. .................................................                             12
      2.6.    The Certification Review Cycle. ...............................................................................                 13
      2.7.    Certification Memo Purpose. .....................................................................................               15
      2.8.    Updating the Templates. ............................................................................................            16

Chapter 3— TEMPLATE STRUCTURE AND USE                                                                                                         17
      3.1.    Interlocking Matrix. ...................................................................................................        17
      3.2.    Consolidation of Multiple Sources. ...........................................................................                  17
      3.3.    Answering Template Line Items. ...............................................................................                  17
      3.4.    Focus on Ends, Not Means. .......................................................................................               17
      3.5.    Assigning Responsibilities. ........................................................................................            17
      3.6.    Certification Template Tracking Tool. ......................................................................                    18
      3.7.    Information Collection, Records, and Forms. ............................................................                        18

Attachment 1— GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION                                                                               19

Attachment 2— ACQUISITION STRATEGY AND SCHEDULE                                                                                               36

Attachment 3— ANALYSIS OF ALTERNATIVES (AOA)                                                                                                  37

Attachment 4— CAPABILITY BASED REQUIREMENTS DOCUMENTS (CBRD)                                                                                  38
AFMAN63-119 20 JUNE 2008                                            3


Attachment 5— THREAT DOCUMENTS                                     40

Attachment 6— INTEGRATED TEST TEAM (ITT) STANDUP AND ITT CHARTER   41

Attachment 7— TEST AND EVALUATION STRATEGY                         42

Attachment 8— AIR FORCE CONCEPTS                                   43

Attachment 9— LOGISTICS SUPPORT CONCEPTS (LSC) AND STRATEGIES      44

Attachment 10— INFORMATION TECHNOLOGY (IT) AND NATIONAL SECURITY
           SYSTEMS (NSS)                                           45

Attachment 11— TEMP OR SAMP                                        47

Attachment 12— TEST PLANS THAT ARE INTEGRATED                      48

Attachment 13— PROGRAM PROTECTION AND SECURITY                     49

Attachment 14— CONTRACTOR TESTING                                  50

Attachment 15— DEVELOPMENTAL TEST AND EVALUATION (DT&E)            52

Attachment 16— SOFTWARE DEVELOPMENT AND MATURITY                   54

Attachment 17— LIVE FIRE TEST AND EVALUATION (LFT&E)               55

Attachment 18— MODELING AND SIMULATION (M&S)                       57

Attachment 19— CONFIGURATION MANAGEMANT PLAN (CMP)                 58

Attachment 20— DEFICIENCY IDENTIFICATION AND RESOLUTION PROCESS    59

Attachment 21— PRODUCTION REPRESENTATIVE TEST ARTICLES             60

Attachment 22— SYSTEM PERFORMANCE                                  61

Attachment 23— OPERATIONAL TEST AND EVALUATION PLAN                62

Attachment 24— TEST AND EVALUATION RESOURCES                       64

Attachment 25— PROGRAMMATIC ENVIRONMENT, SAFETY, AND
           OCCUPATIONAL HEALTH EVALUATION (PESHE)                  65

Attachment 26— OPERATIONAL TEST TEAM TRAINING                      67
4                                               AFMAN63-119 20 JUNE 2008


Attachment 27— SUPPORT EQUIPMENT (SE)                                 68

Attachment 28— SUFFICIENCY OF SPARES                                  69

Attachment 29— SUPPORT AGREEMENTS                                     70

Attachment 30— PACKAGING, HANDLING, AND TRANSPORTATION                71

Attachment 31— PERSONNEL                                              72

Attachment 32— CONTRACTOR SUPPORT                                     73

Attachment 33— TECHNICAL DATA                                         74
AFMAN63-119 20 JUNE 2008                                                                                  5


                                                Chapter 1

                        OVERVIEW OF THE CERTIFICATION PROCESS

1.1. Overview. This AFMAN provides a structured mechanism for identifying and reducing risks associ-
ated with transitioning from developmental test and evaluation (DT&E) to dedicated operational testing.
It establishes a disciplined review and “certification process” in the early stages of acquisition and modi-
fication programs, and culminates in more successful operational test outcomes. The certification process
is a tool to help acquisition managers at all levels identify risks, reach negotiated agreements on issues,
and render more accurate assessments of system readiness to begin dedicated operational testing. The pro-
cess is supported by 32 “templates” based on DOD and Air Force policy, historical information, best prac-
tices, practical advice, and lessons learned from numerous acquisition programs. The certification process
helps document the pursuit of a credible risk reduction program and an effective development program.
This certification process is mandatory and must be implemented as a continuous effort, not a single event
in time.

1.2. Applicability. DODI 5000.2, Enclosure 5, requires the Services to establish a process for evaluating
and determining materiel system readiness for initial OT&E (IOT&E). This process will be used for pro-
grams on the Office of the Secretary of Defense (OSD) Test and Evaluation (T&E) Oversight List, acqui-
sition category (ACAT)-designated programs on the Air Force non-space Acquisition Program Master
List (APML), and space programs designated by the Air Force Service Acquisition Executive (SAE) for
space. Sustainment programs on the Sustainment Program Master List (SPML) and other acquisition
projects or sustainment actions falling below APML or SPML thresholds are highly encouraged to follow
this process.
   1.2.1. DODI 5000.2 requires the SAE to evaluate and determine materiel system readiness to enter
   the dedicated phase of IOT&E. It requires a review of DT&E results; an assessment of the system’s
   progress against the critical technical parameters (CTP) documented in the test and evaluation master
   plan (TEMP); an analysis of identified technical risks to verify that those risks have been mitigated
   during DT&E; and a review of the OT&E entrance criteria specified in the TEMP or other operational
   test plans.
   1.2.2. The National Security Space system acquisition process described in NSS 03-01 is significantly
   different than the acquisition process in DoDI 5000.2 and AFI 63-101. NSS 03-01 uses a streamlined
   acquisition framework that causes the key decision points for NSS acquisition programs to be phased
   earlier than typical DoD 5000-series milestones and decision reviews. However, the basic T&E sup-
   port provided to NSS systems is similar to non-space systems for equivalent decision points. The tem-
   plates in this AFMAN should be modified accordingly. See NSS 03-01, paragraph AP1.1.6 and others,
   for additional guidance.
   1.2.3. In addition, AFI 99-103, Chapters 2, 3, and 6, require Air Force program managers (PM) to use
   this certification process to evaluate system readiness for operational testing in support of a full-rate
   production (FRP) and/or fielding decision for all acquisition programs. The certification process will
   also be used for systems in sustainment that require an FRP and/or fielding decision. Note: Some sys-
   tems, programs, and activities may be exempt from this AFMAN according to AFI 99-103, paragraph
   1.7.
6                                                                         AFMAN63-119 20 JUNE 2008


    1.2.4. This AFMAN provides 32 “templates” in Attachment 2 through Attachment 33 that list spe-
    cific problem or risk areas that could hinder the smooth transition to and execution of dedicated oper-
    ational testing. Use these templates for reviewing important program details in order to assess
    program readiness in sufficient depth. Note: While use of the certification process is mandatory, the
    contents of each template are not mandatory and will not supersede existing DOD or Air Force policy
    or guidance. The templates should be used in parallel with, not substitutes for, formal DOD or Air
    Force policy and guidance.
    1.2.5. For the purposes of this AFMAN, “dedicated operational testing” refers to that phase of opera-
    tional testing that must be conducted independently of developers and users in support of an FRP and/
    or fielding decision. Program offices using an evolutionary acquisition strategy will need to repeat this
    certification process for each increment of capability developed, produced and/or fielded. Note: The
    direction in this AFMAN is based on parameters and descriptions given in AFI 99-103, Chapter 2,
    paragraph 2.6 et seq which explain the differences between various types of operational testing. These
    paragraphs establish the basis for determining when readiness certification will be required. This
    AFMAN is only required for the dedicated operational test portions of an integrated test program. It is
    not required for combined developmental and operational test activities.
    1.2.6. This certification process will be the primary OT&E certification method for all programs, to
    include space systems covered by NSS 03-01, when the Air Force is the lead Service. For programs
    where the Air Force is not the lead Service, Air Force ITT members will adapt the Air Force process
    to flow into the other Service's certification process.
    1.2.7. Prior to the early deployment of prototypes and ACTD/JCTDs, use the appropriate certification
    templates, modified as necessary, to review the system’s capabilities and limitations and its readiness
    for initial deployment. Going through the formal certification process would not be necessary in this
    situation.

1.3. Delegation of Certification Official. Although DODI 5000.2 requires the SAE to evaluate and
determine system readiness for IOT&E, the applicable Air Force SAE (space and non-space) may dele-
gate this authority in writing for assigned programs to a lower milestone decision authority (MDA) such
as a program executive officer (PEO) or his/her Deputy for Acquisition. The applicable SAE will deter-
mine the OT&E Certification Official for ACAT ID programs. OT&E Certification Officials for smaller
programs originating at MAJCOM or Center levels may be delegated to a subordinate level as appropri-
ate. Note: Under no circumstance shall a PM be the OT&E Certification Official for his/her own program.

1.4. Responsibilities. The certification process cuts across organizational lines and brings together stake-
holders from the acquisition, requirements, developer, T&E, and sustainment communities. Other stake-
holder organizations are responsible for providing test data, supporting information, studies, analyses, and
candid feedback for assigned areas in support of the certification process. Each line item in the templates
suggests a single “most likely” office of primary responsibility (OPR) for that item. Additional offices or
organizations may also be involved, but only the OPR is cited. The following organizations or officials (or
their representatives) are required to participate in the certification process.
    1.4.1. OT&E Certification Official. The OT&E Certification Official, with advice from the inte-
    grated test team (ITT), and as designated in the TEMP, will determine the broad scope and require-
    ments for certifying system readiness to begin the dedicated phase of operational testing.
    1.4.2. Program Managers (PM). PMs will:
AFMAN63-119 20 JUNE 2008                                                                                  7


     1.4.2.1. Ensure a robust systems engineering process is the underlying foundation for systems
     development and for reviewing these templates.
     1.4.2.2. Ensure their system is mature and demonstrates stabilized performance in an operation-
     ally relevant environment prior to certification. Additionally, all necessary test support must be
     available and the system must have a high likelihood of a successful operational test.
     1.4.2.3. Designate an OPR for organizing the certification process, gathering information, sched-
     uling reviews, assigning tasks, negotiating consensus on issues and solutions, assembling certifi-
     cation briefings, and drafting the final certification memo according to this AFMAN.
     1.4.2.4. Document the strategy for the certification process in Part III of the TEMP.
     1.4.2.5. Request additional stakeholder organizations to participate in this process as necessary to
     ensure acquisition program success.
  1.4.3. Air Force Operational Test and Evaluation Center (AFOTEC). If AFOTEC is the opera-
  tional test agency (OTA), they will participate in the certification process by assisting the PM and car-
  rying out responsibilities as agreed. They will lead the effort to mobilize resources required for
  dedicated OT&E, and provide advice, test support, and test data to the PM and user throughout the
  development process. Note: For multi-Service programs, the certification policies of the lead Service
  will be used. If any Air Force operational test organization is the lead operational tester, this AFMAN
  will be used. If the Air Force is not the lead Service, this AFMAN may or may not be the governing
  document. Nonetheless, it should be used for Air Force portions of certification activities.
  1.4.4. Major Command (MAJCOM) Operational Test Organizations. I f t h e M A J C O M i s
  responsible for conducting operational testing, they will perform the same certification functions as
  AFOTEC would have performed. MAJCOMs will assist the PM in implementing this certification
  process for force development evaluations (FDE) or operational utility evaluations (OUE) when FRP
  and/or fielding decisions are planned. Note: The acronym “OTA” is used in the templates to denote
  either the MAJCOM operational tester or AFOTEC, whichever applies.
  1.4.5. Lead Operating Command. The lead operating command, or using commands as appropri-
  ate, will participate in the certification process by assisting the PM and operational testers (i.e., AFO-
  TEC or MAJCOM) and carrying out responsibilities as agreed. The lead operating command will
  ensure capability requirements documents are complete and up to date according to AFI 10-601,
  Capabilities Based Requirements Development.
  1.4.6. Responsible Test Organizations (RTO). The RTO or equivalent organization will conduct
  DT&E and support operational testing of systems according to AFI 99-103, AFI 63-101, and MAJ-
  COM policies. The RTO will participate in the certification process by providing sufficient analysis
  results and supporting data, operator comments, and recommendations to the PM to support the PM’s
  responsibilities in paragraph1.4.2..
  1.4.7. HQ USAF Staff. Representatives from SAF/AQ, SAF/US, HQ USAF/TE, and others as
  needed will monitor the certification process for continued effectiveness and periodically update these
  templates as policy changes dictate. These staff members should attend certification proceedings
  when HQ USAF assistance is required.
  1.4.8. OSD Staff. The Office of the Undersecretary of Defense, Acquisition, Technology, and Logis-
  tics (OSD(AT&L)) and OSD/DOT&E staff should closely monitor this process if the program is on
  OSD T&E Oversight.
8                                                                       AFMAN63-119 20 JUNE 2008


    1.4.9. Other Stakeholder Organizations. Other support organizations and participating test organi-
    zations (PTO) should support the PM and the certification process as requested. Joint Interoperability
    Test Command (JITC) staff should be invited to participate for systems with net-ready key perfor-
    mance parameter (NR-KPP) (i.e., interoperability or information assurance) requirements.

1.5. Links to Reference Documents. The most current versions of documents referenced in this
AFMAN are available electronically. For Air Force publications, check http://www.e-publishing.af.mil/
. For DOD publications, check http://www.dtic.mil/whs/directives/.
AFMAN63-119 20 JUNE 2008                                                                                9


                                               Chapter 2

                                 THE CERTIFICATION PROCESS

2.1. Overview. Proper risk management requires the development of a systematic, disciplined plan to
identify problems and risks. A proven risk management technique is to examine the successes, failures,
problems, and solutions of similar or past programs for "lessons learned" that can be applied to current
programs. Another technique is to systematically comb through the entire program using specific decision
criteria based on historical data. The certification process combines these techniques with a system for
assigning responsibility and tracking accountability for results.

2.2. Template Subject Matter. The matrix of 32 certification templates in Figure 2.1., Matrix of Certi-
fication Templates, covers a broad range of subjects that have historically impacted systems transitioning
from DT&E to dedicated operational testing. Not all templates apply equally to every program; however,
all templates should be considered for applicability at each review to ensure every relevant area at that
point in time is covered. The initial template review should reveal where to begin working on long lead
items that usually come to fruition much later in development programs. The templates are arranged in
three notional groups in approximate chronological order: Test Planning and Documentation; System
Design and Performance; and Test Assets and Support. The templates may be re-ordered as desired. These
templates may be used in conjunction with the templates in Department of Defense (DOD) 4245.7-M,
Transition from Development to Production. All templates are designed to increase the visibility of poten-
tial risk factors and facilitate a streamlined, executive-level review.
Figure 2.1. Matrix of Certification Templates.




                                                 10
                                                 AFMAN63-119 20 JUNE 2008
AFMAN63-119 20 JUNE 2008                                                                                  11


2.3. Team Effort. Since any risk reduction process is a team function, PMs must provide the right orga-
nizational structure and continuous motivation to make it effective. Risk is mitigated only when condi-
tions that contribute to risk are adequately addressed. These risk reduction efforts are typically within the
scope, reach, and authority of certification process participants to effect necessary changes. These
changes will typically occur at levels not normally visible to senior decision makers on a day-to-day basis.

2.4. Tailoring the Process. As early as practical, PMs and OT&E Certification Authorities should tailor
the certification process to their need for information. The Certification Review Cycle, described in para-
graph 2.6., should be repeated as often as necessary.
   2.4.1. Templates Not Program Specific. Since the templates are not program specific, PMs and
   OT&E Certification Authorities may tailor them, with operational tester and operational command
   assistance, to fit specific programs or groups of programs. Some templates may require greater or
   lesser emphasis depending on the program and its phase of development. The templates give PMs
   maximum flexibility in focusing and structuring their reviews without losing sight of the original
   objective--providing an executive-level review of the program.
   2.4.2. Tailoring Level of Detail. PMs may attach additional information or levels of detail to the
   templates at their discretion. Some examples might be exit and pass-fail criteria, action plans, require-
   ments thresholds, lists of acquisition regulations and standards, watch lists, breakdowns of specific
   line items, and points of contact. Additional templates can be developed to cover other areas. Addi-
   tionally, aggregation of templates and template line items can reduce redundancy and help managers
   concentrate on known risk areas. In short, tailor each certification program to attain the best results.

2.5. Continuous Process. The certification process must be viewed as a continuous effort, not a single
event in time. It is not tied to any particular acquisition milestone or decision review; however, the final
certification of system readiness briefing must be completed no later than 45 days prior to the planned
start of dedicated operational testing, or as mutually agreed between the PM and operational testers. Any
dedicated operational testing that supports an FRP and/or fielding decision must be supported by a readi-
ness certification. Use the following guidelines:
   2.5.1. Starting Early. Templates may be reviewed in any order that makes sense for the program and
   phase of development. All templates will be initially reviewed and considered for applicability. Those
   that are clearly not relevant to the program may be set aside. Templates previously set aside could
   become relevant again later as program dynamics change (e.g., when a capabilities based require-
   ments document (CBRD) is re-issued after an insertion of new technology). To be most effective, the
   certification process must begin as early as practical in new development programs.
   2.5.2. Series of Reviews. The certification process is a series of reviews culminating in a final readi-
   ness briefing as shown in the notional diagram in Figure 2.2., Notional Timing of Certification Pro-
   cess Reviews. For example, certification reviews (and briefings, if required) should be planned as
   progress checks prior to early operational assessments (EOA) or before each operational assessment
   (OA). The final review and briefing should be scheduled not later than 45 days prior to the start of
   dedicated operational testing, or as mutually agreed. This series of reviews is designed to aid in
   resolving problems or correcting deficiencies as soon as they are discovered, rather than waiting until
   the final certification review and briefing where late remedial action could cause delays in the start of
   dedicated operational testing.
12                                                                        AFMAN63-119 20 JUNE 2008


Figure 2.2. Notional Timing of Certification Process Reviews.




NOTES:
All acronyms in this figure are defined in Attachment 1.
For applicability of this figure to NSS systems, see paragraph 1.2.1.
     2.5.3. Initial Review. Early on, the PM may concentrate on templates grouped under Test Planning
     and Documentation as shown in Figure 2.1. These templates address pre-Milestone B areas of the
     acquisition process where early fixes to problems generate large future paybacks. The System Design
     and Performance group of templates focuses on activities that must be complete or nearly complete
     prior to MS C. The Test Assets and Support group of templates helps ensure all required assets come
     together before dedicated operational testing begins. All line items in each template are arranged chro-
     nologically as much as possible. The order of the templates may be changed as desired.
     2.5.4. ITT Involvement. Early in the development program, each program’s ITT should consult with
     their OT&E Certification Official to determine how to structure and tailor the certification process.
     The ITT should recommend the best forum for conducting the reviews and how frequently they
     should be done. Note: The ITT may not be the appropriate group for conducting the certification
     review itself due to the high-level nature of ITT membership versus the detailed nature of the material.
     A suggestion is to form a special Operational Test Readiness Review Group consisting of the stake-
     holders outlined in paragraph1.4..
AFMAN63-119 20 JUNE 2008                                                                                   13


   2.5.5. Frequency of Reviews. The ITT should recommend to the OT&E Certifying Official how to
   tailor the reviews to the needs of the program. In general, the frequency of reviews should increase as
   the program approaches the final certification date. Early in the development program, a year between
   reviews may be sufficient, but as dedicated operational testing draws near, reviews should be spaced
   at much closer intervals. Certification reviews should be planned prior to all operational test activities
   such as an operational assessment (OA).
   2.5.6. Final Certification. As a minimum, a final certification review and briefing should occur not
   later than 45 calendar days (or as mutually agreed) prior to the start of dedicated operational testing.
   This lead time helps ensure sufficient time to fix weak areas before starting dedicated operational test-
   ing, and for preparation of the certification of readiness memo. A certification of readiness memo
   must be sent to the lead operational test organization a minimum of 15 days prior to the scheduled
   start of dedicated operational testing. These times ensure the operational testers have a minimum of
   two weeks to finalize their T&E resources and schedules. Longer or shorter times may be negotiated
   by mutual agreement.

2.6. The Certification Review Cycle. A systematic series of candid "review-assessment-negotia-
tion-reporting" cycles will promote meaningful dialogue among developers, the operational tester, and the
operational command(s). The certification review OPR will periodically issue a call for roundtable meet-
ings, create an open forum for discussion, consolidate inputs from all participating organizations, and
report results to participants and the OT&E Certification Official.
   2.6.1. Pre-Certification Reviews. A series of thorough reviews of all operational capability-based
   requirements and resource needs is the first step in assessing a program's readiness to begin dedicated
   operational testing. Suggested review points are shown at the bottom of Figure 2.2. Each participant
   (i.e., subject matter expert) should review assigned areas of responsibility and intensify ongoing
   efforts to reach unmet goals. The purpose of multiple early reviews is to keep the PM and the OT&E
   Certification Official better informed as the program nears final certification. Thus, key issues and
   risks that impact operational testing can be identified earlier, and quality, timely direction and feed-
   back attained from the PM and OT&E Certification Official.
       2.6.1.1. Subject matter experts should compare demonstrated system performance to required
       system performance and compare available resources to required resources. A coherent, complete
       linkage should extend from system/program requirements down through the planned methods and
       resources for demonstrating technical and operational performance. Any flaws, inconsistencies,
       contradictions, voids, or disconnects are potential issues and areas of risk. Accurate and complete
       inputs are needed from all participants.
       2.6.1.2. For more complex programs and systems of systems, a greater number of pre-certifica-
       tion reviews may be needed before the final certification review and briefing.
   2.6.2. Assessment. The reviewer should next assess the shortfalls identified in the templates for
   impacts on the dedicated operational test program. Candid assessments of the system's readiness (i.e.,
   the risk of not passing dedicated operational testing) are crucial to the success of the certification pro-
   cess.
       2.6.2.1. Standard for Judging Readiness. Every template and template line item uses the same
       “ideal standard” for assessing system readiness and risk level: "Will the system be ready for and
       successfully complete dedicated operational testing in this area?" This judgment should be based
14                                                                          AFMAN63-119 20 JUNE 2008


        on the most current operational capability based requirements document, or on sound professional
        judgment if an operational requirement is not at issue. Any available exit criteria should be
        reviewed against the relevant military standards, specifications, and requirements. The cumulative
        total of all judgments about these risks will indicate if the complete system is ready for dedicated
        operational testing. This candid assessment is the heart of the certification process.
        2.6.2.2. Develop Exit Criteria. Certification process participants must know what events must
        occur to achieve program goals before dedicated operational testing begins. Specific and testable
        performance-based exit criteria should be developed for each identified deficiency or issue. Satis-
        faction of the exit criteria in terms of demonstrated, stabilized system performance is the best
        means to ensure readiness for dedicated operational testing. If possible, use an "end-to-end system
        integration test" before starting dedicated testing to make DT&E more operationally relevant and
        to serve as a predictor of future operational performance. Subjective value judgments backed up
        by sound technical and military judgment may also be necessary. Areas judged "not ready" will
        require explanation and an action plan to reach the exit criteria.
        2.6.2.3. If Standards Are Not Met. Some template line items may not reach the "ideal standard"
        (i.e., are not expected to be ready for dedicated operational testing) after close scrutiny. For exam-
        ple, technical orders (TO) are often unavailable, produced late, or incomplete at the start of dedi-
        cated operational testing. Limitations to test may remain despite best efforts to rectify shortfalls.
        Although few unavoidable departures from the ideal standard may occur, these areas still require
        constant, long-term management attention. Negotiation of exit criteria and action plans will be
        required.
        2.6.2.4. Deferred Requirements. If an evolutionary acquisition strategy is used, some capability
        requirements (and therefore the operational testing of those requirements) may require deferment
        to a later increment. These deferments may result from program cost-schedule-performance trade
        offs. Deferment of requirements must be coordinated and documented between the user and PM
        and eventually reflected in operational capabilities documents. Deferment of any operational test-
        ing will be summarized in the final certification briefing and memo.
     2.6.3. Negotiation. Risk areas persisting after repeated reviews are likely to impact the conduct of
     operational testing. Certification process participants must negotiate workaround plans and solutions,
     or agree to some limitations on dedicated operational testing. The program office is the focal point for
     attaining negotiated consensus on managing risks. Workarounds and solutions must be in the best
     interests of the Air Force. Operational test organization officials must be satisfied that the strength,
     objectivity, and independence of operational testing will not be compromised, while the program
     office must retain sufficient management flexibility to find optimal solutions. Again, sound military
     and technical judgment is required to reach a corporate Air Force decision on when to proceed into
     dedicated operational testing.
     2.6.4. Reporting. The program office is responsible for consolidating all participants' inputs and
     observations and preparing the certification briefing and/or report. Explicit action plans and exit crite-
     ria should be developed for each deficient area.
        2.6.4.1. Final Certification Briefing. The length and format of the certification briefing are dis-
        cretionary and should be tailored to fit the acquisition or modification program. The order of the
        templates may be changed as desired. The final product should be an executive-level review of the
        entire program conveying enough information for senior decision-makers to make informed judg-
AFMAN63-119 20 JUNE 2008                                                                                    15


       ments of system readiness. The review must broaden senior leadership's perspective to the
       "macro" level where overall program risk is assessed along with supporting details, if required.
       2.6.4.2. Reporting to the OT&E Certification Official. After reviewing the briefing or report,
       the PM will forward it to the OT&E Certification Official who is responsible for final certification
       of system readiness. The PM should brief the OT&E Certification Official not later than 45 days
       prior to the planned start of dedicated operational testing. Representatives from appropriate levels
       of the operational command(s), operational tester(s), RTO, and other participating organizations
       are required to attend the briefing.
       2.6.4.3. Certifications for Evolutionary Acquisition (EA) Programs. Systems may be devel-
       oped using an EA strategy and fielded in increments of increasing capability over extended time
       periods. These systems require a final certification of readiness for each increment, followed by
       dedicated operational testing for that increment. The final certification for follow-on increments
       will be briefed to the OT&E Certification Official and a certification memo sent in the same man-
       ner and format as for prior certifications.

2.7. Certification Memo Purpose. The certification of readiness memo documents in writing the level
of agreement among certification process participants and specifies the extent of system readiness for
dedicated operational testing within stated constraints. It confirms the certification process was properly
followed and that OT&E entrance criteria were attained. The certification memo must be sent a minimum
of 15 days before the scheduled start of OT&E. It serves as a quantifiable benchmark of projected capa-
bilities against which to check operational test results.
   2.7.1. Contents. The OT&E Certification Official should not simply enumerate what was ready for
   dedicated operational testing and what was not ready, but summarize the critical areas and processes
   accomplished. The PM must organize the certification memo to parallel the program's tailored certifi-
   cation process and discuss any agreed-upon deferments or limitations to operational testing. As a min-
   imum, the PM must address the following areas:
       2.7.1.1. Briefly describe the dedicated operational testing, OT&E entrance criteria, and which
       acquisition process phase(s) and increment the memo supports. Include anticipated operational
       test start and end dates.
       2.7.1.2. Briefly describe how the certification program was structured and executed.
       2.7.1.3. List the templates (or line items, if necessary) that are fully certified as ready for dedi-
       cated operational testing without caveats or limitations.
       2.7.1.4. List the templates (or line items, if necessary) that are not ready or have qualifications and
       caveats and explain why. Describe any areas of elevated risk and how they were mitigated.
       Describe any proposed action plans, workarounds, and exit criteria, if required.
       2.7.1.5. List any test limitations or test deferrals, the rationale, and future plans to clear the limi-
       tations and/or deferrals. Note that approval of deferred items does not eliminate or alter the
       requirement for operational testing of those areas. Deferred items must be tested in subsequent
       operational testing, or the operational requirement document must be changed.
       2.7.1.6. List any other system attributes not ready for OT&E or not expected to meet operational
       requirements (e.g., known deficiencies).
16                                                                         AFMAN63-119 20 JUNE 2008


        2.7.1.7. List any major areas of disagreement with the operational test organization (OTA or
        MAJCOM), user(s), or other participants and the rationale.
     2.7.2. Addressees. The PM will summarize the final certification briefing in a memo to the opera-
     tional test organization commander, with information copies to SAF/AQ or SAF/US as appropriate,
     HQ USAF/TE, HQ USAF/A3/5, HQ AFMC/A3/A2/5 or AFSPC/A3/A5 as appropriate, the capability
     director, the PEO, the RTO, operational/using MAJCOMs, and other participants. The memo will be
     released by the OT&E Certification Official.
     2.7.3. Certification Acknowledgment. The operational test organization commander will acknowl-
     edge the certification memo before commencing dedicated operational testing. The acknowledgment
     memo allows the operational test commander the opportunity to concur or non-concur with the OT&E
     Certification Official’s assessment, and restate any reservations or positions on unresolved issues.
     However, operational test commanders are obligated to start testing. Send the acknowledgment memo
     to the addressees listed in paragraph2.7.2.
     2.7.4. Decertification and Recertification. Despite the developer's best efforts, systems may fail to
     perform as planned, and continuation of dedicated operational testing is not in the best interests of the
     Government. The OT&E Certification Official may decertify the system and return it to DT&E. Test
     organizations may recommend decertification to the PM and/or OT&E Certification Official. A decer-
     tification memo is required to the addressees listed in paragraph2.7.2. Before the system resumes ded-
     icated operational testing, the OT&E Certification Official must again certify the system via memo
     according to paragraph 2.7. after appropriate corrective actions have been taken. If a system is decer-
     tified, all relevant templates should be revisited and the process tailored, if necessary, to improve
     future certification reviews of the system.
     2.7.5. Alternative to Decertification. For system problems of a less serious or temporary nature, the
     operational test organization may “pause” or stop testing for a brief time to assess the the problem and
     determine if additional DT&E is warranted. Note: A series of “pauses” may indicate more serious
     problems that require system decertification.

2.8. Updating the Templates. The certification process and templates are expected to mature through
feedback from certifications and as the acquisition process continues to evolve. Further changes will
result from advanced technologies, improved test and evaluation methods, revised acquisition procedures,
and restructure of the DOD test and evaluation. All certification process users should forward their obser-
vations and suggested improvements to HQ USAF/TEP and SAF/AQXA. Feedback is essential to keep
the process and templates up to date.
AFMAN63-119 20 JUNE 2008                                                                                   17


                                                 Chapter 3

                                 TEMPLATE STRUCTURE AND USE

3.1. Interlocking Matrix. The templates form a matrix of interlocking subject areas spanning an entire
acquisition or modification program. Each template introduces order and helps reduce risk in a specific
phase or aspect of a program. Some duplication and cross-referencing between templates are necessary
because acquisition and modification programs rely on many overlapping activities. Decisions about risk
in one area often affect other areas. Cross-referencing also facilitates broad area reviews as well as special
subject area reviews. Closely associated templates are cited (e.g., “See A15”) to help find parallel infor-
mation in other templates. Note: Acronyms and abbreviations are not spelled out the first time used in the
templates (Attachment 2 through Attachment 33) in order to reduce template size and volume. The tem-
plates are intended as checklists to facilitate the review and help structure an executive-level briefing. All
acronyms and abbreviations are described in Attachment 1.

3.2. Consolidation of Multiple Sources. Each template consolidates as much practical information as
possible from multiple sources into a succinct "checklist." Only a few of the most important AFIs, CJC-
SIs, DODIs and DODDs are cited as footnotes to each template since complete document lists are imprac-
tical for this type and level of review, and different groups of documents may apply to different programs.
Programmatic and regulatory details are left to OPRs and collateral agencies more thoroughly conversant
with specialized guidance. Citation of minimum detail should help PMs, OT&E Certification Authorities,
testers, and users stay squarely focused on quality and readiness issues at the executive-level of review.

3.3. Answering Template Line Items. Each template contains line items phrased as statements of fact
rather than questions. Each line item should elicit a brief summary of program status in that subject area
rather than a superficial "yes" or "no" response. The word “system” refers to software as well as hardware
components of the program under review. The entire group of statements covers the template subject area,
but further analysis may be required in certain cases. Line items may be answered individually or in
groups depending on how the certification OPR, ITT, and/or PM tailor the certification process. Each
template can function as a "tailored checklist" and as a road map for future activities in preparation for
dedicated operational testing. As a general rule, aggregation of line items (and even whole templates)
should increase as the review and briefing rise through the chain of command. The templates may be
re-ordered as desired.

3.4. Focus on Ends, Not Means. The templates emphasize "what must be done" rather than "how to do
it." No specific problem solving methods are advocated, leaving PMs maximum flexibility to implement
their own "best practices." Certification participants are free to discuss and decide on the best way to rem-
edy identified problems. The templates focus on the ends, and the participants focus on the means.

3.5. Assigning Responsibilities. A single lead OPR is suggested for each line item on all templates to
assist PMs and other participants focus responsibility and increase accountability for results. Final deter-
mination of each OPR should be made as required to improve organizational efficiency based on who is
best suited to complete each task or final product. Final approval authority for some line items may lie at
higher levels. While other agencies are expected to participate on a collateral basis, multiple OPRs and
offices of collateral responsibility (OCR) are not listed since responsibility would be defocused, and all
variations between programs cannot be covered. Once identified and agreed upon, the OPR must produce
18                                                                        AFMAN63-119 20 JUNE 2008


a high quality review in the assigned areas and gain the required level of participation from OCRs. The
PM is the OPR for ensuring the entire certification process is properly executed.

3.6. Certification Template Tracking Tool. An automated certification process tracking tool for all
templates is available on the HQ USAF/TE portion of the Air Force Portal. Modify this tool as needed to
match any changes made to the templates. Also see SAF/AQ’s web site, www.safaq.hq.af.mil/.

3.7. Information Collection, Records, and Forms.
     3.7.1. No information collections are created by this publication.
     3.7.2. Program records created as a result of the processes prescribed in this publication are main-
     tained according to AFMAN 37-123 (will convert to AFMAN 33-363) and disposed of according to
     the AFRIMS RDS located at https://afrims.amc.af.mil/rds_series.cfm.
     3.7.3. Prescribed Forms: No forms are prescribed by this publication.
     3.7.4. Adopted Forms:
     AF IMT 847, Recommendation for Change of Publication
     SF368, Product Quality Deficiency Report



                                                Sue C. Payton
                                                Assistant Secretary of the Air Force (Acquisition)
AFMAN63-119 20 JUNE 2008                                                                           19


                                            Attachment 1

             GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION

References
Title 10, U.S.C. §139, §2302(5), §2366, and §2399, Operational Test and Evaluation of Defense Acquisi-
tion Programs
DODD 3200.11, Major Range and Test Facility Base (MRTFB)
DODD 5000.1, The Defense Acquisition System
DODI 5000.2, Operation of the Defense Acquisition System
DOD 4245.7-M, Transition from Development to Production
DODD 5200.1, DOD Information Security Program
DOD 7000.14-R, Department of Defense Financial Management Regulations (FMRS), Vol 2A
DODI 8500.2, Information Assurance (IA) Implementation
DODI 5810.01, DOD Information Assurance Certification and Accreditation Process (DIACAP)
National Security Space (NSS) Acquisition Policy 03-01
Joint Publication (JP) 1-02, Department of Defense Dictionary of Military and Associated Terms
CJCSI 3170.01, Joint Capabilities Integration and Development System
CJCSM 3170.01, Operation of the Joint Capabilities Integration and Development System
CJCSI 6212.01, Interoperability and Supportability of Information Technology and National Security
Systems
AFDD 1-2, Air Force Glossary
AFPD 10-28, Air Force Concept Development
AFI 10-601, Capabilities Based Requirements Development
AFI 10-602, Determining Mission Capability and Supportability Requirements
AFI 10-401, Air Force Operations Planning and Execution
AFI 14-111, Intelligence in Force Modernization
AFI 14-206, Modeling and Simulation
AFI 16-1001, Verification, Validation and Accreditation (VV&A)
AFI 16-1002, Modeling and Simulation (M&S) Support to Acquisition
AFI 21-102, Depot Maintenance Management
AFI 21-115 (Interservice), Product Quality Deficiency Report Program
AFI 21-302 (Interservice), Technical Data Requirements for Logistics Support
AFI 21-303, Technical Orders
20                                                                   AFMAN63-119 20 JUNE 2008


AFI 25-201, Support Agreements Procedures
AFI 32-7061, The Environmental Impact Analysis Process
AFI 32-7086, Hazardous Materials Management
AFPD 33-1, Information Resources Management
AFPD 33-2, Information Assurance Program
AFI 36-2201, Developing, Managing, and Conducting Training
AFPD 33-3, Information Management
AFMAN 37-123, Management of Records (will become AFMAN 33-363)
AFI 40-402, Protection of Human Subjects in Biomedical and Behavioral Research
AFPD 63-17, Technology and Acquisition Systems Security Program Protection
AFI 63-101, Operation of the Capabilities Based Acquisition System
AFI 63-104, The SEEK EAGLE Program
AFI 63-107, Integrated Product Support Planning and Assessment
AFI 63-125, Nuclear Certification Program
AFI 63-501, Air Force Acquisition Quality Program
AFI 63-1101, Modification Management
AFI 63-1201, Life Cycle Systems Engineering
AFI 65-601, Vol I, Budget Guidance and Procedures, Chapter 14
AFI 90-901, Operational Risk Management
AFPD 91-4, Directed Energy Weapons (DEW) Safety
AFI 91-202, The US Air Force Mishap Prevention Program
AFI 91-204, Safety Investigations and Reports
AFI 99-103, Capabilities Based Test and Evaluation
AFI 99-109, Test Resource Planning
TO 00-35D-54, USAF Deficiency Reporting, Investigation, and Resolution
TO 00-5-1, Air Force Technical Order System
TO 00-5-3, Air Force Technical Manual Acquisition Procedures
MIL-STD-882D, Standard Practice for System Safety
National Security Telecommunications and Information Systems Security Policy (NTISSP) No.11 and
No.12
Memorandum of Agreement [MOA] on Multi-Service Operational Test and Evaluation (MOT&E) and
Operational Suitability Terminology and Definitions
Glossary, Defense Acquisition Acronyms and Terms, 11th ed, Sept 2003, Defense Acquisition University
AFMAN63-119 20 JUNE 2008                                                                  21


Defense Acquisition Guidebook
DOD Guide for Achieving Reliability, Availability, and Maintainability
IT Lean Guidebook
Test and Evaluation Management Guide, 5th ed., Jan 2005, Defense Acquisition University

Abbreviations and Acronyms
A—Attachment
ACAT—Acquisition Category
ADM—Acquisition Decision Memorandum
AFDD—Air Force Doctrine Document
AFI—Air Force Instruction
AFMAN—Air Force Manual
AFMC—Air Force Materiel Command
AFMSRR—Air Force Modeling and Simulation Resource Repository
AFOTEC—Air Force Operational Test and Evaluation Center
AFPD—Air Force Policy Directive
AFRIMS RDS—Air Force Information Management System Records Disposition Schedule,
https://afrims.amc.af.mil/rds_series.cfm
AFROCC—Air Force Requirements for Operational Capabilities Council
AFSPC—Air Force Space Command
Ao—Operational—Availability
AoA—Analysis of Alternatives
APB—Acquisition Program Baseline
APML—Acquisition Program Master List
BDRSK—Battle Damage Repair Spares Kit
C—a symbol for "contractor"
C4—Command, Control, Communications, and Computers
C4ISR—Command, Control, Communications, Computers, Intelligence, Surveillance and
Reconnaissance
CBRD—Capabilities Based Requirements Document (this acronym used for this AFMAN only)
CDD—Capability Development Document
CDR—Critical Design Review
CJCSI—Chairman of the Joint Chiefs of Staff Instruction
22                                                              AFMAN63-119 20 JUNE 2008


CJCSM—Chairman of the Joint Chiefs of Staff Manual
CLS—Contractor Logistics Support
CMP—Configuration Management Plan
COA—Courses of Action
COI—Critical Operational Issue
COMSEC—Communications Security
COOP—Continuity of Operations Plan
COTS—Commercial-Off-The-Shelf
CPD—Capability Production Document
CTF—Combined Test Force
CTP—Critical Technical Parameters
DAA—Designated Approving Authority
DAB—Defense Acquisition Board
DAG—Defense Acquisition Guidebook
DAU—Defense Acquisition University
DCR—DOTMLPF Change Recommendation
DEW—Directed Energy Weapons
DIA—Defense Intelligence Agency
DIACAP—DOD Information Assurance Certification and Accreditation Process
DITSCAP—DOD Information Technology Security Certification and Accreditation Process
DOD—Department of Defense
DODD—Department of Defense Directive
DODI—Department of Defense Instruction
DOTMLPF—Doctrine, organization, training, materiel, leadership and education, personnel, and
facilities
DR—Deficiency Report, Deficiency Reporting
DRR—Design Readiness Review
DT&E—Developmental Test and Evaluation
EA—Evolutionary Acquisition
EDM—Engineering Development Model
EOA—Early Operational Assessment
e.g.—for example
ESOH—Environment, Safety, and Occupational Health
AFMAN63-119 20 JUNE 2008                                      23


et. seq.—and all that follows
FCA—Functional Configuration Audit
FDE—Force Development Evaluation
FMRS—Financial Management Regulations
FOC—Full Operational Capability
FOT&E—Follow-on Operational Test and Evaluation
FRP—Full-Rate Production
FSA—Functional Solutions Analysis
GFE—Government Furnished Equipment
HPT—High Performance Team
HQ USAF—Headquarters, United States Air Force
IA—Information Assurance
IAVA—Information Assurance Vulnerability Alert
ICD—Initial Capabilities Document
ICS—Interim Contractor Support
ID—Identification
i.e.—that is
IOC—Initial Operational Capability
IOT&E—Initial Operational Test and Evaluation
ISP—Information Support Plan
IT—Information Technology
ITT—Integrated Test Team
JITC—Joint Interoperability Test Command
JOpsCs—Joint Operations Concepts
JP—Joint Publication
JPG—Joint Planning Guidance
JRMET—Joint Reliability and Maintainability Evaluation Team
JROC—Joint Requirements Oversight Council
KPP—Key Performance Parameter
LCMP—Life Cycle Management Plan
LFT&E—Live Fire Test and Evaluation
LRIP—Low-Rate Initial Production
24                                                           AFMAN63-119 20 JUNE 2008


LRU—Line-Replaceable Unit
LSC—Logistics Support Concept
MAC—Mission Assurance Category
MAJCOM—Major Command
MDA—Milestone Decision Authority
MDAP—Major Defense Acquisition Program
MIPRB—Material Improvement Program Review Board
MOA—Memorandum of Agreement
MOE—Measure of Effectiveness
MOP—Measure of Performance
MOS—Measure of Suitability
MOT&E—Multi-Service Operational Test and Evaluation
MOU—Memorandum of Understanding
MRSP—Mobility Readiness Spares Package
MS—Milestone
MSSP—Modeling and Simulation Support Plan
M&S—Modeling and Simulation
NCOW-RM—Net-Centric Operations and Warfare Reference Model
NDI—Non-Developmental Item
NEPA—National Environmental Policy Act
NetRA—Network Risk Assessment
NLT—not later than
NR-KPP—Net-Ready Key Performance Parameter
NSS—National Security Space, National Security System
OA—Operational Assessment
OCR—Office of Collateral Responsibility
OPSEC—Operations Security
OPR—Office of Primary Responsibility
OSD—Office of the Secretary of Defense
OSS&E—Operational Safety, Suitability, and Effectiveness
OTA—Operational Test Agency
OT&E—Operational Test and Evaluation
AFMAN63-119 20 JUNE 2008                                                          25


OTRR—Operational Test Readiness Review
OUE—Operational Utility Evaluation
OV—Operational View
PEO—Program Executive Officer
PESHE—Programmatic Environment, Safety, and Occupational Health Evaluation
PM—Program Manager
PMD—Program Management Directive
PPBE—Planning, Programming, Budgeting, and Execution
PPP—Program Protection Plan
PTO—Participating Test Organization
QOT&E—Qualification Operational Test and Evaluation
QT&E—Qualification Test and Evaluation
R&D—Research and Development
R&M—Reliability and Maintainability
RCT—Requirements Correlation Table
RDT&E—Research, Development, Test and Evaluation
RFP—Request for Proposal
RM&A—Reliability, Maintainability, and Availability
RSR—Requirements Strategy Review
RTO—Responsible Test Organization
SAE—Service Acquisition Executive
SAF—Secretary of the Air Force
SAMP—Single Acquisition Management Plan
SCG—Security Classification Guide
SE—Support Equipment
SEP—Systems Engineering Plan
SF—Standard Form
SISSU—Security, Interoperability, Supportability, Sustainability, and Usability
SORAP—Source of Repair Assignment Process
SOW—Statement of Work
SPG—Strategic Planning Guidance
SPML—Sustainment Program Master List
26                                                                    AFMAN63-119 20 JUNE 2008


SSAA—Systems Security Authorization Agreement
STAR—System Threat Assessment Report
SV—System View
T&E—Test and Evaluation
TDS—Technology Development Strategy
TEMP—Test and Evaluation Master Plan
TIPT—Test Integrated Product Team
TO—Technical Order
TPP—Technology Protection Plan
USAF—United States Air Force
USD—Undersecretary of Defense
V&V—Verification and Validation
VV&A—Verification, Validation, and Accreditation

Terms

NOTES:
See AFI 10-601and AFI 63-101 for definitions of terms relating to the requirements and acquisition pro-
cesses.
A common understanding of terms is essential to effectively implement this instruction. In some cases,
definitions from multiple sources are offered where they may be of value. Italicized words and notes in
parenthesis are not part of the formal definition and are offered only for clarity.
For additional terms and definitions not listed below, see Joint Publication (JP) 1-02, Department of
Defense Dictionary of Military and Associated Terms. An unofficial source is the Test and Evaluation
Management Guide, 5th edition, Defense Acquisition University (DAU) Press.
Acquisition Category (ACAT)—Acquisition categories determine the level of review, decision
authority, and applicable T&E policies and procedures. They facilitate decentralized decision making and
execution, and compliance with statutorily imposed requirements. See DODI 5000.2, Enclosure 2 for
details.
Capability Based Requirements Document—Any formal requirements document (i.e., ICD, CDD,
CPD, or DCR) used to support the Joint Capabilities Integration and Development System. [This
definition is used solely to support this AFMAN.]
Capability Based Testing—A mission-focused methodology of verifying that a capabilities solution will
enable operations at an acceptable level of risk. Capabilities-oriented evaluations are emphasized
throughout system testing in addition to traditional evaluations of system performance measured against
specification-like requirements. It requires understanding Concept of Operations and involves developing
T&E strategies and plans to determine whether a capability solution option merits fielding. (AFI 99-103)
Combined Testing—See Integrated Testing.
AFMAN63-119 20 JUNE 2008                                                                                  27


Covered System—1. A vehicle, weapon platform, or conventional weapon system that includes features
designed to provide some degree of protection to users in combat; and this is a major system within the
meaning of that term in Title 10 §2302(5). (Title 10 §2366) 2. All categories of systems or programs
identified in Title 10 §2366 as requiring live fire test and evaluation. In addition, non-traditional systems
or programs that do not have acquisition points referenced in Title 10 §2366, but otherwise meet the
statutory criteria. Note: The definitions of “covered system,” “major munitions program,” and “covered
product improvement program” are encompassed in the single DOD term “covered system.” (Defense
Acquisition Guidebook, Chapter 9, which includes conventional munitions programs for which more than
1,000,000 rounds are planned to be acquired; or a modification to a covered system that is likely to affect
significantly the survivability or lethality of such a system.)
Covered Product Improvement Program—See Covered System.
Critical Operational Issue (COI)—1. Operational effectiveness and operational suitability issues (not
parameters, objectives, or thresholds) that must be examined during operational testing to determine the
system’s capability to perform its mission. (paraphrased from DAU’s Test and Evaluation Management
Guide) 2. A key question that must be examined in operational test and evaluation to determine the
system's capability to perform its mission. Testers normally phrase a COI as a question to be answered in
evaluating a system's operational effectiveness or suitability. (AFI 99-103)
Critical Technical Parameter (CTP)—Measurable critical system characteristics that, when achieved,
allow the attainment of operational performance requirements. They are technical measures derived from
operator requirements. Failure to achieve a critical technical parameter should be considered a reliable
indicator that the system is behind in the planned development schedule or will likely not achieve an
operational requirement. (paraphrased from Defense Acquisition Guidebook)
Dedicated Operational Testing—Operational test and evaluation that is conducted independently from
contractors, developers, and operators and used to support production or fielding decisions. (AFI 99-103)
Deficiency Report (DR)—The generic term used within the USAF to record, submit, and transmit
deficiency data which may include, but is not limited to a Deficiency Report involving quality, materiel,
software, warranty, or informational deficiency data submitted using the SF368, Product Quality
Deficiency Report, or equivalent format. (T.O. 00-35D-54, USAF Deficiency Reporting, Investigation,
and Resolution)
   Category I Deficiency—Those which may cause death, severe injury, or severe occupational illness;
   may cause loss or major damage to a weapon system; critically restricts the combat readiness capabil-
   ities of the using organization; or which would result in a production line stoppage.
   Category II Deficiency—Those that impede or constrain successful mission accomplishment (sys-
   tem does not meet minimum operational requirements but does not meet the safety or mission impact
   criteria of a Category I deficiency). It may also be a condition that complements, but is not absolutely
   required for, successful mission accomplishment. The recommended enhancement, if incorporated,
   will improve a system’s operational effectiveness or suitability.
   Enhancement—A condition that improves or complements successful mission accomplishment but
   is not absolutely required. The recommendation, if incorporated, will enhance a system’s operational
   safety, suitability and/or effectiveness. An enhancement report should not be designated as such solely
   due to an “out-of-scope” condition as described in contractual requirements.
28                                                                        AFMAN63-119 20 JUNE 2008


Deployment—1. The movement of forces within operational areas. 2. The relocation of forces and
materiel to desired operational areas. Deployment encompasses all activities from origin or home station
through destination. (JP 1-02)
Developmental Test and Evaluation (DT&E)—Test and evaluation conducted to evaluate design
approaches, validate analytical models, quantify contract technical performance and manufacturing
quality, measure progress in system engineering design and development, minimize design risks, predict
integrated system operational performance (effectiveness and suitability) in the intended environment,
and identify system problems (or deficiencies) to allow for early and timely resolution. DT&E includes
contractor testing and is conducted over the life of the system to support acquisition and sustainment
efforts. (Defense Acquisition Guidebook)
Early Operational Assessment (EOA)—An operational assessment (OA) conducted before MS B. An
EOA assesses the design approach sufficiently early in the acquisition process to assure it has the
potential to fulfill operator requirements. See Operational Assessment.
Evaluation Criteria—Standards by which the accomplishment of required technical and operational
effectiveness and/or suitability characteristics, or resolution of operational issues, may be addressed.
(Defense Acquisition Guidebook)
Evolutionary Acquisition—Evolutionary acquisition is the preferred DOD strategy for rapid acquisition
of mature technology for the user. An evolutionary approach delivers capability in increments,
recognizing, up front, the need for future capability improvements. The objective is to balance needs and
available capability with resources, and to put capability into the hands of the user quickly. The success of
the strategy depends on consistent and continuous definition of requirements, and the maturation of
technologies that lead to disciplined development and production of systems that provide increasing
capability towards a materiel concept. The approaches to achieve evolutionary acquisition require close
collaboration between the user, tester, and developer. (DODI 5000.2) They include:
Fielding—The decision to acquire and/or release a system to operators in the field. (AFI 99-103)
Follow-on Operational Test and Evaluations (FOT&E)—The continuation of IOT&E or QOT&E
activities past the full-rate production decision. FOT&E answers specific questions about unresolved
COIs or completes areas not finished during the IOT&E or QOT&E. It ensures the initial system
acquisition process is complete. (AFI 99-103)
Force Development Evaluation (FDE)—The operational test and evaluation of fielded, operational
systems during the sustainment portion of the system life cycle after acceptance for operational use. The
focus is on maintaining or upgrading operational systems after the initial acquisition process is complete.
An FDE also supports acquisition of MAJCOM-managed systems. (AFI 99-103)
Full-Up, System-Level Testing—Testing that fully satisfies the statutory requirement for “realistic
survivability testing” or “realistic lethality testing” as defined in Title 10 §2366. (Defense Acquisition
Guidebook, Appendix 3)
Increment—A militarily useful and supportable operational capability that can be effectively developed,
produced or acquired, deployed, and sustained. Each increment of capability will have its own set of
threshold and objective values set by the user. (CJCSI 3170.01 and AFI 10-601)
Information Assurance (IA)—Information operations that protect and defend information and
information systems by ensuring their availability, integrity, authentication, confidentiality and
AFMAN63-119 20 JUNE 2008                                                                                    29


non-repudiation. This includes providing for restoration of information systems by incorporating
protection, detection and reaction capabilities. (CJCSI 3170.01)
Information Support Plan (ISP)—[The plan] used by program authorities to document the IT and NSS
needs, objectives, interface requirements for all non-ACAT and fielded programs. (CJCSI 6212.01)
Initial Operational Test and Evaluation (IOT&E)—See Operational Test and Evaluation.
Integrated Testing—Any combination of two or more types of testing used to achieve greater test
efficiency, reduced cost, and schedule savings without compromising the objectives and needs of the
participating test organizations. (AFI 99-103)
Integrated Test Team (ITT)—A cross-functional team of empowered representatives from multiple
disciplines and organizations and co-chaired by operational testers and the program manager. The ITT is
responsible for developing the T&E strategy and TEMP, assisting the acquisition community with T&E
matters, and guiding the development of integrated test plans. There is one ITT for each acquisition
program. (AFI 99-103)
Interoperability—The ability of systems, units or forces to provide data, information, materiel and
services to and accept the same from other systems, units or forces and to use the data, information,
materiel and services so exchanged to enable them to operate effectively together. (CJCSI 3170.01)
Joint Reliability and Maintainability Evaluation Team (JRMET)—T h e t e a m r e s p o n s i b l e f o r
collecting, analyzing, and categorizing R&M data during DT&E and OT&E. It is chaired by the program
office during DT&E and the operational tester during dedicated operational testing. The JRMET includes
representatives from the supporting and operating commands, the DT&E and OT&E test teams, and,
when appropriate, system contractor personnel and nonvoting members.
Life Cycle Management Plan (LCMP)—The implementation, management and oversight by the
designated PM of all activities associated with the acquisition, development, production, fielding,
sustainment and disposal of a DOD weapon or materiel system across its life cycle. (AFI 63-107)
Live Fire Test and Evaluation (LFT&E)—The firing of actual weapons (or surrogates if actual
weapons are not available) at components, subsystems, sub-assemblies, and/or full-up, system-level
targets or systems to examine personnel casualties, system vulnerabilities, or system lethality; and the
evaluation of the results of such testing. (Defense Acquisition Guidebook, Chapter 9)
Logistics Support Elements—A composite of all support considerations necessary to ensure the
effective and economical support of a system for its life cycle. It is an integral part of all other aspects of
system acquisition and operation. (JP 1-02) Note: The ten logistics support elements are: maintenance
planning; manpower and personnel; supply support; support equipment; technical data; training and
training support; computer resources support; facilities; packaging, handling, storage, and transportation;
and design interface. Formerly known as Integrated Logistics Support. (AFI 10-602, Determining Mission
Capability and Supportability Requirements)
Logistics Supportability—The degree to which the planned logistics support allows the system to meet
its availability and wartime usage requirements. Planned logistics support includes the following: test,
measurement, and diagnostic equipment; spare and repair parts; technical data; support facilities;
transportation requirements; training; manpower; and software. (Defense Acquisition Guidebook)
Logistics Test and Evaluation—The test methodology, criteria, and tools for evaluating and analyzing
the ten logistics support elements as they apply to a system under test. The objective is to influence the
30                                                                        AFMAN63-119 20 JUNE 2008


design through applying the logistics support elements as early as possible in the acquisition cycle. This
testing integrates the evaluation and analysis efforts of RM&A, human factors engineering, and logistics
test, and is an integral part of T&E reports. (AFI 99-103)
Low-Rate Initial Production (LRIP)—Production of the system in the minimum quantity necessary (1)
to provide production-configured or representative articles for operational tests pursuant to §2399; (2) to
establish an initial production base for the system; and (3) to permit an orderly increase in the production
rate for the system sufficient to lead to full-rate production upon the successful completion of operational
testing. Note: The LRIP quantity should not exceed 10 percent of the total number of articles to be
produced as determined at the milestone B decision. (Title 10 §2400)
Maintainability—The capability of an item to be retained in or restored to a specified condition when
maintenance is performed by personnel having specified skill levels, using prescribed procedures and
routines, at each prescribed level of maintenance and repair. (Defense Acquisition Guidebook)
Major Munitions Program—See Covered System.
Measurable—Having qualitative or quantitative attributes (e.g., dimensions, velocity, capabilities) that
can be ascertained and compared to known standards. (See Testable.)
Measure of Effectiveness (MOE)—A qualitative or quantitative measure of a system's performance or a
characteristic that indicates the degree to which it performs the task or meets a requirement under
specified conditions. MOEs should be established to measure the system’s capability to produce or
accomplish the desired result. (AFI 99-103)
Measure of Performance (MOP)—A quantitative measure of a system’s capability to accomplish a
task. Typically in the area of physical performance (e.g., range, velocity, throughput, payload).
Measure of Suitability (MOS)—A quantitative or qualitative measure of a system’s capability to
support mission or task accomplishment with respect to reliability, availability, maintainability,
transportability, supportability, training, and other suitability considerations. (AFI 99-103)
Military Utility—The military worth of a system performing its mission in a competitive environment
including versatility (or potential) of the system. It is measured against the operational concept,
operational effectiveness, safety, security, and cost/worth. Military utility estimates form a rational basis
for making management decisions. (Glossary, Defense Acquisition Acronyms and Terms)
Multi-Service—Involving two or more military Services or DOD components. (AFI 99-103)
Multi-Service Operational Test and Evaluation (MOT&E)—OT&E conducted by two or more
Service OTAs for systems acquired by more than one Service. MOT&E is conducted according to the
T&E directives of the lead OTA, or as agreed in a memorandum of agreement between the participants.
(AFI 99-103)
Objective—An operationally significant increment above the threshold. An objective value may be the
same as the threshold when an operationally significant increment above the threshold is not significant or
useful. (AFI 10-601)
Operational Assessment (OA)—An analysis of potential operational effectiveness and suitability made
by an independent operational test activity, with operator support as required, on other than production
systems. The focus of an operational assessment is on significant trends noted in development efforts,
programmatic voids, areas of risk, adequacy of requirements, and the ability of the program to support
adequate operational testing. Operational assessments may be made at any time using technology
AFMAN63-119 20 JUNE 2008                                                                                    31


demonstrators, prototypes, mockups, engineering development models, or simulations, but will not
substitute for the dedicated OT&E [sic] necessary to support full production decisions. (Defense
Acquisition Guidebook)
Operational Availability (Ao)—A measure of the degree to which an item is in the operable and
committable state at the start of a mission when the mission is called for at an unknown (random) time.
(DAU’s Test and Evaluation Management Guide)
Operational Effectiveness—Measure of the overall ability to accomplish a mission when used by
representative personnel in the environment planned or expected for operational employment of the
system considering organization, doctrine, tactics, supportability, survivability, vulnerability and threat.
(CJCSI 3170.01E)
Operational Suitability—The degree to which a system can be placed and sustained satisfactorily in
field use with consideration given to availability, compatibility, transportability, interoperability,
reliability, wartime usage rates, maintainability, environmental, safety and occupational health risks,
human factors, habitability, manpower, logistics, supportability, logistics supportability, natural
environmental effects and impacts, documentation, and training requirements. (CJCSI 3170.01E)
Operational Test Agency (OTA)—An independent agency reporting directly to the Service Chief that
plans and conducts operational tests, reports results, and provides evaluations of effectiveness and
suitability on new systems. (DODD 5000.1, The Defense Acquisition System) Note: Each Service has one
designated OTA: The Air Force has the Air Force Operational Test and Evaluation Center (AFOTEC).
The Navy has the Operational Test and Evaluation Force (OPTEVFOR). The Army has the Army Test and
Evaluation Command (ATEC). The Marine Corps has the Marine Corps Operational Test and Evaluation
Agency (MCOTEA). (AFI 99-103)
Operational Test and Evaluation (OT&E)—1. The field test, under realistic combat conditions, of any
item of (or key component of) weapons, equipment, or munitions for the purpose of determining the
effectiveness and suitability of the weapons, equipment, or munitions for use in combat by typical
military users; and the evaluation of the results of such test. (Title 10 §139(a)(2)) 2. Testing and evaluation
conducted in as realistic an operational environment as possible to estimate the prospective system's
operational effectiveness and operational suitability. In addition, OT&E provides information on
organization, personnel requirements, doctrine, and tactics. It may also provide data to support or verify
material in operating instructions, publications, and handbooks.
Operational Testing—A generic term describing the test and evaluation options and levels of effort
available to an operational test organization. (AFI 99-103)
Operational Utility Evaluation (OUE)—OUEs are evaluations of military capabilities conducted to
demonstrate or validate new operational concepts or capabilities, upgrade components, or expand the
mission or capabilities of existing or modified systems. (AFI 99-103)
Operator—Refers to the operating command which is the primary command operating a system,
subsystem, or item of equipment. Generally applies to those operational commands or organizations
designated by Headquarters, US Air Force to conduct or participate in operations or operational testing,
interchangeable with the term "using command" or “user.” In other forums the term “warfighter” or
“customer” is often used. (AFI 10-601)
Oversight—Senior executive-level monitoring and review of programs to ensure compliance with policy
and attainment of broad program goals. (AFI 99-103)
32                                                                     AFMAN63-119 20 JUNE 2008


Oversight Program—A program on the OSD T&E Oversight List for DT&E, LFT&E, and/or OT&E.
The list includes all ACAT I (MDAP) programs, ACAT II (major system) programs, and any other
programs selected for OSD T&E Oversight. These programs require additional documentation and have
additional review, reporting, and approval requirements. (AFI 99-103)
Participating Test Organization (PTO)—Any test organization required to support a lead test
organization by providing specific T&E data or resources for a T&E program or activity. (AFI 99-103)
Production Representative/Production Configuration—A system that can be used for initial
operational test and evaluation (IOT&E), such as a mature engineering development model (EDM), or a
low-rate initial production (LRIP) system in its final configuration, conforming to production
specifications and drawings. System-level critical design review (CDR), qualification testing, and
functional configuration audit (FCA) should have been completed. While desirable, the item does not
have to be manufactured on a formal production line to be production representative. (Glossary, Defense
Acquisition Acronyms and Terms)
Program Manager (PM)—1. The designated individual with responsibility for and authority to
accomplish program objectives for development, production, and sustainment to meet the user’s
operational needs. The PM shall be accountable for credible cost, schedule, and performance reporting to
the MDA. (DODD 5000.1) 2. Applies collectively to system program directors, product group managers,
single managers, acquisition program managers, and weapon system managers. Operating as the single
manager, the PM has total life cycle system management authority. Note: This AFI uses the term “PM”
for any designated person in charge of acquisition activities prior to MS A (i.e., before a technology
project is officially designated an acquisition program).
Prototype—A model suitable for evaluation of design, performance, and production potential. (JP 1-02)
Note: The Air Force uses prototypes during development of a technology or acquisition program for
verification or demonstration of technical feasibility. Prototypes may not be representative of the final
production item. (AFI 99-103)
Qualification Operational Test and Evaluation (QOT&E)—A tailored type of IOT&E performed on
systems for which there is little to no RDT&E-funded development effort. Commercial-off-the-shelf
(COTS), non-developmental items (NDI), and government furnished equipment (GFE) are tested in this
manner. (AFI 99-103)
Qualification Test and Evaluation (QT&E)—A tailored type of DT&E for which there is little to no
RDT&E-funded development effort. Commercial-off-the-shelf (COTS), non-developmental items (NDI),
and government furnished equipment (GFE) are tested in this manner. (AFI 99-103)
Recoverability—Following combat damage, the ability to take emergency action to prevent loss of the
system, to reduce personnel casualties, or to regain weapon system combat mission capabilities. (Defense
Acquisition Guidebook)
Reliability—The ability of a system and its parts to perform its mission without failure, degradation, or
demand on the support system. (Defense Acquisition Guidebook)
Research, Development, Test and Evaluation (RDT&E)—The type of funding appropriation (3600)
intended for research, development, test and evaluation efforts. (DOD 7000.14-R, Department of Defense
Financial Management Regulations (FMRS), Vol 2A, and AFI 65-601, Budget Guidance and Procedures,
Vol I) Note: The term “research and development” (R&D) broadly covers the work performed by a
government agency or the private sector. “Research” is the systematic study directed toward gaining
AFMAN63-119 20 JUNE 2008                                                                                   33


scientific knowledge or understanding of a subject area. “Development” is the systematic use of the
knowledge and understanding gained from research for the production of useful materials, devices,
systems, or methods. RDT&E includes all supporting test and evaluation activities. (AFI 99-103)
Responsible Test Organization (RTO)—The lead government developmental test organization on the
ITT that is qualified to conduct and responsible for overseeing DT&E. (AFI 99-103)
Risk—1. A measurable probability of consequence associated with a set of conditions or actions.
(Glossary, Defense Acquisition Acronyms and Terms) 2. Probability and severity of loss linked to hazards.
(JP 1-02) 3. A subjective assessment made regarding the likelihood or probability of not achieving a
specific objective by the time established with the resources provided or requested. It also refers to overall
program risk. (Defense Acquisition Guidebook)
Safety Release—A formal document issued to a user test organization before any hands-on use or
maintenance by personnel. The safety release indicates the system is safe for use and maintenance by
typical user personnel and describes the system safety analyses. Operational limits and precautions are
included. The test agency uses the data to integrate safety into test controls and procedures and to
determine if the test objectives can be met within these limits. (DAU’s Test and Evaluation Management
Guide)
Seamless Verification—A concept for structuring test and evaluation (T&E) to more effectively support
the requirements and acquisition processes so new capabilities are brought to operators more quickly.
Seamless verification promotes using integrated testing procedures coupled with tester collaboration in
early requirements definition and system development activities. It shifts T&E away from the traditional
"pass-fail" model to one of providing continuous feedback and objective evaluations of system
capabilities and limitations throughout system development. (AFI 99-103)
Specification—A document intended primarily for use in procurement which clearly and accurately
describes the essential technical requirements for items, materials, or services, including the procedures
by which it will be determined that the requirements have been met. Specifications may be prepared to
cover a group of products, services, or materials, or a single product, service, or material, and are general
or detail specifications. (Glossary, Defense Acquisition Acronyms and Terms)
Support Equipment (SE)—
   Peculiar SE—SE under development in support of the system being tested.
   Common SE—Fielded SE that supports existing systems used in dedicated OT&E.
   Unique SE—Contractor or Government furnished SE for RDT&E use only.
Survivability—The capability of a system and crew to avoid or withstand a man-made hostile
environment without suffering an abortive impairment of its ability to accomplish its designated mission.
Survivability consists of susceptibility, vulnerability, and recoverability. (Defense Acquisition Guidebook)
Sustainment—1. The provision of personnel, logistic, and other support required to maintain and prolong
operations or combat until successful accomplishment or revision of the mission or of the national
objective. (JP 1-02) 2. The Service's ability to maintain operations once forces are engaged. (AFDD 1-2,
Air Force Glossary) 3. Activities that sustain systems during the operations and support phases of the
system life cycle. Such activities include any investigative test and evaluation that extends the useful
military life of systems, or expands the current performance envelope or capabilities of fielded systems.
34                                                                      AFMAN63-119 20 JUNE 2008


Sustainment activities also include T&E for modifications and upgrade programs, and may disclose
system or product deficiencies and enhancements that make further acquisitions necessary.
Testable—The attribute of being measurable with available test instrumentation and resources. Note:
Testability is a broader concept indicating whether T&E infrastructure capabilities are available and
capable of measuring the parameter. The difference between testable and measurable may indicate a test
limitation. Some requirements may be measurable but not testable due to T&E infrastructure shortfalls,
insufficient funding, safety, or statutory or regulatory prohibitions. (AFI 99-103)
Test and Evaluation (T&E)—The act of generating empirical data during the research, development or
sustainment of systems, and the creation of information through analysis that is useful to technical
personnel and decision makers for reducing design and acquisition risks. The process by which systems
are measured against requirements and specifications, and the results analyzed so as to gauge progress
and provide feedback. (AFI 99-103)
Test and Evaluation Master Plan (TEMP)—Documents the overall structure and objectives of the
T&E program. It provides a framework within which to generate detailed T&E plans and it documents
schedule and resource implications associated with the T&E program. The TEMP identifies the necessary
developmental, operational, and live-fire test activities. It relates program schedule, test management
strategy and structure, and required resources to: COIs; critical technical parameters; objectives and
thresholds documented in the requirements document; and milestone decision points. (DAU’s Test and
Evaluation Management Guide) Note: All references to the TEMP in this AFMAN include the SAMP or
LCMP, whichever is applicable according to AFI 63-107, Integrated Product Support Planning and
Assessment. The TEMP should be included as an annex to the LCMP.
Test and Evaluation Organization—Any organization whose designated mission includes test and
evaluation. (AFI 99-103)
Test and Evaluation Strategy—The overarching integrated T&E plan for the entire acquisition program
that describes how operational capability requirements will be tested and evaluated in support of the
acquisition strategy. Developed prior to Milestone A, the T&E strategy addresses modeling and
simulation, risk and risk mitigation, development of support equipment, and identifies how system
concepts will be evaluated against mission requirements, among other things. The T&E strategy is a
precursor to the test and evaluation master plan. (AFI 99-103)
Test Deferral—The delay of testing and/or evaluation of a specific critical technical parameter,
operational requirement, or critical operational issue to a follow-on increment. (AFI 99-103)
Test Integrated Product Team (TIPT)—Any temporary group consisting of testers and other experts
who are focused on a specific test issue or problem. There may be multiple TIPTs for each acquisition
program. (AFI 99-103)
Test Limitation—Any condition that hampers but does not preclude adequate test and/or evaluation of a
critical technical parameter, operational requirement, or critical operational issue during a T&E program.
(AFI 99-103)
Test Resources—A collective term that encompasses all elements necessary to plan, conduct, and collect/
analyze data from a test event or program. Elements include test funding and support manpower
(including temporary duty costs), test assets (or units under test, test asset support equipment, technical
data, simulation models, test beds, threat simulators, surrogates and replicas, special instrumentation
peculiar to a given test asset or test event, targets, tracking and data acquisition, instrumentation,
AFMAN63-119 20 JUNE 2008                                                                               35


equipment for data reduction, communications, meteorology, utilities, photography, calibration, security,
recovery, maintenance and repair, frequency management and control, and base/facility support services.
(DAU’s T&E Management Guide)
Test Team—A group of testers and other experts who carry out integrated testing according to a specific
test plan. Note: A combined test force (CTF) is one way to organize a test team for integrated testing.
(AFI 99-103)
Threshold—A minimum acceptable operational value below which the utility of the system becomes
questionable.
User—See Operator.
Verification, Validation and Accreditation (VV&A)—VV&A is a continuous process in the life cycle
of a model or simulation as it gets upgraded or is used for different applications. (AFI 16-1002, Modeling
and Simulation (M&S) Support to Acquisition)
   Verification—Process of determining that M&S accurately represent the developer’s conceptual
description and specifications.
    Validation—Rigorous and structured process of determining the extent to which M&S accurately
represents the intended “real world” phenomena from the perspective of the intended M&S use.
    Accreditation—The official determination that a model or simulation is acceptable for use for a spe-
cific purpose.
Waiver—A decision not to conduct OT&E required by statute or policy. (AFI 99-103)
36                                                                       AFMAN63-119 20 JUNE 2008


                                               Attachment 2

                            ACQUISITION STRATEGY AND SCHEDULE

A2.1. Ensure early operational tester (AFOTEC or MAJCOM) involvement when developing the acqui-
sition strategy to ensure the T&E Strategy can provide needed support. (PM) (See A7)

A2.2. Develop realistic, achievable, “event-driven” acquisition and test schedules and ensure they are
"harmonized" throughout all program documents. "Success oriented" schedules should be avoided. (PM)

A2.3. Congressional and PPBE schedule constraints must be incorporated into the acquisition schedule.
(PM)

A2.4. Ensure sufficient and timely RDT&E funding and procurement appropriations are programmed
during each budget cycle to keep the program in technical balance. (PM, OTA)

A2.5. Plan the final certification briefing not later than 45 days (or as mutually agreed) prior to starting
dedicated operational testing to allow sufficient time to address any remaining issues. (PM)

A2.6. Schedule sufficient numbers of certification reviews using the certification process. Frequency of
reviews should increase as the program nears the start of dedicated OT&E. (PM)

A2.7. Resolve open issues, particularly with requirements, sufficiently early to permit orderly planning
and transition to dedicated OT&E. (PM)

A2.8. If an EA strategy is used, a clear distinction must exist between increments for determining what
will be tested, produced and/or fielded. (PM, User) (See A4, A7)
     A2.8.1. Operational capabilities are clearly assigned to specific increments.
     A2.8.2. Provisions exist for developing and operationally testing subsequent increments after the ini-
     tial increment is complete.

A2.9. Ensure means exist for efficiently changing the contract in response to changes in the CBRD. (PM)

A2.10. Ensure the PMD reflects the most current program direction. (PM)


_______________________
Primary References:
DODI 5000.2
DAG, Chapters 2 & 9
AFI 63-101
AFI 99-103
AFI 10-601
AFMAN63-119 20 JUNE 2008                                                                            37


                                            Attachment 3

                             ANALYSIS OF ALTERNATIVES (AOA)

A3.1. The AoA (if required) may require updating, re-validation, and approval at the appropriate level
prior to each MS. (User)

A3.2. All reasonable alternatives must be objectively described. The military worth of the final set of
alternatives must be clearly identified. (User)
   A3.2.1. All relevant costs must be identified, preferably using objective engineering and business
   estimates using accepted Air Force cost analysis principles and processes. (PM)
   A3.2.2. All assumptions and constraints must be explicitly identified and supported by the latest
   CBRD, other requirements document AoA guidance documents, or reasonable basis determined by
   the AoA sponsoring agency. (User)
   A3.2.3. Acceptable ranges of performance must be established using rigorous cost-benefit, trade-off,
   and sensitivity analyses to show decision makers when and where certain degradations in system cost
   or performance yield outcomes that no longer satisfy the mission need. (User)

A3.3. MOEs must reflect operational utility and show how they were derived from the requirements doc-
uments. (User)
   A3.3.1. MOEs at the operational task level must be "testable" in order to develop DT&E and OT&E
   plans and concepts. MOEs must be developed as early as possible and agreed to between user and
   tester. (User)
   A3.3.2. The AoA’s MOEs, MOPs, and other criteria must be linked to system performance thresholds
   stated in the latest threat and requirements documents and "track" throughout the program's develop-
   ment. (User)


______________________
Primary references:
AFI 10-601
38                                                                     AFMAN63-119 20 JUNE 2008


                                             Attachment 4

                CAPABILITY BASED REQUIREMENTS DOCUMENTS (CBRD)

A4.1. The appropriate CBRD (i.e., ICD, CDD, or CPD) must be coordinated and approved at appropriate
levels prior to each milestone, after major program changes, and early enough to develop the OT&E test
concept and plan. (User)

A4.2. The CBRD must be based on the SPG, JPG, Joint Vision, JOpsCs, Air Force Vision, and long-range
planning inputs from Joint and Air Force concepts. (User)

A4.3. The CBRD’s capabilities must accurately flow down through the AoA, acquisition strategy, and
TEMP to the OT&E concept and OT&E plan. (User)

A4.4. The proposed system design must satisfy projected operational requirements in the CBRD and
SPG. (PM)

A4.5. The system must provide the needed capabilities against the most current validated threat described
in the system’s threat documents. (PM)
     A4.5.1. Ensure M&S requirements are identified early to enable programmed funding. (PM) (See
     A18)

A4.6. Joint, multi-national, multi-departmental, or multi-service uses described in the CBRD must be
addressed during the system's development. (PM)

A4.7. All thresholds and objectives must be stated in operational terms and defined in measurable, bene-
ficial increments of capability. (User)

A4.8. CBRDs must be stated in such a manner that "testable" MOEs and MOPs can be developed. MOEs
must be quantitatively measurable through analytically-based evaluation methods when possible. (User)

A4.9. An RCT must be attached that accurately summarizes the system characteristics and capabilities
described in the CBRD. The RCT must be up-to-date and in the proper format. (User)

A4.10. All CTPs, KPPs, MOEs, MOSs, threats, definitions, and other criteria must be consistent (harmo-
nized) between the most current support documents (e.g., CBRD, system threat assessment, AoA, Air
Force concepts, APB). (User)

A4.11. If increments of operational capability are planned, the CPD must be updated to describe the next
increment prior to development of the OT&E plan. (User)

A4.12. Changes must be finalized and open issues resolved early enough to ensure no adverse impacts on
the successful completion of dedicated OT&E. (User)

A4.13. The CBRD and RCM must contain a complete audit trail documenting rationale for all require-
ments changes, including changes from the APB. (User)
AFMAN63-119 20 JUNE 2008                                                                                  39


A4.14. Systems with no requirement to “protect users in combat” according to Title 10 §2366 must not
be listed as “covered systems.” (User)

A4.15. The CBRD must state the appropriate mission assurance category and confidentiality levels as
defined in DODI 8500.2. (User)
   A4.15.1. IA capabilities must state levels of assurance (Mission Assurance Category and Confidenti-
   ality Level) necessary by which protection, detection, reaction and restoration capabilities ensure
   appropriate availability, integrity, authentication, confidentiality, and non-repudiation of system infor-
   mation. (User) (See A10)


_____________________
Primary references:
CJCSI 3170.01
CJCSI 6212.01
AFI 10-601
AFI 10-602
AFI 99-103
40                                                                      AFMAN63-119 20 JUNE 2008


                                              Attachment 5

                                       THREAT DOCUMENTS

A5.1. Threat assessment document(s) must remain valid and current with updates made prior to each MS.
(User)

A5.2. The system threat assessment document must be approved by AF/A2. For ACAT I programs, the
STAR must be validated by DIA. (User)

A5.3. The system’s threat assessment document(s) must be consistent with current DOD threat projec-
tions and accurately reflected in the CBRD and AoA. (User)

A5.4. Sufficient threat detail must be provided to support system R&D and the development of realistic
operational mission scenarios in support of the ITP, OT&E plan, and schedules. (PM)
     A5.4.1. All threats must be described in system-specific terms. (PM)
     A5.4.2. Threat "shot doctrine" and employment tactics must be described. (PM)
     A5.4.3. The "reactive" threat and potential countermeasures must be described. (PM)
     A5.4.4. Sources for projections and areas of uncertainty must be cited. (PM)




__________________________
Primary References:
CJCSI 3170.01
AFI 10-601
AFI 14-111
AFMAN63-119 20 JUNE 2008                                                                              41


                                             Attachment 6

               INTEGRATED TEST TEAM (ITT) STANDUP AND ITT CHARTER

A6.1. New-start programs should direct establishment of an ITT in the initial ADM. (Every program
requires an ITT regardless of how long the program has been in existence.) (PM)

A6.2. A current charter describes ITT activities, membership, goals, responsibilities, and operating pro-
cedures. (PM)
   A6.2.1. The ITT is co-chaired by representatives from the program office and the operational test
   organization. (PM)
   A6.2.2. The ITT charter covers the entire life cycle of the program. (PM)
   A6.2.3. If the system comes under an overarching ITT of related systems, the ITT Charter must have
   provisions for managing multiple systems. (PM)

A6.3. The ITT directs formation of sub-groups to handle specific tasks and responsibilities. (ITT)

A6.4. Research is done to find and nominate an RTO to the PEO or decision review authority. (ITT)

A6.5. A common T&E data base is established. (PM)
   A6.5.1. Common standards for data measurement and common formulas, definitions, and terms are
   established. (PM)




_______________________
Primary References:
AFI 99-103
42                                                                      AFMAN63-119 20 JUNE 2008


                                              Attachment 7

                               TEST AND EVALUATION STRATEGY

A7.1. The outline and contents parallel the outline and contents of the TEMP template in the Defense
Acquisition Guidebook. (ITT)

A7.2. All T&E organizations and types of test activities (M&S, interoperability certification, information
assurance testing, LFT&E, contractor testing, etc.) are fully integrated into an efficient continuum. (ITT)
    A7.2.1. Contractor-conducted vs Government-conducted DT&E are clearly distinguished and mutu-
    ally supportive. (PM, RTO)
    A7.2.2. Oversight of all DT&E is described and who will do it. (RTO)
    A7.2.3. Acceptable Service test processes and procedures are used. (PM)

A7.3. The requirements strategy (as reflected in the ICD, CDD, and CPD) and acquisition strategy are
fully supported. (PM) (See A2)

A7.4. Risk identification and mitigation methods and techniques are described. (PM)

A7.5. If an EA strategy is used, a clear distinction must exist between increments for determining what
will be tested, produced and/or fielded. (PM, User) (See A2, A4)

A7.6. The known MOEs, MOSs, CTPs, and COIs are listed. (ITT)

A7.7. Operational testing must be scheduled at key points prior to key decisions when transitioning
between development stages. (PM)
   A7.7.1. OAs are scheduled to support long-lead and low-rate production decisions; increments of EA
   programs; and the PM’s periodic need for updated data about system capabilities. (PM)
   A7.7.2. The proper form of OT&E (e.g., IOT&E) is scheduled to support a beyond-LRIP and/or field-
   ing decision. (OTA)

A7.8. Inputs and coordination are obtained from OSD(AT&L) and OSD/DOT&E if the program is on
OSD T&E Oversight. (PM)

A7.9. All decision reviews (i.e., Milestone C, LRIP, FRP, fielding) are supported by an operational test
and evaluation. (ITT)


___________________________
Primary References:
DODI 5000.2, Enclosure 5
DAG, Chapter 9
AFI 99-103
AFMAN63-119 20 JUNE 2008                                                                             43


                                            Attachment 8

                                     AIR FORCE CONCEPTS

A8.1. The Air Force concepts must describe expected system employment concepts, strategies, methods,
and tactics in concert with the latest SPG and CBRD. (User)
   A8.1.1. Sufficient detail must permit early development of operationally realistic scenarios and tac-
   tics for the OT&E test concept and test plans. (User)

A8.2. Operational effectiveness requirements, criteria, thresholds, objectives, and definitions in the
CBRD must accurately flow down (be linked) to the Air Force concepts, which must in turn be linked to
the OT&E test concept and OT&E plan. (User)
   A8.2.1. Changes in the CBRD, system threat assessment document, AoA, logistics support concepts
   (LSC), and TEMP must be analyzed for potential impacts on the Air Force concepts, which in turn
   affect T&E plans. (User)

A8.3. Air Force concepts must support development of operationally relevant DT&E and OT&E scenar-
ios. (PM)




_________________________
Primary References:
CJCSI 3170.01
AFI 10-601
AFI 10-401
AFPD 10-28
44                                                                      AFMAN63-119 20 JUNE 2008


                                              Attachment 9

                  LOGISTICS SUPPORT CONCEPTS (LSC) AND STRATEGIES
A9.1. LSCs must describe the optimal system maintenance strategies, concepts, and methods based on
the CBRD’s requirements. (User)
    A9.1.1. The system must use an acceptable inter-Service, organic, and/or contractor mix. (PM)
    A9.1.2. LSCs must identify potential high-risk and problem areas (such as long lead items, TOs, sup-
    port equipment, training). (User)
A9.2. Logistics and readiness criteria, thresholds, objectives, and definitions in the CBRD must accu-
rately flow down (be linked) to the LSCs, which must in turn be linked to the MOEs and MOPs in the
OT&E concept and plan. (User)
A9.3. LSC strategies and plans must be sufficiently detailed to support early development of the OT&E
concept and OT&E plan. (User)
A9.4. Realistic suitability test scenarios that support the integrated test plan must be developed from the
LSC and other Air Force concepts. (PM)
A9.5. The system must be supportable in dedicated OT&E using the LSC's strategies and plans. (PM)
A9.6. The system's design must successfully address the quantitative and qualitative constraints identi-
fied in the LSC. (PM)
A9.7. The DOTMLPF elements must be sufficient to support the LSC and maintenance plan during ded-
icated OT&E. (OTA)
A9.8. The SORAP has determined the optimal maintenance posturing decisions needed to support warf-
ighter operational requirements. (PM)
A9.9. A reliability and maintainability (R&M) growth plan should be developed and coordinated if
required. (PM)
A9.10. A LCMP must integrate the acquisition and product support strategies throughout the system’s
entire life cycle. The LCMP must support Milestone B and follow-on decisions. (PM) Note: Space sys-
tems are exempt from this requirement. Note: After June, 2005, SAMPs and Product Support Plans will
be combined into LCMPs.



_________________________
Primary References:
AFI 10-602
AFI 21-302(I)
AFI 63-107
AFPD 10-28
Reliability, Availability, Maintainability (RAM) Primer (formerly DOD 3235.1-H, Test and Evaluation of
System Reliability, Availability and Maintainability, A Primer)
AFMAN63-119 20 JUNE 2008                                                                               45


                                            Attachment 10

     INFORMATION TECHNOLOGY (IT) AND NATIONAL SECURITY SYSTEMS (NSS)

A10.1. The NR-KPP is defined in the CBRD, consists of testable characteristics, and contains perfor-
mance measures required for the timely, accurate, and complete exchange and use of information. (User)
   A10.1.1. Architecture products (e.g., OV-5, OV-6, and SV-1 to SV-7) are developed and available to
   the test community. (User)
   A10.1.2. Key interface profiles are identified and complied with as applicable. (User)
   A10.1.3. IA capabilities are planned and designed into system specifications and configurations using
   the latest threat estimates. (PM)
       A10.1.3.1. DAA and an IA manager have been assigned. (PM)
       A10.1.3.2. MAC and confidentiality levels are identified in requirements documents and the
       TEMP. (User) (See A4)
       A10.1.3.3. The IA strategy is complete and available to the T&E community. (PM)
       A10.1.3.4. DIACAP (or equivalent process) is implemented and the T&E community invited to
       observe process activities. (PM, RTO)
   A10.1.4. The HPT’s architecture expert has ensured compliance with the Net-Centric Operations and
   Warfare Reference Model (NCOW-RM) and provided a compliance statement to the program office.
   (User)

A10.2. The ISP, SCG, and all DIACAP documents (including CONOPS, Information System Security
Policy, Certification Test and Evaluation Plan and Procedures, Certification and Accreditation Statement,
System Documentation, COOP, and Disaster Recovery Plan) are complete and available to the T&E com-
munity as early as possible. (PM)
   A10.2.1. ISP, SCG, DIACAP documentation, and PPP are consistent with the TEMP, T&E Strategy,
   and support T&E execution activities. (PM)

A10.3. System IA training for DAA representatives, Information Systems Security Officer, security sys-
tems administrators, and users is available and completed. (PM)
   A10.3.1. Fully trained blue teams are used to conduct passive and active scans to reveal system/net-
   work vulnerabilities, verify system protection and detection capabilities, and complete a NetRA or
   equivalent as outlined in the TEMP and ISP. (RTO)
   A10.3.2. Fully trained red teams are used as opposition forces to conduct penetration testing for
   assessing the IA posture of the system/network as part of a vulnerability analysis assessment or equiv-
   alent as outlined in the TEMP and ISP. (RTO)

A10.4. NetRA and other interoperability or net-ready certification activities are complete. (PM, RTO,
JITC)
   A10.4.1. All developer/test passwords, password scripts, and accounts in use during system develop-
   ment are deleted prior to operational testing. (PM)
46                                                                    AFMAN63-119 20 JUNE 2008


     A10.4.2. Compliance with IA vulnerability alerts (IAVA) will not impact any other type of system
     certification or potentially invalidate test data. (PM, RTO)
     A10.4.3. Data passed to and from other interoperable systems must be compatible. (PM)
     A10.4.4. JITC has provided an OTRR Interoperability Statement as required. (PM)
     A10.4.5. Applicable items of the SISSU checklist have been addressed. (PM)
     A10.4.6. An Interim Authority to Operate or Approval to Operate memo has been provided by the
     DAA. (PM)

A10.5. The USAF Electromagnetic Compatibility Program and Radio Frequency Spectrum Management
guidelines are complied with. (PM)

A10.6. Other systems and subsystems required to interoperate with the test articles (including external
systems) are available. (PM, OTA) (See A24)




________________________
Primary References:
CJCSI 6212.01
DODI 8500.2
DODI 8510.01
AFPD 33-1
AFPD 33-2
National Security Telecommunications and Information Systems Security Policy (NTISSP) No.11 and
No.12
IT Lean Guidebook
AFMAN63-119 20 JUNE 2008                                                                           47


                                           Attachment 11

                                         TEMP OR SAMP

A11.1. The TEMP must be updated, coordinated, and approved at appropriate levels prior to each MS and
after major program changes. (PM) (See A9) Note: SAMPs must transition to LCMPs beginning in 2005.
Existing SAMPs may remain in effect until updated to an LCMP.
    A11.1.1. Open issues must be addressed and resolved before submission to HQ USAF. Changes
    required by OSD or other decision authorities must be incorporated as agreed. (PM)
    A11.1.2. Coordination must be timely and efficiently planned to minimize chances of late rejection
    and negative impacts on dedicated OT&E. (PM)

A11.2. Level of detail must be appropriate for the stage of development, and “TBDs” eliminated as much
as possible. (PM)

A11.3. The TEMP must accurately reflect the most recent CBRD, system threat assessment document,
LSC, Air Force concepts, and AoA. (PM)

A11.4. The TEMP must clearly summarize relationships between: 1) the T&E Strategy, program sched-
ule, and required resources; and 2) CBRD parameters, COIs, CTPs, MOEs, MOSs, other evaluation crite-
ria, and MS decision points. (PM)
     A11.4.1. The OT&E concept and plan must be executable in terms of structure, schedule, and
     resources. (OTA)
     A11.4.2. Dedicated OT&E test resource shortfalls or limitations potentially impacting dedicated
     OT&E must be identified and discussed in the TEMP. (OTA)
     A11.4.3. Describe the M&S assets needed for dedicated OT&E. (OTA)
     A11.4.4. Ensure M&S assets are VV&A'ed and their use is approved. (OTA, PM) (See A18)
     A11.4.5. If LFT&E is required, include the LFT&E strategy in the TEMP. (PM) (See A17)

A11.5. The TEMP must describe what DT&E, OT&E, or integrated T&E has done or will do to ensure
the system has the potential to meet operational requirements in dedicated OT&E. (PM)
    A11.5.1. Show how all critical issues and MOEs and MOSs will be addressed in dedicated OT&E.
    (OTA)

A11.6. Rationale and provision must be made for any planned OT&E deferred beyond dedicated OT&E
into FOT&E or follow-on increments. (PM)


___________________________
Primary References:
DAG, Chapter 9
AFI 99-103
AFI 63-101
AFI 63-107
48                                                                      AFMAN63-119 20 JUNE 2008


                                             Attachment 12

                              TEST PLANS THAT ARE INTEGRATED

A12.1. Ensure integrated test planning starts as early as practical to make T&E schedules more efficient
and eliminate duplication of effort. (PM)
     A12.1.1. A rigorous SEP identifies how T&E will be used to achieve program goals and technical
     results. (PM)
     A12.1.2. DT&E and OT&E plans and concepts are structured so that OT&E can capture and use
     DT&E data to reduce OT&E requirements. (ITT) (See A14, A15, A23, A27)
     A12.1.3. Dedicated operational test and developmental test objectives are not compromised. (ITT)

A12.2. Definitions, formulas, and evaluation criteria used to determine operational effectiveness and suit-
ability must be consistent between all individual test plans and T&E documents. (ITT)

A12.3. OAs are planned at strategic points in the development program. (OTA) (See A11)

A12.4. A common T&E data base is used to archive all T&E data from all test organizations. (PM)
     A12.4.1. Parameters and formats are agreed upon between all test teams. (ITT)
     A12.4.2. Test item configurations are rigorously controlled. (PM) (See A19, A23)

A12.5. Integrated test matrices depict all T&E events and who will accomplish them. (ITT)
     A12.5.1. Duplication and voids in testing are minimized. (ITT) (See A23)
     A12.5.2. A prudent number of backup resources (e.g., test assets, funds) are available to supplement
     all testing if planned integrated DT&E/OT&E data is unusable or unavailable. (PM) (See A23)




_______________________
Primary References:
DODD 5000.1
DODI 5000.2, Enclosure 5
DAG, Chapters 2 and 9
AFI 99-103
AFMAN63-119 20 JUNE 2008                                                                           49


                                           Attachment 13

                          PROGRAM PROTECTION AND SECURITY

A13.1. The system's SCG and PPP are current. (PM)
   A13.1.1. Computer system security protection measures are accomplished and current. (PM)
   A13.1.2. The TPP (if required) is linked to the PPP and is current. (PM)

A13.2. The system OPSEC plan must be current and system engineering security requirements accom-
plished. (PM)
   A13.2.1. Identify and resolve any disconnects between Service and system SCGs. (PM)
   A13.2.2. Ensure secure communications and/or frequencies (if required) are in place to support sys-
   tem-level DT&E and dedicated OT&E. (PM)
   A13.2.3. Ensure data encryption, COMSEC materials, and encoding devices are available if required.
   (PM)
   A13.2.4. Ensure security measures and requirements (such as TEMPEST and HAVE HEMP) are
   accomplished and current. (PM)

A13.3. Identify security constraints and their impacts on dedicated OT&E. (OTA)

A13.4. Security clearances and required security and COMSEC training for test team personnel must
support the dedicated OT&E plan and schedule. (OTA)

A13.5. Security requirements from the SISSU checklist must be addressed. (PM)




_______________________
Primary References:
DAG
AFPD 63-17
50                                                                       AFMAN63-119 20 JUNE 2008


                                              Attachment 14

                                       CONTRACTOR TESTING

A14.1. Ensure all system specifications and contractor requirements reflect the latest CBRD. (PM)

A14.2. Ensure comprehensive contractor test plans for development, qualification, and production accep-
tance testing are in place. (C)
     A14.2.1. Requirements and specifications must flow down accurately and clearly from prime con-
     tractors to subcontractors. (C)
     A14.2.2. Contractor test strategies and methods must determine if all aspects of the specification and
     the CBRD can be met. (RTO)
     A14.2.3. Test events should be performed under operationally relevant conditions and scenarios as
     much as possible. (C)
     A14.2.4. Sub-system and system pass/fail specification thresholds must be directly traceable to the
     most current CBRD. (PM)
     A14.2.5. A realistic, attainable, event-driven test schedule must be proposed and funded. (C)
     A14.2.6. All contractor test data must be available in the system’s common T&E data base. (PM)
     A14.2.7. Ensure contractor testing is included in the ITP and described in the T&E Strategy and/or
     TEMP. (PM) (See A11, A12)

A14.3. Contractor testing must demonstrate that the system and/or components are meeting the CTPs at
prescribed threshold levels and within defined time frames at each step in development. (C)
     A14.3.1. Government systems engineering analysis should determine if test results support achieve-
     ment of the spec and if the system is projected to meet operational requirements. (RTO)

A14.4. Ensure contractor personnel will not be involved in dedicated OT&E except where permitted by
law. (OTA) (See A32)

A14.5. Available government facilities are used in contractor testing wherever cost-effective, available,
and feasible. (PM)

A14.6. A deficiency resolution system must be in place and accessible to all test organizations to identify,
track, and resolve test failures. (C) (See A20)
     A14.6.1. The contractor’s DR process must be compatible with the Government’s DR process. (PM)
     A14.6.2. All test failures and resultant system design changes must be documented and analyzed.
     Tests must be repeated as necessary to certify specification compliance. (C)
     A14.6.3. Document all changes to specification threshold (pass/fail) values and rationale. (PM)

A14.7. Contractor T&E data and information must be available in the required formats for Government
review for impacts on DT&E and dedicated OT&E. (PM)
AFMAN63-119 20 JUNE 2008                                                                           51


A14.8. Planned contractor testing must be completed according to the contract before government accep-
tance and dedicated OT&E. (C)
   A14.8.1. Contractor testing deferred beyond government acceptance of the system is documented for
   final certification of system readiness. (PM)




_______________________
Primary References:
AFI 99-103
TO 00-35D-54
52                                                                         AFMAN63-119 20 JUNE 2008


                                               Attachment 15

                       DEVELOPMENTAL TEST AND EVALUATION (DT&E)

A15.1. CBRD requirements must accurately flow down into Government DT&E plans and be demon-
strated during contractor and government DT&E. (PM)

A15.2. When design-cost-performance trade-offs are made that may impact CBRD requirements, user
concurrence must be obtained and documented where appropriate. (PM)

A15.3. The DT&E schedule and testing must be planned and executed to allow sufficient time to certify
system OT&E readiness, start and complete dedicated OT&E before FRP or fielding. (PM)
     A15.3.1. DT&E must validate that contractor testing is complete, or that a plan exists to finish testing.
     (RTO) (See A14)
     A15.3.2. Sufficient suitability testing must be conducted to permit credible predictions about system
     RM&A. (RTO) (See A9)
     A15.3.3. The system must demonstrate the capability to satisfy each of the 14 elements of operational
     suitability and the ten logistics support elements. (PM) (See A9, A25, A27, A30, A32, A33)

A15.4. A government-run DR system must be in place in support of DT&E and OT&E for identifying,
tracking, reporting, and resolving DRs. (PM) (See A20)

A15.5. A formal process is in place to control and track system configuration during DT&E that will sup-
port dedicated OT&E. (PM) (See A19)
     A15.5.1. The system design must be stabilized sufficiently early with no major changes implemented
     in the OT&E test articles. (PM) (See A21, A22, A19)

A15.6. Sufficient operationally relevant DT&E must be done, culminating in a "dress rehearsal" in the
final phase, to determine if CBRD requirements can be met before dedicated OT&E. (RTO)
     A15.6.1. Sufficient testing must be done with other systems to support end-to-end security, informa-
     tion assurance, and interoperability certifications. (PM) (See A10)
     A15.6.2. Required levels of performance must be demonstrated in the intended operational environ-
     ment based on the CBRD, Air Force concepts, strategies, and plans. (PM) (See A22)

A15.7. LFT&E results (if required) must be available before start of dedicated OT&E. (PM) (See A17)

A15.8. SEEK EAGLE should be completed for “threshold systems” prior to dedicated OT&E. If not,
ensure flight clearances are available for use in dedicated OT&E. (PM)

A15.9. For integrated T&E, minimize duplication and voids in testing and the use of facilities. (OTA)
(See A11, A12, A14, A15, A23, A24)
     A15.9.1. DT&E data formats and parameters are compatible with other tests to maximize data avail-
     ability in the common data base and usability for OT&E. (OTA) (See A12)
AFMAN63-119 20 JUNE 2008                                                                             53


A15.10. An agreed-upon plan and rationale must exist (e.g., in the TEMP) for testing any areas or capa-
bilities deferred past the start of dedicated OT&E. (PM)
   A15.10.1. If there are any incomplete test areas, explain why and give impacts on dedicated OT&E.
   (RTO)

A15.11. Ensure sufficient interim DT&E results and conclusions are available to support certification of
readiness for operational testing. (RTO) (See A14)




_______________________
Primary References:
AFI 99-103
AFI 63-104
54                                                                      AFMAN63-119 20 JUNE 2008


                                             Attachment 16

                         SOFTWARE DEVELOPMENT AND MATURITY
A16.1. System functionality and maturity must be developmentally tested at the system level prior to
starting dedicated OT&E. (PM) (See A21)
    A16.1.1. If there are interoperability requirements, DT&E must take place at the system-of-systems
    level. (PM) (See A10)
A16.2. Define software-related exit criteria for MS B. These criteria may be modified and/or criteria
added/deleted in response to CBRD changes during system development. (PM)
A16.3. Develop and implement a "requirements traceability" metric to measure adherence of software
products (to include architecture, design, and code) to the CBRD. (PM) (See A4)
A16.4. System level integration testing of software and hardware-software-firmware interfaces must be
monitored, documented, and completed. (PM) (See A33)
A16.5. Known software and firmware discrepancies affecting system performance or the dedicated
OT&E must be properly documented and appropriate corrective action(s) taken. (PM) (See A20)
  A16.5.1. The software must be analyzed for safety critical functions and determined acceptable for
  operational use. (PM)
A16.6. Sufficient regression testing must be done at the unit, integration, and system-of-systems level to
ensure changes do not result in additional defects. (PM)
A16.7. Ensure effective software configuration management and control procedures are in place. (PM)
(See A19)
A16.8. Software manuals and documentation must be validated and up-to-date with the current software
baseline in support of dedicated OT&E. (PM) (See A33)
A16.9. Software and firmware configurations must be fully documented and "frozen" before starting ded-
icated OT&E. Changes must not be implemented during dedicated OT&E that would impact the configu-
ration being fielded or produced. (PM) (See A19)
A16.10. The software must be stable (i.e., operate error free for a reasonable length of time prior to ded-
icated OT&E). (PM) (See A22)
A16.11. Facilities, tools, and manpower must be sufficiently representative to support the OT&E plan
and schedule, and fielding of the software. (PM) (See A9, A27)


_______________________
Primary References:
TO 00-35D-54
CJCSI 6212.01
DODD 5200.1
DODD 8500.2
AFI 16-1001
AFI 16-1002
AFI 63-1201
AFMAN63-119 20 JUNE 2008                                                                             55


                                           Attachment 17

                         LIVE FIRE TEST AND EVALUATION (LFT&E)

A17.1. Review the most current threats and operational scenarios in the CBRD, threat documents, Air
Force concepts, and AoA to determine if the system is a "covered system.” (PM)
   A17.1.1. Consult AF/TEP, users, and OSD/DOT&E (in that order) for concurrence with the determi-
   nation of covered system status. (PM)

A17.2. If the system is a covered system, determine LFT&E scope and complete a cost-benefit analysis.
(PM)

A17.3. If full-up LFT&E is determined to be cost-effective and practical, develop an LFT&E strategy, to
include the level of funding, and submit to OSD/DOT&E for approval. (PM)
   A17.3.1. Describe the LFT&E strategy in the TEMP and submit individual plans for “full-up system
   level” LFT&E to OSD/DOT&E. (PM)
   A17.3.2. Fully integrate the LFT&E strategy and plans into the overall T&E Strategy and integrated
   test plan. (PM) (See A7, A11, A12)
   A17.3.3. Plan for and fund LFT&E to be completed before start of dedicated OT&E. (PM)

A17.4. If full-up LFT&E is determined not to be cost effective and practical, prepare an LFT&E waiver
request and an alternate LFT&E plan for the decision review authority or PEO and OSD/DOT&E
approval before MS B. (PM)
   A17.4.1. Describe the alternate vulnerability/lethality strategy in the TEMP. (PM)
   A17.4.2. Plan for and fund “alternate” LFT&E to be completed before start of dedicated OT&E. (PM)

A17.5. Deficiencies identified during LFT&E that are to be corrected must be tracked and retested prior
to certification for dedicated OT&E. (PM)

A17.6. Fully comply with all system-specific congressional direction regarding LFT&E. (PM)

A17.7. With regard to threat systems for LFT&E:
   A17.7.1. Threat "shot doctrine" and employment tactics must reflect the contents in the CBRD, Air
   Force concepts, and threat documents. (PM)
   A17.7.2. Threat systems and threat models must be VV&A’ed before use in LFT&E. (PM) (See A18)
   A17.7.3. Identify limitations in the test threats and voids in covering the threat spectrum. Describe
   proposed fixes. (PM)
   A17.7.4. Where limitations exist in test threat systems, obtain approval to fill gaps with M&S and
   alternative systems. (PM)

A17.8. Develop a data reduction and common database for using all validated threat test data throughout
the integrated test plan. (PM)
56                        AFMAN63-119 20 JUNE 2008




_______________________
Primary References:
DAG, Chapter 9
AFI 99-103
AFMAN63-119 20 JUNE 2008                                                                            57


                                           Attachment 18

                            MODELING AND SIMULATION (M&S)

A18.1. Ensure M&S requirements are identified in CBRDs in order to obtain funding and support for
their development. (User) (See A4)

A18.2. Develop an MSSP that links M&S requirements to the capabilities being developed and tested
throughout the program (from the AoA through the MS C decision). (PM)
   A18.2.1. Identify as early as possible the M&S support requirements, to include funding, over the
   entire system life cycle. The MSSP must address continuing ownership and maintenance of M&S
   assets after system fielding. (PM)
   A18.2.2. Identify M&S linkages with planned interfacing and interoperable systems. (PM)
   A18.2.3. Check for archived M&S tools (e.g., with AFMSRR) before building new M&S resources.
   (PM)

A18.3. Ensure M&S assets will be available and usable for T&E as required. Testers must receive ade-
quate training as required. (PM)

A18.4. Develop an M&S V&V plan with a comprehensive schedule that supports the integrated test plan
and the dedicated OT&E plan and schedule. (PM)
   A18.4.1. Scenarios, test tools, and analysis tools required for DT&E must be adequately documented.
   (PM)
   A18.4.2. The design engineering notebook data must be reviewed. Physics models can be V&V'ed,
   whereas operations analyses are subjectively V&V'ed. Empirical test data should be used to establish
   model credibility. (PM)
   A18.4.3. Any M&S used to support dedicated OT&E must be accredited. (OTA)

A18.5. If M&S will generate results used to support a fielding and/or FRP decision on an OSD T&E
Oversight program, OSD/DOT&E must approve its use in dedicated OT&E. (OTA)




_______________________
Primary References:
DODI 5200.40
AFI 14-206
AFI 16-1001
AFI 16-1002
58                                                                       AFMAN63-119 20 JUNE 2008


                                              Attachment 19

                         CONFIGURATION MANAGEMANT PLAN (CMP)

A19.1. A CMP must be a key element of a rigorous systems engineering process. (PM)
     A19.1.1. The systems engineering process must be used for all system components and support items
     (e.g., hardware, software, support equipment, spares, GFE). (PM) (See A21)

A19.2. A CMP and configuration control mechanism must be used to ensure the orderly transition from
one decision review to the next, and from development to production. (PM)
     A19.2.1. The Government must have sufficient control or oversight over the configuration to ensure
     changes do not invalidate the results of dedicated OT&E. (PM) (See A21)
     A19.2.2. The exact system configuration must be traceable throughout the program. (PM)

A19.3. If known deficiencies remain in test articles before start of dedicated OT&E, the CMP must
describe strategies for managing the following areas: (See A20, A21)
     A19.3.1. System form, fit, and function must not be adversely affected as a result of each deficiency.
     (PM)
     A19.3.2. The impacts of fixing before versus after dedicated OT&E must be assessed. (PM)

A19.4. The system configuration must be stable and production representative before the start of dedi-
cated OT&E. (PM) (See A21, A16)




_______________________
Primary References:
AFI 21-102
AFI 63-1101
AFI 63-1201
AFMAN63-119 20 JUNE 2008                                                                            59


                                           Attachment 20

               DEFICIENCY IDENTIFICATION AND RESOLUTION PROCESS

A20.1. A contractor-operated DR process, if established, will augment the official Government DR pro-
cess. (C)

A20.2. A Government-run DR process must be established and open to all stakeholders for promptly
identifying, reporting, tracking, and resolving system deficiencies. (PM)

A20.3. A MIPRB must ensure resolution of all DRs and list the impacts to dedicated OT&E. (PM)

A20.4. A DR Review Board will periodically review, validate, and prioritize all open DRs. (PM)
   A20.4.1. DRs should be rank-ordered, and the most critical worked first or as agreed by the user(s),
   operational tester, and RTO. (PM)

A20.5. Open DRs from DT&E must not preclude successful conduct of dedicated operational testing and
the achievement of operational requirements. (PM)
    A20.5.1. Dedicated operational test results will not be invalidated due to deferred DR resolution.
    (PM)
    A20.5.2. System form, fit, and function must not be affected if dedicated OT&E is conducted with
    any known deficiencies. (PM) (See A21)
    A20.5.3. The DR analysis process must be complete and coordinated with users and testers prior to
    the start of dedicated OT&E. (PM)

A20.6. Known DRs or capabilities deferred beyond the start of dedicated OT&E must be reviewed and
prioritized by a DR Review Board and an impact analysis performed. (PM)
    A20.6.1. Category I DRs must be fixed and closure verified according to an agreed upon plan. (PM)
    A20.6.2. Category II DRs must be fixed and closure verified, or suitable work-arounds provided.
    (PM)

A20.7. For DRs that cannot be resolved prior to start of dedicated OT&E, a plan exists for testing
deferred capabilities and fixes after dedicated OT&E is done. (PM)
    A20.7.1. The plan addresses how open DRs are tracked from increment to increment after IOT&E is
    complete. (PM)

A20.8. A JRMET and a Test Data Scoring Board must be established to review all RM&A data. (ITT)


_______________________
Primary References:
AFI 63-501
TO 00-35D-54
AFI 99-103
60                                                                      AFMAN63-119 20 JUNE 2008


                                             Attachment 21

                       PRODUCTION REPRESENTATIVE TEST ARTICLES

A21.1. Test articles (to include support equipment, software, GFE) must be as production-representative
as possible to support the dedicated OT&E and schedule. (PM)
     A21.1.1. Sufficient quantities of test articles must be available for dedicated OT&E. (PM)
     A21.1.2. Test articles must have achieved stabilized performance. (PM)
     A21.1.3. Test article requirements should be provided as early as possible to ensure sufficient lead
     time for procurement. (OTA)

A21.2. Assess any configuration differences between pre-production and production test articles and the
expected impact on the validity of dedicated OT&E. (OTA)

A21.3. Other systems and subsystems required to interoperate with the test articles (including external
systems) must be available to permit testing in an operationally realistic manner. (OTA) (See A10)
     A21.3.1. These systems must be production representative or as close to production representative as
     possible. (PM)
     A21.3.2. A process must be in place to manage interoperability with other required systems and sub-
     systems. (PM) (See A10, A19)
     A21.3.3. Embedded test instrumentation must be transparent to system performance and test execu-
     tion. (PM)




_______________________
Primary References:
DODI 5000.2, Enclosure 5
DAG
AFMAN63-119 20 JUNE 2008                                                                               61


                                             Attachment 22

                                     SYSTEM PERFORMANCE

A22.1. The system must demonstrate it is capable of meeting the CBRD’s requirements (i.e., is opera-
tionally effective and suitable) in its intended operational environment using operationally relevant sce-
narios. (PM)
   A22.1.1. MOEs, MOSs, MOPs, thresholds, objectives, and other test criteria in CBRDs must be
   reviewed. (PM)
   A22.1.2. The system must demonstrate that it will meet criteria for FRP and/or fielding. (PM)

A22.2. System DT&E must demonstrate that known deficiencies have been identified and corrected,
fixes verified, or otherwise resolved. (PM) (See A20)
   A22.2.1. Any remaining problem areas must have minimal to no impact on the outcome of dedicated
   OT&E. (PM)

A22.3. System integration problems must be corrected to allow operators to satisfy mission requirements.
The system must be ready for system-or mission-level testing. (PM)
   A22.3.1. Integration among system components, subsystems, and external systems must optimize
   total system design and performance capabilities. (PM)

A22.4. If the system was planned with an evolutionary acquisition strategy, describe what capabilities are
lacking at this time and when they will be implemented. (PM)

A22.5. LFT&E (if required) must be complete and achieve required (acceptable) levels of system surviv-
ability or lethality. (PM) (See A17)
62                                                                       AFMAN63-119 20 JUNE 2008


                                              Attachment 23

                         OPERATIONAL TEST AND EVALUATION PLAN

A23.1. The OT&E test concept (if required) must be developed and briefed as early as feasible (but not
later than 120 days before start of dedicated OT&E). (OTA) (See A12)
     A23.1.1. The OT&E test concept must describe the characteristics of the operations and support envi-
     ronments and test scenarios the system will encounter in dedicated OT&E. (OTA)
     A23.1.2. The OT&E concept and OT&E plan must be developed from the strategies in the LSC and
     other Air Force concepts. (OTA) (See A8, A9)

A23.2. The dedicated OT&E plan must be developed, coordinated, and approved as early as feasible. If
on OSD OT&E Oversight, OSD/DOT&E must approve the adequacy of the test plan NLT 60 days prior
to dedicated OT&E start. (OTA)

A23.3. A dedicated phase of rigorous, operationally realistic OT&E must be planned. (OTA)
     A23.3.1. Sufficiently realistic testing, to include realistic scenarios, must emulate expected combat
     and peacetime environments. (OTA)
     A23.3.2. COIs, MOEs, and MOSs must have clearly defined linkages to the CBRD, AoA, and threat
     documents and be summarized in the TEMP. (OTA)
     A23.3.3. The elements of operational suitability and all logistics support elements must be addressed.
     (PM) (See A9, A25, A27, A30, A32, A33)
     A23.3.4. Open issues and disconnects (e.g., with test methodologies, requirements, and MOEs) must
     be resolved prior to OT&E start. (OTA)
     A23.3.5. Definitions, formulas, models, scenarios, and evaluation criteria must be standardized as
     much as possible between all test plans for the system. (OTA) (See A12, A15)
     A23.3.6. M&S assets planned for dedicated OT&E should be as consistent as possible with the M&S
     assets used for the AoA and DT&E. (OTA) (See A3, A15)
     A23.3.7. The OT&E plan must be coordinated with the other Service OTAs as required and their
     inputs integrated into the lead OTA plan. (OTA)

A23.4. All T&E resources (e.g., M&S support, test articles, training, fault analysis, test facilities and
ranges, contracting) must be identified. (OTA) (See A12, A24, A26, A27, A28, A29, A31, A32, A33)

A23.5. An integrated DT&E and OT&E test plan must ensure the following: (ITT) (See A11, A12)
     A23.5.1. DT&E and dedicated OT&E test objectives are not compromised as a result of integrated
     testing. (OTA)
     A23.5.2. DT&E data can supplement dedicated OT&E as much as possible. (RTO)
     A23.5.3. Test item configurations are rigorously controlled. (PM) (See A19)
     A23.5.4. Duplication and voids in testing are minimized. (OTA)
AFMAN63-119 20 JUNE 2008                                                                                63


   A23.5.5. A prudent number of backup resources (e.g., test assets, funds) are available to supplement
   dedicated OT&E if planned integrated DT&E/OT&E data is unusable or unavailable. (PM)
   A23.5.6. A plan exists for dry running test procedures. (OTA)

A23.6. All OT&E limitations are described (e.g., lack of test articles, time, system capabilities, insuffi-
cient realism) that may impact the FRP or fielding decision. (OTA)
   A23.6.1. Describe how these limitations (and any waivers) will be addressed in subsequent incre-
   ments, FOT&E, FDE, and beyond. (OTA)

A23.7. Threat "shot doctrine" and employment tactics must accurately reflect Air Force concepts and
CBRD. (PM)
   A23.7.1. The PM V&V’s threat systems and M&S assets, and the OTA accredits them before use in
   dedicated OT&E. (PM, OTA) (See A18)
NOTE: The term OT&E includes IOT&E, FOT&E, MOT&E, QOT&E, FDE, and OUE as defined in
AFI 99-103.




_______________________
Primary References:
DODI 5000.2, Enclosure 5
DAG
AFI 99-103
64                                                                         AFMAN63-119 20 JUNE 2008


                                               Attachment 24

                                TEST AND EVALUATION RESOURCES

A24.1. T&E infrastructure shortfalls are identified in the T&E Strategy and TEMP. HQ USAF/TER is
informed of shortfalls. (PM)
     A24.1.1. Sufficient resources and funding must be available to start and sustain the planned OT&E
     program. (OTA)

A24.2. Test ranges and facilities are properly equipped, manned, funded, scheduled, and personnel
briefed before start of dedicated OT&E. (OTA)

A24.3. Realistic targets (or V&V’ed simulators) must be in the most current operational configuration(s)
and available in sufficient quantities. (PM) (See A17, A18)
     A24.3.1. Ensure test threat systems and related support, including countermeasures and M&S assets,
     are identified and programmed as early as possible. (OTA)
     A24.3.2. Sufficient threat densities, either in open-air or indoor facilities, must rigorously stress the
     system in as realistic a combat environment as possible. (OTA) (See A17)

A24.4. Adequate test instrumentation and data reduction capabilities must be identified, funded, sched-
uled, and support agreements negotiated on utilization rates and data requirements. (OTA)

A24.5. M&S assets (including simulators, test drivers, and scenarios) are accredited, scheduled, and
available to support the OT&E plan and schedule. (OTA)

A24.6. An Environmental Impact Study or assessment (if required) addressing Federal, State, Air Force,
and local regulations must be completed and approved or waivers granted. (PM) (See A25)

A24.7. Other systems and subsystems required to interoperate with the test articles, including external
systems are available. (OTA) (See A10)




_______________________
Primary References:
DOD 7000.14-R, Vol 2A
AFI 65-601, Chapter 14
AFI 99-109
AFMAN63-119 20 JUNE 2008                                                                                65


                                             Attachment 25

       PROGRAMMATIC ENVIRONMENT, SAFETY, AND OCCUPATIONAL HEALTH
                          EVALUATION (PESHE)

A25.1. The system must be capable of being operated and maintained in its intended operational environ-
ment during dedicated operational testing with an acceptable level of ESOH risks. (PM)

A25.2. All ESOH hazards with an assessed mishap risk level of “Serious” or “High” must be mitigated to
an acceptable level and a safety release provided to the OTA before start of dedicated operational testing.
(PM)
   A25.2.1. The LSC and other Air Force concepts must be reviewed and ESOH constraints and limita-
   tions resolved. (PM)
   A25.2.2. All system-related ESOH risks have been accepted at the appropriate management level
   prior to exposing people, equipment, or the environment to known hazards. A safety release must be
   provided to the OTA before start of dedicated operational testing. (PM)
   A25.2.3. The safety release must transmit system ESOH hazard data to the operators, maintainers,
   and testers. (PM)
   A25.2.4. Environmental impacts must be identified and mitigated or eliminated consistent within
   cost, schedule, and technical performance considerations. Data about environmental hazards must be
   provided to the OTA to support analyses in compliance with NEPA. (PM)

A25.3. Verified preliminary TOs, and technical and procedural manuals that identify ESOH risks and
mitigation measures must be available to support the dedicated operational test plan and schedule. (PM)
(See A33)

A25.4. Operator and maintenance personnel must have ESOH training completed in time to support the
OT&E plan and schedule. (OTA) (See A26)

A25.5. Formal certifications may be required from the following sources (among others): (PM)
   A25.5.1. Non-nuclear Munitions Safety Board
   A25.5.2. Directed Energy Weapons Safety Board
   A25.5.3. Flight Safety Board
   A25.5.4. Airframe Certification
   A25.5.5. Range Safety
   A25.5.6. Nuclear Weapons Center
   A25.5.7. Institutional Review Board for protection of human subjects in testing

A25.6. Obtain operational flight clearances or waivers for systems requiring release or jettison from air-
craft. (OTA) (See A15)
66                                                                 AFMAN63-119 20 JUNE 2008


A25.7. Ensure OSS&E policies and principles are employed throughout the system life cycle. (PM)


_______________________
Primary References:
DoDI 5000.2, E5 & E7
DAG
MIL-STD-882D
AFI 32-7061
AFI 32-7086
AFI 40-402
AFI 63-125
AFI 63-1201
AFI 91-202
AFI 91-204
AFI 90-901
AFMAN63-119 20 JUNE 2008                                                                             67


                                           Attachment 26

                            OPERATIONAL TEST TEAM TRAINING

A26.1. OT&E test team training requirements and assets are identified early and in sufficient detail.
(OTA)
   A26.1.1. For multi-Service and multi-national systems, any additional training requirements are iden-
   tified. (OTA)

A26.2. Required training must be adequately contracted for, funded, and scheduled to ensure completion
when required in the OT&E plan and schedule. (PM)
   A26.2.1. OT&E test team personnel must be proficiently trained in required T&E procedures and/or
   operational skills (i.e., Type I training) before the start of dedicated OT&E. (OTA)
   A26.2.2. Training must include normal and emergency operations to operate and maintain the sys-
   tem(s) according to the LSC and other Air Force concepts. (PM)

A26.3. Dry run test procedures before start of dedicated OT&E. (OTA)




_______________________
Primary References:
AFI 36-2201, V2
68                                                                      AFMAN63-119 20 JUNE 2008


                                             Attachment 27

                                    SUPPORT EQUIPMENT (SE)

A27.1. Peculiar, common, and unique SE must be identified as early as feasible. (PM)

A27.2. Peculiar SE and its required support (e.g., technical data, spares) must meet the maintenance times
and capabilities stated in the CBRD. (PM) (See A22)
     A27.2.1. Peculiar SE must be available in required quantities to support the OT&E plan and schedule.
     (PM)
     A27.2.2. Peculiar software SE and its supporting technical data, compilers, manuals, etc., must be
     available if the Government maintains the software. (PM)

A27.3. Peculiar SE must be in production representative configurations and fully interoperable and com-
patible with the system(s) it supports. (PM) (See A21)
     A27.3.1. Assess any configuration differences between preproduction and production peculiar SE
     and the expected impact on the validity of dedicated OT&E. (PM) (See A19)
     A27.3.2. The Government must have positive control or oversight over SE configurations. (PM) (See
     A19)

A27.4. Common SE and unique SE must be identified and available in the required quantities to support
the OT&E plan and schedule. (User)

A27.5. SE training must be accomplished or scheduled to support the OT&E plan and schedule. (PM)
(See A26, A32)
AFMAN63-119 20 JUNE 2008                                                                               69


                                             Attachment 28

                                     SUFFICIENCY OF SPARES

A28.1. Sufficient spares must be available to support test assets, test scenarios, and SE according to the
OT&E plan and schedule. Support levels must be based on the total number of expected operational test
events and hours. (PM) (See A9)

A28.2. Spares repair procedures and capabilities (for blue suit and/or CLS) must be in place to support
the OT&E plan and schedule. (PM) (See A32)

A28.3. Provision must be made for timely failure confirmation and repair action reports to the OT&E test
team. (PM) (See A32, A33)

A28.4. The management concepts for primary operating stocks, war readiness spares support, and for
battle damage repair must be estimated prior to OT&E plan development. (PM)
   A28.4.1. Candidate spares for two-level maintenance must be identified. (PM)
   A28.4.2. Spare levels for MRSP and BDRSK must be identified if required. (User)

A28.5. A logistics support plan must be developed that accurately reflects the LSC and other Air Force
concepts. (PM) (See A9)
   A28.5.1. Identify the risks and limitations in the spares that support dedicated OT&E. For spares with
   limited availability, define how quickly they must be replenished. (PM)
   A28.5.2. The projected number of spares and rates of replenishment must support the ops tempo of
   the dedicated OT&E. (PM)
70                                                                        AFMAN63-119 20 JUNE 2008


                                              Attachment 29

                                      SUPPORT AGREEMENTS

A29.1. MOUs and MOAs should establish the availability of test and support resources needed for the
OT&E plan and schedule. (OTA)
     A29.1.1. For multi-Service testing, comply with the terms of the "MOA on Multi-Service Operational
     Test and Evaluation (MOT&E) and Operational Suitability Terminology and Definitions." (OTA)

A29.2. Host base support agreements should be established for using required ranges, test facilities, air-
space, frequencies, etc., and base support functions such as supply, transportation, and billeting. (OTA)

A29.3. Support agreements should be established with other Government agencies for such functions as
data processing, failure analysis, communications, and security. (OTA)
     A29.3.1. Obtain agreements for testing interoperability, information assurance, network risk assess-
     ments, etc. (ITT) (See A10, A11)

A29.4. Potential for conflict of interest must be strictly avoided, mitigated, or neutralized before any con-
tractor is allowed to participate in the support of dedicated OT&E. (OTA) (See A32)




_______________________
Primary References:
AFI 25-201
AFMAN63-119 20 JUNE 2008                                                                               71


                                             Attachment 30

                      PACKAGING, HANDLING, AND TRANSPORTATION

A30.1. Shipping containers, packaging, handling, and transportation components and methods must be
fully qualified and expected to meet the CBRD’s requirements. (PM)
   A30.1.1. Operationally relevant maintenance demonstrations and scenarios must be used. (PM)

A30.2. Adequate numbers of production representative shipping containers, packaging, and transporta-
tion vehicles must be used to transport test articles to the dedicated OT&E sites. (PM)

A30.3. Formal or preliminary technical data must be verified and available to support the dedicated
OT&E plan and schedule. (PM) (See A33)

A30.4. Shipping, transportation, receiving, and storage arrangements must be in place with the contractor
and host base transportation offices to ensure timely shipping, receiving, and resource protection of test
and support assets. (OTA)

A30.5. OT&E test team maintenance personnel must be adequately trained. (PM) (See A26)
72                                                                       AFMAN63-119 20 JUNE 2008


                                             Attachment 31

                                              PERSONNEL

A31.1. Identify OT&E test team personnel requirements, including software maintenance skills and secu-
rity clearances. The number of personnel and skill levels must be representative of the field (i.e., reflect
the operational environment). (OTA)

A31.2. Written agreements must be in place establishing the sources for required personnel. (OTA) (See
A12, A29)

A31.3. Estimates of maintenance requirements (in terms of man hours and personnel) for LRUs, sub-
systems, and the full system must be available. (PM) (See A9, A25)

A31.4. Contractor support (ICS and CLS) must be identified. (PM) (See A32)

A31.5. Required training, including Type I training, must be completed or scheduled for completion to
support the dedicated OT&E plan and schedule. (PM) (See A26)
AFMAN63-119 20 JUNE 2008                                                                                 73


                                             Attachment 32

                                     CONTRACTOR SUPPORT

A32.1. All contractor assistance or services required to support dedicated OT&E must be identified in the
OT&E test plan, TEMP, RFP, and SOW. (OTA)
   A32.1.1. The potential for conflict of interest must be strictly avoided, mitigated, or neutralized
   before any “system contractor” is allowed to participate in the support of dedicated OT&E. (OTA)

A32.2. OSD approval must be obtained for the following types of “system contractor” involvement in
dedicated OT&E: (OTA)
   A32.2.1. Contractor maintenance and support actions may be of the same type that will be performed
   as part of ICS or CLS after the system is deployed. (OTA)
   A32.2.2. Contractor conduct and reporting of failure analyses to assist in isolating causes of test fail-
   ures. (OTA)
   A32.2.3. Contractor provision of system-unique test equipment, test beds, test facilities, instrumenta-
   tion, data collection, and analysis. (OTA)
   A32.2.4. Contractor logistics support and training (Type I) if such services have not yet been devel-
   oped and are not available from Government sources. (OTA)

A32.3. “ System contractor” report generation procedures must be established for depot-level repair and
maintenance actions. (PM) (See A20, A14)

A32.4. Support contractor services must be established for any required data collection, reduction, and
analysis capabilities needed in dedicated OT&E that are not performed by the Government. (OTA) (See
A11, A12, A23, A29)



_______________________
Primary References:
Title 10 §2399
AFI 25-201
74                                                                      AFMAN63-119 20 JUNE 2008


                                             Attachment 33

                                         TECHNICAL DATA

A33.1. Operator and maintainer technical data (i.e., TOs, engineering drawings, specifications, standards,
process and user manuals, technical reports, catalog items) are available to support the OT&E plan and
schedule. (PM)
     A33.1.1. Technical data from other interoperable systems must be available to support the OT&E
     plan. (PM) (See A12)
     A33.1.2. Technical data required to evaluate system suitability and software supportability must be
     available. (PM)
     A33.1.3. Sufficient information is provided for successfully operating and maintaining the system.
     (PM)

A33.2. Formal or verified preliminary TOs and technical data must be available for use in dedicated
OT&E. (PM)

A33.3. A Technical Order Management Agency must be in place to manage TO deliveries, changes, and
other TO requirements. (PM)
     A33.3.1. Procedures must be established to process changes to technical data and TOs. (PM)




_______________________
Primary References:
AFI 21-303
TO 00-5-1
TO 00-5-3
