BY ORDER OF THE COMMANDER                                       EDWARDS AIR FORCE BASE
EDWARDS AIR FORCE BASE                                               INSTRUCTION 99-101

                                                                                7 AUGUST 2013

                                                                             Test and Evaluation

                                                                          412 TW TEST PLANS

             COMPLIANCE WITH THIS INSTRUCTION IS MANDATORY

ACCESSIBILITY: Publications and forms are available on the e-Publishing website at
               www.e-Publishing.af.mil for downloading or ordering

RELEASABILITY: There are no releasability restrictions on this publication

OPR: 412 TENG                                                           Certified by: 412 TW/CT
                                                                               (Carter Wilkinson)
Supersedes: AFFTCI 99-101, 12 July 2012                                                 Pages: 26


This Instruction implements the test plan policies from Air Force Policy Directive (AFPD) 99-1,
Test and Evaluation Process, Air Force Instruction (AFI) 99-103, Capabilities-Based Test and
Evaluation, Department of Defense Directive (DODD) 5000.1, The Defense Acquisition System
and DOD Instruction (DODI) 5000.2, Operation of the Defense Acquisition System (collectively
called the DOD 5000-series) and supersedes Air Force Flight Test Center Instruction (AFFTCI)
99-101, AFFTC Test Plans, to reflect organization and office symbol changes resulting from the
5 Center Construct and the 412th Test Wing (412 TW) merger. In accordance with AFMCI 99-
103, Test Management, this instruction establishes the responsibilities and local procedures for
the development, review, and approval of test plans for the 412 TW, contractor and joint 412
TW/contractor test projects. The ultimate goal is to develop technically robust test plans which
provide relevant, high-confidence planning to decision-makers. It applies to all units, agencies,
and contractors for which a 412 TW unit is the responsible test organization (RTO) or
participating test organization (PTO). It also applies to USAF Test Pilot School plans and to test
plans where 412 TW resources will be used or at risk, even when the 412 TW has no technical
responsibility. To the extent that the US Air Force Reserve, ANG or Civil Air Patrol would
conduct tests where 412 TW resources are at risk, this instruction would also apply to those
plans. This instruction addresses development, review, and approval of test plans from a
technical point of view. Safety planning and review for tests is covered by AFFTCI 91-105,
AFFTC Test Safety Review Process. Technical reporting is covered by AFFTCI 99-103, AFFTC
Technical Report Program. Test personnel must follow good security practices in order to
protect sensitive and classified information. Mark classified or sensitive data (For Official Use
Only) IAW DoDM 5200.01, DoD Information Security Program, and share with appropriately
cleared individuals on a “need to know” basis. The Air Force Supplement to DODD 5400.7-R,
DOD Freedom of Information Act (FOIA) Program, states the public will be allowed to inspect,
 2                                                                         EDWARDSAFBI99-101 7 AUGUST 2013


review, and receive copies of Air Force records. This applies to all Air Force records except for
those exempt from release under the Act, such as classified records. Information should be
released only after the clearance and release processes described in AFI 35-102, Security and
Policy Review Process, are followed. Air Force personnel must ensure that any
information/records to be provided outside official DoD channels, including foreign nationals,
must be released in accordance with the provisions of AFI 35-102, Security and Policy Review
Process, and have the approval of the release authority. If a written request for DoD records has
been made by any person, organization, business, but not including a Federal Agency, that either
explicitly or implicitly invokes the FOIA, it must be processed by the local FOIA Requester
Service Center in accordance with DoDR 5400.7-R_AFMAN-33-302, DoD Freedom of
Information Act Program. Ensure that all records created as a result of processes prescribed in
this publication are maintained in accordance with (IAW) Air Force Manual (AFMAN) 33-363,
Management of Records, and disposed of IAW Air Force Records Information Management
System (AFRIMS) Records Disposition Schedule (RDS). There are no prescribed forms for the
processes described in this instruction. Refer recommended changes and questions about this
publication to the Office of Primary Responsibility (OPR) using the AF Form 847,
Recommendation for Change of Publication; route AF Form 847s from the field through the
appropriate functional’s chain of command.

SUMMARY OF CHANGES

This document has been revised, including instruction number change, to reflect changes to
organizations. This version reflects the changes made as a result of the AFMC 5 Center
construct reorganization: the re-designation of the Air Force Flight Test Center (AFFTC), as the
Air Force Test Center (AFTC) and identification of the 412 TW as the organization responsible
for testing and reporting on developmental testing at Edwards AFB and associated units.
Primary changes include removing AFFTC organization references and replacing with 412 TW
organization and the removal of options for higher level report approval at the AFTC level.

       1.     General. ..................................................................................................................    3
       2.     Responsibilities. .....................................................................................................        4
       3.     Procedures. .............................................................................................................      5
       4.     Test Plan Changes. .................................................................................................          11

Attachment 1—GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION                                                                              13

Attachment 2—412 TW TEST PLAN PROCESS OVERVIEW                                                                                              16

Attachment 3—TEST PLAN CONTENT CHECKLIST                                                                                                    17

Attachment 4—DATA ANALYSIS PLAN CONTENT CHECKLIST                                                                                           19

Attachment 5—COMMON TECHNICAL RISKS                                                                                                         21

Attachment 6—TECHNICAL REVIEW BOARD (TRB) MEETING APPROACH                                                                                  23
EDWARDSAFBI99-101 7 AUGUST 2013                                                                    3


Attachment 7—SAMPLE TECHNICAL REVIEW MEMORANDUM WHEN 412 TW IS
             THE RESPONSIBLE TEST ORGANIZATION                                                    24

Attachment 8—SAMPLE TECHNICAL REVIEW MEMORANDUM WHEN 412 TW
             DOES NOT HAVE ANY TECHNICAL RESPONSIBILITIES                                         26


1. General. Test plans come in a variety of formats and styles but must contain the following
content: objectives, instrumentation requirements, data analysis plan, test techniques, test points,
limitations, and management information. Test plans document why tests will be accomplished
(the objectives), how tests will be accomplished (the test techniques and test points), what data
will be acquired (the instrumentation requirements), how data will be used to answer the
objectives (the data analysis plan), why the test may not be fully successful (limitations as
applicable), and when and what type of report is needed (management information). A test can
be a ground or flight activity to gather specific information, answer a customer’s question, or
provide information not wholly covered by an approved instruction/training manual.
   1.1. As noted in AFI 99-103 and the Air Force Test and Evaluation Guidebook, an
   Integrated Test Team (ITT) is formed during the Concept Refinement phase or equivalent
   point in time and is chaired by the Program Manager (PM) and the operational test
   organization representative. This ITT facilitates formal early tester involvement and is
   responsible for all T&E planning, execution, and reporting for the program. The ITT
   establishes subgroups (herein referred to as “test teams”) such as Test Integrated Product
   Teams, Combined Test Forces, and other teams to carry out specific T&E tasks. Test plans
   generated by 412 TW units or by other parties (often the case when a 412 TW unit is PTO)
   are required for all tests.
   1.2. In order to ensure proper and adequate preparation and planning, a thorough technical
   review is required of test plans and associated data analysis plans. In accordance with AFI
   99-103, when a 412 TW unit is RTO, the 412 TW will provide government Developmental
   Test and Evaluation (DT&E) results. To ensure the DT&E mission is accomplished
   effectively, the reporting requirements, the report schedule, and the data analysis plan will be
   reviewed in addition to other elements of the test plan as part of the technical review process.
   1.3. Further information on the roles of the RTO and PTO can be found in AFI 99-103.
   Guidance on the local test reporting process can be found in AFFTCI 99-103. Changes to
   existing test plans must also be reviewed. Lastly, an independent safety review of all test and
   safety plans is required to identify potential hazards, determine risk mitigations, and assess
   residual risk in accordance with AFFTCI 91-105 for all tests using 412 TW assets. The
   document package containing the test and safety plan is referred to here and in AFFTCI 91-
   105 as the “test package” to more appropriately place the documentation ownership and
   responsibility on the test team. Both technical and safety reviews (up to and including
   approval) must be completed before testing begins. Senior leadership will coordinate,
   approve, or review the test package for information, according to the risk level of the tests. If
   a test package includes tests at more than one risk level, the test team may request approval
   of lower risk test activities prior to the package being approved at the higher risk level if the
   steps specified in chapter 7 of AFFTCI 91-105 are followed.
 4                                                    EDWARDSAFBI99-101 7 AUGUST 2013


2. Responsibilities.
     2.1. Senior Leadership (412 TW Authorities).
        2.1.1. Coordinate, approve, and review test packages.
        2.1.2. Assign a 412 TW test unit as a liaison to assist non-412 TW organizations seeking
        approval to test at the 412 TW.
        2.1.3. Ensure compliance with this instruction.
     2.2. 412th Test Wing Technical Director (412 TW/CT).
        2.2.1. Set policy for the test plan development, review, and approval process.
        2.2.2. Maintain this instruction. Solicit lessons learned so they may be incorporated into
        future test plans.
        2.2.3. As the Office of Primary Responsibility (OPR) for technical reviews, ensure that
        all test plans under 412th Test Wing cognizance involving 412 TW resources receive an
        independent technical review. The Technical Director (412 TW/CT) may keep technical
        review authority (TRA) at the wing level or may designate a Test Wing engineering
        group, engineering squadron or equivalent as the TRA. Normally, if not 412 TW/CT, the
        TRA will be a technical leader from one of the following organizations: 412th Test
        Engineering Group (412 TENG), 412th Electronic Warfare Group (412 EWG), 771st
        Electronic Warfare Assessments Test Squadron (771 TS), 772nd Electronic Warfare
        Operations Test Squadron (772 TS), 773rd Flight Systems Integration Test Squadron
        (773 TS), 775th Avionics and Armament Integration Test Squadron (775 TS), 412th
        Logistics Test Squadron (412 MXLS), or the USAF Test Pilot School (TPS).
            2.2.3.1. The TRA should have expertise in all the test areas to allow adequate review
            of the test plan or shall request assistance from the appropriate 412 TW organizations.
            2.2.3.2. The TRA will ensure an independent technical review is accomplished in
            accordance with AFI 99-103 and this instruction.
     2.3. Technical Reviewers (Test experts from 412 TW engineering, operations, and
     maintenance/logistics organizations).
        2.3.1. Support the 412 TW test plan technical review process IAW this instruction.
     2.4. Combined Test Force (CTF) Commanders (or equivalent).
        2.4.1. Ensure test teams are familiar and compliant with this instruction.
        2.4.2. Support the 412 TW test plan technical review process. The CTF test conductors
        and project pilots will support technical reviews of the projects to which they are
        assigned. When requested, CTF project pilots and other personnel are required to provide
        independent reviews of other test programs or activities.
     2.5. CTF Chief Engineer (or equivalent).
        2.5.1. Train, guide, and direct test teams to ensure satisfactory test plan development.
        2.5.2. Request expert technical assistance from the appropriate engineering offices
        within the 412th Test Wing as early as possible during test plan development to ensure
        the best possible test plan.
EDWARDSAFBI99-101 7 AUGUST 2013                                                                 5


      2.5.3. Ensure 412 TW engineering personnel prepare detailed test plans or make
      thorough and timely test plan inputs through test team meetings (or other means) to the
      entities who are preparing test plans so that the resulting plans comply with this
      instruction.
      2.5.4. Ensure test plans are prepared in compliance with this instruction and are of
      sufficient quality before they are submitted for technical review. Ensure test plans are
      submitted for technical reviews in time to allow comprehensive technical and safety
      reviews and to meet program requirements and schedules. Copies of the complete test
      plan and associated documents (as given in section 3.3.1.2) will be provided as soon as
      they are ready for a technical review but normally no earlier than 60 days before the start
      of testing and no later than five working days before the scheduled technical review.
      2.5.5. Ensure completion of technical review action items and incorporation of
      recommended test plan changes. When noncompliance items exist (described in section
      3.4.2), document the rationale for noncompliance and incorporate this document into the
      test package.
      2.5.6. Ensure proper test plan distribution after approval.
   2.6. Test Team.
      2.6.1. Allocate sufficient time and resources to generate a test plan and complete the
      process described in this instruction.
      2.6.2. Identify a Project Test Lead as the test team point of contact and focal point for
      ensuring that requirements of this instruction, AFI 99-103, and current AFMC
      supplements are met prior to and during all phases of test.
   2.7. Test Expert Organizations (412 TENG, 412 EWG, 771 TS, 772 TS, 773 TS, 775 TS,
   412 MXLS, USAF TPS).
      2.7.1. Provide expertise to aid in test planning, develop analysis capabilities, support
      technical reviews, and train testers in all aspects of the process described in this
      instruction.
      2.7.2. Set policy for test plan and technical review training to be implemented by
      subordinate units.
3. Procedures.
   3.1. Overall Process. Detailed test plan development involves three general steps: planning
   tests, technical reviews of test plans, and submitting test plans for approval. Attachment 2
   shows the overall process for detailed test plan development. The test team is responsible for
   planning the test and drafting the test plan document. When the Chief Engineer deems the
   plan is ready for independent review, he/she submits it with a request for review to 412
   TW/CT. The independent technical review is conducted by 412 TW/CT or his/her delegate.
   After the review is completed, recommendations and action items are provided to the test
   team. The test team provides the updated version of the plan along with their responses to
   recommendations and action items to the technical review authority (TRA). The TRA
   generates a technical review memorandum (TRM) to document the status of the test plan and
   any residual technical risk. The following paragraphs amplify significant parts of the process.
   It is important to note that final approval of the test plan usually occurs after the test team
6                                                    EDWARDSAFBI99-101 7 AUGUST 2013


    submits the test plan along with the TRM and draft safety plan as a test package and
    completes the coordination and approval process described in AFFTCI 91-105.
    3.2. Planning Tests. To plan tests, test team members should begin the effort with the goals
    of understanding the various customers and their requirements; understanding the system
    under test; understanding test and evaluation in the planned engineering disciplines, and
    translating this understanding into realistic test, analysis, and safety plans. As mentioned in
    section 2 of AFFTCI 91-105, safety planning and test planning are integral and iterative
    processes. Therefore, as the test plan is being developed, the test team should consider and
    incorporate risk control measures. Similarly, analysis, report planning, and test planning are
    also integral and iterative processes. So, as the test plan is being developed, the test team
    should consider and incorporate analysis and reporting requirements.
       3.2.1. RTO. The RTO will ensure that a test plan is prepared to meet the objectives
       mutually agreed to by the 412 TW, the system program manager (PM) or other customer
       and participating agencies. The PM, the end-user, and the contractor often have different
       needs from a development test effort. The contractor may need to gather data to support
       the development of flight manuals, simulators, or other deliverables. The PM may need
       results to address milestone decisions. The end-user may need the system to perform a
       variety of special missions. All of these requirements may also have different need dates.
       The test plan should address these diverse needs in the most effective manner possible. In
       addition to appropriate discussion with the PM, end-user, and contractor representatives,
       the author should take advantage of available program documentation. Specifications,
       operational requirements, and management plans, such as the Capabilities Based
       Requirements Document (CBRD), the Operational Requirements Document (ORD), and
       the Test and Evaluation Master Plan (TEMP) should be used as sources for objectives,
       issues, and items that require management emphasis. If necessary, a number of test team
       meetings should be held with appropriate organizations (including customers,
       contractors, and participating organizations) to ensure a test plan is well-developed and
       ready for a technical review.
       3.2.2. Qualities of a Good Test Plan. A good test plan will have sufficient information
       for an experienced flight test discipline engineer to take over the program with ease. In
       addition, an operations flight test engineer should be able to use the plan to develop test
       cards. An experienced engineer/test pilot/reviewer should be able to easily understand the
       technical aspects of the program and assess the risks/benefits. Lastly, management should
       be able to discern the overall technical approach being taken. Regardless of format, the
       test plan must contain: objectives, instrumentation requirements, data analysis plan, test
       techniques, test points, limitations, and management information (including when and
       what type of report is needed). To prepare a test plan with the appropriate content, the
       author should review attachment 3 and consult with the appropriate technical experts and
       senior engineers while developing the test plan.
           3.2.2.1. Data Analysis Plan. A significant element leading to the success or failure of
           a test is the data analysis plan. The data analysis plan must answer the questions that
           led to the test requirement. For example, if the test is needed to develop a simulator,
           the analysis plan will likely involve gathering and integrating available models and
           predictive data, requiring more time and resources than many other types of tests.
           Regardless of format, the data analysis plan must clearly indicate: required
EDWARDSAFBI99-101 7 AUGUST 2013                                                                 7


         parameters, collection methods, initial quality check methods, algorithms and tools to
         be applied, tool validation methods, analysis methods, completion criteria, evaluation
         criteria (if applicable), evaluation products, and statistical relevance (if applicable).
         The data analysis plan author should determine the resources required to ensure the
         data production and analysis schedules keep pace with execution. To prepare a data
         analysis plan with the appropriate content, the author should review attachment 4.
         Again, the author should involve the appropriate technical experts and senior
         engineers while developing the data analysis plan.
         3.2.2.2. Test Conduct and Reporting Details. The ability to generate a technical
         report is tied to the ability of the test team to execute the test and analysis in
         accordance with a good test plan. The test team should consult with the appropriate
         discipline engineer technical experts and senior engineers to gain understanding of a
         number of test conduct details. Details such as: when a test must be repeated, what
         settings must be used when running an existing analysis tool, what comments to elicit
         during certain qualitative tests, and what configuration information to track to
         determine when a new baseline ground procedure or test must be performed.
  3.3. Review of the Plan.
     3.3.1. Establishment of a Technical Review and Technical Review Board (TRB).
         3.3.1.1. All test plans under 412th Test Wing cognizance involving 412 TW
         resources will receive an independent technical review. This along with the safety
         review described in AFFTCI 91-105 complies with AFI 99-103, paragraph 6.5 which
         states that “Independent government technical and safety personnel will examine the
         technical and safety aspects of T&E plans that involve government resources prior to
         commencement of test activities.”
         3.3.1.2. When a test project is ready for independent review, the chief engineer will
         send a request for technical review to the 412 TW/CT. Typically, the request for
         review will be emailed to the 412 TW/CT with a copy of the test plan. In the request,
         the chief engineer may indicate which engineering offices are recommended as most
         appropriate to participate in the review and may include them in the courtesy copy
         (“cc”) of the email message. The chief engineer will ensure that the following items
         are submitted to the reviewers as part of or alongside the test plan: the data analysis
         plan, the reporting requirements, and the analysis/report schedule. Test Pilot School
         (TPS) student Test Management Project (TMP) test plans are automatically assigned
         TPS/CT as TRA unless TPS/CT determines that a higher level of TRA is required.
         3.3.1.3. As mentioned previously, the 412 TW/CT may keep TRA at the Wing level
         or may designate an engineering group or squadron as the TRA. In cases where a test
         plan incorporates many different disciplines or has high leadership interest, a TRA
         above the squadron level (e.g., at the Group or Wing level) may be chosen. If the
         TRA does not have sufficient expertise in the appropriate areas, they shall request
         assistance from 412 TW operations, engineering, maintenance, and/or logistics
         organizations. All of these organizations will support the technical reviews as
         requested by the TRA. The intent of a technical review is to establish a group of
         experienced personnel not intimately associated with the project to provide an
         independent technical assessment and executive review of the plan.
8                                            EDWARDSAFBI99-101 7 AUGUST 2013


    3.3.1.4. The TRA shall conduct a technical review as specified in the responsibilities
    section of this instruction, and follow the procedures as specified in this section. To
    aid in the technical review, the TRA and other reviewers should reference attachment
    3 for test plan content, attachment 4 for data analysis plan content, and attachment 5
    for common technical risks. In accordance with AFI 99-103, “as a minimum,
    technical reviews will assess test requirements, techniques, approaches, and
    objectives” and “will ensure that environmental analyses have been completed as
    required by AFI 32-7061, The Environmental Impact Analyses Process, and are
    referenced in the test plan.”
    3.3.1.5. Throughout the technical review process, it is important that the TRA and
    the test team communicate clearly with respect to recommendations, action items, and
    closure processes.
    3.3.1.6. Technical review boards (TRBs) are the normal method of accomplishing a
    technical review. This is especially true when: there are multiple engineering
    disciplines involved; the test, analysis, or system under test are especially complex;
    envelope expansion or other elevated risk testing is involved; and/or new test or
    analysis methods will be applied. An independent technical review is always required
    where the 412 TW has technical responsibility. The 412 TW/CT will approve when
    the review method will be anything other than a TRB.
       3.3.1.6.1. A TRB is a face-to-face meeting that includes project personnel and the
       test plan reviewers. The TRA will ensure that the board is made up of experienced
       personnel to include at least operations personnel (from 412 OG) and engineering
       personnel (from 412 TENG or 412 EWG). In addition, personnel from the
       statistical consulting flight (in the 812 TSS) will be invited to participate in all
       TRBs. The TRB members will be chosen on the basis of their experience in the
       areas addressed in the test plan. The expertise should not be limited to that
       dictated by the test program objectives, but should be broad enough for these
       individuals to critically review all aspects of the planned test. Ideally, the
       operations and engineering personnel on the TRB should be the same as those
       who will participate in the safety review process. Therefore, the chief engineer
       should inform the TRA of any personnel already identified as safety reviewers.
       The TRA may select those personnel or others who would best fit the technical
       review requirements. The chief engineer, operations, and engineering personnel
       responsible for conducting and supporting the test program will participate in the
       review as consultants. Appropriate customer and contractor representatives may
       also be invited.
       3.3.1.6.2. The TRA determines the date for the TRB. The test team shall ensure
       that notification of the time, date, and location of the TRB are provided to all
       necessary attendees at least five working days before the meeting. The test team
       shall ensure the complete test plan with data analysis plan is distributed to all
       participants at least five working days before the TRB meeting. In addition, the
       test team shall ensure that the associated report plans, including schedules, be
       distributed to all TRB members at least five working days before the TRB
       meeting.
EDWARDSAFBI99-101 7 AUGUST 2013                                                              9


          3.3.1.6.3. When a TRB meeting is convened, it should follow the general
          approach given in attachment 6. The TRB chair has the authority to alter the order
          and flow of the review. Ideally, all those required for the TRB will meet in one
          location. When this is not possible, at a minimum, the TRB chair, members, and
          key test team personnel will meet in one location with other parties available via
          telephone conference call. If a conference call is necessary, the test team will
          make the appropriate arrangements. A chair, usually the TRA, will lead the TRB
          meeting. Key reviewers will be the TRB members. The test team shall provide a
          member to be note-taker throughout the review. The test team shall generate an
          attendance sheet listing all the participants and their role in the TRB.
          3.3.1.6.4. The TRB meeting will begin with a briefing by test team personnel.
          The test team briefing will answer the “who, what, where, when, why, and how”
          of their test plan and approach. The briefing will include a summary of the test
          objectives, the customer(s), the reporting requirements, status of the development
          of the test item and operational limitations, proposed overall test procedures,
          instrumentation,      data     handling,      data     analysis      plans,     test
          execution/analysis/reporting schedules, real-time and post-flight data
          requirements and the philosophy used in establishing these items. Potentially high
          risk tests from a technical standpoint will be identified and their relationship to
          sequence of testing and known development problems will be addressed.
          3.3.1.6.5. After the test team briefing, the TRB meeting will proceed to a detailed
          technical review of the test plan. The detailed review begins with an opportunity
          for the TRB chair or other reviewers to provide overall comments and/or raise any
          significant issues. Significant issues could be related to a number of items,
          including: the program objectives; status of preparation and planning; likelihood
          of successful test execution; predicted test item characteristics associated with
          individual tests; adequacy of test matrix and test procedures; technical go/no-go
          criteria; compatibility of instrumentation and analysis methods with objectives
          and reporting plans; alternate courses of action; resources needed and available;
          test team training; and other items important to test planning, execution, or
          reporting. Preliminary aspects of safety planning for the test program should be
          discussed. After general comments and issues have been discussed sufficiently,
          the TRB chair will lead the group in a page-by-page review of the test plan. The
          TRB chair will ensure that all participants concentrate on the technical content
          and refrain from discussing inputs related to spelling, grammar, or style. These
          types of inputs should instead be provided by directing reviewers to provide their
          marked-up plans to the test team at the conclusion of the meeting. Any such
          marked-up test plan should have the reviewer’s name on the cover so that the test
          team may contact the reviewer to resolve questions. Action items and
          recommendations will be identified throughout the TRB. These actions could
          involve additional studies or analyses, qualification tests, unique recommended
          training, data analysis plan modifications, or changes to the test plan. Changes to
          the test plan include the addition, deletion, or modification of test points. All
          changes to the test plan, other than strictly editorial, must be discussed and agreed
          upon by the TRB members during the TRB meeting or during the action item
10                                                EDWARDSAFBI99-101 7 AUGUST 2013


            closure phase.
            3.3.1.6.6. At the end of the TRB meeting, the test team note-taker will read the
            action items and recommendations captured in their notes. The TRB chair and
            members will ensure that all the action items and recommendations have been
            properly captured so that the test team can begin addressing them promptly after
            the meeting. The team will provide their notes, the project briefing, and
            attendance list to the TRB chair at the end of the meeting, or as soon as possible
            thereafter. The TRB chair will use these documents along with inputs from the
            other board members to draft a summary document of the TRB. The draft TRB
            summary document will cover significant discussion topics and include the action
            items and recommendations. The test team will provide updates to the TRB
            members as they address the action items and recommendations. The TRB chair
            and members will assess the test team responses and update the TRB summary
            document to reflect the appropriate status of the action items. Ideally, the test
            team will be able to close all action items and incorporate all the TRB
            recommendations in a timely fashion before proceeding to the final safety review
            phase. The TRB summary will contain an overall assessment of the test plan and
            address each of the above items as appropriate. The TRB summary, including a
            list of attendees, will accompany the test plan during final coordination and
            approval.
            3.3.1.6.7. When an alternative to the TRB meeting is approved by 412 TW/CT,
            the TRA shall ensure that a thorough independent review is accomplished and any
            feedback is provided to the responsible test team. Test teams should hold their
            own preliminary test plan review, but this does not constitute the independent
            review required by this instruction. When a TRB meeting is not convened, the test
            team will support the detailed technical review process by maintaining
            communication with the TRA and providing responses to reviewer questions and
            requests as soon as possible.
  3.4. Finishing and Submitting Test Plan for Approval.
     3.4.1. Technical Review Memorandum. Following the technical review, the TRA will
     generate a technical review memorandum (TRM) to the test executing organization
     (TEO) chief engineer summarizing the technical review. When the 412 TW is the RTO,
     the TRM will indicate whether a test plan is considered technically adequate to meet the
     stated objectives and whether there are any significant technical risks. When the 412 TW
     is PTO and does not have technical analysis and reporting responsibilities, the TRM will
     indicate whether a test plan is a reasonable use of 412 TW resources and whether there
     are any significant technical risks. The TRA will provide a copy of the TRM to the 412
     TW/CT.
         3.4.1.1. Reference attachment 7 for TRM guidance when the 412 TW has technical
         responsibility; reference attachment 8 when it does not. The TRA will use the TRM
         to document: who conducted the review; the current technical status and risks of the
         test plan; any additional recommendations and/or open action items; whether the test
         team considered the statistical relevance of their approach in the test design; and the
         test reporting plans. In the TRM, the TRA should classify test plan status as follows:
EDWARDSAFBI99-101 7 AUGUST 2013                                                              11


          adequate to meet the test objectives; adequate to meet the test objectives after
          recommended changes are incorporated; appropriate use of 412 TW resources (used
          only where the 412 TW has no technical responsibility); inadequate to meet the test
          objectives; or inappropriate use of 412 TW resources. If either of the last two
          situations exists, the TRA must notify the 412 TW/CT as soon as possible.
          3.4.1.2. When a TRB meeting is held, the TRM attachments will, at a minimum,
          include the TRB summary document with action items and attendance list. The action
          items will be given along with the test team responses and closure status as judged by
          the TRA.
          3.4.1.3. The test team will modify the test plan to incorporate the appropriate
          recommendations from the technical review. If there are recommendations or action
          items which are not addressed in a test plan revision, the exceptions will be noted in
          the TRM. The test team will provide reasons for the exceptions to the TRA so that
          they can be incorporated in the TRM. In the case of exceptions to recommendations
          or action items, the TRA will notify the 412 TW/CT when providing the copy of the
          TRM so that increased technical risk can be noted and addressed.
       3.4.2. Package Preparation. Following the issuance of the TRM, the TEO chief engineer
       will ensure that the test team prepares a package that includes the test plan, signature
       page, TRM, and, if any exceptions exist, a memorandum for record explaining reasons
       for exceptions to the recommendations or action items. These documents will normally
       be included in the test and safety documentation package (aka “test package”) as
       specified in AFFTCI 91-105. If there are exceptions to technical review
       recommendations or action items, it is highly advisable to coordinate with the TRA in
       advance of submitting the test package to prevent delays in the approval cycle.
   3.5. Test Plan Approval. Approval of the test plan will normally be accomplished
   concurrently with the approval of the test package including the AFFTC Form 5028. Senior
   leadership will coordinate, approve, or review the test package for awareness, according to
   the risk level of the tests. The approval authority for the test plan is the same as for the
   AFFTC Form 5028. If a test package includes tests at more than one risk level, the test team
   may request approval of lower risk test activities prior to the package being approved at the
   higher risk level if they follow the steps specified in chapter 7 of AFFTCI 91-105. The only
   signatures required on the test plan document are those of the author and the chief engineer.
   3.6. Distribution of the Test Plan. The chief engineer shall ensure distribution of the
   approved test plan to: Defense Technical Information Center (DTIC), the Edwards AFB
   Technical Research Library, the AFTC History Office (HO), the TRA, and to organizations
   requiring the plan for participation or support. Test Pilot School student TMP test plans are
   part of the instructional process and thus will not be distributed to DTIC, AFTC/HO, or the
   Edwards AFB Technical Research Library. These plans will be archived, if required, within
   TPS and are considered TPS internal-use documents.
4. Test Plan Changes. before or during a test program, changes can occur which necessitate a
revision of the original test plan. These changes can be due to: unexpected simulator, lab,
ground, or flight test results; unexpected safety impacts; changes in management emphasis or
specification requirements; or other causes. The chief engineer shall ensure that the test team
documents the change and the reasons for it. For both minor and major changes, the office
 12                                                 EDWARDSAFBI99-101 7 AUGUST 2013


responsible for the original technical review (the original TRA) shall be provided a copy of the
modified test plan and the change documentation. The chief engineer shall consult with the TRA
to determine whether a change is minor or major.
   4.1. Minor Changes. Minor changes include: changing flight conditions of test points as
   long as they remain close to existing points and within the envelope of test points approved
   in the original plan; adding test points within the envelope of test points and technical scope
   approved in the original plan; and deleting test points if preliminary results validated by a
   technical expert show they are unnecessary and are not part of a safety build-up. Other than
   the previously mentioned consultation review to determine change category, a minor change
   does not require a new technical review. The chief engineer must generate a memorandum
   amendment documenting the change, the reasons for the change, consultation with the TRA,
   and the determination that changes are minor. The format and coordination/approval process
   for the memorandum amendment shall comply with AFFTCI 91-105. The test team shall
   provide a copy of the memorandum amendment to the TRA and add the memorandum
   amendment to the test package when it is approved.
   4.2. Major Revisions. A major revision is defined to be any change to test objectives,
   technical approach or test methodology, or substantive changes to test procedures or test
   scope. For a major change, the technical review and approval process are identical to that for
   the original test plan. Based on the change documentation provided by the chief engineer, the
   TRA will determine whether a TRB meeting is required. In determining the need for a TRB
   meeting, the TRA will consider the extent of the changes, results of testing to date, predicted
   results of proposed testing, and other pertinent details. Whether a serial technical review is
   accomplished or a TRB meeting is held, the TRA will document the results of the technical
   review in a TRM, as mentioned in paragraph 3.4.1. The TRM must be included in the test
   package when it is amended in accordance with AFFTCI 91-105.
   4.3. Change/Revision Approval, Closure, and Distribution. Approval of the test package
   amendment        will      be      obtained        before      testing     (IAW      AFFTCI
   91-105). The chief engineer shall ensure distribution of the approved test plan to
   organizations requiring the plan for participation or support. As indicated in AFFTCI 91-105,
   the closure amendment is the final step in the overall safety review process. At the time the
   closure amendment is generated, the final version of the test plan as well as any relevant
   lessons learned shall be distributed, at a minimum, to DTIC, the Edwards AFB Technical
   Research Library, the AFTC History Office, and the TRA.




                                            MICHAEL T. BREWER
                                            Brigadier General, USAF
                                            Commander
EDWARDSAFBI99-101 7 AUGUST 2013                                                            13


                                       Attachment 1
         GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION

References
AFPD 99-1, Test and Evaluation Process, 22 July 1993
AFI 99-103, Capabilities-Based Test and Evaluation, 26 February 2008; incorporating change 2,
20 March 2009
DODD 5000.1, The Defense Acquisition System, 12 May 2003
DODI 5000.2, Operation of the Defense Acquisition System, 8 December 2008
AFMCI 99-103, Test Management, 22 November 2004
AFFTCI 91-105, AFFTC Test Safety Review Process, 25 July 2012
AFFTCI 99-103, AFFTC Technical Report Program, 9 July 2010
DoDM 5200.01, DoD Information Security Program, 24 February 2012
DoDD 5400.07, DoD Freedom of Information Act (FOIA) Program, 2 January 2008
AFI 35-102, Security and Policy Review Process, 20 October 2009
AFMAN 33-363, Management of Records, 1 March 2008
AFI 32-7061, The Environmental Impact Analysis, 12 March 2003, certified current 2 April 2010
AFPD 33-3, Information Management, 8 September 2011
EAFBI 99-224, Test Center Deficiency Reporting, 27 October 2010
AFI 99-106, Joint Test and Evaluation Program, 26 August 2009
AFI 99-107, Test Pilot School (PA), 17 January 2013
Adopted Form
AFFTC Form 5028, Test Project Safety Review (Initial of Amendment), 1 April 2008
AF Form 847, Recommendation for Change of Publication, 22 September 2009

Abbreviations and Acronyms
AFB—Air Force Base
AFFTC—Air Force Flight Test Center
AFFTCI—Air Force Flight Test Center Instruction
AFI—Air Force Instruction
AFMAN—Air Force Manual
AFMC—Air Force Materiel Command
AFPD—Air Force Policy Directive
AFRIMS—Air Force Records Information Management System
 14                                             EDWARDSAFBI99-101 7 AUGUST 2013


AFTC—Air Force Test Center
BAF—Benefield Anechoic Facility
cc— courtesy copy
CBRD—Capabilities Based Requirements Document
CTF—combined test force
DoD—Department of Defense
DT&E—developmental test and evaluation
DTIC—Defense Technical Information Center
EAFB—Edwards Air Force Base
EAFBI—Edwards Air Force Base Instruction
GMC—general minimizing considerations
HUD—head-up display
IAW—in accordance with
IFAST—Integrated Facility for Avionics Systems Testing
MFR—memorandum for record
OFP—Operational Flight Program
OPR—Office of Primary Responsibility
ORD—Operational Requirements Document
OT&E—operational test and evaluation
PM—program manager
PTO—participating test organization
ROE—Rules of engagement
RDS—Records Disposition Schedule
RTO—responsible test organization
SG—Systems Group
SRB—Safety Review Board
TBD—to be determined
TEMP—Test and Evaluation Master Plan
TEO—test executing organization
THA—test hazard analysis
T.O.—– technical order
TOC—theory of constraints
EDWARDSAFBI99-101 7 AUGUST 2013   15


TPS—Test Pilot School
TR—technical report
TRA—technical review authority
TRB—technical review board
TRM—technical review memorandum
TW—Test Wing
USAF—United States Air Force
 16                                          EDWARDSAFBI99-101 7 AUGUST 2013


                                   Attachment 2
                    412 TW TEST PLAN PROCESS OVERVIEW

Figure A2.1. 412 TW Test Plan Process Overview
EDWARDSAFBI99-101 7 AUGUST 2013                                                                    17


                                          Attachment 3
                           TEST PLAN CONTENT CHECKLIST

Figure A3.1. Test Plan Content Checklist
General: The following is not intended to drive a format. It is intended to provide guidance on
appropriate content in test plans. Have you reviewed other relevant guidance (AFI 99-103 and
AFFTCI 91-105)? Is an in-house review with all team members planned prior to requesting
review for technical adequacy?
1. Why is this test needed? When is it needed? Who needed it? Who is doing it?
    - Short background on who requested test and for what purpose
    - Summary of program schedule and definition of roles (RTO, PTO, etc)

2. What are the test objectives? What is being tested?
   - Has the test item and/or related test item changes been adequately described?

3. How will the test objectives be met?
   - What types of tests? (test techniques)
   - What part of the envelope? (flight conditions)
   - What store loadings/configurations? (internal and external stores, payloads, etc.)
   - What aerodynamic configurations? (gear, flaps, slats, bay doors, sweep, etc.)
   - What data will be collected and analyzed? (data analysis plan)
       · Aircraft parameters, sources, sample rates, update rates, accuracies
       · Hand-recorded information, records, questionnaires, ratings, criteria
       · Other information – audio recordings, video recordings, chase data, etc.
       · How will the data be analyzed – tools, methods, models, simulations, statistical
         techniques, etc.
   - Test approach, prerequisites and buildup criteria for point-to-point or envelope clearance?

4. What are the reporting requirements? What is the customer’s burning question?
   - Who wants a report, why, what’s the required content, and when is it needed?

5. Where will the test be conducted? Are there other important factors to consider?
   - Deployments to other test sites?
   - Unique test requirements?
   - Facility requirements? (Weight & Balance Hangar, BAF, IFAST, etc.)

6. Is the test plan detailed enough?
   - For another experienced engineer to take over the program using the test plan
   - For an ops engineer to write test cards and conduct tests from it with minimal additional
     help
   - For a technical reviewer to understand the technical approach
   - For an experienced engineer/pilot to easily understand the program and assess the
     risk/benefit of doing the test as planned
 18                                                   EDWARDSAFBI99-101 7 AUGUST 2013


7. Is the test plan effective?
   - Are the test objectives reasonable?
   - Are the objectives, test techniques, instrumentation, data analysis plan, and report
     compatible so that, if you accomplish all the tests correctly and record the data and analyze
     the data, you will be able to answer the test objectives in the final technical report?
   - Are there other test approaches (analysis, lab, ground or flight), test techniques, test points
     or analysis approaches which will produce similar or better data at less cost or risk?
   - Has the homework been done or planned? Instrumentation calibration, tool development,
     tool verification/validation, experienced/trained test team, control room setup?
EDWARDSAFBI99-101 7 AUGUST 2013                                                                                        19


                                                   Attachment 4
                         DATA ANALYSIS PLAN CONTENT CHECKLIST

Figure A4.1. Data Analysis Plan Content Checklist
General: The following is not intended to drive a format. It is intended to provide guidance on appropriate content in
data analysis plans.
Regardless of format, the data analysis plan must clearly indicate: required parameters, collection methods, initial
quality check methods, algorithms and tools to be applied, tool validation methods, analysis methods, completion
criteria, evaluation criteria (if applicable), evaluation products and statistical relevance (if applicable). All of these
must be related to answering the customer’s burning questions.
1. What parameters are needed for the analysis? How will they be collected?
  - Aircraft parameters: orange wire or bus data?
     Associated Details:
     · What/where is the source for each parameter?
     · What is the update rate of the parameter? What is the refresh rate of the parameter?
     · What is the sample rate of the parameter?
     · Is there any filtering (or other manipulation) of the parameter by the source system?
     · Is there any filtering (or other manipulation) of the parameter by the instrumentation system?
     · What is the range of the parameter?
     · What is the resolution of the parameter?
     · What is the accuracy of the parameter?
     · How is the parameter calibrated? When was it calibrated? Will it need to be calibrated again? When? What
        were the calibration results? What analysis of the calibration results is needed?
     · Will the parameter be telemetered and available real-time? Will it be for safety of flight or safety of test?
     · Will the parameter be recorded and available post-flight?
     · Will the parameter only be available on a cockpit display? If so, has a data collection sheet been developed?
        Will it be completed by the aircrew, the test conductor, or other engineers? Does the data collection sheet
        include enough information to correlate the hand-recorded data with other recorded information (such as:
        HUD tape, instrumentation data tape, etc)?
  - Will qualitative data be collected?
     · Have questionnaires been developed and reviewed by engineering and ops personnel? Are they well-
        established or will they need to be validated during baseline tests?
     · Will comments from aircrew, maintainers, or other personnel need to be captured during the test? Will they be
        recorded automatically? Will they need to be captured by a note-taker or scribe?
     · Will Cooper-Harper ratings, pilot induced oscillation ratings, or other rating scales be used? If so, have the test
        techniques been adjusted appropriately? Have criteria been established or will they need to be determined
        during baseline tests? Has collection methodology been established to avoid inappropriate rating
        assignments?
  - Is other information needed for analysis?
     · Is other test aircraft data needed including weight and balance data, configuration information, fuel sample
        data, audio recordings, video recordings, memory downloads, or maintenance logs?
     · Is information needed from support aircraft including photographs, video recordings, audio recordings, pacer
        data, chase aircraft engineering data, configuration logs, maintenance information, etc?
     · For either of the above categories: Who has the information? Who, on the test team, will collect this
        information? How will it be distributed to other team members? How will it be used in the analysis?
2. How will you know the data you collect are good?
  - What methods will be used to quality check parameters? (“Truth” data, predictions/models, comparisons, tools)
  - Who will accomplish the checks?
  - What guidance and training on quality checks is available?
  - When will you know the parameters collected are good? If they aren’t good, what steps are necessary to
    determine where parameters went bad (at the source, after filtering, when recorded, when telemetered, etc)?
    Depending on where parameters went bad, can they be corrected? If not, will test repeats be possible? If not,
 20                                                               EDWARDSAFBI99-101 7 AUGUST 2013


    will you be able to answer the customer’s burning question? If you can answer the question but at a lower level
    of confidence, will the customer be satisfied?

3. How will data be analyzed?
  - What sampling tool will provide the initial parameters (aka “first generation data”) in proper engineering units,
    at the proper rates? What settings are required for the tool to function properly? Are the required settings
    understood and documented for each type of test and set of parameters?
  - Which sets of initial parameters are inputs to analytical tools which calculate or generate necessary “second
    generation” parameters? What other inputs or settings are required to properly run these tools? Are the required
    inputs and settings documented for each tool, test, and parameter set?
  - Will the evaluation involve models or simulations? Are all the required models (aerodynamic, propulsive, flight
    controls, fuel system, stores system, thermal, etc.) available? Is there documentation for each of the models
    explaining proper use, assumptions, and limitations? Have test cases for each of the models been provided and
    run? Will the models/simulations be updated with flight test data? What will the update process involve?
  - Will the evaluation involve statistical techniques? Have the actual or potential statistical techniques been
    identified? Are the required statistical tools and expertise available to do the statistical analysis? If a statistical
    technique is not being used, has the reason been clearly stated?
4. What will be used to determine if the right data have been collected? What will be used to determine when
    enough data have been collected?
  - What method will be used to judge test technique/maneuver quality? Are there any parameters which will
    indicate that the test item is in the proper “state” at the start of the test? If not, can the states be determined by
    the aircrew? If available to the aircrew, what will the process be to check and document the state? Will
    maneuver inputs be compared to standards and/or predictions? Are the flight conditions (M, V, H, Nz, Phi,
    Theta, Psi, etc.) and the acceptable ranges documented? Are there expected results which must be achieved
    (flight conditions, cockpit indications, task completion, etc.) and can these be monitored or must they be noted
    by the aircrew?
  - How will the test team know when enough data have been collected for specific test points?
  - How will the test team know when enough data have been collected for a related series of tests?
  - How will the test team know when enough data have been collected for the whole test?
5. How will the analytical results be used to meet the customer’s needs?
  - Will the team use results to document a system’s behavior at specific conditions? Objective: “Demonstrate”
  - Will the team use results to evaluate the system against requirements? Objective: “Evaluate”
  - If there are evaluation objectives, what criteria will be used to determine levels of performance? (satisfactory,
     unsatisfactory, etc.) Has the customer agreed to the criteria?
  - What final results are expected? Are the results all the customer needs to answer the burning question?
     (Example: characterization tests may be used when the customer wants an update to the flight manual or a
     simulator).
  - How will the final results lead to conclusions and/or recommendations? Are these needed to address the
     customer’s burning question? (Example: evaluation tests may be used when the customer wants to know
     whether a system will meet a key performance parameter that feeds into a milestone decision.)
6. Is the data analysis plan detailed enough?
  - For another experienced engineer to take over the program using the data analysis plan
  - For a technical reviewer to understand the technical approach
  - For an experienced engineer to easily understand the program and assess the risk/benefit of doing the test as
     planned
7. Is the data analysis plan effective?
  - Are the objectives, test techniques, instrumentation, data analysis plan, and report plan compatible so that, if you
    accomplish all the tests correctly and record the data and analyze the data, you will be able to answer the test
    objectives in the final technical report?
  - Are there other test approaches (analysis, lab, ground or flight), test techniques, test points or analysis
    approaches which will produce similar or better data at less cost or risk?
  - Has the homework been done or planned? Instrumentation calibration, tool development, tool
    verification/validation, experienced/trained test team, control room setup?
EDWARDSAFBI99-101 7 AUGUST 2013                                                                                     21


                                                  Attachment 5
                                    COMMON TECHNICAL RISKS

Figure A5.1. Common Technical Risks
General: The following is not intended to drive a review. It is intended to provide guidance on
possible technical risks in test plans based on past experiences. Those who generate or review
test plans should reference other relevant guidance (AFI 99-103 and AFFTCI 91-105) and should
research related past test efforts for lessons learned to minimize technical risks.
1. Not knowing or misunderstanding how the system under test will be used in the field.
   - Unclear communications between the test team and the Systems Group (SG), end-user, contractor, and other
     interested parties may result in doing the wrong test.
        · For example, if the SG needs the system to be used without additional maintenance actions after a new
          part is installed or new fuel is used (so that the system is ready to meet fast-response operations), then the
          test should be accomplished by installing the new part or using the fuel and not performing maintenance
          actions which may alter the potential characteristics of the system (such as calibrations, re-trimming, fine-
          tuning, etc). This may mean that the test will require special permission to deviate from “established
          technical order (T.O.) procedures” and/or that the SG may need to change those established T.O.
          procedures.
2. Not knowing or misunderstanding what led to the system development effort.
   - Only through detailed discussions between the test team and the contractor and SG can a test team learn about
     the system under test. For developing systems, you need to determine whether a system change is a planned
     evolution or is in response to a specific issue. This knowledge should drive test point development.
        · For example, sometimes a change is made to the system to address a problem found in a simulation study,
          lab test, ground/flight test event, or operational mission but is inserted as one of many changes that are
          part of a scheduled Operational Flight Program (OFP) release. Only by digging into the details of each
          change can you find out specifics that should be used to tailor test techniques and test conditions. You
          want to be sure to accomplish the test at the conditions where the problems first arose in addition to any
          predicted “worst-case” conditions.
3. Not properly translating the customer’s burning question into test objectives.
   - An objective may seem to match the customer’s burning question but may be too vague. In this case, it should
     probably be an overall test approach or general objective and there should be specific objectives underneath
     tied directly to test techniques, test conditions, instrumentation requirements, and data analysis procedures.
        · For example, the customer may want to accomplish some tests to gain confidence in a new system.
          However, that is too vague to be an acceptable objective or even a good test approach. Reliability and
          maintainability databases take years to develop fully. However, if they have information on what results
          would increase or lower their confidence in the system, then that can be used to drive some specific
          objectives, test techniques, and analysis procedures. And, if the customer knows where the old system
          had problems or where the new system may have problems, or where the new system will be used most
          often, that can help drive test conditions.
4. Not knowing the details of the answer the customer wants.
   - The test “was successful” but the report may deliver an incomplete answer. Ask the customer how accurate the
     results need to be and what confidence they desire in the conclusion. Explain how accuracy and confidence
     requirements drive instrumentation requirements, analysis procedures, test techniques, and the scope of
     testing.
        · For example, the customer may have the budget and schedule for a demonstration effort – going to a test
          condition a few times with a poorly-instrumented aircraft and noting if anything bad happens or not. This
          effort would not be enough for us to state that the system has eliminated a past problem. To reach that
          conclusion, the frequency of past problems as well as the conditions associated with those past problems
          would have to be known and would have to be used to develop a test matrix. This would best be done by
          applying a design of experiments approach to be able to also report the confidence in the conclusion.
        · Another example, the customer may want to get data for inclusion in a flight manual update. The existing
          data are accurate to a specific level. The test program must be able to obtain data which are at least as
 22                                                            EDWARDSAFBI99-101 7 AUGUST 2013


         accurate as the existing data – which will drive instrumentation requirements, test techniques, and data
         analysis procedures. Also, to provide appropriate data for a flight manual update, it is likely that models
         (aerodynamic, propulsive, flight controls, fuel system, etc.) used in flight manual development would be
         needed as part of the data standardization procedures.
5. Not putting realism into the test and report schedule.
   - The customer always wants the test and report done quickly but we must honestly and politely convey what a
     realistic test and report schedule would be to begin a dialogue.
        · For example, if a customer is told that the test matrix was a certain size in order to address the multitude
          of issues they had raised, they may decide to prioritize the issues and be willing to accept a test which
          only addresses the top-priority issues. Or, if the customer is informed that, to get the level of accuracy
          they required, an instrumentation modification, more time-consuming test techniques, and/or more
          extensive analysis procedures are needed, they may be willing to make the necessary schedule
          adjustments. Or, if the customer is informed that, to get the type of recommendation required to support a
          milestone decision, an evaluation approach rather than a demonstration approach must be taken which
          requires more time and resources, they may accept those increases. To properly adjust the schedule for
          these increases, the theory of constraints (TOC) network should be modified to capture the additional
          analysis tasks before, during, and after execution phases of the test program. Work a day-to-day TOC
          network to coordinate a timely, technically accurate and credible technical report to support a 42-day or
          less timeline.
6. Not getting the right tools to do the job.
   - The customer wants answers to support milestone decisions but we will not have all the tests done in time
     based on the execution plan. Are there tools which could improve the analysis to support the milestone
     decision briefing?
       · For example, safe build-up approaches for envelope expansion drive the schedule and tend to be lengthier
         than the customer desires. If there is a milestone decision looming, the customer may think there are few
         options but to accelerate the test effort. However, if models (aerodynamic, structural, propulsive, weight
         and balance, etc) are made available before the start of the test program, sometimes they can be used to
         develop predictions which can be validated with the available test data and provide some limited but
         valuable answers until the envelope expansion effort is completed.
7. Inadequate instrumentation.
   - The test team developed a list of parameters needed for the test and those parameters are available on the
     aircraft, but they do not tie to the data analysis plan. Usually the parameter names will match the data analysis
     plan but they may not be from the correct onboard data source, be sampled at the desired rate, be in the correct
     engineering units, be refreshed at the desired rate, be sampled at the same time as other parameters, be
     accurate, have a backup parameter, have a truth source, or have enough resolution.
        · For example, the data analysis plan may call for normal acceleration and the parameter list includes it.
          However, the specific analysis requires this to be the normal acceleration in feet per second squared used
          as an input to the third page of the flight control system control laws but the normal acceleration pulled
          off the bus was in g’s and was from a different accelerometer package than that used by the flight control
          system. To be sure you have the right instrumentation and parameters, the inputs to each data analysis
          tool must be well-understood, including the assumptions of the tools.
8. Forgetting the warfighter.
   - The test plan may meet the customer’s stated requirements but may not answer the questions and requirements
     of the warfighter if the operational requirements and uses are not properly considered.
        · For example, the test plan may be well laid out to address specification requirements for the contractor
          and key performance parameters for a milestone decision briefing by the customer but may not include
          testing in an integrated manner under operationally representative conditions. This has, in the past, led to
          failures in operational test and evaluation (OT&E) as well as in the field. When found late, the cost of
          correcting deficiencies has been more expensive than if they had been found and addressed in
          developmental test. The warfighter deserves the best test effort we can provide.
EDWARDSAFBI99-101 7 AUGUST 2013                                                                                     23


                                                   Attachment 6
                TECHNICAL REVIEW BOARD (TRB) MEETING APPROACH

Figure A6.1. Technical Review Board (TRB) Meeting Approach
Before the Meeting:
1. Have all the TRB members been given the test plan – including test information sheets, data analysis plans,
     report plans, schedules and other supplemental material – with sufficient time to review the materials prior to
     the TRB?
2.   Does the test team have all the right people (government and contractor testers, subject matter experts, customer
     representatives, participating test organization representatives, etc.) available to support the TRB? Is the test
     team prepared for the meeting – with detailed information on the system under test, the test objectives (and their
     relationship to the “burning question”), the test approach, the data analysis approach, the reporting
     requirements, the program schedule, the roles of all the players, and the readiness to test?

During the Meeting:
1. Administration
     - sign-in sheet
     - introductions, identify board members
     - note taker assigned
     - location of rest rooms, etc.
2.   Rules of Engagement (ROE)
     - time available (until board members or key project personnel have to leave)
           · technical not safety review
           · safety issues that affect technical planning ok
           · may lead to recommended actions for SRB
     - no wordsmithing, provide marked copy of test plan to authors
     - all TBDs resolved prior to final signoff
     - test team to provide explicit responses to TRB actions items – will be added to TRM
3.   Project Brief
4.   Overall Comments & Significant Issues (Schedule review with time allocated to the technical report theory of
     constraints schedule, including data analysis to support a timely, technically adequate and credible report
     delivered to the customer.)
5.   Page-by-page review
6.   Safety discussion with optional review of draft safety package, primarily general minimizing considerations
     (GMCs) and test hazard analyses (THAs)
7.   Review action items
8.   Project will draft notes for TRB chair
     - Chair & board members may add more words from personal notes
9.   Closing comments

After the Meeting:
1. Test team: provides notes, project brief, attendance list, and action items to TRB chair; updates test plan as
     required and provides to TRB chair; works action items and provides updates to TRB chair. If applicable,
     prepares exceptions memorandum for any action items not closed when TRM issued.
2.   TRB chair: drafts summary document with attendance list and action items; coordinates with TRB members to
     finalize; prepares technical review memorandum and attaches TRB notes and action items.
 24                                                                  EDWARDSAFBI99-101 7 AUGUST 2013


                                                      Attachment 7
       SAMPLE TECHNICAL REVIEW MEMORANDUM WHEN 412 TW IS THE
                   RESPONSIBLE TEST ORGANIZATION

Figure A7.1. Sample Technical Review Memorandum when 412 TW is the Responsible
Test Organization

                                              DEPARTMENT OF THE AIR FORCE
                                           HEADQUARTERS 412TH TEST WING (AFMC)
                                            EDWARDS AIR FORCE BASE CALIFORNIA

                                                                                                                  dd Mmm 200y

MEMORANDUM FOR xxx FLTS/EN

FROM: 77x TS/Exxx
      307 E. Popson Avenue
      Edwards AFB CA 93524

SUBJECT: Technical Review of the ____________ Test Plan, xyzabc (412 TW JON xxyyzzaa)

1. As delegated by the 412 TW/CT, the subject test plan was reviewed by Mr. _____, 77xTS/Exxx, ____________
Technical Expert; Ms. _________, 77xTS/Exxx, ___________ Flight Chief; … A Technical Review Board (TRB) was
held on _________ (reference attachment 1 for minutes and action items). (Or: The review was conducted in an informal
fashion with the test team.) The test plan was modified to address the reviewers’ concerns. (Or: with the following
exceptions, the test plan was modified to address the reviewers’ concerns….)

2. The overall test objective is to …. Note whether the team considered the statistical relevance of their approach in the
test design and why this was or was not considered. Discuss any relevant issues related to reduced or increased technical
risk. For example: Since this test is limited in scope to ..., the applicability of the findings will be limited. The customer
has accepted this risk in attached memo. Other example: Since results from a previously conducted risk reduction effort
are available and applicable, the scope of this test, while less than typically required, is considered reasonable.

3. The responsible test organization is the 412th Test Wing, Air Force Test Center, Edwards AFB, CA. Testing will be
executed by the _____ test team, part of the _________ Combined Test Force, led by the ____th Flight Test Squadron at
Edwards AFB, CA.

4. The subject test plan is adequate to meet the test objectives. (If this is not true, indicate the shortfalls.) The test results
will be documented in a technical report (TR). As the discipline most involved in the test, ______ will lead the analysis
and reporting efforts. _____ _______ (661-27x-yyyy) will be the report team lead and team members will include _____
engineer, ____ engineer, project pilot and ____ (loadmaster, crew chief, others, if applicable), at a minimum. Please
contact _________ (661-27x-yyyy) with any questions.


                                                                       _____ __. _______
                                                                       Chief, __________ Flight

2 Attachments:
1. Technical Review Board Attendance List
2. Technical Review Board Notes and Action Items

cc:
412TW/CT
EDWARDSAFBI99-101 7 AUGUST 2013                                                                 25


                                  Notes and Action Items from
                        Technical Review Board, DD MMM 20YY, for
                                 (Insert Test Plan Title) Test Plan
     This document was UPDATED with Action Item Closure status on DD MMM 20YY
The formal technical review board was … (provide an overview of the TRB). Could point out
where things ran smoothly and where things did not. Could point out whether test team delivered
test plan and related materials to the reviewers in adequate time to prepare. Could discuss
whether the reviewers took extra efforts to provide feedback to the test team in advance of the
meeting. Could end with an indication of whether the test plan was in pretty good shape with
mostly minor changes needed or not.

Address major topics of discussion. Try to identify discussions which resulted in significant
recommendations or action items.

…

…

Action items were reviewed at the end of the meeting and are included below.

                                     TRB ACTION ITEMS:
1.       Provide the details on … .
     Response: Test team provided the details to the reviewers.
     Status: Closed.

2.       Determine whether … should be incorporated into the test plan.
     Response: Test team decided these items were not related to meeting the technical objectives
     and thus should not be incorporated into the test plan.
     Status: Closed.

3.       Rewrite sections … consistent with inputs from … reviewer.
     Response: Test team made changes and provided them to appropriate TRB member.
     Status: Closed.

4.       Change … .
     Response: Test team did not have time to complete the action item due to higher priority
     tasks. Test team does not consider recommended changes critical to test success.
     Status: Open. Item will be addressed in exceptions memorandum.

5.       Research … issue to determine appropriate corrections to … .
     Response: Test team was unable to reach the knowledgeable party at … . Corrections will be
     made at a later date and coordinated with TRB members.
     Status: Open. Item will be addressed in exceptions memorandum.
 26                                                                EDWARDSAFBI99-101 7 AUGUST 2013


                                                     Attachment 8
SAMPLE TECHNICAL REVIEW MEMORANDUM WHEN 412 TW DOES NOT HAVE
                ANY TECHNICAL RESPONSIBILITIES

Figure A8.1. Sample Technical Review Memorandum when 412 TW does not have any
Technical Responsibilities

                                          DEPARTMENT OF THE AIR FORCE
                                       HEADQUARTERS 412TH TEST WING (AFMC)
                                        EDWARDS AIR FORCE BASE CALIFORNIA
                                                                                                              dd Mmm 200y

MEMORANDUM FOR 412 TW/CT (Mr. Carter Wilkinson)

FROM: 77x TS/Exxx
      307 E. Popson Avenue
      Edwards AFB CA 93524

SUBJECT: Technical Review of the ____________ Test Plan, xyzabc (412 TW JON xxyyzzaa)

1. As delegated by 412TW/CT, the subject test plan was reviewed by Mr. _____, 77xTS/Exxx, ____________ Technical
Expert; Ms. _________, 77xTS/Exxx, ___________ Flight Chief; … A Technical Review Board (TRB) was held on
_________ (Reference attachment 1 for minutes and action items). (Or: The review was conducted in an informal fashion
with the test team.) The test plan was modified to address the reviewers’ concerns. (Or: With the following exceptions,
the test plan was modified to address the reviewers’ concerns….)

2. The overall test objective is to …. Note whether the team considered the statistical relevance of their approach in the
test design and why this was or was not applied. Discuss any issues related to whether the test is a reasonable and an
effective use of 412 TW resources.

3. The responsible test organization is the ____ (Some other organization) in ______, ___. The Participating Test
Organization is the 412th Test Wing, Air Force Test Center, Edwards AFB, CA which is responsible for mission planning,
test execution and test safety. Testing will be executed by the _____ test team, part of the ____ Combined Test Force, led
by the __th Flight Test Squadron at Edwards AFB, CA. The Total System Program Responsibility resides with _________
(Contractor organization or organizations).

4. The test plan is considered a reasonable use of 412 TW resources. Technical reporting is not an 412 TW responsibility
and is not addressed in the subject plan. Please contact _________ (661-27x-yyyy) with any questions.


                                                                   _____ __. _______
                                                                   Chief, __________ Flight

2 Attachments:
1. Technical Review Board Attendance List
2. Technical Review Board Notes and Action Items

cc:
412TW/CT
