BY ORDER OF THE COMMANDER                            AIR FORCE OPERATIONAL TEST AND
AIR FORCE OPERATIONAL TEST AND                    EVALUATION CENTER PAMPHLET 99-104
EVALUATION CENTER
                                                                            24 SEPTEMBER 2013

                                                                               Test and Evaluation

                                                      AFOTEC OPERATIONAL SUITABILITY
                                                          TEST AND EVALUATION GUIDE

              COMPLIANCE WITH THIS PUBLICATION IS MANDATORY

ACCESSIBILITY: Publications and forms are available on the e-Publishing website at
               www.e-Publishing.af.mil for downloading or ordering.

RELEASABILITY: There are no releasability restrictions on this publication.

OPR: AFOTEC A-2/9A                                                    Certified by: AFOTEC A-2/9
                                                                       (Col Marcus R. Schulthess)
Supersedes:    AFOTECPAM 99-104,                                                       Pages: 113
               9 Nov 2010


AFOTECPAM 99-104 provides a compilation of “best practices” and corporate wisdom of “how
to” efficiently and effectively develop the operational suitability portion of the test concept, test
plan and final report. It provides definitions of common terms and measures and identifies
processes related to suitability test and evaluation. This guide will equip test teams with a solid
foundational knowledge of the concepts and procedures pertaining to operational suitability test
and evaluation. Detachments, test teams and support personnel should use it as a “how to” guide
in conjunction with guidance provided in AFOTECMAN 99-101, Operational Test Processes
and Procedures, and the AFOTEC OT&E Guide. Refer recommended changes and questions
about this publication to the Office of Primary Responsibility (OPR) using the AF IMT 847,
Recommendation for Change of Publication; route AF IMT 847’s from the field through the
appropriate functional’s chain of command. The use of the name or mark of any specific
manufacturer, commercial product, commodity, or service in this publication does not imply
endorsement by the Air Force. Ensure that all records created as a result of processes prescribed
in this publication are maintained in accordance with AF Manual (AFMAN) 33-363,
Management of Records, and disposed of in accordance with the Air Force Records Disposition
Schedule (RDS) located at https://www.my.af.mil/gcss-af61a/afrims/afrims. The use of the
name or mark of any specific manufacturer, commercial product, commodity, or service in this
publication does not imply endorsement by the Air Force.
 2                                                                   AFOTECPAM 99-104 24 SEPTEMBER 2013


SUMMARY OF CHANGES

Readers should thoroughly review this significantly revised publication. Four significant
changes are the updates to reference materials, linkage to high level DoD Reliability,
Availability and Maintainability guidance, the incorporation of AFOTEC’s process
improvements and latest best practices. The formatting now conforms to the direction in AFI 33-
360, Publications and Forms Management, Incorporating through 7 Feb 2013.

Chapter 1—INTRODUCTION                                                                                                                       6
       1.1.    Intent and Organization. .........................................................................................             6
       1.2.    Operational Suitability Defined. ............................................................................                  6
       1.3.    Significance of OT&E for System Suitability. ......................................................                            6
       1.4.    Supporting RAM Documentation. .........................................................................                        8
       1.5.    Related Issues ........................................................................................................        9

Chapter 2—RELIABILITY, AVAILABILITY AND MAINTAINABILITY (RAM)                                                                                10
       2.1.    Introduction. ...........................................................................................................     10
       2.2.    Repairable and Non-repairable Systems in the Context of RAM Planning and
               Analysis. ................................................................................................................    10
       2.3.    Dissecting Total Equipment Time. ........................................................................                     10
Figure 2.1.    Linear Breakdown of Total Equipment Time ........................................................                             11
Figure 2.2.    Dendritic Breakdown of Total Equipment Time ...................................................                               12
       2.4.    Analytical Products. ...............................................................................................          12
Figure 2.3.    Notional State Graph .............................................................................................            13
Table 2.1.     Example Failure and Repair Data ..........................................................................                    13
       2.5.    Probability Distributions. .......................................................................................            13
       2.6.    Reliability. ..............................................................................................................   14
       2.7.    Aspects of Reliability. ............................................................................................          15
       2.8.    Availability. ...........................................................................................................     17
       2.9.    Maintainability. ......................................................................................................       17
       2.10.   Other RAM Guidance: ...........................................................................................               19

Chapter 3—LOGISTICS SUPPORTABILITY                                                                                                           21
       3.1.    Introduction. ...........................................................................................................     21
Table 3.1.     Logistics Support and Operational Suitability Linkage .........................................                               21
       3.2.    Definitions. ............................................................................................................     22
       3.3.    Logistics Support Concepts. ..................................................................................                23
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                                                          3


      3.4.   Other Topics Related to Logistics Support. ...........................................................                         24

Chapter 4—USAGE RATES                                                                                                                       27
      4.1.   Introduction. ...........................................................................................................      27
      4.2.   Definitions. ............................................................................................................      27
      4.3.   Sources of Usage Rate Data. ..................................................................................                 28
      4.4.   Usage Rate Evaluation Considerations. .................................................................                        28
Table 4.1.   SPA Requirements .................................................................................................             29
Table 4.2.   Example Test Data .................................................................................................            29
      4.5.   Usage Rate Measures. ............................................................................................              30

Chapter 5—COMPATIBILITY AND INTEROPERABILITY                                                                                                31
      5.1.   Introduction. ...........................................................................................................      31
      5.2.   Definitions. ............................................................................................................      31
      5.3.   Planning For Test. ..................................................................................................          32
Table 5.1.   Notional Factors and Descriptors Affecting EMC OT&E Planning ......................                                            34
      5.4.   Test Execution and Reporting Considerations. ......................................................                            35

Chapter 6—TRANSPORTABILITY                                                                                                                  36
      6.1.   Introduction. ...........................................................................................................      36
      6.2.   Definitions. ............................................................................................................      36
      6.3.   Transportability Concepts. .....................................................................................               36
      6.4.   Transportability OT&E Planning Considerations. .................................................                               37

Chapter 7—SAFETY AND ENVIRONMENTAL IMPACTS AND EFFECTS                                                                                      41
      7.1.   Introduction. ...........................................................................................................      41
      7.2.   ESOH. ....................................................................................................................     41
      7.3.   ESOH Council (ESOHC). ......................................................................................                   41
      7.4.   ESOH Statutory Compliance. ................................................................................                    41
      7.5.   ESOH-Management System (ESOH-MS). ............................................................                                 42
      7.6.   ESOH Planning and Review Process. ....................................................................                         42
      7.7.   HSP. .......................................................................................................................   43
Table 7.1.   Risk Level Matrix ..................................................................................................           43
      7.8.   ESOHCB. ...............................................................................................................        44
Table 7.2.   ESOH Operational Impact Matrix .........................................................................                       44

Chapter 8—HUMAN FACTORS (HF)                                                                                                                46
4                                                                 AFOTECPAM 99-104 24 SEPTEMBER 2013


     8.1.    Introduction. ...........................................................................................................   46
     8.2.    Definitions. ............................................................................................................   46
     8.3.    HF/HSI Policies. ....................................................................................................       46
     8.4.    HF/HSI Areas of Expertise. ...................................................................................              46
     8.5.    HF/HSI Test Measures. ..........................................................................................            46
     8.6.    HF/HSI Test Plan Inputs. .......................................................................................            47
     8.7.    HF/HSI Test Execution. .........................................................................................            47
     8.8.    Questionnaire Support. ..........................................................................................           47

Chapter 9—DOCUMENTATION                                                                                                                  49
     9.1.    Introduction. ...........................................................................................................   49
     9.2.    Definitions. ............................................................................................................   49
     9.3.    Documentation Test Planning Considerations. ......................................................                          49
     9.4.    Documentation Measures. ......................................................................................              50
     9.5.    Documentation Data Requirements. ......................................................................                     50
     9.6.    Documentation Evaluation Considerations. ...........................................................                        51

Chapter 10—TRAINING                                                                                                                      52
     10.1.   Introduction. ...........................................................................................................   52
     10.2.   Training and Training Support. ..............................................................................               52
     10.3.   Training OT&E Methodology. ..............................................................................                   52
     10.4.   Training Considerations. ........................................................................................           53

Chapter 11—MODELING AND SIMULATION (M&S)                                                                                                 54
     11.1.   Introduction. ...........................................................................................................   54
     11.2.   Models and Simulations. ........................................................................................            54

Chapter 12—HOW TO TAILOR TESTING                                                                                                         56
     12.1.   Introduction. ...........................................................................................................   56
     12.2.   Tailoring Suitability Testing for Systems (General). .............................................                          56
     12.3.   Tailoring Suitability Testing for Joint Programs. ..................................................                        57

Chapter 13—MANAGING OPERATIONAL SUITABILITY DATA                                                                                         58
     13.1.   Introduction. ...........................................................................................................   58
     13.2.   Data Management Procedures. ..............................................................................                  58
     13.3.   Data Management and Analysis Plan (DMAP). ....................................................                              58
     13.4.   Tasks for Suitability Personnel. .............................................................................              59
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                                                         5


      13.5.   Other Data Sources. ...............................................................................................         60
      13.6.   JRMET. ..................................................................................................................   62
      13.7.   Test Data Scoring Board (TDSB). .........................................................................                   63

Chapter 14—DEFICIENCY REPORTING                                                                                                           64
      14.1.   Introduction. ...........................................................................................................   64
      14.2.   Deficiency Reporting. ............................................................................................          64
Table 14.1.   Attributes which may affect OSS&E .....................................................................                     64
Table 14.2.   DR Category and Priority Determination ..............................................................                        65

Attachment 1—GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION                                                                             67

Attachment 2—RELIABILITY MEASURES                                                                                                          94

Attachment 3—AVAILABILITY MEASURES                                                                                                         98

Attachment 4—MAINTAINABILITY MEASURES                                                                                                     103

Attachment 5—LOGISTICS MEASURES                                                                                                           107
 6                                                AFOTECPAM 99-104 24 SEPTEMBER 2013



                                             Chapter 1

                                         INTRODUCTION

1.1. Intent and Organization.
     1.1.1. The intent of this guide is to provide a general overview of suitability and useful
     information to test team members assigned the responsibilities of planning, executing, and
     analyzing tests involving suitability areas of interest. This pamphlet is not intended to
     replace or supersede regulatory or statutory requirements found in other documents. This
     guide is meant to educate newcomers to Operational Test and Evaluation (OT&E) about the
     peculiarities of evaluating a system for operational suitability.
     1.1.2. The main body of this guide provides a translation of real-world operations to
     operations in OT&E. For example, some metrics from real-world operations are not as
     meaningful in OT&E due to smaller sample sizes and/or shorter time periods of
     consideration. It is important for analysts to know this information to understand the sources
     of data used to assess suitability.
     1.1.3. The attachments include sample test measures and provide detailed guidance for
     collecting and analyzing data. It is important for the operators/maintainers to know this
     information to understand how the data are to be collected and the goal of data collection.
     1.1.4. Detachments and test teams should tailor suitability testing to their specific systems.
1.2. Operational Suitability Defined. The Defense Acquisition Guidebook (DAG) contains
this definition of operational suitability: “The degree to which a system can be placed
satisfactorily in field use with consideration given to availability, compatibility, transportability,
interoperability, reliability, wartime usage rates, maintainability, safety, human factors,
manpower supportability, logistics supportability, natural environmental effects and impacts,
documentation, and training requirements.” Some elements of this definition, such as safety,
human factors, compatibility, interoperability and environmental effects, also apply to
operational effectiveness. The current “Memorandum of Agreement (MOA) on Multi-Service
Operational Test and Evaluation (MOT&E) and Operational Suitability Terminology and
Definitions” and the AFOTEC OT&E Guidebook provide the same definition for operational
suitability as the DAG. The MOA is located on the AFOTEC Intranet page and provides further
policy, terms and definitions for the Service Operational Test Agencies (OTA) to use in
suitability evaluations.
1.3. Significance of OT&E for System Suitability.
     1.3.1. During OT&E, test team members will evaluate operational suitability along with
     operational effectiveness for a given system in order to provide a complete operational
     perspective to the warfighter and determine the overall mission capability of a system. The
     test team will evaluate the areas listed in the definition above to search for suitability
     shortfalls, identify them and document the results. In the end, operational suitability testing
     is essential because the suitability shortfalls identified in OT&E provide the warfighter with
     key information that plays a large role in determining the overall mission capability of a
     system. Thus, a carefully orchestrated suitability analysis is necessary for the Milestone
     Decision Authority (MDA) to make an informed fielding decision for a given system.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                7


  1.3.2. OT&E examines many facets of suitability for a given system. Some of these
  suitability areas may include the effects the system will have on operations tempo, what
  support personnel will be required to maintain the system and what level of logistics support
  will be needed for the system. In addition, the OT&E results will identify how deployable
  the system is, how often it breaks, how easy it is to fix, how time-consuming it is to fix and
  what type of training maintainers will need to support the system. Furthermore, OT&E will
  examine the type of storage and transportation needed for the system at the main and forward
  operating bases. OT&E will also provide the warfighter with a substantial amount of human
  factors data on the system’s ease of use, what type of issues the user may have with the
  system’s design, the effectiveness of system training and the impact the new system might
  have on the user’s day-to-day mission. A well-planned, properly executed OT&E is key to
  identifying suitability shortfalls of a given system.
  1.3.3. Proper suitability training for operational testers is of paramount importance, because
  suitability analysis can be difficult and costly to perform. Some areas of suitability such as
  reliability might be particularly difficult to test because of the limited number of hours or test
  articles available for the test. If the system under test (SUT) simply does not fail during the
  allotted time, a conclusion about mean time between critical failures will be difficult to report
  without other avenues of analysis. Because of difficulties such as this, the test team must
  have a solid plan for conducting suitability analysis, along with a well-defined methodology
  and a list of the models they will employ to fill any gaps in the data collected from the actual
  test. It is important for the test team to have a solid understanding of testing for operational
  suitability before planning the operational test in order to take into account the full scope of
  suitability testing.
  1.3.4. To the warfighter, a system’s suitability performance could be a major factor that
  determines the life-cycle cost of the system. Money is not the only cost issue. A system with
  suitability issues could dramatically increase the number of maintainers needed to sustain it,
  so fielding it could have an impact on manpower. Furthermore, a system with major human
  factors issues might increase operator workload dramatically. For instance, if a new
  chemical suit is very effective at providing protection against chemical agents, yet is
  significantly more thermally burdensome than the currently fielded chemical suit, the
  warfighter will need to weigh the added protection against the drawbacks of the excessive
  thermal burden. In addition, if a system needs to be stored in a climate-controlled
  environment, that requirement might add significant cost to its deployment in the theater if a
  special facility is required to store it. Moreover, if a new chemical agent detector has a high
  false alarm rate for detecting chemical/biological agents, the impact on base operations could
  be catastrophic. Alternatively, the commander may lose confidence in the detector and thus,
  the system becomes useless for its intended purpose. These examples represent some
  potential operational impacts that suitability issues may pose for a given system. It is the
  responsibility of the operational test community to discover these suitability shortfalls before
  the system is fielded in order to reduce the warfighter’s risk in purchasing a system that
  might be extremely expensive in the long run.
  1.3.5. In addition to increasing life-cycle cost, manpower, facilities and workload, suitability
  issues could also have a significant impact on mission capability. For example, during
  OT&E, reliability, availability and maintainability data will be gathered and analyzed. If a
  system has a large mean repair time, but also has a large mean time between critical failures,
 8                                                 AFOTECPAM 99-104 24 SEPTEMBER 2013


     this might not impact the system’s mission capability. However, if the system experiences a
     critical failure often and has a lengthy repair time, this could result in low availability.
     Hence, even though the system might be extremely effective when available, it would not be
     mission capable because it is rarely available to support the warfighter.
     1.3.6. OT&E also provides an opportunity to test the system with other systems to identify
     any problems with compatibility and interoperability. Addressing these issues early in the
     program’s acquisition lifecycle has a much lower cost impact as opposed to making
     accommodations after the system is fielded. Regardless of the system under test, it is
     essential to determine whether it will work with the other systems in its intended
     environment. Suitability analysis in OT&E provides the warfighter with a measure to gauge
     the impact of incorporating a new system into the battlespace environment. Again, if the
     system is highly effective but not interoperable with the other support/supported systems, it
     might not be mission capable and worth fielding.
     1.3.7. Another area in which suitability analysis pays dividends is identifying any natural
     environmental hazards or impacts the new system might cause. For example, if the
     combination of four new aircraft engines produces too much noise during OT&E, the refitted
     aircraft might not be able to fly or land in an area that has restrictions against high levels of
     noise. If this impact is not identified before the engine is fielded on the fleet, there may be a
     considerable amount of political fallout from this acquisition decision due to new restrictions
     on the aircraft’s available basing and operating locations. A proper OT&E suitability study
     is invaluable to identify such environmental impacts.
     1.3.8. Suitability issues can be very transparent without a properly planned and well-
     orchestrated OT&E for a system. Developmental Test and Evaluation (DT&E) does not
     provide the full compilation of data for a system under test that the warfighter needs to make
     an informed fielding decision. Although DT&E does not satisfy the requirements for a
     fielding decision, it is important for AFOTEC to be involved with DT&E. By integrating
     DT&E data points into OT&E events, AFOTEC can show the overall growth of the system
     prior to OT&E and the developer can find operationally relevant problems earlier. In
     addition, data from DT&E becomes available and useable for OT&E reporting. Besides
     using the OT&E results to make a fielding decision, the warfighter can use these results to
     help employ a system to accomplish the mission. The operating command can use these
     results to improve the system’s concept of operations (CONOPS), logistics plan,
     maintenance plan, system training for both users and maintainers, as well as determine the
     scope of the system’s footprint and life-cycle cost. A system’s overall mission capability is
     determined by its effectiveness and suitability. Thus, it is very important to properly scope
     and plan for suitability analysis early in the OT&E process, and then carefully conduct the
     suitability analysis during OT&E in order to correctly resolve the overall mission capability
     of a given system.
1.4. Supporting RAM Documentation.
     1.4.1. The August 2005 DoD Guide for Achieving Reliability, Availability and
     Maintainability (RAM) is an informative document for program managers and developers to
     plan for and design RAM into weapon systems early in a program. Although focused on
     understanding the user need, RAM design/redesign and other system engineering processes,
     many of the lifecycle sustainment philosophies may be carried over into the weapon system
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                            9


   operational demonstrations—operational test and evaluation. The guide supports the
   fundamental acquisition management framework principles as documented in the current
   DoD 5000 series. Sections of the DoD Guide for Achieving RAM include information
   relevant to operational test and evaluation. This information has been incorporated into this
   pamphlet. The DoD Guide for Achieving Reliability, Availability and Maintainability, can be
   found        at       <https://acc.dau.mil/adl/en-US/142103/file/27585/DoD-RAMGuide-
   April06%5B1%5D.pdf>.
   1.4.2. The focus of the June 2009 Department of Defense Reliability, Availability,
   Maintainability and Cost Rationale Report Manual (RAM-C manual) is to assist the combat
   developers and program managers integrate top-level sustainment requirements early in the
   requirement generation phase to ensure weapon systems are sustainable and affordable
   throughout its life cycle. This manual also provides, “guidance in how to develop and
   document realistic sustainment KPP/KSA requirements and related supporting rationale,”
   “guidance so the acquisition community understands how the requirements must be
   measured and tested throughout the life cycle,” and “describes processes for DoD to interface
   with Services and programs when developing the sustainment requirements.” As with the
   previous guide, portions of the processes outlined in the RAM-C manual may be carried over
   into the test and evaluation of a weapon system—ultimately achieving satisfactory RAM
   levels. The sections discussing the Life Cycle sustainment KPP and related KSA metrics are
   worth reading to explain their relevance to operational test and evaluation and AFOTEC’s
   position on OT&E of these metrics. The Department of Defense Reliability, Availability,
   Maintainability and Cost Rationale Report Manual can be found at the following link:
   <https://acc.dau.mil/adl/en-US/293762/file/44776/RAM-
   C%20Manual%20Jun%2009.pdf>.
1.5. Related Issues The Chairman of the Joint Chiefs of Staff Instruction (CJCSI 6212.01F)
describes the Net Ready KPP (NR KPP). DOT&E has defined two special interest items (SIIs)
applicable to RAM in systems (especially software-intensive systems): compatibility and
interoperability on the Global Information Grid (GIG) and Information Assurance (IA).
Compatibility and interoperability discussion is included in Chapter 5. At the time of this
revision, the guidance governing IA is evolving (Currently the AFOTEC Operational Testing
(OT) of Information Assurance (IA) Guide, 1 June 2011). A discussion of IA is not included in
this pamphlet in an effort to maintain this document’s currency. A-3I maintains and disseminates
guidance, and should be coordinated with to provide adequate IA testing of systems.
 10                                                 AFOTECPAM 99-104 24 SEPTEMBER 2013


                                              Chapter 2

             RELIABILITY, AVAILABILITY AND MAINTAINABILITY (RAM)

2.1. Introduction. Evaluating a system’s RAM plays a key role in determining whether the
system can be placed satisfactorily in field use. This chapter provides test teams with details
concerning RAM concepts and definitions. RAM test procedures are in the Joint Reliability and
Maintainability Evaluation Team (JRMET) charter for each system under test (SUT).
2.2. Repairable and Non-repairable Systems in the Context of RAM Planning and
Analysis.
   2.2.1. For the purposes of RAM test planning and analysis, a system under test is either
   repairable or non-repairable. Repairable systems are restorable to a functioning status
   whenever a failure occurs. Such repair is corrective maintenance. In addition to this
   corrective maintenance, these systems also receive preventive maintenance (PM). The
   classic example of a repairable system is the propulsion system on an aircraft. Non-
   repairable systems are those that are not repaired once they fail; usually the system is simply
   discarded upon failure. While such systems receive no corrective maintenance, they may or
   may not receive preventive maintenance. Examples of non-repairable sub-systems that
   receive no preventive maintenance are an aircraft’s tires, requiring disposal whenever worn
   out.
   2.2.2. The distinction between repairable and non-repairable systems is important when
   selecting which measures of RAM are appropriate for the system under test. These measures
   in turn drive how total equipment time is broken down and what data the test team needs to
   collect during testing.
      2.2.3. RAM analysis entails characterizing the nature of random variables and making
      inferences regarding expected performance in operational settings based on observations
      made during testing. The random variables with which we deal in RAM applications
      typically involve chronological time: the waiting time until an occurrence of an event, the
      time between the successive occurrences of events, or the time to complete some task are
      examples. The events of interest in RAM applications are typically failures of some sort
      (failures, critical failures (CF), or operational mission failures (OMF)), other events that
      “down” a system, or perhaps the delivery of parts needed to complete a repair job. A task
      duration of interest is the time to restore function to a downed system. This task has several
      subtasks such as diagnosing the problem, obtaining the parts to fix it, actually fixing it, and
      accruing administrative and logistics delays (see Figure 2.1). Depending on the RAM metric,
      subtask time duration may also be of interest. The actual times measured depend on the RAM
      requirements set forth by the user.
2.3. Dissecting Total Equipment Time.
      2.3.1. A key objective for test teams to remember when planning tests for either repairable
      or non-repairable systems is that, in order to conduct a proper RAM analysis after the test,
      they need to be able to account for and break down all equipment time during the test period.
      At the simplest level, it is necessary that test teams collect the proper information during test
      to conclude, after the fact, whether the SUT was “up” or “down” at any given time during the
      test period. Knowing uptime and downtime will enable the computation of one of the more
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                   11


   basic measures of readiness, namely operational availability (Ao). Ao is simply a function of
   the total amount of uptime and the total amount of downtime. To compute RAM measures,
   total uptime and total downtime must each be further divided into several constituent parts.
   Figure 2.1. illustrates how total equipment time is broken into the parts needed to compute
   the various measures of RAM.

                                                                                               Off
                                  Total Time
                                                                                              Time

             Uptime (UT)                               Downtime (DT)
                                                                        Admin/Log
                                            Maintenance                 Delay Time
   Operating Time       Standby Time         Time (MT)                   (ALDT)
       (OT)                 (ST)
                                          Corrective   Preventive   Corrective   Preventive
                                          MT (CMT)     MT (PMT)      ALDT          ALDT




Figure 2.1. Linear Breakdown of Total Equipment Time
   2.3.2. Uptime (UT), for instance, consists of Standby Time (ST)and Operating Time (OT).
   Downtime consists of active maintenance time and administrative/logistical delay time.
   Active maintenance time is subdivided into Corrective Maintenance Time (CMT) and
   Preventive Maintenance Time (PMT).           If the system is non-repairable, corrective
   maintenance falls out of the breakdown and downtime includes only PMT and preventive
   Administrative/Logistic Delay Time (ALDT).
   2.3.3. Figure 2.2. is an alternate presentation of the concept illustrated in Figure 2.1.,
   namely, how total equipment time can be broken down into components. Total time is first
   split into active time and inactive time. Inactive time is not split any further. Active time is
   composed of uptime and downtime. Uptime and downtime are divided into multiple
   categories, and so on. This breakdown is obviously more detailed than the corresponding
   graphic at Figure 2.1. The terminology differs, and accommodates certain possibilities that
   the breakdown in Figure 2.1. does not allow. For example, there is a possibility of doing
   preventive and corrective maintenance on the system during uptime and/or during mission
   time.
 12                                             AFOTECPAM 99-104 24 SEPTEMBER 2013




Figure 2.2. Dendritic Breakdown of Total Equipment Time
   2.3.4. Presenting these two equipment time breakdowns highlights that the proper method to
   use depends on the particular system under test. The breakdowns given in Figures 2.1. and
   2.2. likely would require tailoring to fit other systems under test. The tailoring would depend
   on the specific RAM measures being used (reflecting the complexity of the system and
   whether the system is repairable or not).
   2.3.5. In any case, it is important to clearly define each segment (if using a linear breakdown
   as in Figure 2.1.) or each node (if using a dendritic breakdown in Figure 2.2.) such that they
   are mutually exclusive. The question, “What constitutes active time, inactive time, mission
   time, standby time, and off time?” should be asked to aid these definitions.
2.4. Analytical Products.
   2.4.1. Figure 2.3. depicts a notional “state graph” of a fictional computer system under test.
   Assume that this computer is required to operate continuously, though it may be in a “hot
   standby” mode at times (not performing any of its active functions while turned on). The
   horizontal axis represents time and the vertical axis represents a binary state variable X(t)
   that takes value 1 when the computer is functioning or is assumed functional and takes the
   value 0 when it is not functional (values other than 0 or 1 are not defined). At time t0 the
   computer is booted up and the observation period begins. Initially, the computer is on but
   standing by for a short period of time, which is counted as standby time (ST11), followed by a
   period of operation, counted as operating time (OT11). The computer is considered
   functional (uptime, UT1) during ST11 and OT11, and thus the total uptime at this point in the
   observational period is ST11+OT11 and total downtime is 0. At time t1, the computer
   “crashes” and begins the first period of downtime (DT1) in this test. Maintenance personnel
   immediately begin diagnosing the problem, which is counted as corrective maintenance time
   (CMT11). Parts are required from supply to continue corrective maintenance. The time spent
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                               13


   ordering the parts and waiting for their delivery is counted as administrative and logistics
   delay time (ALDT11). Once they receive the parts, they resume corrective maintenance
   (CMT12) and at t2, the computer is once again up and in standby mode (ST21). Total
   downtime at this point is CMT11+ALDT11+CMT12 (or t2 - t1). This graph would continue
   charting the state of the computer until the test period ends.
                  UT1                                                      UT2
         ST11           OT11                                       ST21   OT21     ST22

  1


 X(t)                                        DT1

                                     CMT11   ALDT11   CMT12

  0
        t0                      t1                            t2
                                                                                   Time, t

Figure 2.3. Notional State Graph
   2.4.2. A tabular form of similar data is in Table 2.1.

Table 2.1. Example Failure and Repair Data
             Downing     Type    Clock         Inter-         Time (hours)       Clock Time
             Event               Time          occurrence     to Restore         (hours)
             (DE)                (hours) of    Time           Function           Back in
                                 DE            (hours)                           Service
                                                              (C/P/A*)
             1           OMF     170           170            10/0/1             181
             2           OMF     242           61             12/0/1             255
             3           OMF     293           38             12/0/1             306
             4           PM      337           31             0/5/0              342
* C/P/A = Corrective/Preventive/Administrative
2.5. Probability Distributions.
   2.5.1. The exponential distribution tends to be overused to describe a failure distribution,
   which may misrepresent the data. A primary assumption of the exponential distribution is a
   constant failure rate; that is, a used item is assumed to be as good as a new item and an item
   is just as likely to fail at the beginning of a test as at the end of a test. Usually, a failure
   distribution has either an increasing failure rate (system fails more often over time) or a
   decreasing failure rate (system fails less often over time). Since reliability is the probability
   that a system will perform satisfactorily for a given time (mission length), it is inappropriate
   to use the exponential distribution without either having relevant a priori data from previous
    14                                                 AFOTECPAM 99-104 24 SEPTEMBER 2013


      testing (e.g. utilizing Developmental Test data or data from a similar system) or waiting to
      model the system failure distribution after test.
      2.5.2. The exponential distribution can be used when testing a system that operates as a
      function of a continuous measure. A continuous measure is marked by an uninterrupted
      response variable (e.g. time, distance, temperature, angle, etc.) and is therefore better at
      describing system performance than a discrete measure. In such tests, the times at which
      failures occur and durations of operating time without failure are important to collect. The
      collected data aids the determination of the failure rate (constant, increasing or decreasing).
      Weibull analysis determines failure rate characteristics. When a system with a constant
      failure rate is also repairable, the Poisson distribution is used to analyze the system’s
      reliability.
      2.5.3. In contrast to a continuous measure, a discrete measure takes on only countable values
      (0, 1, 2, etc.). An example of a discrete measure is the ratio of systems that fail to the total
      number of systems tested during a specified period. A sampling plan is required to define the
      interval of testing. The binomial distribution is appropriate to analyze test articles that result
      in a distinguishable success or failure.
         2.5.4. Many measures of suitability (MOS) are discrete. Due to limitations during OT&E,
         only a certain number of systems, units, or assets can be evaluated under finite temporal,
         environmental, and spatial constraints. For example, a mission capable rate during test may
         be capable reported, but it is not operationally realistic since the utilization rate was less than
         that of the operational fleet. Modeling and simulation may support calculation of the MOSs,
         provided the data required for the model is collected. Such data may include the times at
         which failures occur.
2.6. Reliability.
      2.6.1. Reliability is defined in a variety of ways. The Defense Acquisition Guidebook
      (DAG) defines reliability as, “The ability of a system to perform as designed in an
      operational environment over time without failure.” Air Force Pamphlet (AFPAM) 63-128,
      Guide to Acquisition and Sustainment Life Cycle Management, contains a definition for
      weapon system reliability: “The probability that a system will perform satisfactorily for a
      given time when used under specified operating conditions.” Note that the first definition
      refers to an ability of an item, namely its ability to perform a mission, rather than some
      measure of that ability, as in the second definition. Measures of this type are discussed in
      detail in the AFOTEC Measures Primer, available on the AFOTEC Intranet website. The
      distinction between an ability and a measure of an ability is fundamental to the discussion
      that follows in Section 2.7.
     2.6.2. In general, reliability is a term used to describe quantitatively how failure-free a
     system is likely to be during a given period of operation. The International Organization for
     Standardization (ISO) defines reliability as the ability of an item to perform a required
     function, under given environmental and operational conditions, and for a stated period of
     time (ISO 4802). Rausand and Hoyland 1 further interpret the ISO definition, writing that the
     item to which the ISO definition refers may be any component, subsystem or system that can

1
 Rausand, M., and A. Høyland. 2004. System Reliability Theory: Models, Statistical Methods, and Applications,
second edition. John Wiley and Sons, Hoboken, New Jersey.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                               15


   be considered as an entity and that the function in the definition may be a single function or a
   combination of functions that is necessary to provide a required service.
   2.6.3. The meanings of the phrases “given environmental and operational conditions” and
   “stated period of time” are important to the understanding of reliability. The phrase “given
   environmental and operational conditions” refers to the complete definition of the scenario in
   which the system will operate. For a cargo airframe, these conditions include climatic
   conditions, threats to the system and cargo type (e.g. personnel, equipment, supplies) during
   a selected mission profile. These conditions should reflect operational usage. The phrase
   “stated period of time” refers to the length of the mission described in a mission profile. The
   specification of the length of the interval need not be (and in many cases will not be) a simple
   specification of clock time. For example, a bomber aircraft mission profile will define an
   interval containing X bombs dropped, Y number of sorties generated, and Z hours of
   possessed time. Note that only the flight time in this specification involves clock time. For a
   simpler system, such as a dumb bomb, the interval may include a single event—detonation.
2.7. Aspects of Reliability.
   2.7.1. The basic concept of reliability is that the system performs satisfactorily, where
   “satisfactorily” implies a lack of broad undesirable events and subsequent impacts (risk
   analysis). This general concept has two aspects: mission reliability and logistics reliability.
   Mission reliability refers to the concept of not having undesirable events (i.e. failures) during
   mission time and the immediate impact to that mission. In that sense, mission reliability is
   actually best considered in the context of effectiveness rather than suitability. Logistics
   reliability refers to undesirable events (including potential or pending failures) with the
   impact being a burden on the support system. Note that a particular undesirable event or
   failure could contribute to both mission reliability measures as well as logistics reliability
   measures.
   2.7.2. Mission reliability relates to system effectiveness; logistics reliability relates to the
   burden of owning and operating the system. Measures of mission reliability address only
   those incidents that affect mission accomplishment. Measures of logistics-related reliability
   address all incidents that require a response from the logistics system.
   2.7.3. Mission Reliability.
       2.7.3.1. Measures of mission reliability quantify the probability that a system will
       perform mission essential functions for a period of time under the conditions stated in the
       mission profile. Mission reliability for a single-shot system, i.e., a missile, would not
       include a time period constraint. A system with high mission reliability has a high
       probability of successfully completing the defined mission.
       2.7.3.2. Measures of mission reliability address only those incidents that affect mission
       accomplishment. A mission reliability analysis must include the definition of mission
       essential functions. For example, the mission essential functions for a tanker airframe
       might be to take off, communicate with command and other aircraft, refuel another
       aircraft and land. Requirements that build more specific measures could specify
       minimum speed, maximum time to refuel and communication range.
   2.7.4. Logistics (Maintenance/Supply) Related Reliability. Logistics related reliability
   measures, as indicated above, must be selected so that they account for or address all
    16                                                  AFOTECPAM 99-104 24 SEPTEMBER 2013


      incidents that require a response from the logistics system. Logistics related reliability
      measures may be further subdivided into maintenance related reliability and supply related
      reliability. These measures respectively represent the probability that no corrective
      maintenance or the probability that no unscheduled supply demand will occur following the
      completion of a specific mission profile. Note that the mathematical models used to evaluate
      mission and logistics reliability for the same system may be entirely different.
     2.7.5. AFPAM 63-128 defines both reliability and mission reliability as probabilities (“The
     probability of…”) while it defines logistics reliability as an ability (“The ability of …”).
     Observe that in the former two cases, the definitions refer to a measure of ability (probability
     of…), while in the latter case, the definition refers to the ability itself. Additionally,
     measures of mission reliability are given in AFPAM 63-128, one of which is also called
     mission reliability (Rm)—implying that mission reliability is both an ability as well as a
     measure of that ability. In contrast, logistics reliability is an ability with specific measures of
     that ability defined later in the document.
     2.7.6. For AFOTEC purposes, the definitions of reliability, mission reliability, and logistics
     reliability found in AFPAM 63-128 will be viewed as abilities of the system under test and,
     therefore, will be treated as operational capabilities (OC) of a system. An OC is a system
     attribute or grouping of attributes that users and subject matter experts have identified as
     being crucial to the achievement of critical mission elements and/or operational objectives of
     significant value to the warfighter. With this perspective, the definitions of reliability,
     mission reliability, and logistics reliability found in AFPAM 63-128 will be rewritten in
     terms of OCs, as:
         2.7.6.1. Reliability—The capability of a system to perform satisfactorily for a given
         time when used under specified operating conditions. This suitability OC is frequently
         measured by the time 2 between successive occurrences of unsatisfactory performance.
         Exactly what constitutes unsatisfactory performance for a particular system under test
         must be defined in advance of testing and usually involves various categories of failures
         (failures, critical failures, operational mission failures), each of which must be clearly
         defined.
         2.7.6.2. Mission Reliability—The capability of a system to perform its required function
         for a stated mission duration or for a specified time into the mission. Typical measures of
         this operational capability are mission reliability (note that the term mission reliability
         refers to the OC itself and also to one of the measures of that OC), weapon system
         reliability, and break rate. Note that the OC mission reliability relates to system
         effectiveness rather than suitability.
         2.7.6.3. Logistics Reliability—The capability of a system to perform failure free, under
         specified operating conditions and time without demand on the support system. Typical
         measures of this suitability OC include time (or cycles, miles, etc.) between maintenance,
         demand, or removals.
      2.7.7. Reliability Growth. Reliability growth is under the purview of the program office,
      and not specifically AFOTEC’s responsibility. With that be said, AFOTEC OT&E designs

2
  Throughout this document, time does not necessarily mean clock time. Here time includes indirect measures of
time like miles, rotations, cycles, etc.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                17


   and methods account for reliability growth strategies in operational assessments and
   evaluations. The reliability growth strategy is typically depicted by a reliability graph in the
   form of an idealized reliability growth curve. However, visual depictions of the growth
   strategies will vary widely depending on the selected units of reliability (e.g., MTBF or MR)
   and units of test (e.g., operating/test hours, operating months). Using reliability growth in
   reporting requires careful consideration to avoid inaccurately depicting a system. A-2/9 can
   provide guidance and assistance in developing a strategy that addresses reliability growth.
2.8. Availability. Operational availability is the probability that a system will be ready for
operational use (i.e., any specified purpose) when required. Availability is the desire of
operations and the goal of maintenance. Availability is dependent on reliability, maintainability
and logistics supportability. A goal of the availability assessment is to determine if the system
can meet the user’s availability requirements stated in a capability document (initial capabilities
document (ICD), capability development document (CDD), capability production document
(CPD)) or another requirements document.
   2.8.1. Availability Concepts.
       2.8.1.1. Availability translates the reliability, maintainability and logistics supportability
       characteristics of the system into a measure of interest to the user. It is based on the
       question, “Is the equipment in working condition when it is needed?” The evaluator
       should compare the availability of the system with mission requirements contained in
       existing requirements documents.
       2.8.1.2. Metrics of availability are generally probabilities; i.e., either the probability a
       system is capable of performing its mission or the probability a fleet is capable of
       performing its mission. During initial operational test and evaluation (IOT&E), little or
       no meaningful availability data may be available due to limited amounts of time and
       assets. Under these circumstances, the availability assessment may require extensive
       modeling and simulation. There are some cases where data from integrated test and
       evaluation (IT&E) may help evaluate availability.
2.9. Maintainability. Maintainability is the ability of an item to be retained in or restored to a
specified condition when personnel having specified skills, using prescribed procedures and
resources at each prescribed level of maintenance and repair perform maintenance. A commonly
used working definition states that maintainability is the inherent characteristic of a design that
determines the type and amount of maintenance required to retain that design in, or restore it to,
a specified condition. In this definition, “retained in” refers to preventive maintenance, while
“restored to” addresses corrective maintenance.
   2.9.1. Maintainability Concepts.
       2.9.1.1. Maintainability, reliability, and other logistics support drivers are major system
       characteristics that impact availability. While maintainability is important as a factor of
       availability, it also merits substantial consideration as an individual system characteristic.
       Maintainability is a factor of the design process and an inherent design characteristic that
       is quantitative and qualitative in nature and, therefore, lends itself to specification,
       demonstration and trade-off analysis.
       2.9.1.2. Maintenance refers to all actions required to retain an item in, or restore it to, a
       specified condition. This includes servicing, diagnosis, removal, repair, installation,
18                                           AFOTECPAM 99-104 24 SEPTEMBER 2013


     modification, modernization, overhaul, rebuild, test, reclamation, inspection and
     condition determination.
     2.9.1.3. Assessing maintainability, such as maintenance time, direct maintenance work
     hours and system downtime are collected. Data is then reported as means, either divided
     by some operational frequency such as flying hours, sorties or maintenance
     events/actions, or categorized by subsystems to highlight areas needing the most
     attention.
     2.9.1.4. During an OT&E, both quantitative and qualitative aspects of maintainability are
     addressed.
        2.9.1.4.1. Quantitative statistics for maintainability evaluation can be expressed as
        maintenance downtime per sortie, maintenance ratio (e.g., maintenance work hours
        per flying hour), total required work force (e.g., maintenance personnel (MP) per
        operational unit), time to restore a system to operational status (mean downtime), etc.
        2.9.1.4.2. Qualitative aspects of maintainability include accessibility, serviceability,
        ease or difficulty of maintenance, safety, and human factors associated with
        maintenance actions. These factors affect the quantity, skill levels and specialty
        codes of MP and the test equipment required to maintain the system. Qualitative
        evaluations of maintainability are usually done by experienced maintenance
        technicians using subjective judgment and are supported by quantitative
        maintainability measures.
 2.9.2. Maintenance Demonstrations. Testers may conduct maintenance demonstrations
 (M-demos) during DT&E and OT&E. During DT&E, the developmental test team conducts
 M-demos to demonstrate compliance with specifications. During OT&E, the test team may
 conduct M-demos for data gathering if done in the operational environment. Considerations
 concerning the use of M-demos follow.
     2.9.2.1. M-demos performed during OT&E are “staged” maintenance events done in an
     operational environment to obtain quantitative maintainability information not otherwise
     available during OT&E. These are sometimes described as ease-of-maintenance
     demonstrations (removal and replacement of components, or performance of tasks).
     Other potential M-demos can be performed by intentionally inserting faulty components
     into the system to assess troubleshooting and repair capability. This is especially
     important when testing highly reliable systems where test exposure is small compared
     with the expected time between failures. Demonstrations may be used to quantify
     maintenance times for tasks that will not be required during the course of the test.
     2.9.2.2. Many times the contribution of M-demos in an OT&E is limited because they
     account for the operational frequency and mode of failures. Additionally, M-demos do
     not give data on induced and no-defect failures. When employing M-demos in OT&E:
        2.9.2.2.1. Clearly define and scope demo(s) in the OT&E test plan.
        2.9.2.2.2. Coordinate with the using command(s).
        2.9.2.2.3. Ensure M-demos are planned in detail within the Data Management and
        Analysis Plan (DMAP) so as to not interfere with planned operational missions. Data
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              19


          from actual maintenance events that occur during the operational test can be used in
          satisfying requirements for M-demos.
          2.9.2.2.4. Conduct M-demos in an environment that simulates as closely as possible
          the operational and maintenance environment planned for the item (e.g., blue-suit
          maintenance with no contractor involvement, cold weather maintenance with
          operators wearing cold weather gear, high ops tempo environments, etc.)
          2.9.2.2.5. Representative working conditions, tools, support equipment, spares,
          facilities and technical publications that would be required during operational service.
          2.9.2.2.6. Videotape for documentation and analysis.
      2.9.2.3. Some data collected during M-demos may be included in RAM calculations,
      other data may not. For example, the repair times from an M-demo in which a failure is
      induced or inserted may be included in the calculation of mean repair time. However,
      including the maintenance event in the calculation of mean time between maintenance
      (MTBM) is inappropriate.
   2.9.3. Integrated Diagnostics. Integrated Diagnostics (ID) (or simply diagnostics) are
   “monitoring/recording devices and software…providing the capability for fault detection and
   isolation, (including false alarm mitigation) to signal the need for maintenance. [They]
   should include user friendly features to convey system status and the effect on mission
   capabilities to the operator and maintainer” (DAG).
      2.9.3.1. The purpose of integrated diagnostics is to provide a cost-effective capability to
      accurately detect and isolate all faults known or expected to occur in weapons systems.
      In wartime, this becomes significant in that timely identification of critical failures leads
      to rapid repair with minimal troubleshooting to support combat turn-around times.
      2.9.3.2. The term “diagnostics” is often a general term to cover all means of determining
      that a system fault has occurred, and the means to determine where the fault is and to
      isolate it to a repairable or replaceable portion of the system. There are many other terms
      relating to this area, including built-in test (BIT) with built-in test equipment (BITE),
      built-in test and fault isolation test (BIT/FIT), and automatic test equipment (ATE).
      2.9.3.3. The key to integrated diagnostics is the successful consideration and integration
      of the functions of detection, isolation, verification, recovery, recording and reporting, in
      a comprehensive and cohesive fashion, with the operator and with support functions that
      may be automatically, semi-automatically and/or manually controlled.
2.10. Other RAM Guidance:
   2.10.1. The Chairman of the Joint Chiefs of Staff Instruction (CJCSI 3170.01H) directs (and
   the Manual for the Operation of the Joint Capabilities Integration and Development System
   further defines) a Sustainment KPP and two supporting KSAs for weapon system acquisition.
   The four metrics defined include: Materiel Availability (KPP); Materiel Reliability (KSA);
   Operation and Support Cost (KSA); and Mean Down Time. AFOTEC currently identifies
   measures and collects data for both Materiel Reliability and Mean Down Time. Although
   AFOTEC recognizes the value of the metrics to improve weapon system acquisition via
   increased lifecycle sustainment, testing and evaluating of Materiel Availability and Operation
   and Support Cost is beyond AFOTEC’s scope and capability. The scope of these metrics are
20                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


 fleet wide and affect the overall life cycle as opposed to individual units and short, near-term
 time periods typically seen in OT. Because of this constraint, AFOTEC does not have access
 to all necessary information to test and evaluate these metrics. AFOTEC does not directly
 test and evaluate the defined metrics (Operation and Support Cost, Materiel Availability),
 however, there may be related indirect metrics (e.g., Operational Availability (Ao)) that
 contribute to these lifecycle metrics.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                21


                                            Chapter 3

                              LOGISTICS SUPPORTABILITY

3.1. Introduction. Logistics supportability is the degree to which the planned logistics support
(test measurement and diagnostic equipment, spare and repair parts, technical data, support
facilities, transportation requirements, training, manpower, and computer resources) allows
meeting system availability and wartime usage requirements. Planning for a logistics
supportability evaluation is discussed in this chapter. The program office and major command
(MAJCOM) use the product support elements in AFPAM 63-128 to design and develop the
system. AFOTEC bases its suitability test on these documented supportability items. Table 3.1.
shows the relationship between the elements of logistics support and operational suitability.

Table 3.1. Logistics Support and Operational Suitability Linkage
   Logistics Support Elements                         Operational Suitability
   (AFPAM 63-128)                                     (AFOTECPAM 99-104)
   Sustaining/Systems Engineering                     Safety
                                                      Human Factors
                                                      Compatibility
                                                      Natural Environmental Effects and Impacts
   Design Interface                                   Availability
                                                      Compatibility
                                                      Interoperability
                                                      Reliability
                                                      Maintainability
                                                      Human Factors
   Supply Support                                     Wartime Usage Rates
                                                      Logistics Supportability
   Maintenance Planning and Management                Maintainability

   Support Equipment/Automatic Test Systems           Logistics Supportability

   Facilities                                         Logistics Supportability
   Packaging, Handling, Storage, and Transportation   Transportability
                                                      Logistics Supportability
   Technical Data Management/ Technical Orders        Documentation
                                                      Logistics Supportability
   Manpower and Personnel                             Manpower Supportability
                                                      Logistics Supportability
   Training                                           Training
                                                      Logistics Supportability
   Computer Resources                                 Logistics Supportability

   Protection of Critical Program Info/Anti-Tamper    Availability
                                                      Reliability
 22                                               AFOTECPAM 99-104 24 SEPTEMBER 2013


3.2. Definitions. The terms that follow are suitability areas that we must evaluate to assess
mission capability.
   3.2.1. Sustaining/Systems Engineering—The technical effort required to support an in-
   service system in its operational environment to ensure continued operation and maintenance
   of the system with managed risk.
   3.2.2. Design Interface—Considers what is needed to integrate the logistics-related
   readiness, combat capability, systems commonality, and supportability design parameters
   into system and equipment design.
   3.2.3. Supply Support—The process conducted to determine, acquire, catalog, receive,
   store, transfer, issue, and dispose of secondary items necessary for the support of end items
   and support items. The process includes initial support (provisioning) and follow-on
   requirements (routine replenishment).
   3.2.4. Maintenance Planning and Management—Documents the process conducted to
   develop and establish maintenance concepts and requirements for the life-cycle. The process
   should consider all elements of maintenance support necessary to keep systems and
   equipment ready to perform assigned missions. This includes all levels of maintenance and
   implementation of those levels; includes any partnering, organic, and contract support.
   3.2.5. Support Equipment/Automatic Test Systems—All equipment (mobile and fixed)
   required to support the operation and maintenance of a materiel system. This includes
   associated multi-use end items, ground handling and maintenance equipment, tools,
   meteorology and calibration equipment, test equipment, and automatic test equipment. It
   includes the acquisition of logistics support for the support and test equipment itself.
   3.2.6. Facilities—The permanent, semi-permanent or temporary real property assets
   required to support the materiel system, including conducting studies to define types of
   facilities or facility improvements, location, space needs, utilities, environmental
   requirements, real estate requirements, and equipment.
   3.2.7. Packaging, Handling, Storage, and Transportation—The resources, processes,
   procedures, design considerations, and methods to ensure that all system, equipment, and
   support items are preserved, packaged, handled, and transported properly, including
   environmental considerations, equipment preservation requirements for short- and long-term
   storage, and transportability.
   3.2.8. Technical Data Management/Technical Orders—Recorded information regardless
   of form or character of a scientific or technical nature. This includes data rights, data
   management strategy, engineering data, drawings and associated documents, specifications,
   and the scientific or technical information (recorded in any form or medium) necessary to
   operate and/or maintain the defense system.
      3.2.9. Manpower and Personnel—The identification and acquisition of military and
      civilian personnel with skills and grades required to operate and support a materiel system
      over its lifetime at peacetime and wartime rates.
      3.2.10. Training—The processes, procedures, curricula, techniques, training devices,
      simulators, other equipment, and software necessary to train civilian and active duty/reserve
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             23


   duty personnel to operate and support/maintain the defense system; includes acquisition,
   installation, operation, and support of training equipment/devices.
   3.2.11. Computer Resources—The facilities, hardware, software, documentation,
   manpower and personnel needed to operate and support stand alone and embedded computer
   systems.
   3.2.12. Protection of Critical Program Info/Anti-Tamper—The efforts and provisions
   required to protect sensitive information identified in the Program Protection Plan during
   operations and sustainment.
3.3. Logistics Support Concepts.
   3.3.1. Product Support. Product support focuses on the entire life cycle of support needed
   to acquire and sustain a weapon system. The concept is implemented by the Department of
   Defense (DoD) Performance-based Logistics (PBL) strategy which seeks to link product
   support to weapon system performance by optimizing system availability and minimizing
   cost and the logistics footprint. PBL applies to both retail (base level) logistics operations
   and wholesale (depot) logistics operations. According to the AFI, PBL shall be implemented
   for new acquisition category (ACAT) I and II systems. In addition, the AFI mandates
   development of plans and processes such as the Life Cycle Management Plan (LCMP).
   3.3.2. Combat Support. Agile combat support (ACS) includes actions taken to create,
   effectively deploy and sustain US military power anywhere—at any initiative, speed and
   tempo. ACS is technologically superior, robust, flexible, and fully integrated with
   operations. ACS capabilities include provisions for and protection of air and space
   personnel, assets and capabilities throughout the full range of military operations. ACS is the
   foundation for global air and space power engagement and the linchpin that ties together Air
   Force distinctive capabilities. It is the foundational and cross cutting US Air Force system of
   support that enables Air Force operational concepts and the capabilities that distinguish air
   and space power—speed, flexibility, and global perspective.
   3.3.3. Contractor Logistics Support (CLS)/Interim Contractor Support (ICS). CLS/ICS
   includes the collection of logistics support activities provided under contract to a using
   command. CLS supports a system, subsystem, modification or equipment throughout the
   term of the contract. ICS is a temporary support function to provide initial logistics support
   until an organic capability is in place. CLS/ICS may include software maintenance; on- and
   off-equipment maintenance; maintenance management; maintenance planning; supply
   support; transportation, packaging, and handling; storage; and facility maintenance. NOTE:
   Air Force policy restricts using the developmental contractor during OT&E. They may
   participate only to the extent that is planned for them to be involved in the operation,
   maintenance, and other support of the system when the system is deployed in combat.
   3.3.4. Integrated Logistics Support (ILS). ILS planning provides the basis to assess the
   planned logistics support. This involves investigating the various elements of ILS being
   planned for the system. The ability to assess ILS elements depends on the test environment.
   For example during IOT&E, support equipment and technical data may not be available or
   representative of the operational items. Further, the contractor typically provides supply
   support. Review of the LCMP and any supporting plans are critical to understand the
   planned logistics support for the system.
 24                                                AFOTECPAM 99-104 24 SEPTEMBER 2013


   3.3.5. Phased Logistics Support. Systems go through a transition period from production
   to full operational use, therefore there should be proper time phasing of transitioning
   different aspects of logistics support capability. Phased logistics support begins with
   establishing an initial capability at the first designated operational site and then replicating
   this capability at other operational sites until the system is totally fielded. Once fielded, the
   focus shifts to maintaining the logistics support capability throughout the system’s life cycle.
   There are three considerations for this concept to work successfully, especially during the
   startup period:
         3.3.5.1. Depending on the type of equipment, test planners should expect a greater
         number of corrective maintenance (CM) actions occurring immediately after equipment
         delivery. Early support should consider the possibility of realizing a logistics burden
         beyond what was initially planned.
         3.3.5.2. Formal training and system familiarization should occur; however, with this
         training comes increased possibility of operator or maintainer-induced failures.
         3.3.5.3. For large and complex systems, the user may be able to successfully operate the
         system at the time of activation but is unable to provide full logistics support at all levels
         of maintenance.
3.4. Other Topics Related to Logistics Support. Certain systems due to their functions and
operational role must be designed for protection from nuclear effects and/or from chemical,
biological and radiological (CBR) contamination. There are inherent logistics considerations
associated with achieving such protection. The logistical requirements of battle damage repair
are another important consideration for system effectiveness and suitability.
      3.4.1. Hardness Maintenance/Hardness Surveillance (HM/HS).
         3.4.1.1. HM/HS Concept. The Air Force expends many dollars of scarce acquisition
         resources to ensure selected systems will be usable in trans- or post-attack nuclear
         environments. These selected systems typically have a nuclear survivability requirement.
         As such, they require an HM/HS concept that should be developed and documented as
         part of the system’s LCMP. The concept should address each level of maintenance and
         highlight new skills that may be needed, HM/HS-peculiar support equipment that should
         be developed or procured, and related areas such as training, technical data, and spares.
         Specialized system, subsystem, line-replaceable unit (LRU) and component-level
         hardness testing that are planned or required should also be addressed. The focus of
         HM/HS concept may be restricted to looking at the effects on hardness critical items
         (HCIs). These are hardware items at any indenture level that are critical to nuclear
         hardness capability. Improper design, manufacture, assembly, modification, installation,
         removal or repair can degrade this capability. HCIs typically include items such as
         electromagnetic pulse gaskets, Zener diodes, surge arresters, and other specialized
         components/subcomponents selected for their nuclear hardness properties.
         3.4.1.2. HM/HS Evaluation/Assessment. Planning for an HM/HS evaluation and
         assessment includes system familiarization and documentation review, involvement in
         DT&E test planning, and OT&E test plan and data management and analysis plan
         (DMAP) preparation.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                         25


        3.4.1.2.1. Systems Familiarization and Documentation Review. One of the
        cornerstone tasks of HM/HS assessment planning. Specifically focus on identifying
        periodic maintenance inspection procedures and select those that can be performed
        during operational testing.
        3.4.1.2.2. Involvement in DT&E Test Planning. Nuclear effects testing are
        normally the responsibility of developmental testers. OTAs provide support to these
        test efforts to ensure test articles are configured and operated in an operationally
        realistic environment such that OT&E data related to the nuclear effects testing are
        sufficiently met. Close coordination with DT&E test planners is essential. OTAs
        may require independent OT&E contractor support to assist with an HM/HS
        assessment. Contractor support assistance may be beneficial due to the specialized
        technical nature of planning, testing, limitations, and requirements involving the
        nuclear enterprise. One of the deliverables often required of the OT&E contractor,
        primarily on major weapon systems, is a nuclear assessment plan (NAP). The NAP is
        a structured approach to OT&E nuclear assessment planning. Test teams should
        contact A-2/9 for AFOTEC Technical Paper 13.1, Introduction to the Operational
        Nuclear Survivability Assessment Process.
        3.4.1.2.3. OT&E Plan and DMAP. HM/HS is normally addressed as a measure of
        effectiveness (MOE) of a nuclear survivability system characteristic or under logistic
        supportability. Regardless of how the test plan is structured, the basic areas of
        assessment are usually the same. Ensure documentation of necessary resources for
        accomplishing the HM/HS evaluation in the test resource plan (TRP).
     3.4.1.3. Other HM/HS Considerations. The HM/HS evaluation should focus on the
     ability of military personnel to perform the required HM/HS procedures (during periodic
     maintenance inspection intervals) and the ability of the logistics support system to
     support HM/HS. The data for this evaluation should come from DT&E, documentation
     review and operational test experience. Also, evaluate the ability to detect hardening
     degradations.
  3.4.2. CBR Contamination.
     3.4.2.1. CBR Contamination Concept. Consider the use of CBR warfare protective
     clothing and equipment when estimating the utility of a system operated under CBR
     contamination threat conditions. Requirements documents may require the new system
     be operable and maintainable by personnel wearing cold weather clothing or the chemical
     warfare ensemble configured for a specific protective posture. Demonstration of this
     capability is usually a part of M-demos. If an M-demo is required for OT&E, there are
     two key questions that focus on developing a CBR contamination concept: 1) Can the
     SUT accomplish its assigned task under CBR contamination threat conditions?, and 2)
     Can the SUT be properly and safely operated and maintained by personnel wearing CBR
     contamination protective clothing? The latter is the focus of the suitability effort and
     involves a careful look at human factors issues, safety, maintainability and
     documentation. Proper planning and preparation of resources to implement a CBR M-
     demo becomes a critical task of the suitability personnel.
     3.4.2.2. CBR Contamination Capability. The LCMP should define a program to
     ensure CBR contamination capability is not compromised during the system life cycle.
26                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


     Careful attention to loss of configuration control, use of improper spares or repair parts,
     performance of inappropriate maintenance or repair, or hardness degradations due to
     normal operations, maintenance and environments is important. These characteristics
     apply to both the prime equipment and support equipment. Logistic evaluators should
     review the requirements document and maintenance concept with CBR contamination
     issues in mind. The four key questions to ask are:
        3.4.2.2.1. Can the SUT be properly and safely operated and maintained by personnel
        wearing CBR contamination protective clothing?
        3.4.2.2.2. Is an M-demo required to answer this question?
        3.4.2.2.3. If an M-demo is required, what is the scope of the M-demo (i.e., is there a
        need to demonstrate full capability or will the demonstration of a partial capability
        suffice)?
        3.4.2.2.4. Is the system and program documentation adequate to plan a
        comprehensive M-demo?
     3.4.2.3. Additional CBR Considerations. Focus the M-demo planning effort to capture
     CBR contamination-specific data on human factors (HF), safety and maintainability
     impacts. Determine when and where the M-demo should be done. Consider the
     following: additional training required for test team personnel performing the M-demo,
     protective clothing/ensembles in the right quantity to perform the M-demo, video
     documentation, and security considerations that may impact the M-demo. Finally, ensure
     significant aspects of the M-demo are accounted for in the Test and Evaluation Master
     Plan (TEMP).
 3.4.3. Battle-Damage Repair (BDR).
     3.4.3.1. BDR Concept. The BDR concept is based on demonstrating the
     accomplishment of BDR procedures. This can also be done using M-demos, during
     DT&E, OT&E, or through IT&E.
     3.4.3.2. BDR Procedures. The LCMP should address the acquisition of BDR
     procedures, technical orders, supplies, tools, manuals and training to ensure rapid return
     of battle damaged systems. BDR plans may also address HM/HS.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             27


                                           Chapter 4

                                       USAGE RATES

4.1. Introduction. Usage rates impact operational tasks of deploying a system, sustaining
deployed operations and maintaining employment or operational readiness. Usage rates, their
derivation, measures and methodologies are discussed in this chapter.
4.2. Definitions.
   4.2.1. Unit Type Code (UTC)—A five-character alphanumeric code associated with a
   particular type of unit. It designates a specific capability. A UTC is the basic building block
   used in force planning and the deployment of Air Expeditionary Task Forces (AETF). A
   UTC depicts a force capability with personnel and/or equipment requirements. The
   assignment of a UTC categorizes each type of organization into a class or kind of unit having
   common distinguishing characteristics. Planners use UTCs to document total manpower and
   logistics requirements needed to support the national military strategy during deliberate,
   crisis action, and rotational planning. For a complete definition and breakdown of UTCs and
   their purpose, reference AFI 10-401, Air Force Operations Planning and Execution.
   4.2.2. Utilization Rate (UTE)—A UTE is a unit of use (i.e., sorties, operating hours, etc.)
   over a defined baseline (i.e., possessed hours, time period, number of aircraft, etc). The unit
   of measure for each part of the formula is unique to the end item being measured. Aircraft
   systems, ground-based communication systems, and ground-launched missile systems are all
   measured differently. Example formulas for most applications are in AFPAM 63-128,
   Utilization Rate (UR).
   4.2.3. Mission Capability (MISCAP)—A MISCAP defines the mission a UTC is capable
   of accomplishing. It contains the following: 1) Type and amount of workload the UTC is
   capable of performing, 2) The type of base where the UTC may be employed (bare base,
   main operating base, forward operating base, or advanced operating base in accordance with
   Joint Publication (JP) 1-02, DoD Dictionary of Military and Associated Terms), 3) Other
   UTCs which are required to support the defined capability, and 4) Any other information
   pertinent to that UTC. The MISCAP is the only part of the UTC that could be classified.
   More detailed information on the MISCAP portion of the UTC is in AFI 10-401.
   4.2.4. Sortie Generation Rate (SGR)—SGR is an aircraft-unique description of a specific
   by-day generation requirement, as opposed to a UTE rate which is an average usage rate over
   a specified period. The intent of SGR is to show the capability to meet the by-day Air
   Tasking Order (ATO) requirements derived from operation plans (OPLAN).
   4.2.5. Wartime Usage Rate—A quantitative statement of the projected manner in which the
   system is to be used in its intended wartime environment. It describes the projected intensity
   at which the system is to be used in accomplishing wartime missions. The statement is
   usually formatted as a UTE or SGR. Wartime usage rate may also be referred to as a surge
   capability. In accordance with Air Combat Command Instruction (ACCI) 21-165, Aircraft
   Flying and Maintenance Scheduling Procedures, a surge (during home-station ops) is
   defined as an increase in flying schedule by fifty percent or greater.
 28                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


   4.2.6. Wartime Mission—Actions required to be performed by the system as defined by the
   user in CONOPS, OPLANs or other support concepts.
4.3. Sources of Usage Rate Data.
   4.3.1. To properly evaluate the ability of a system to achieve a given usage rate, testers must
   first begin with the requirements documentation related to the system. Compare CDD
   requirements with UTCs and the corresponding MISCAPs designated for the system or for
   the items it supports. If the system under test does not have a UTC or fit within an existing
   one, usage rate data will be required from the warfighter/user’s CONOPS or relevant
   requirements document. However, if the system does have a prescribed UTC, then
   requirements documents may contain either a requirement to increase the UTC usage rate by
   a specific amount or usage rates required to support taskings. The relationship of UTCs to an
   item’s intended usage in the field could help testers form relevant evaluations of systems.
   Understanding the elements of a UTC and the information it provides will ensure the test is
   structured around the system’s intended operational environment, with the right levels of
   support and usage rates.
   4.3.2. UTCs are elements of a capability available to combatant commanders to plan for and
   execute operations. Types of UTCs include independent, dependent and supporting UTCs;
   each should be considered for initial test designs. UTCs are applicable to all types of
   military operations, from direct combat to operations other than war. UTCs and
   peacetime/wartime usage rates come from the warfighter/user. When using UTCs as
   references for test development, ensure they are current. Consider all applicable UTCs in
   which the system may be tasked, as UTCs affect UTEs. For example, the UTC designator
   for a tanker aircraft would start with “3Y”, which defines its deployment capability as a
   “Refueling Aircraft.”
   4.3.3. The primary source for testers to determine proper usages rates from a UTC is the
   MISCAP. The MISCAP will describe the intended capability a UTC is to provide to the
   Combatant Commander. The MISCAP, when compared to the logistics requirements listed
   in the UTC, frames the capability within the support package intended to enable it. The
   support package details will include listings for spare parts, manpower (by Air Force
   specialty code [AFSC] and skill level), support equipment and aerospace ground equipment
   for all on- and off-equipment maintenance. For the tanker example, the MISCAP would
   include the Air Force’s capability to provide global reach. The tanker’s MISCAP may
   include the craft’s effective range, fuel storage capacity, the types of bases to which that unit
   may be deployed, and other UTCs that will be required to support the system (e.g. a UTC
   code beginning in “9AD” that designates an “Air Refueling Headquarters” may be necessary
   to support the operation of the system).
4.4. Usage Rate Evaluation Considerations.
   4.4.1. Utilization Rate versus Sortie Generation Rate.
      4.4.1.1. The definitions of UTE and SGR are in Section 4.2. It is easy to confuse the two
      requirements or their measures—some people even use the terms interchangeably.
      Nevertheless, SGR is typically a more definitive requirement that applies directly to an
      end item’s intended use in a specified environment. SGR may be defined for peacetime
      or wartime. During the standard Air Tasking Order cycle, planners levy sortie
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                 29


      requirements on wings and units per day. Each day, deployed units report capabilities
      through their chain of command; Air Planners in the Air and Space Operations Center use
      these reports to develop plans for executing the air war. A UTE rate captures the average
      rate over a period of time. In contrast, SGR is a specific by-day requirement.
      4.4.1.2. Tables 4.1. and 4.2. contain example test data and illustrate how the same data
      set, based on the differences in the definitions, may or may not meet requirements. Table
      4.1 contains example Sortie per Aircraft (SPA) values for the requirement, “Sortie
      Generation Rate shall equal the Sortie per Aircraft.”

Table 4.1. SPA Requirements
                                         Days          SPA
                                         1-5           4
                                         6-10          3
                                         11-30         2.5

Table 4.2. Example Test Data
                   Day             1       2       3         4     5       AVG
                   SPA             5       3       5         3     5       4.2

      4.4.1.3. Considering the sample data in Table 4.2., if the requirement was for a UTE rate
      of 4 over a period of five days, the displayed average of 4.2 exceeds that requirement and
      would be rated favorably. However, the requirement as given is for an SGR, and a close
      examination of the data shows peaks and dips in sortie performance, with two out of five
      days not meeting the requirement of 4.
      4.4.1.4. To understand the operational relevance of numbers such as these, consider a
      deployed unit of 12 F-15Cs. Planners depend on the unit to provide 48 sorties per day on
      days 1 through 5 of a conflict (4 sorties per aircraft X 12 aircraft = 48 sorties). If the
      performance matched that of the example test data on day 1, the F-15s exceeded
      expectations providing 60 sorties for tasking. However, on day 2 that number drops to
      only 36 sorties, 12 short of the planned requirement of 48.
   4.4.2. Usage Rates and Modeling and Simulation. Availability is accurately characterized
   when compared to a given usage rate. If a reported availability was the factor of a low UTE
   rate, then the number may be inflated. Accurate availability should be derived when
   compared to the usage rates that the user defines. A natural limitation to test environments is
   the inability to construct complete scenarios that encompass all of the factors of the intended
   environment. In an effort to provide these answers under test constraints, modeling and
   simulation (M&S) is often used. Availability and usage rate models are usually one in the
   same, and run with different controlled inputs. M&S is used by framing the intended
   environment with the correct level of assets (i.e., number of aircraft), manpower, spare parts,
   support equipment and facilities, etc., along with the inherent reliability characteristics of the
   system under test. Model outputs using these defined characteristics should provide usage
   rate data relevant to the ability of the support concept and reliability characteristics to support
 30                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


   user requirements. Sensitivity analysis on specific effects of the constraints could be run
   (given the model has the proper level of fidelity) to provide users and decision makers with
   valuable inputs and characterization of system performance and limitations.
4.5. Usage Rate Measures. The measures listed below are for reference only. Actual measures
should be rooted in the verbiage of user requirements documents. If the interpretation of these
documents is vague or difficult to discern, the test team should make every effort to receive
clarification in writing from the user.
   4.5.1. Flying Hours per Life Unit (FH/LU)—Number of flying hours from wheels up to
   wheels down, collected over a specified period.
   4.5.2. SGR—Total number of sorties per aircraft per day. This is usually a number like 3 or
   2.4.
   4.5.3. Operation Hours per Life Unit—The number of operating hours collected over the
   OT&E period (or deemed OT relevant) divided by the life unit of interest. This is applicable
   to ground systems.
   4.5.4. UTE—For aircraft, the average number of sorties or hours flown per authorized or
   chargeable aircraft per relevant time period, such as a day or month. This is usually a number
   such as 18 or 21 per month.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             31


                                           Chapter 5

                     COMPATIBILITY AND INTEROPERABILITY

5.1. Introduction. Compatibility and interoperability during test planning, execution, and
reporting are discussed in this chapter. The definitions of these terms and related key terms
follow.
5.2. Definitions.
   5.2.1. Compatibility—The capability of two or more items or components of equipment or
   material to exist or function in the same system or environment without mutual interference.
   There are various types of compatibility (each with different meanings). Hence, it is
   important to know what type needs consideration during test. A few common types are
   electrical, electromagnetic, human-systems interface, and physical.        Electromagnetic
   compatibility (EMC) is the primary type of compatibility for OT.
   5.2.2. Interoperability— The ability to operate in synergy in the execution of assigned
   tasks (JP 1-02). Functionally, this is the ability of systems, units, or forces to provide
   services to and accept services from other systems, units, or forces and to use the services so
   exchanged to enable them to operate effectively together. Interoperability is achieved when
   information or services can be exchanged directly and used effectively by all applicable
   system users. See Chairman of the Joint Chiefs Of Staff Instruction (CJCSI) 6212.01E,
   Interoperability and Supportability of Information Technology and National Security
   Systems, and the DAG, Section 7.3., for additional guidance on interoperability. These
   references introduce the Net Ready Key Performance Parameter (NR KPP). Interoperability
   is a DOT&E special interest item (SII). There is also an NR KPP study report (available as
   of 8 Mar 13 at https://acc.dau.mil/CommunityBrowser.aspx?id=126642) which may be
   helpful in planning the interoperability evaluation.
   5.2.3. EMC—The ability of systems, equipment, and devices that utilize the electromagnetic
   spectrum to operate in their intended operational environments without suffering
   unacceptable degradation or causing unintentional degradation because of electromagnetic
   radiation or response. EMC involves the application of sound electromagnetic spectrum
   management; system, equipment and device design configuration that ensures interference-
   free operation; and clear concepts and doctrines that maximize operational effectiveness (see
   the DAG). Electromagnetic Environmental Effects and Spectrum Management (E3/SM) is a
   special interest item for AFOTEC. Considerations include transmitter power, emission
   bandwidth, modulation scheme (frequency hopping), spread spectrum (frequency
   modulation), tuning range, duty cycle, receiver sensitivity, antenna type (parabolic, horn,
   whip), beam-width, gain, polarization, height and directivity (directional, omni-directional).
   5.2.4. Mutual Interference—Mutual interference refers to an electromagnetic spectrum
   user’s receipt of electromagnetic energy from emitters in the operational environment that
   causes unacceptable performance degradation to that user’s systems. The key word is
   “unacceptable,” since the nature of electromagnetic fields and phenomena precludes
   exclusion of unintended reception under all circumstances.
   5.2.5. Physical Compatibility—The “form and fit” in the design and integration of
   components and subsystems into the overall weapon system. Considerations include
 32                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


   interconnecting cabling mechanical linkages, signal interface connectors and weapon
   interlock devices.
5.3. Planning For Test. As defined above, there are both effectiveness and suitability aspects
of compatibility and interoperability (C-I). The system developer should document requirements
for C-I testing in the DT portion of the TEMP (see the DAG, Section 9.5). The OT planner
should consider CI at the operational level above the often purely technical characteristics of
system design. A key element of the OT planner’s job is the consideration of the performance of
the weapon system in its intended operational environment with other friendly weapon systems
deployed to accomplish warfighter tasks. The OT planner should accomplish coordination with
outside agencies such as the Joint Spectrum Center and Joint Interoperability Test Command
(JITC).
   5.3.1. Compatibility.
       5.3.1.1. Compatibility concerns the capability of the equipment in the system to operate
       with each of the required supporting equipment items such as electrical power generation,
       air conditioning, hydraulic power subsystems, etc. It also addresses the interface with
       logistics support items including test equipment, servicing equipment, maintenance
       stands, and elements of the transportation systems. Compatibility includes considerations
       for physical, functional, electrical/electronic, environmental conditioning, human factors
       and electromagnetic environmental effects. Although much of detailed compatibility
       testing falls in the domain of DT, there may be occurrences of compatibility problems
       uncovered in OT. For this reason, OT planners should:
          5.3.1.1.1. Identify any special resource/system requirements for testing compatibility
          during OT in the TEMP.
          5.3.1.1.2. Be aware of modifications/upgrades potential for introducing compatibility
          problems, especially if the upgrade involves integrating non-developmental items into
          the system.
          5.3.1.1.3. Be attentive of procedures being used (i.e., the compatibility of two
          systems may depend on how the procedures are followed).
       5.3.1.2. As stated earlier, EMC is the primary type of compatibility for OT. Many
       weapon systems utilize the electromagnetic spectrum for command, control, information
       transfer, telemetry or guidance. Efficient and effective transmission and receipt of
       commanding, controlling, and telemetry information often becomes the critical link to
       successful task accomplishment. When a weapon system is employed in a joint theater of
       operations and is degraded or negated by electromagnetic interference (EMI) from
       friendly forces, that weapon system’s capability is reduced as surely as if it were
       degraded or negated by enemy action. The importance of developing weapon systems
       that can utilize the electromagnetic spectrum effectively has led to DoD-level interest in
       properly designing weapon systems with consideration of EMC. Here are some basic
       principles of EMC:
          5.3.1.2.1. EMC seeks to control or limit the undesirable effects of EMI on systems in
          a specific environment. EMI occurs when an electromagnetic signal intended for one
          receiver (circuit) reaches and impacts another receiver (circuit) for which it was not
          intended. Successful control of EMI will have the characteristics of reducing the
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                        33


       levels or magnitudes of undesired voltages and/or currents in the affected system to a
       point where performance degradation is minimized or acceptable.
       5.3.1.2.2. The OT planner will be more interested in verifying EMC during scenarios
       involving intersystem and system-to-environment situations:
          5.3.1.2.2.1. Component. The EMI cause and effect is wholly constrained to an
          electronic subsystem such as a radio receiver or computer or electronic equipment
          rack.
          5.3.1.2.2.2. Intrasystem. EMI cause and effects are located within a larger
          system such as an aircraft or missile. An example of this type of EMI scenario
          would be an aircraft’s weather radar that interferes with the operation of an
          onboard inertial navigation computer.
          5.3.1.2.2.3. Intersystem. EMI cause exists outside the weapon system in a
          constrained environment essential for the operation of the weapon system. An
          example would be a nearby ground-based air traffic control radar that interferes
          with an aircraft’s ultrahigh frequency (UHF) air-to-ground radio during the
          aircraft’s flight operation.
          5.3.1.2.2.4. System-to-Environment. This is the most complex environment for
          EMI interactions. EMI sources and receivers (victims) are part of a major
          deployment of many weapon systems as in a theater of operations.
       5.3.1.2.3. User requirements start the process of developing systems with good EMC.
       A battlefield task that requires forces to detect or operate in widely separated
       formations will often utilize the electromagnetic spectrum in some form to
       accomplish this type of mission. An example might be to detect enemy armored
       vehicles day or night in the presence of smoke and dust. The designer may consider
       some form of radar type system optimized for use in the stated environment. Some
       required design choices are the output power, effective radiated power and frequency
       range of the radar. Each of these design factors will have an implication for the
       eventual EMC of the deployed system.
       5.3.1.2.4. The process of determining the frequency range of operation is one of the
       most important aspects in a design that will have good EMC characteristics. Contact
       the installation frequency monitor and the appropriate communication guide to aid in
       determining frequency availability. Consider the threat emitters likely to be present
       in the operational environment, friendly emitters deployed with the system, and the
       existing usage of the electromagnetic spectrum by host nations if EMC is to be
       preserved. Other factors are required power, antenna type and effective radiated
       power, modulation scheme, and electronic combat countermeasures.                Early
       identification of potential co-channel and in-band interference situations will allow
       design changes at a point in the acquisition cycle where changes cost.
       5.3.1.2.5. Table 5.1. is a matrix of factors and descriptors affecting EMC that may
       help in OT&E planning for the EMC evaluation. Consider other factors and
       descriptors for the particular system under test.
 34                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


Table 5.1. Notional Factors and Descriptors Affecting EMC OT&E Planning
      Factors                           Descriptors
      Type of Warfare Encountered       Conventional; Nuclear; Guerrilla
      Command, Control, and             Two Or More; One Only (Single Site)
      Communications Network Size
      Mission                           Strategic; Tactical; Special Operations
      Path Obstacles                    Clear; Obscured; Knife-Edge Diffraction
      Climate                           Temperate; Desert (Arid); Arctic
      Interference Sources              Jamming: Adaptive, Broadband, Spot;
                                        Industrial/Man-Made Noise; Friendly Emitters
      Communication Density             Low; High
      Mission Duration                  Round The Clock; Scheduled;
                                        Random/Unscheduled

   5.3.2. Interoperability. The acquisition strategy should describe the treatment of
   interoperability requirements. If an evolutionary acquisition strategy involves successive
   increments satisfying time-phased capability needs, the program manager should address
   each increment and the transitions from increment to increment. A NR KPP, of which
   interoperability is a part, is required for all Information Technology and National Security
   Systems defense acquisition and procurement programs. There are two general aspects of
   interoperability: information related, and non-information related.
        5.3.2.1. Information related interoperability deals with the exchange of information
        within the delivered system and between the delivered system and other systems
        necessary for mission accomplishment (to include other Services, interagencies and
        coalition partners as applicable). For a given system, there could easily be hundreds,
        thousands, even millions of communication pathways in which information can be
        exchanged. Not all require examination during testing. Usually, key components of the
        system are checked for compliance with engineering and technical standards according to
        the system’s integrated architecture views. Testing also focuses on the exchange of key
        information between key network nodes. Because OT planners do not test for
        compliance (DT does this), the focus for OT is on assisting the DT community during
        IT&E, following AFMAN 63-119, Certification of System Readiness for Dedicated
        Operational Testing, interoperability template guidance on readiness for OT, and noting
        any interoperability problems during OT. For programs that require joint interoperability
        certification, OT planners will include the JITC in the OT. For specifics concerning JITC
        involvement, see Section 3.f. of the “Memorandum of Agreement (MOA) on MOT&E.”
        The MOA is located on the AFOTEC Intranet page.
        5.3.2.2. Non-information related interoperability deals with fuel formulation, mechanical
        connectors, armament, power characteristics and other technology considerations that are
        usually addressed during DT. OT should focus on human-machine interface and assess
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             35


      the impact of any non-information related interoperability problems as they arise during
      test.
5.4. Test Execution and Reporting Considerations.
   5.4.1. EMC of operational systems has traditionally been measured in OT&E during analysis
   of degradation or failures of the system. When analysis revealed that EMI caused or had a
   high probability of causing the observed faults, this was recorded and allocated in scoring the
   OT&E test data. This provided only a point-in-time estimate and did not provide conclusive
   evidence of future system performance in the operational environment. Modeling and
   simulating the expected operational environment using proven EMC analysis models will
   provide an important contribution in determining the effectiveness or suitability of the
   weapon system in the operational environment.
   5.4.2. Interoperability evaluation involves addressing the ability to exchange information
   across an interface; conformance to standards defined for format, language, syntax,
   vocabulary, and interface operating procedures; and information exchanged among
   command, control, communications, computers and intelligence (C4I) systems using
   message text formats, tactical digital information links, or other combat data links for its
   timeliness, accuracy, and usability.
   5.4.3. Consider obtaining the following information/data before or during test:
      5.4.3.1. Results of developmental testing of C-I. Tests that demonstrate the degree of
      susceptibility of receiver systems to the type of potential interference are important. Also
      important are tests that evaluate intrasystem integration (as on a satellite, airframe or
      missile) and the resistance of receiver subsystems to conducted interference conditions.
      5.4.3.2. A list of C-I issues that would impact certification to go to OT&E (see AFMAN
      63-119).
      5.4.3.3. The use of special facilities, instrumentation, and simulation required to
      assess/evaluate C-I.
      5.4.3.4. A list of data needed by JITC to complete their recommendation for Net Ready
      certification.
      5.4.3.5. Common frequency usage between the source and receiver.
      5.4.3.6. Time when the source and receiver are using the same frequency resource(s).
      5.4.3.7. Spatial (distance and bearing) relationships of all receivers in relation to
      emitters.
 36                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


                                           Chapter 6

                                   TRANSPORTABILITY

6.1. Introduction. System transportability is a key factor in rapid deployment of a system and,
once deployed, sustaining the system at wartime utilization rates in the deployment theater.
Transportability, therefore, is a dynamic concept, intimately related to system CONOPS and unit
mission tasks. No single methodology can be used to measure transportability in every situation;
rather, the suitability evaluator must consider factors relating to the system, its mission, the
environment, support infrastructure and other factors., A combination of techniques are then
used to determine the suitability of the system with respect to transportability. This chapter
provides general considerations that may be used in planning transportability evaluation.
6.2. Definitions.
   6.2.1. Transportability—The capability of material to be moved by towing, self-propulsion,
   or carrier through any means such as railways, highways, waterways, pipelines, oceans,
   space, and airways. Full consideration of available and planned transportation assets,
   mobility plans/schedules and the impact of system equipment/support items on the strategic
   mobility of operating military forces is required to achieve this capability.
   6.2.2. Packaging, Handling, Storage, and Transportation (PHS&T)—One of the ten
   integrated logistics support (ILS) elements; the resources, processes, procedures, and design
   considerations and methods to ensure that all system, equipment, and support items are
   preserved, packaged, handled, and transported properly, including environmental
   considerations, equipment preservation requirements for short- and long-time storage, and
   transportability.
   6.2.3. System—For transportability evaluation purposes, the system includes not only prime
   mission equipment, but also all associated support equipment, personnel, spares, and any
   other dedicated materiel required for the system to perform operational suitability tasks.
6.3. Transportability Concepts. The concept of transportability has two major components: A
physical or dimensional component and a readiness component.
   6.3.1. The physical or dimensional component focuses on ensuring that the physical
   dimensions and characteristics of system hardware are compatible with and fall within the
   carrying capacity of the planned transportation mode. The unit of analysis is the individual
   system and its associated items. This component generally involves related packaging,
   handling, and storage (PH&S) concepts focused on the capability to package, handle, and
   preserve all system equipment and support items. Mission, design specifications, item
   configuration, safety, geographic and environmental considerations, and packaging and
   preservation concepts influence this component.
   6.3.2. The readiness component focuses on system deployability and sustainability. It
   focuses on how system physical characteristics, CONOPS, maintenance concept and other
   factors react with deployed unit missions and deployed resources to determine the ability to
   perform operational suitability tasks. The units of analysis include both the individual
   system and the entire fleet or collection of individual items. Operational test planning for the
   readiness component of system transportability should: proceed from an understanding of
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                            37


   how the system will be tasked to deploy, timing of deployment; cargo processing
   infrastructure at deployed locations, immediacy of operational tasking upon initial arrival at
   deployed locations, the relationship between system demands on the transportation system,
   and available transportation resources.
   6.3.3. The concepts below are useful in putting specific user transportability requirements
   into practice. These concepts are valuable, as they apply to transportability, rather than
   simply satisfying ICD/CDD/CPD requirements (i.e., number of XYZ pallets, etc.). Other
   sources should be utilized such as system CONOPS, operational plans, and data on available
   infrastructure.
      6.3.3.1. Operational Transportability. Operational transportability is a notional
      concept of the degree to which an item is movable by specified transportation means
      using given transportation assets. The following should be considered when working to
      develop transportability requirements and measures with the user: how many things must
      be transported; how quickly must they be transported; what personnel, materiel handling
      assets and facilities will be required at points of departure, inter-modal nodes, and
      destinations; and storage and packaging requirements.
          6.3.3.1.1. For example, the user often may have an idea of transportability
          requirements in terms of C-17-equivalent airlift loads, or total pallet positions.
          OT&E personnel should work with the user to develop the operational
          transportability concept that will focus analysis of transportability requirements.
      6.3.3.2. Operational Sustainment Transportation Burden. A notional concept
      expressed as the degree to which the operation of a system at specified rates/intensities
      for a specified duration will drive specific transportation infrastructure requirements. An
      example would be intermediate maintenance support items, such as an engine test cell
      capability for a particular fighter engine, which are planned to be deployed as engine
      spares are depleted -- the movement of the test cell would be a transportation requirement
      driven by the fighter.
6.4. Transportability OT&E Planning Considerations.
   6.4.1. OT&E evaluators may assess items which range from the acceptability of packing
   techniques and materials for transporting spare parts (particularly hazardous material, cure-
   dated and fragile items) to equipment required for reassembly at destination. Specific areas
   depend upon characteristics of the system under test; the technical order (T.O.) concept
   provides a framework for selecting individual items. Common test planning considerations
   include: protection from weather and rough handling; requirements for outsize components;
   unique packaging, crating, or handling needed for such components; amount of provisioning
   needed to meet rapid deployment requirements; requirements for shipping and storage
   containers; and requirements for handling, carrying, and protective devices required for dust,
   shock, impact, moisture, etc.
   6.4.2. Physical/Dimensional Component Testing. The physical/dimensional component of
   system transportability is generally examined during developmental testing. However,
   OT&E planners should work closely with the developer, user and system program office
   (SPO) to ensure all requirements are clear.
38                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


     6.4.2.1. For example, the developing contractor may be able to demonstrate successful
     loading of a system aboard an aircraft during daylight in mild weather conditions.
     Although this data may prove relevant to OT, it does not mean that the user’s personnel
     will meet with the same success when attempting to load the system under more
     challenging weather or lighting conditions.
     6.4.2.2. Similarly, design requirements aimed at making a system transportable by a
     single individual may be shown to have been met during DT&E and IT&E, but the
     operational suitability question is, “Can the individual carry the system and still perform
     other required operational duties, or is there a negative impact on the individual’s combat
     effectiveness?” The OT&E suitability personnel should work to ensure that DT&E and
     IT&E address such questions, and if not, ensure they are included in OT&E events.
     OT&E interests in the physical/dimensional component of transportability generally fall
     within the following areas:
         6.4.2.2.1. Can the system be prepared for transportation?
         6.4.2.2.2. Can the system be physically transported?
             6.4.2.2.2.1. Can the system be processed by available material handling
             equipment?
             6.4.2.2.2.2. Can the system be physically loaded, carried by and unloaded from
             truck, rail, ship, commercial aircraft, C-130, C-5, C-17 and/or space transportation
             system?
             6.4.2.2.2.3. Is the system compatible with the 463L pallet system?
         6.4.2.2.3. What limitations does the system place on the transport system?
             6.4.2.2.3.1. Are there special handling considerations (hazardous or perishable
             cargo, security requirements, couriers, etc.)?
             6.4.2.2.3.2. Do cargo incompatibilities exist?
         6.4.2.2.4. What is the weight and cube of the system?
             6.4.2.2.4.1. How much is outsize or oversize?
             6.4.2.2.4.2. How many pallet positions or equivalents are required?
         6.4.2.2.5. Can the system withstand transportation-induced handling/environmental
         stresses without damage?
         6.4.2.2.6. Can the system be unpacked, reassembled, set up, and initiate operations?
         6.4.2.2.7. What items must be transported to the deployed location to operate and
         sustain the system, and what is their transportability?
         6.4.2.2.8. What supplies, equipment, personnel and facilities are required in the
         above actions?
         6.4.2.2.9. How much time is allotted, available and required in the above actions?
  6.4.3. Readiness Component Testing. Testing the readiness component of system
  transportability requires an integration of physical/dimensional data with an understanding of
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             39


  unit missions, operational plans, deployed infrastructure and other issues to answer the
  following refinements of the basic transportability questions:
     6.4.3.1. Can the system be prepared for transportation (and transported) with the
     personnel, equipment, and facilities specified in the CONOPS, under operationally
     realistic conditions, within times required by operational plans? Individually? As a
     fleet?
     6.4.3.2. Can the system withstand operationally realistic transport conditions in all
     expected transport modes without damage?
     6.4.3.3. Can the system be unpacked, reassembled and set up with the personnel,
     equipment, and facilities specified in the CONOPS, under operationally realistic
     conditions, within times required by operational plans? Individually? As a fleet?
     6.4.3.4. Can the items required to operate and sustain the system be transported to the
     deployed location with the personnel, equipment, and facilities specified in the CONOPS
     or plan, under operationally realistic conditions, within times required by operational
     plans? Individually? As a fleet?
  6.4.4. Test Activities and Scenarios. Answering the transportability question may require
  integration and aggregation of data from numerous differing sources. Qualitative measures
  of the suitability of the planned PH&S capability may be captured by standard questionnaires
  to obtain expert opinions in this area. Modeling and simulation may be required to fully
  understand the impact of deploying the entire fleet of a new system on the global
  transportation network and the flow of other assets. Suitability evaluation personnel should
  work to ensure that all suitability testing requirements are fully incorporated into test plans,
  to include dedicated suitability events and required modeling and simulation efforts.
  Additional considerations for planning test actions, events and scenarios include:
     6.4.4.1. The suitability test planner should incorporate deployment activities into the
     mission scenario that will be used during operational testing. While it may not be
     necessary to actually deploy the system during operational testing, all aspects of
     preparing the system and its support equipment for deployment should be exercised to
     evaluate manpower, material and time requirements.
     6.4.4.2. Time to prepare the system and support equipment for transport should be
     evaluated and assessed. If deployment by different modes of transportation is required,
     preparation times for the most likely mode should be evaluated. Typically, the user will
     state the requirement in terms of the system and/or support equipment being prepared for
     transport in not more than “x” hours from formal notification to deploy.
     6.4.4.3. There are unique factors to loading aircraft with system support equipment that
     cannot be captured by modeling and simulation. Although a volumetric model shows no
     problems when loading a given aircraft, support equipment may fail based on un-
     modeled ramp angle and expansion joint interference with equipment casters. When
     possible, actual airlift assets should be used to evaluate, by actual loading, the amount of
     airlift assets required to deploy the system and its support equipment.
         6.4.4.3.1. If the requirement is, for example, to deploy the system in four or fewer
         C17 equivalent loads, it is not necessary to bring four C-17 aircraft to the test
40                                           AFOTECPAM 99-104 24 SEPTEMBER 2013


        location. One or two aircraft and multiple loads would appropriately demonstrate the
        transportability requirement and may provide a more controlled data collection
        environment. AF logistics software systems such as Logistics Module (LOGMOD)
        and Automated Air Load Planning System (AALPS) can help determine airlift
        requirements.
     6.4.4.4. If it is not possible to evaluate such transportability requirements by actual
     demonstration, as a bare minimum, a thorough and detailed load plan for the entire initial
     deployment package should be prepared. Air Mobility Command personnel should
     participate in both the load planning and evaluation.
     6.4.4.5. Upon completion of the deployment-related transportability evaluation, the
     system and/or support should be placed into operation (as if it had just arrived at the
     deployment location) to demonstrate time requirements related to beddown, setup,
     reassembly, etc.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              41


                                           Chapter 7

              SAFETY AND ENVIRONMENTAL IMPACTS AND EFFECTS

7.1. Introduction. Regulations governing safety and health in OT&E and AFOTEC’s process
to achieve compliance with those regulations are discussed in this chapter. SUT environment,
safety and occupational health (ESOH) considerations shall not be viewed as OT&E evaluation
criteria. For the criteria to be real or valid, the SUT must be able to pass or fail. In the
operational scenario, however, any person recognizing an unsafe potential (failure) must
intervene and “knock-it-off” before the failure event—there is no other option. Therefore, the
“ESOH” criteria cannot be valid for operational test. Test teams are expected to describe
perceived SUT ESOH failure potentials to give the full operational picture to users but we will
not evaluate to those potentials. Many tests involve potentially dangerous activities. In addition,
management is responsible for providing proper training in health and safety matters, equipment,
controls, and the environment to ensure the safety of all personnel and equipment involved. The
ESOH certification board (ESOHCB) safety certification is AFOTEC management’s record of
reviewing SUT ESOH issues, of mitigating those issues to acceptable levels and when necessary
of raising the decision level suitably to the increased residual risk. On occasion, the Navy as
OTA, will construct a Safety critical operational issue (COI) or other “safety” criteria for a SUT.
Individual test teams will have to deal with this very issue and attempt to measure to the
unrealistic criteria which asks, “Is this system safe for use by typical users in the typical
environment.” The only acceptable answer is “Yes,” or the test must be incomplete because the
test team cannot have completed an “Unsafe” test.
7.2. ESOH. The three basic principles of ESOH are to sustain readiness, leverage resources,
and to be a good neighbor. The test director (TD) is responsible to provide a safe and healthy
workplace, enhance mission accomplishment, preserve resources and minimize risks—on and off
the installation or public lands. AFOTEC Safety (SE) supports these responsibilities with subject
matter experts assigned to the core/test teams.
7.3. ESOH Council (ESOHC). The Secretary of the Air Force and AF Chief of Staff directed
all AF organizational levels to develop a comprehensive ESOH management system to
systematically pursue the Secretary of Defense’s significant accident reduction objective. The
ESOHC is the senior environment, safety and health steering group at each level of command.
The AFOTEC ESOHC meets at least semi-annually to sustain a systematic, interdisciplinary
approach to ESOH and ensure core mission areas integrate this approach into planning,
budgeting and decision-making. Membership includes representatives from all two-letter
offices. The Commander serves as chairperson of the ESOHC, which normally occurs during
the semi-annual Commanders Conference. The required minimum agenda is in AFI 90-801,
Environment, Safety, and Occupational Health Councils.
7.4. ESOH Statutory Compliance.
   7.4.1. The AFOTEC ESOH-Management System addresses the requirements of Executive
   Order 13423, Strengthening Federal Environmental, Energy, and Transportation, the
   National Environmental Policy Act (NEPA), the Occupational Safety and Health Act, and
   other laws brought to bear under these broad umbrellas. AFI 99-103, Capabilities-Based Test
   and Evaluation, requires independent government technical and safety personnel to examine
 42                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


   the technical and safety aspects of test and evaluation (T&E) plans that involve government
   resources prior to commencement of test activities. The Environmental and Occupational
   Health and Safety Plan (HSP) documents compliance to protect test teams and AFOTEC
   from a legal challenge or a violation. Publishing the HSP accomplishes compliance to the
   following additional statutes: Occupational Safety and Health Act of 1970; Executive Order
   12196 Occupational Safety and Health Programs for Federal Employees; 29 Code of Federal
   Regulations (CFR) 1960, Basic Program Elements for Federal Employee Occupational
   Safety and Health Programs and Related Matters; 29 CFR 1910, Occupational Safety and
   Health Programs; Air Force Policy Directive (AFPD) 32-70, Environmental Quality; AFPD
   48-1, Aerospace Medical Program (Health); AFPD 63-1, Acquisition and Sustainment Life
   Cycle Management; AFPD 90-8, Environment, Safety & Occupational Health Management
   and Risk Management; AFPD 91-2, Safety Programs; and AFPD 99-1, Test & Evaluation
   Process.
   7.4.2. To help cover these areas during test development, the Test Director should include a
   statement along these lines in charter type documents: Provide AFOTEC with relevant
   operational safety, suitability and effectiveness (OSS&E) (ref. AFI 63-1201, Life Cycle
   Systems Engineering) related information to include but not limited to the following:
   Current/draft OSS&E Assurance Plan; System Safety Plan, System Level Assessment; and
   Programmatic Environment, Safety, and Occupational Health (PESHE) plans/assessments
   along with related Operating Instructions on Internal Modification Processing, Configuration
   Control, Deficiency Reporting Process & Procedures, Engineering Change Proposal Process
   & Procedures, Materiel Improvement Project Review Board Process & Procedures, etc.
7.5. ESOH-Management System (ESOH-MS). The ESOH-MS is the starting point for every
AFOTEC test activity and the ticket to a safe, compliant activity. ESOH-MS is comprised of a
collection of tools that includes Standard Work templates, Systems Safety analysis databases,
and the ESOH certification process, ultimately resulting in an ESOH Certification for Test. The
system aggregates data through the life of a program, maintains individual records of each test
activity, certifies safety and environmental planning and supports the “Readiness for Dedicated
Operational Test” decision. Prior to or in Initial Test Design (ITD), the AFOTEC/SE point of
contact (POC) will initiate the ESOH-MS for the core team to capture known technology and
ESOH factors and develop an initial technology risk assessment. Then, as the test team develops
each operational test concept, the safety office POC will initiate an activity-specific certification
for each test activity. The ESOH-MS process will be available to the test director and has the
capability to branch out to safety subject matter experts and other test experts as required to
accomplish a thorough analysis and develop the identified hazard list with mitigations. Early
initiation of ESOH-MS is critical to allow adequate time to request environmental analysis and
scope test risks. Since complex environmental analyses can take years to complete, the TD
should consult with the safety office POC early in test concept to determine whether a “long
pole” may exist. The system connects the team and safety office subject matter experts to
develop risk factors, assess installation environmental documentation, and determine the activity
risk level. ESOH-MS facilitates all of these phases and builds the HSP.
7.6. ESOH Planning and Review Process. AFOTEC/SE assigns safety POCs to advise and
consult on ESOH-related matters with the Detachment/SE, and attends program ESOH meetings.
The SE POC leads the initial technological review in pre-planning phases and can guide the test
teams during site safety and environmental compliance planning for test execution. Programs
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                43


should conduct an ESOH review at least once a year to review changes and catch up on issues.
The ESOH review composition is tailorable. Consider including the program office, sponsor,
AFOTEC/SE POC, Det/SE and others as needed. Safety-related concerns are normally
identified during OT planning, combined DT/OT events, and at system safety working group
meetings or system safety design review meetings. Identified hazards must be controlled or
mitigated to acceptable levels before start of test, and ESOH-MS is designed for this purpose.
7.7. HSP.
    7.7.1. The safety office POC uses the ESOH-MS to build the HSP interactively with the test
    team. The HSP covers environmental and occupational health and safety issues while
    documenting compliance with laws and regulations. It is the key document evaluated at the
    ESOHCB. The HSP is the TD’s (or the onsite responsible person’s) daily pretest/deployment
    briefing guide. When used, the HSP assures statutory compliance and produces a solid
    foundation for test site Operational Risk Management considerations.
    7.7.2. Hazards are identified in the HSP, each with an initial risk level (Low, Medium, High
    and Extremely High) that is controlled or mitigated as well as practicable to reduce the risk.
    Table 7.1. is an aid tailored from AFPAM 90-902, Operational Risk Management (ORM)
    Guidelines and Tools, which can be used to help assess ESOH risk levels for each type of
    hazard. This is a subjective process and should not be confused with the deficiency report
    (DR) or systems safety processes. Residual risk is the remaining mishap risk that exists after
    control measures are in place. The HSP finally considers all residual risks to assess an
    overall risk level for mission impact to test activities which determines the risk acceptance
    authority. At the detachment the appropriate operational impact risk acceptance decision
    authority for Low risk, no impact to test activities is the test director; for Medium risk,
    minimal impact to test activities is the director of operations; and for High risk, substantial
    impact to test activities is the commander. The AFOTEC/CC is the risk acceptance authority
    for Extremely High risk, severe impact to test activities. This decision authority can accept
    the risk and proceed or elevate (or consult other General Officers prior to making a risk
    decision in the case of the AFOTEC Commander) the decision to the next higher authority
    level. Whenever any identified hazard’s control measure(s) cannot be implemented, the risk
    level reverts back to the initial (higher) risk, which in-turn may require elevating the decision
    level before proceeding with the activity.

Table 7.1. Risk Level Matrix
Mishap Risk Index                                                 AFOTEC Mission Impact
Severity Probability   I                  II                 III             IV
                       Catastrophic       Critical           Moderate        Negligible
A – Frequent             EXTREMELY
B – Likely                   HIGH               HIGH
C – Occasional                                                   MEDIUM
D – Seldom                                                                              LOW
E – Unlikely
 Risk Assessment         Technology         Impact to Test
Value (AFPAM90-803)     (Mil Std 882E)        Activities        Mission Impact Acceptance Level
 Extremely High              High              Severe        AFOTEC Commander
       High                Serious           Substantial     Detachment Commander
 44                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


      Medium             Medium             Minimal        Detachment Director of Operations
       Low                Low                 No           Detachment Test Director

7.8. ESOHCB.
   7.8.1. The ESOHCB assembles to consider all outstanding ESOH risk factors, to formalize
   the risk acceptance decision authority, and to certify the environmental, safety and
   occupational health analyses in a certification memorandum. The ESOHCB is accomplished
   about four weeks before test readiness review (TRR), or before test start if TRR is not
   required. The TD works with the safety office POC, and the Detachment safety office to
   prepare and coordinate the ESOHCB, which the safety office chairs. The ESOHCB may be
   conducted in-person, teleconference or video teleconference. The board structure is
   tailorable to the programmatic situation; for example, the ESOHCB for an ESOH-
   controversial program could demand a formal meeting with multiple attendees whereas a
   well-structured and well-documented program with significant but acknowledged risks could
   be as simple as a teleconference between the TD, Detachment safety office POC, safety
   office POC, Director of Safety and the chairperson. The ESOHCB focuses on the safety of
   test, technology and environment as they pertain to the scope of the test activities. The Board
   pays particular attention to the HSP as the TD’s primary ESOH field source for ESOH
   hazards and mitigations, and emergency and mishap notifications. Additional sources
   include AFMAN 63-119, Attachment 25, Programmatic Environmental, Safety, and
   Occupational Health (PESHE) criteria; Safety deficiency reports (DR); environmental
   considerations; deployment issues; system safety deficiencies related to the system under
   test; occupational health; infrastructure support requirements; and others.
   7.8.2. The ESOHCB gives the test activity an overall risk assessment based on identified
   hazards and mitigations applied. The ESOHCB then considers how those risks will
   potentially impact test activities. Table 7.2. outlines ESOH-related impact potential for
   mishap risk factors and establishes the appropriate risk decision authority. The Director of
   Safety assigns the final risk in the form of a certification memorandum (ESOHCB Memo)
   and signs the memo. The appropriate risk decision authority for the test activity will accept
   the risk and approve the ESOHCB Memo. After risk acceptance and approval, a copy of the
   ESHOCB Memo will be forwarded as “info” to the next higher decision authority as level of
   risk dictates (i.e. TD to DO to Det/CC to AFOTEC/CC). Coordination and approval of the
   ESOHCB Memo will be accomplished prior to TRR for OT&E or prior to test activity start.

Table 7.2. ESOH Operational Impact Matrix
   No Mission Impact: No ESOH hazards have been identified that impact test activities.
   Known hazards identified have been adequately controlled (mitigated) to acceptable
   levels. The potential ESOH-related capability impacts are negligible. The Test Director
   or higher is the risk decision authority to accept the hazard controls.
   Minimal Mission Impact: Some ESOH hazards have been identified with minimal
   impact on test activities. Known hazards have been adequately identified and controlled
   (mitigated) to an acceptable level. Some conditions will not be resolved prior to the test
   but sufficient mitigation measures are in place to reduce the operational objective
   limitations. The Detachment Director of Operations or higher is the appropriate level to
   accept the limitations.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                            45


  Substantial Mission Impact: Some ESOH hazards have been identified with substantial
  impact on test activities. Known hazards have been adequately identified and the
  majority were resolved or mitigated. ESOH-related operational objective impacts were
  mitigated to acceptable levels. Some will not be resolved by test completion that may
  limit mission capability and may potentially influence affected test results. The
  Detachment Commander or higher is the appropriate level to accept and manage the
  limitations.
  Severe Mission Impact: There are ESOH shortfalls identified with severe impact on test
  activities. The majority of known hazards have been identified and controlled (mitigated)
  to an acceptable level. However, some ESOH-related operational objectives have the
  potential to limit test capabilities and will not be resolved in time. If it is deemed
  necessary to proceed with the test considering these significant limitations, it may be
  appropriate that Senior (General Officer) level external involvement occur along with the
  AFOTEC Commander accepting responsibility to proceed with test execution.
 46                                            AFOTECPAM 99-104 24 SEPTEMBER 2013



                                          Chapter 8

                                  HUMAN FACTORS (HF)

8.1. Introduction. A broad introduction to the scope of issues and methods employed in the
OT&E of HF/human systems integration (HSI) issues are discussed in this chapter, and should
be used as a guide to the different types of T&E services and support available from HF/HSI
analyst. For more specific information on questionnaires, see the AFOTEC Questionnaire
Guide, available on the AFOTEC Intranet website.
8.2. Definitions.
   8.2.1. HF—HF is the science/discipline that applies information about human behavior,
   abilities, limitations and other characteristics to the design of systems for effective human
   use.
   8.2.2. HSI—HSI covers seven human integration areas of concern (domains) that are
   incorporated into the design of acquisition programs. Although in OT&E HSI focuses on HF
   engineering (HFE), it also incorporates several areas not normally considered in the strict
   domain of human factors. The seven domains of HSI are manpower and personnel; training;
   safety and occupational health; human factors engineering; survivability; and habitability.
   Although the Air Force has no specific requirement for acquisition programs to have a
   dedicated HSI plan or program, look for HSI to be incorporated into the systems engineering
   plan (SEP) or the LCMP. When the term HSI is used in an Air Force document, it refers to
   all the domains with a primary interest in the requirement to incorporate HFE considerations
   into the acquisition.
8.3. HF/HSI Policies. DoDI 5000.02, Enclosure 8, Human Systems Integration, paragraph 1,
specifically mandates human factors engineering considerations, as a part of human-systems
integration, and must be included in all acquisition programs: “The PM shall have a plan for HSI
in place early in the acquisition process to optimize total system performance, minimize total
ownership costs, and ensure that the system is built to accommodate the characteristics of the
user population that will operate, maintain, and support the system.” The DAG further expands
on this requirement in Chapter 6.
8.4. HF/HSI Areas of Expertise. Detachment or A-9I HF analysts can assist test teams with
the formulation of HF aspects of OT&E and provide required analytical support. This aid can
help in the following functional areas: HF issues, ergonomics (workstation design or “switch
technology”), anthropometrics (size, accommodation or fit), workload and fatigue, human
performance and situational awareness evaluations, computer usability, personal equipment
usability, human-machine interface, training, and questionnaire development and administration.
8.5. HF/HSI Test Measures. The first step is to identify a HF/HSI analyst for assistance on the
program. The HF/HSI analyst will review documents and consult with the TD and/or the test
team to develop recommendations for appropriate HF/HSI test measures. A large number of HF
measures are subjective yet quantitative; in other words, they utilize near-interval scale
questionnaire methodologies and are analyzed using descriptive statistics. Observational
methods may also be used by the test team. Sometimes, interviews may be conducted to obtain
desired data or users may be asked to choose from a response among a set of categories, which
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                               47


are examples of qualitative data. HF questionnaires are usually web-based using AFOTEC’s
enterprise software solution (Vovici). Once appropriate measures are developed and approved,
they will be incorporated into test planning documents.
8.6. HF/HSI Test Plan Inputs. COIs are critical elements or operational objectives of the
mission that support mission accomplishment. When formulating each specific measure it
should be descriptive enough to show its relation to the associated COI. These MOEs/MOSs can
be distributed throughout the test plan and listed underneath multiple COIs. They may require
multiple pieces of data/information to resolve them satisfactorily. Examples of appropriately
stated HF MOEs/MOSs are:
   8.6.1. MOE: Operator ratings of control and display usability.
   8.6.2. MOE: Crew ratings of situational awareness.
   8.6.3. MOS: Maintainer ratings of ease of maintenance.
8.7. HF/HSI Test Execution. Each test team will determine the roles and responsibilities for
their team’s evaluation of HF/HSI during test execution. Test teams should have draft
questionnaires reviewed by the HF/HSI analyst for quality well ahead of the start of OT&E
(ideally during the test planning phase). Test teams should also have the HF/HSI analyst review
and comment on the HF/HSI portions of DMAPs and detailed test procedures (DTPs). In
general, HF questionnaires are administered as close to the event/activity as possible (e.g., at the
end of test mission, or a maintenance demonstration).
8.8. Questionnaire Support. The Detachment HF/HSI analyst and A-9I personnel ensure
AFOTEC questionnaires used during operational test are of the highest quality and conform to
AFOTEC best practices and technical standards. The HF/HSI analyst working with the test
program will work with the TD and test team to provide questionnaire software training,
instruction in the development of questionnaires, and review the formatting of questionnaires.
The HF/HSI analyst should be involved in this process at the earliest possible date. The
following is the recommended TD/test team procedure:
   8.8.1. Identify MOEs/MOSs which may require questionnaire data.
   8.8.2. Consult with the HF/HSI analyst about appropriateness of questionnaires as a source
   of data.
   8.8.3. Ask the HF/HSI analyst to provide the test team with the appropriate questionnaire
   software, handbooks, examples of questionnaires, etc.
   8.8.4. Consult with the HF/HSI analyst about appropriate format for administering
   questionnaires.
   8.8.5. Develop draft questionnaires for collection of data to address appropriate test
   measures.
   8.8.6. Have the HF/HSI analyst review draft questionnaires.
   8.8.7. Format questionnaires in the desired method of administration (computer-based, web-
   based, paper, interviews etc.).
   8.8.8. Consult users for comments on correctness of content in the questionnaires and any
   other additional suggestions.
48                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


  8.8.9. Perform a dry run of questionnaire administration.
  8.8.10. Have the test team collect questionnaire data during test execution.
  8.8.11. Identify the test team personnel who will analyze and evaluate questionnaire data for
  test report. Consult the HF/HSI analyst for assistance.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             49


                                           Chapter 9

                                    DOCUMENTATION

9.1. Introduction. Documentation is the critical link between the user and the system and
forms the foundation of training, maintenance instructions, operational procedures, and
continuity. This chapter provides general guidance and considerations, which should be used in
planning documentation evaluation.
9.2. Definitions.
   9.2.1. Documentation—Documentation includes operator and maintenance instructions,
   repair parts lists, and support manuals, as well as manuals related to computer programs and
   system software such as the software load instruction, user manuals, and systems
   administrator manuals. The focus is not only what the deliverables are, but how user-friendly
   and informative the documents are in completing the mission.
   9.2.2. Technical Data—Recorded information, regardless of form or character, of a
   scientific or technical nature. Documentation of computer programs and related software is
   technical data; computer programs and related software are not technical data. Financial data
   or other information related to contract administration is also included.
   9.2.3. Technical Manual (TM)—A publication that contains instructions for the
   installation, operation, maintenance, training, and support of weapon systems, weapon
   system components, and support equipment. TM information may be presented in any form
   or characteristic including, but not limited to, hard copy, audio and visual displays, magnetic
   tape, disks, and other electronic devices. A TM normally includes operational instructions,
   maintenance instructions, parts lists or parts breakdown, and related technical information or
   procedures exclusive to administrative procedures. T.O.s that meet the criteria of this
   definition may also be classified as TMs.
9.3. Documentation Test Planning Considerations.
   9.3.1. Technical data are the link between personnel and equipment. Traditionally, they
   have been paper products, but the current USAF trend is toward automation (i.e. digital
   technical data). Assess the usability, completeness, correctness and understandability of the
   technical data. The assessment of technical data is usually based on operator or maintainer
   subjective evaluations collected using questionnaires. Guidance can be obtained from HF
   analysts located at the Detachments or A-9I and can provide detailed assistance in producing,
   administering and evaluating survey instruments related to technical data.
   9.3.2. Suitability personnel should ensure they have access to and familiarity with data
   deliverables available from the SPO or other program sources. The data deliverables process
   should be reviewed before test design activities begin.
   9.3.3. Suitability personnel should monitor SPO technical data validation planning and
   implementation efforts to ensure verified technical data are available for dedicated OT&E.
   9.3.4. Validated technical data are required to begin OT&E. (AFOTECMAN 99-101; T.O.
   00-5-1, Air Force Technical Order System; and, T.O. 00-5-3, AF Technical Order Life Cycle
   Management)
 50                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


   9.3.5. Suitability personnel should also be knowledgeable of the SPO and users’ plans for
   verification of technical data, plans for providing formal publications to units, and
   information media/style (paper or digital, checklist, pocket size, military, commercial, etc.)
   being considered. These factors will have an impact on the usefulness of the documentation.
   9.3.6. Consider Joint Computer-Aided Acquisition and Logistics System (JCALS)
   requirements for technical data. Future systems should be as paperless as possible.
   9.3.7. Documentation should identify Hardness Maintenance/Hardness Surveillance
   (HM/HS) processes and hardness critical items (HCIs).
   9.3.8. Documentation should contain an adequate number of illustrations, reference any
   special tool and test equipment required, have a table of contents and an index, and reference
   other documents such as T.O.s needed to complete tasks.
   9.3.9. Installation, hardware and safety provisions between related manuals should be
   consistent. Troubleshooting procedures and illustrated parts breakdown should be addressed.
   9.3.10. Documentation should be readable at the required level of understanding.
   9.3.11. The test team should participate and provide inputs through the program office into
   the review of the technical data specifications, tabletop reviews of technical data, and the
   contractor’s technical data development effort. The team should evaluate the technical data
   during its day-to-day use, and participate and provide inputs to the program office during in-
   process reviews to ensure all previously found discrepancies are corrected.
   9.3.12. Do not confuse the scope of a documentation evaluation done as part of an
   operational assessment (OA) with that for an OT&E. For an OA, the scope encompasses
   reviewing program-related documentation (ICD, CDD, etc.), as well as technical
   documentation. For an OT&E, the documentation evaluation focuses on technical manuals.
9.4. Documentation Measures.
   9.4.1. The documentation evaluation is primarily subjective in nature and typical measures
   include:
      9.4.1.1. MOS: Maintainer ratings of technical orders
      9.4.1.2. MOS: Operator ratings of manuals
   9.4.2. Potential objective measures (especially for software intensive systems) include:
      9.4.2.1. MOS: Percentage of tasks or procedures documented. This metric indicates
      how complete the documentation is at the time of OT&E. The percentage can be tracked
      over time to indicate a (maturing or non-maturing) trend.
      9.4.2.2. MOS: Percentage of procedures or tasks that are erroneous. This metric
      indicates the extent of unclear tasks, tasks that have too much detail, insufficient detail,
      etc.
9.5. Documentation Data Requirements. Data requirements usually include:
   9.5.1. Air Force Technical Order (AFTO) Form 158, Technical Order Review Comment
   Sheet, for preliminary T.O.s.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              51


   9.5.2. AFTO Form 22, Technical Manual (TM) Change Recomendation and Reply, or AFTO
   Form 27, Preliminary Technical Order (PTO) Publication Change Request, submitted in
   accordance with (IAW) T.O. 00-5-1. These are the DRs for technical data.
   9.5.3. Proposed and actual delivery schedules for publications and for validation and
   verification plans and schedules.
   9.5.4. Standard questionnaire formats and assistance, which can be obtained from the Det
   HSI technical lead or A-2/9 personnel.
   9.5.5. Minutes and worksheets from any specification reviews, prepublication reviews or
   other such meetings.
9.6. Documentation Evaluation Considerations. Evaluation/assessment of technical data
should determine the adequacy of the technical data to support mission requirements and to
identify technical data deficiencies. Technical data can be evaluated/assessed in terms of the
usability, completeness, correctness and understandability of the data for their intended use. This
is a qualitative assessment requiring judgmental criteria.
 52                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


                                            Chapter 10

                                           TRAINING

10.1. Introduction. The evaluation of training will focus on the adequacy of operator and
maintainer training, the transfer of training to the system’s tasks, and training in general for the
system under test. This training is important because inadequacies may impact effectiveness and
suitability measures. In addition, if the system under test is a training system, the test team may
evaluate training planning since training implementation typically occurs after OT. Training
requirements and general considerations when conducting training evaluations are discussed in
this chapter.
10.2. Training and Training Support. Training for a system under test consists of the
procedures, techniques, training devices, and equipment used to train civilian, active duty, and
reserve military personnel to operate and support the materiel system. This includes operator
and maintainer training, new equipment training, initial, formal, and on-the-job training. During
OT&E, the planned training program, along with any training devices and equipment, should be
evaluated.
10.3. Training OT&E Methodology. The test methodology includes the following activities
performed by test team evaluators:
   10.3.1. Review technical, reliability, and maintainability (R&M) data on system hardware,
   software, and support equipment to identify training-related issues.
      10.3.2. Examine    training created/delivered by developer for content completeness and
      adequacy.
      10.3.3. Review operational, maintenance, logistics and training support concepts to
      determine any unusual or adverse implications on training.
      10.3.4. Interview operations and maintenance personnel to identify areas where equipment
      design, configuration or complexity may impact training.
      10.3.5. Interview training personnel to assess the adequacy of training materials, equipment
      and technical data.
   10.3.6. Perform over-the-shoulder observations of operations and scheduled/unscheduled
   maintenance tasks to identify problems encountered by personnel and determine whether or
   not the problems are training-related.
   10.3.7. Develop and administer questionnaires/surveys to trained personnel upon completion
   of training courses and, as appropriate, operational test scenarios.
   10.3.8. Provide training evaluation status reports to the TD.
   10.3.9. Pay special attention to certification/recurring training requirements such as
   Weapons Loading, Engine Run Certification, etc.
   10.3.10. Provide the Det human systems integration (HSI) analyst with the information to
   perform a thorough technical review of all test plans/reports and test plan/report briefings.
   Provide A-9I with the information to perform a headquarters-level technical review of all test
   plans/reports and test plan/report briefings.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              53


10.3.11. Use surveys, interviews, or other appropriate techniques to evaluate/assess the transfer
of training to the system tasks that are required for correct performance on the job.
10.4. Training Considerations.
   10.4.1. The OT&E effort focuses on whether personnel with system training can operate,
   maintain, and support the system in its intended environment. This includes an assessment of
   specialties and skill levels required to perform base-level tasks, as well as the need for new
   specialties or unique training requirements for existing specialties to support system-unique
   requirements. It also includes evaluating training conducted prior to and in support of OT&E
   (usually Type 1) and providing information to assist in refining training requirements,
   technical training materials, and facilities required to support systems during operational use.
   It is sometimes possible to perform actual or inferred correlation between performance on
   training tasks and the tasks performed during operational test, which amounts to a measure of
   training transfer using an objective methodology. A training deficiency exists when the
   training provided does not address the skills needed to operate or maintain the system.
   10.4.2. Maintenance training should be analyzed for all levels of maintenance and include
   the training program and technical orders, as well as training aids, simulators, and support
   equipment used at each maintenance level.
   10.4.3. The Det HSI analyst can assist with the identification of training issues and
   measures, methods and resources for performing training evaluations. In addition, there are
   training questionnaires that are tailorable to your test program. Examples of appropriately
   stated training MOSs are:
       10.4.3.1. MOS: Maintainer ratings of technical training course.
       10.4.3.2. MOS: Maintainer ratings of on-the-job training.
       10.4.3.3. MOS: Operator ratings of unit-level training.
   10.4.4. Air Education and Training Command Studies and Analysis Squadron (AETC/SAS)
   can provide assistance in conducting training assessments on larger, more complex training
   systems/acquisitions. AETC/SAS is located at Randolph AFB, Texas and can be reached at
   210-652-5229 or DSN 487-5229.
 54                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


                                          Chapter 11

                         MODELING AND SIMULATION (M&S)

11.1. Introduction. Suitability M&S used in OT&E is discussed in this chapter. M&S tools
and several legacy tools available during the different phases of operational testing are also
discussed. Selection of specific M&S tools for use during OT&E is the responsibility of the
Detachment, with support from A-2/9 & A-5/8 analysts and the Operations Directorate (A-3) test
program manager. Specific guidance for using M&S is available in AFI 161001, Verification,
Validation and Accreditation (VV&A), and AFI 16-1002, Modeling and Simulation (M&S)
Support to Acquisition. Additional guidance for OT&E is available in AFI 99-103 and
AFOTECMAN 99-101.
11.2. Models and Simulations.
   11.2.1. In addition to real data available from automated data system reports, logistics
   management information (LMI) reports, and committee/team agreements, suitability
   evaluators often require M&S to help plan the suitability test, help perform an OA, and/or
   augment field test results. This is especially true if the OT&E lacks test realism because of
   various test limitations (e.g., not enough spares). Any M&S (in-house developed, contractor-
   developed, commercial off-the-shelf, or other) used in support of OT&E requires sufficient,
   documented verification and validation to support accreditation of the specific application
   and test process it supports (AFOTECMAN 99-101). Currently available suitability M&S
   languages and tools include:
       11.2.1.1. Weibull++. Part of ReliaSoft’s suite of reliability software products
       (http://www.reliasoft.com/products.htm), Weibull++ performs life data analyses
       utilizing multiple lifetime distributions, including all forms of the Weibull distribution,
       with a clear and concise interface geared toward reliability engineering.
       11.2.1.2. BlockSim. Another tool in ReliaSoft’s suite of reliability software products,
       BlockSim provides a comprehensive platform for system reliability, maintainability,
       availability, optimization, throughput, life-cycle cost and related analyses using the exact
       system reliability function and/or discrete event simulation. BlockSim provides
       sophisticated and flexible capabilities to model systems using a reliability block diagram
       (RBD) or fault tree analysis (FTA) approach.
       11.2.1.3. Reliability Growth Analysis (RGA). Developed in a joint effort between Dr.
       Larry Crow (http://www.reliasoft.com) and ReliaSoft Corporation, the leading authority
       in the field of RGA, along with key development partners in government and industry
       (including the U.S. Navy and John Deere), RGA combines comprehensive and powerful
       reliability growth analysis software with fielded (repairable) systems analysis capabilities
       for determining the optimum overhaul time and other results without the detailed data
       sets that would normally be required. Note: as of 26 Mar 2013, ReliaSoft software is not
       authorized on Air Force networks
       11.2.1.4. Matrix Laboratory (MATLAB)/Simulink. A product of The MathWorks,
       MATLAB (which stands for matrix laboratory) is a high-level language and interactive
       environment that performs computationally intensive tasks more efficiently than
       traditional programming languages. Capabilities include graphics features to visualize
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                55


     engineering and scientific data including 2-D and 3-D plotting functions, 3-D volume
     visualization functions, tools for interactively creating plots, and the ability to export
     results. Simulink is a platform for multi-domain simulation and Model-Based Design of
     dynamic systems. It provides an interactive graphical environment and a customizable
     set of block libraries that allow an analyst to accurately design, simulate, implement, and
     test control, signal processing, communications, and other time-varying systems. Visit
     www.mathworks.com for more information.
     11.2.1.5. Crystal Ball. Crystal Ball is a user-friendly, graphically oriented forecasting
     and risk analysis program that takes the uncertainty out of decision-making. This tool is
     useful for aggregating distributions when no known analytical method exists. Visit
     www.oracle.com for more information
  11.2.1.6. Rapid Availability Prototyping for Testing Operational Readiness
  (RAPTOR).        ARINC commercially markets RAPTOR which models system RAM
  characteristics based on a system’s reliability block diagram.         Visit
  www.arinc.com/products/raptor/index.html for more information.
     11.2.1.7. ReALOps. ReALOps is an integrated tool to perform reliability, availability
     and logistics analysis. The tool was developed by SAIC to produce maintenance metric
     indicators and outputs (such as MC, Ao, MTBM, MTBR, MTBCF, etc.) from
     probabilistic and exponential distributions.
  11.2.2. Some government-owned legacy models support M&S to varying degrees. Use of
  these models and tools will generally require modification and subsequent verification,
  validation, and accreditation for specific test program use. Examples include Logistics
  Composite model and Aircraft Readiness Model (ARM) The AFOTEC Catalog of Modeling,
  Simulation, and Analysis Tools located on A-2/9’s section of the the AFOTEC intranet
  contains links to various M&S repositories. These repositories in turn contain M&S
  descriptions and POC information.
  11.2.3. Test team members should consider these tools for their applications to OT&E.
  During OT&E, these tools may be applied in ways that are not generally seen in the field.
  11.2.4. Since tool availability, as well as the guidance that governs M&S, change frequently,
  it is important to consult the latest version of the instructions outlined in paragraph 11.1. It is
  good practice to contact A-2/9 to obtain information and assistance on current tools available,
  technical support, and direct M&S support.
 56                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


                                           Chapter 12

                                 HOW TO TAILOR TESTING

12.1. Introduction. General guidance on tailoring suitability testing is provided in this chapter.
Definitions and equations for specific suitability measures can be found in Attachment 5,
Sections A6A, A6B and A6C of this publication, AFPAM 63-128, and “Memorandum of
Agreement on Multi-Service Operational Test and Evaluation and Operational Suitability
Terminology and Definitions”.
12.2. Tailoring Suitability Testing for Systems (General). Tailoring suitability OT&E
involves identifying system characteristic measures that relate to the MOSs such as those defined
in Attachment 5, Sections A6A, A6B and A6C of this publication, establishing data requirements
for these measures, arranging to obtain the data from various sources, and planning the
evaluation of operational suitability. Each of these tasks should relate to the overall framework
established by the test concept. Discussion on how to accomplish the tailoring of tasks is given
in the remainder of this pamphlet.
      12.2.1. Data Requirements. Data required to calculate an MOS need to be measurable and
      collectible in test or the MOS needs to be answered using modeling and simulation. Data
      may vary with the test environment.
         12.2.1.1. During IOT&E and qualification operational test and evaluation (QOT&E), the
         test environment may not represent the intended operational environment. As a result,
         direct measurement (or demonstrated values) may be augmented with data from similar
         systems, testing, and contractor estimates to produce operationally meaningful estimates.
         Simulation models are then used to estimate measures such as Ao. Examples of these
         data include logistics delay times (LDTs), administrative delay times (ADTs), expected
         manpower, expected sparing, expected support equipment, number of each type of
         mission attempted (simulators), number of each type of mission completed (simulators),
         environmental factors (ground communication/weather systems), expected inventory
         size/authorization, and delivery schedules (munitions).
         12.2.1.2. During follow-on operational test and evaluation (FOT&E), the test
         environment is generally representative of the intended operational environment.
         Therefore, the following data may be used directly to calculate the MOSs: fully mission
         capable (FMC) hours; partially mission capable (PMC) hours; non mission capable
         (NMC) hours; number of sorties, number and type of assigned aircraft, personnel, and
         support equipment; number of possessed hours/days; uptime hours; downtime hours
         (ground communication/weather systems); and inventory size (munitions).
      12.2.2. Data Sources. As with data, data sources vary with the test environment.
         12.2.2.1. During IOT&E, data are available from contractor reports, other evaluation
         areas of the test, operational and maintenance concepts, weapon system and equipment
         LMI, automated data systems (such as Integrated Maintenance Data System (IMDS),
         G081, Systems Effectiveness Data System (SEDS), Micro-OMNIVORE, and integrated
         weapon system management (IWSM) databases), and similar systems. When using data
         obtained outside of OT&E, ensure that the SUT was production representative and
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             57


      evaluated in the operationally relevant environment, otherwise the data may be
      invalidated for OT&E use.
      12.2.2.2. During FOT&E, data are available from IMDS status data, from FOT&E
      logbooks, and other automated data systems.
   12.2.3. Evaluation of Suitability Elements.
      12.2.3.1. Once the MOSs are identified and agreed to by test participants, the method of
      evaluation is determined. Evaluation may be accomplished by directly measuring MOSs
      in test and/or, where appropriate, measuring MOSs and using M&S to extend the results
      of the field test. Once measured, the MOSs are compared to the user’s threshold as stated
      in the requirements document. The rating methodology and reporting differ for an OA,
      an operational utility evaluation (OUE), and an OT&Erefer to the AFOTECMAN 99-101
      for specific guidance on rating MOSs and COIs.
      12.2.3.2. The overall suitability evaluation (i.e., is the system suitable) should relate to
      the test concept by answering how well the suitability attributes contribute to the critical
      elements or operational objectives of the mission. If the system performs its tasks well or
      meets its required functional characteristics, then answer the overall question
      affirmatively .
12.3. Tailoring Suitability Testing for Joint Programs. The “Memorandum of Agreement on
Multi-Service Operational Test and Evaluation and Operational Suitability Terminology and
Definitions” governs OT&E of multi-service programs. This MOA is located on the AFOTEC
Intranet page. Each participating OTA should provide their OT&E requirements for
incorporation into the TEMP and/or OT plan.
 58                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


                                          Chapter 13

                   MANAGING OPERATIONAL SUITABILITY DATA

13.1. Introduction. Suitability data management used in OT&E is discussed in this chapter. A
discussion of common analysis tools and several automated tools available during test concept
development, test planning and test execution is also provided. Selection of specific suitability
data tools for aircraft, command, control, communications, computer, and intelligence (C4I) and
space systems is the responsibility of the test director (TD) with support from A-2/9 analysts.
13.2. Data Management Procedures.
   13.2.1. Data management is the process of identifying, collecting, processing, analyzing,
   reporting, storing and disposing test-related data. An effective data management system
   results in efficient use of resources and credible information upon which to draw test
   conclusions. Collecting reams of data is not necessarily effective. The test team should plan
   to collect the minimum amount of data required to support measuring the measures of
   suitability (MOSs).
   13.2.2. AFOTEC test teams use data management methods based on lessons learned from
   previous tests, the test plan, test team resources, and on-going program events. Before the
   test begins, detachment suitability personnel review the test plan to identify required data and
   determine whether the data will be collected by the test team or from activities conducted by
   external organizations (e.g., contractors and development testers). If data is used from tests
   conducted by others, test plans of those agencies should be reviewed to determine whether
   OT data requirements will be satisfied and production-representative testing occurred in an
   operationally realistic environment. If AFOTEC requirements are satisfied, coordinate with
   the external agency and collect the data for analysis. It is acceptable to use test and analysis
   reports from other agencies at the highest possible level of aggregation. If test team
   participation is required, they should jointly develop and coordinate plans to collect and
   analyze data.
13.3. Data Management and Analysis Plan (DMAP). Refer to AFOTECMAN 99-101 and the
DMAP template, located on the AFOTEC Intranet website.
   13.3.1. The DMAP is initiated during the Test Concept phase and refined during the Test
   Planning phase. The DMAP is the primary tool to ensure required data are identified,
   recorded, collected, reduced, processed, verified, analyzed and evaluated to support each test
   event. The DMAP is designed to be a working document used in the actual conduct of a test
   and is not intended for external coordination. It should provide a flexible means of updating
   test data requirements, data presentation format, planned analysis techniques, and captures
   the overall scope and methodology of the test program. It should address:
       13.3.1.1. Dissemination of data between different locations.
       13.3.1.2. Data processes to avoid duplication of effort.
       13.3.1.3. Focal points and responsibility centers for data management.
       13.3.1.4. Who will collect the data, how the data will be analyzed, how the data will be
       presented, application of detailed test procedures (DTPs), etc.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             59


       13.3.1.5. Calculations and data processing equipment to be used. The analysis
       techniques and evaluation procedures should spell out who will do what with which
       technique.
       13.3.1.6. Disposition policies for all test data describing the exact procedures and media
       to be used in storage.
   13.3.2. The DMAP is generally organized by COI, but may be tailored depending on the
   requirements of the test (see DMAP Template located on the AFOTEC Intranet website).
   13.3.3. The DMAP should specifically address any data requirements that differ from
   common practice or require special attention from the test team and specify in detail the data
   requirements, procedures, and definitions for these requirements. The DMAP should also
   address any data collection requirements unique to the test.
   13.3.4. Special attention should be given to the use of data from a developer. Data
   requirements should be coordinated with the system program office (SPO) at the earliest
   possible date to ensure they are specified by contract. Where possible, develop specific
   techniques to ensure the contractor data are representative of the production-ready system
   under operational conditions.
   13.3.5. The DMAP should include provisions for maintaining the data system. An example
   is external agency programming support available throughout the test program when required
   (as when the data processing system is developer-supplied). For any specially developed
   data programs, system maintenance requirements should receive special attention and should
   be specified in detail.
13.4. Tasks for Suitability Personnel.
   13.4.1. Specify the suitability database(s) in the AFOTEC test plan. The quality of any
   database output will be only as good as the quality of the data input to that database (garbage
   in/garbage out). Database selection should take into account measures of interest required to
   be supported by the data, quantity of data available and known data quality of the database.
   Products from Air Force Materiel Command (AFMC) data systems can be of significant
   benefit for logistics assessments during test planning and conduct. However, AFMC reports
   are often published approximately 60 days after the closeout date for each monthly or
   quarterly cycle. This reporting delay can significantly limit their usefulness in preparing
   evaluation reports at the end of test. Examples of typical database systems are:
       13.4.1.1. Micro-OMNIVORE. The AFOTEC-developed Micro-OMNIVORE enables
       the user to consolidate selected maintenance and operating time data and to perform
       detailed statistical analyses. Micro-OMNIVORE accepts data as input from IMDS,
       SEDS, and IMDS for airlift (GO81) (see all below).
       13.4.1.2. SEDS. SEDS is an Air Force Test Center RAM data acquisition, storage,
       retrieval, reporting, and analysis system developed primarily for test purposes (especially
       DT&E of aircraft systems). SEDS has many common elements with the standard Air
       Force maintenance data collection system, IMDS, but it is more useful because it
       provides a capability for a narrative presentation of discrepancies and corrective actions,
       delay codes, required AFSCs, ground support equipment data, diagnostic data, and
       technical data.
 60                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


         13.4.1.3. IMDS. IMDS, formerly known as Core Automated Maintenance System
         (CAMS), is the Air Force’s standard system for collecting maintenance data on
         operational Air Force systems. IMDS products can provide specific quantitative data for
         failures and discrepancies, maintenance actions, and maintenance work-hours for a
         specific calendar period.
         13.4.1.4. GO81. An automated maintenance management system used primarily by Air
         Mobility Command.
         13.4.1.5. Reliability    Engineering       Management          Information System
         (REMIS). REMIS is an Air Force standard data system for reliability information,
         which aggregates data and provides additional reports and analysis.
         13.4.1.6. IWSM Database. The program office may establish an IWSM database,
         containing LMI data, schedules, DR status, etc.
         13.4.1.7. Equipment Inventory, Status and Utilization Reporting System. AFI 21-
         103, Equipment Inventory, Status and Utilization Reporting, provides detailed historical
         records for systems availability and utilization reporting.
         13.4.1.8. Standard Base Supply System (SBSS). The SBSS is an automated inventory
         accounting system useful in analyzing organic supply support. In cases where supply
         support is not organic (for example, IOT&E), other supply measurement systems may
         need to be developed.
         13.4.1.9. Manual Databases. Manual databases include maintenance logbooks and
         videotape libraries.
         13.4.1.10. Test Team-Developed Databases. In some cases, no automated maintenance
         database exists and/or the system is too complex to rely on manual methods such as
         logbooks and video libraries. Alternatively, outputs from many of the above databases
         are best consolidated into one, for data analysis. The test team may develop databases
         using commercial off-the-shelf software (e.g., Excel or Access).
      13.4.2. In the test resource plan (TRP), include requirements for computers, programmer
      support (e.g. Access database programmers), video recorders, scanners, storage media,
      instrumentation and similar resources.
      13.4.3. Unique automated data processing requirements, such as data processing equipment
      or contractor support, should be coordinated with the AFOTEC communications and
      information team (A-6).
      13.4.4. Sustainment representatives should participate with test team personnel on the Joint
      Reliability and Maintainability Evaluation Team (JRMET) and Test Data Scoring Board
      (TDSB). The primary representative is the TD, assisted by the analyst/engineer. While
      AFOTEC is not bound by the JRMET’s interpretation of the data, an agreement is usually
      reached and a common database is established for use by all participating agencies. In cases
      where agreement is not reached, the specific items should be coded in the common RAM
      database for separate DT&E and OT&E analysis.
13.5. Other Data Sources. During multi-service test, AFOTEC personnel may use Army or
Navy data systems, particularly if the Air Force is not the lead service (“MOA on MOT&E” This
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             61


MOA is located on the Policy Center page under the AFOTEC Policy menu item of the
AFOTEC Intranet home page)
   13.5.1. Army Data Systems. Suitability personnel participating in MOT&E with the Army
   as lead, should obtain a copy of Department of Army Pamphlet (DAP) 73-1, Test and
   Evaluation in Support of Systems Acquisition. This document describes the Army Test
   Incident        Reporting       System         and          is       available      at
   http://www.apd.army.mil/jw2/xmldemo/p73_1/head.asp as of 8 Mar 2013.
   13.5.2. Navy Data Systems.
      13.5.2.1. Naval Aviation Maintenance and Materiel Management (Aviation 3-M)
      System. Aviation 3-M combines RAM and logistics supportability data into one system.
      13.5.2.2. Navy Ship’s Maintenance and Materiel Management System (3-M). Ship’s
      3-M system is analogous to the aviation 3-M systemit contains active and mothballed
      equipment, a maintenance data processing system, and an alteration management system
      for shipboard configuration control.
   13.5.3. Government-Industry Data Exchange Program (GIDEP). The GIDEP is a
   cooperative activity between government and industry participants to automatically exchange
   technical data essential in the research, development, production and operational life cycle of
   systems and equipment. The GIDEP maintains specialized data banks, which are available to
   government and industry.
   13.5.4. Defense Logistics Studies Information Exchange (DLSIE). DLSIE is chartered by
   DoDI 5154.19, Defense Logistics Studies Information Exchange (DLSIE). DLSIE collects,
   organizes, stores, and disseminates information pertaining to logistics studies, models,
   management information, and related documentation, which may benefit the DoD logistics
   management and research community. By reviewing the DLSIE collection, logistics
   research activities can avoid spending defense dollars on previously completed research.
   13.5.5. Defense Technical Information Center (DTIC). DTIC, a primary field activity of
   the Defense Logistics Agency, is the central banking institution for DoD’s collection of
   research and development (R&D) information in virtually all fields of science and
   technology. DTIC has the mission to exploit the contents of its collection to answer three
   basic questions relative to the research, development, test and evaluation (RDT&E) program
   of the DoD: what research is being planned; what research is currently being performed; and
   what results were realized by completed research? The AFOTEC historian (HO) is the focal
   point for interfacing with DTIC. HO maintains a significant number of DTIC documents and
   periodically receives a catalog of new publications from DTIC.
   13.5.6. Logistics Management Information (LMI). LMI consists of summaries and data
   required by the government to perform acquisition logistics management functions. The
   LMI is important to suitability test planning and execution because it is the major source of
   data used to develop the support system. The LMI should provide detailed information on
   how the SPO intends to address each of the 10 integrated logistics support (ILS)
   elementsextremely useful in the OT&E suitability analysis effort. It also shows the detailed
   factors on which the support system is based. These can be used to reveal potential problem
   areas.
62                                                  AFOTECPAM 99-104 24 SEPTEMBER 2013


13.6. JRMET.
  13.6.1. Air Force policy, including AFI 99-103, dictates test and operational data collection
  and analysis systems be complementary to each other to verify RAM performance
  throughout the system or equipment life cycle. The purpose of the JRMET is to review raw
  RAM data for accuracy, completeness, and operational relevance.
     13.6.2. The AFMC program manager (PM) is responsible for establishing a JRMET (or
     similar team), to assist in reliability and maintainability data collection, analysis, verification,
     and categorization during DT&E and OT&E. The system program office (SPO) establishes
     and chairs the JRMET during DT&E and IT&E. If for any reason the program office
     chooses not to chair or participate, AFOTEC may chair the JRMET. During dedicated
     OT&E, the AFOTEC TD (or designated representative) chairs the JRMET. Participants
     include representatives from the supporting and operating commands, the DT&E and OT&E
     test teams, and developer when appropriate.
     13.6.3. Specific responsibilities of JRMET participants are specified in the JRMET charter.
     The charter also states policy for the joint use of RAM data, exchanges of RAM information,
     classification criteria for system-related failures/faults, and administrative procedures for
     conducting JRMET meetings. The intent of having a charter is to avoid duplication of effort.
     If a charter is not established, the JRMET functions normally will be assigned within the
     SPO. A-2/9 has several JRMET charter templates for use when interfacing with the SPO.
     13.6.4. JRMET meetings serve as an ideal forum for reviewing RAM data, familiarization
     with RAM terminology, obtaining joint agreement on the use of RAM data, and ensuring
     data accuracy. JRMET meetings should be held periodically as RAM data are collected.
     Actual operational suitability T&E data reduction, analysis, and evaluation are normally not
     done at the meetings; rather, the JRMET logistics personnel should accomplish this after
     each JRMET meeting to obtain estimates of applicable MOSs.
     13.6.5. AFOTEC involvement in the JRMET entails understanding the data for OT&E
     purposes and assisting the SPO and developer in data interpretation from an operational
     perspective. Whereas AFOTEC is not bound to contractual interpretation of test data,
     agreements are usually reached to establish a common database for use by all participating
     agencies. As test team members may be unfamiliar with the JRMET organization and
     functions, they should review the JRMET charter and convene a pre-JRMET meeting
     consisting of suitability, and logistics personnel, and appropriate operating command
     personnel. The pre-JRMET meeting is normally held one day before the formal JRMET
     meeting. Key questions to answer at the pre-JRMET/JRMET meetings are:
        13.6.5.1. Is a failure relevant or test specific? Treat all failures as relevant until the
        developer or SPO proves otherwise.
        13.6.5.2. Did the failure critically impact the mission?
        13.6.5.3. Do computed values of RAM measures relate to contractual and/or operational
        requirements?
        13.6.5.4. Are values of RAM measures expressed as progress points on a growth curve,
        end of test values, or as mature system values?
        13.6.5.5. Was the failure software-related?
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              63


       13.6.5.6. Has a failure-related DR been written, or is one needed?
   13.6.6. During IOT&E, software failures should be scored the same as hardware failures
   since both have a similar impact on mission performance. Software failures may be tracked
   separately for the purpose of providing additional insights, but all software failures should be
   counted toward reliability metrics. Exclusion of system failures or maintenance actions in
   RAM calculations is at the discretion of the JRMET, but should be documented. AFOTEC
   measures maintainability at the organizational or field level. Any maintenance at the field
   level in support of software counts as repair time. This includes time to reboot or restore the
   system, data backups, system administration and configuration management.
   13.6.7. In summary, the JRMET allows AFOTEC to reconcile data differences with the SPO
   or contractor. If agreement cannot be reached on scoring specific data, AFOTEC reserves
   the right to flag the data for OT&E use and report using the flagged data. Other agencies
   also benefit by gaining the latest test information that may be used in updating logistics
   plans, LMI, initial provisioning estimates and life-cycle cost estimates. Although the main
   concern of the JRMET is scoring RAM data, the TD verifies all test data. A-2/9 maintains a
   framework for JRMET charters and provides assistance in establishing JRMET function.
13.7. Test Data Scoring Board (TDSB). The TDSB is a government-only forum to score and
categorize the data reviewed and accepted by the JRMET. The purpose of the TDSB is to
remove perception of developer bias in the data-scoring process. The PM establishes and chairs
the TDSB during DT&E and IT&E. During dedicated OT&E, the AFOTEC TD (or designated
representative) chairs the TDSB. Participants include representatives from the supporting and
operating commands and the DT&E and OT&E test teams, but exclude the developer.
Restrictions stated in Public Law (US Code, Title 10) prohibit all system developers from TDSB
participation. Although the main concern of the TDSB is scoring RAM data, the TD verifies all
test data. If agreement cannot be reached on scoring specific data, AFOTEC reserves the right to
flag data for OT&E use and report using the flagged data. A-2/9 may provide assistance in
establishing a TDSB.
 64                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


                                          Chapter 14

                                DEFICIENCY REPORTING

14.1. Introduction. Deficiency reports may arise while testing and evaluating any of the
operational suitability elements. This chapter discusses deficiency categories and AFOTEC’s
roles and responsibilities with respect to deficiency reporting.
14.2. Deficiency Reporting.
   14.2.1. AFOTEC is responsible for supporting the Deficiency Report (DR) process by
   reporting deficiencies found during OT&E. The integrated test team (ITT) charter designates
   a sub-group to define and document DR strategy and procedures consistent with T.O. 00-
   35D-54 and AFI 99-103 (T.O. 00-35D-54 and AFI 99-103 contain detailed information
   about the DR process). DR strategy and accompanying procedures are established prior to
   milestone B. Deficiency report status is included in the final OT&E report. Deficiencies that
   impact the operational safety, suitability and effectiveness (OSS&E) of systems or equipment
   in development, test or employment are required to be reported (see Table 14.1.). Deficient
   conditions are identified according to criteria and report type and categorized according to
   their impact to mission.

Table 14.1. Attributes which may affect OSS&E
          Compatibility                                     Malfunction
          Design                                            Quality
          Difficulty of operation or maintenance            Reliability
          Effectiveness                                     Reparability
          Environmental                                     Safety
          Expense of operation or maintenance               Security
          Fidelity/conformity of technical publications     Suitability
          Human Factors                                     Survivability
          Integration                                       Training Fidelity
          Interoperability                                  Undocumented features
          Logistics supportability                          Utility
          Maintainability                                   Vulnerability

   14.2.2. Category (CAT) I deficiencies require the immediate attention and response of the
   system Program Manager and Chief/Lead Engineer to mitigate risk and/or limit/resolve
   mission impact. Strict application of CAT I criteria is essential. If a Category I condition is
   noted or suspect, assess safety, mission or operational impact and include a detailed
   statement outlining the safety, mission or operational impact to the system or end item.
   Category II deficiencies impede or constrain successful mission accomplishment (system
   impacts OSS&E but does not meet the safety or mission impact criteria of a Category I
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                65


   deficiency). Category II deficiencies may also include recommended enhancements that
   improve or complement successful mission accomplishment, but are not absolutely required.
   An enhancement report should not be designated as such solely due to an “out-of-scope”
   condition as described in contractual requirements (see Table 14.2.). Program Managers will
   not close deficiencies as enhancements solely because they are “out of scope” to contractual
   requirements. Enhancement deficiencies will be assessed as a potential requirement and
   prioritized based upon their impact to OSS&E.

Table 14.2. DR Category and Priority Determination
Annotate the DR Category (I or II) and the corresponding priority. Submit a Category I DR and
assign the corresponding priority when a condition:
CAT I     Priority      Impact
          Emergency     If uncorrected, may cause death, severe injury, or severe occupational
                        illness and no workaround is known; or, if uncorrected, may cause
                        major loss or damage to equipment or a system and no workaround is
                        known; or, prevents the accomplishment of an essential capability or
                        critically restricts OSS&E, to include required interaction with other
                        mission critical platforms or systems, and no acceptable workaround is
                        known.
          Urgent        Adversely affects an essential capability or negatively impacts OSS&E
                        and no acceptable workarounds are known or adversely affects
                        technical, cost or schedule risks to the project or to life cycle support of
                        the system, or, results in a production line stoppage and no acceptable
                        workaround is known.

When the condition does not meet the safety or mission impact criteria of a Category I report,
submit a Category II DR with the corresponding priority when the condition:
CAT II     Priority    Impact
           Urgent      Adversely affects an essential capability or negatively impacts OSS&E
                       and adequate performance is achieved through significant compensation
                       or acceptable workaround and or adversely affects technical, cost or
                       schedule risks to the project or to life cycle support of the system, but an
                       acceptable workaround is known.
           Routine     Does not affect an essential capability but may result in user/operator
                       inconvenience or annoyance. Adequate performance is achieved through
                       minimal compensation. Results in inconvenience or annoyance for
                       development or maintenance personnel, but does not prevent the
                       accomplishment of the task. Adequate performance is achieved through
                       minimal compensation. Any other effect, i.e., enhancements having little
                       or no impact to OSS&E under current requirements.
 66                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


NOTES:
1. Careful consideration should be given in assigning the category and corresponding priority
recommendation to accurately define the deficiency’s impact.
2. Prior to test, the test team and program office shall ensure understanding and consensus of
priority definitions. If required, definitions may be further defined in the local operating
procedures to support the individual test program.
3. T&E deficiency category and priority will be determined by the test director. Subsequent
changes may occur only with consensus of primary Materiel Improvement Project Review Board
(MIPRB) members (program office, lead operating command, and applicable test director). See
AFI 99-103, Capabilities-Based Test and Evaluation for additional T&E information.
4. Originators/Originating Points should consider and document factors such as cost, schedule
and performance risks; availability of spares; difficulty of operation or maintenance, repair, or
replacement; system redundancy; associated trends; secondary failures or damages; and
environmental impacts among other possible factors.
5. Workarounds refer to approved/authorized alternate procedures which could include, but are
not limited to: manual processes, order of task accomplishment, more restrictive or intensive
procedures, and the use of back-up or redundant systems or processes, etc.

   14.2.3. The following are typical OT&E DR concerns which require follow-up during and
   after completion of testing: verify program managers do not close out DRs without
   consensus of the stakeholders (e.g., users and operational testers); if a contractor DR
   database is used, verify seamless portability into the government system has been
   accomplished; and ensure open DRs are transferred to program office POCs upon completion
   of AFOTEC involvement.
   14.2.4. Classified Deficiency Reporting. Ensure coordination with the applicable PM
   representative and provide information per the applicable program security guide.




                                              SCOTT D. WEST
                                              Major General, USAF
                                              Commander
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                      67


                                       Attachment 1
         GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION

References
ACCI 21-165, Aircraft Flying and Maintenance Scheduling Procedures, 22 Apr 2008
AFI 10-401, Air Force Operations Planning and Execution, 07 Dec 2006
AFI 16-1001, Verification, Validation and Accreditation (VV&A), 01 Jun 1996
AFI 16-1002, Modeling and Simulation (M&S) Support to Acquisition, 01 Jun 2000
AFI 21-101, Aircraft and Equipment Maintenance Management, 26 Jul 2010
AFI 21-103, Equipment Inventory, Status and Utilization Reporting, 26 Jan 2012
AFI 32-1024, Standard Facility Requirements, 14 Jul 2011
AFI 33-360, Publications and Forms Management, 07 Feb 2013
AFI 38-201, Management of Manpower Requirements and Authorizations, 26 Sep 2011
AFI 63-101, Integrated Life Cycle Management, 07 Mar 2013
AFI 63-1201, Life Cycle Systems Engineering, 23 Jul 2007
AFI 90-801, Environment, Safety, and Occupational Health Councils, 25 Mar 2005
AFI 99-103, Capabilities-Based Test and Evaluation, 26 Feb 2008
AFI 16-1001, Verification, Validation and Accreditation (VV&A), 01 Jun 1996
AFI 16-1002, Modeling and Simulation (M&S) Support to Acquisition, 01 Jun 2000
AFPAM 63-128, Guide to Acquisition and Sustainment Life Cycle Management, 05 Oct 2009
AFPAM 90-902, Operational Risk Management (ORM) Guidelines and Tools, 14 Dec 2000
AFPD 32-70, Environmental Quality, 20 Jul 1994
AFPD 48-1, Aerospace Medicine Enterprise, 23 Aug 2011
AFPD 63-1, Integrated Life Cycle Management, 03 Jul 2012
AFPD 90-8, Environment, Safety, & Occupational Health Management and Risk Management,
02 Feb 2012
AFPD 91-2, Safety Programs, 03 Jul 2012
AFPD 99-1, Test and Evaluation Process, 22 Jul 1993
AFOTEC Operational Test and Evaluation (OT&E) Guide, 8th Edition, 10 Sep 2012
Refer requests for copies of the OT&E Guide to AFOTEC A-3E, 1251 Wyoming Blvd SE,
Kirtland AFB, NM 87117, AFOTEC.A3.Workflow@kirtland.af.mil DSN: 246-5238
AFOTEC Questionnaire Guide, 02 Feb 2009
AFOTECMAN 99-101, Operational Test Processes and Procedures, 11 Oct 2012
AFMAN 33-363, Management of Records, 01 Mar 2008
 68                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


AFMAN 63-119, Certification of System Readiness for Dedicated Operational Test and
Evaluation, 20 Jun 08
Air Force Records Disposition Schedule (RDS) at https://www.my.af.mil/gcss-
af61a/afrims/afrims
Army’s List of Supportability Metrics, https://www.logsa.army.mil/lec/
CJCSI 6212.01E, Interoperability and Supportability of Information Technology and National
Security Systems, 15 Dec 2008
CJCSI 6212.01F, Net Ready Key Performance Parameter (NR KPP), 21 Mar 2012
CJCSI 3170.01H, Joint Capabilities Integration and Development System, 10 Jan 2012
DAP 73-1, Test and Evaluation in Support of Systems Acquisition, 30 May 2003
Defense Acquisition Guidebook (DAG), 01 Nov 2012
Department of Army Pamphlet (DAP) 73-1, Test and Evaluation in Support of Systems
Acquisition, 30 May 2003
DISA website at www.disa.mil
DoD Directive 5000.59, DoD Modeling And Simulation (M&S) Management, 08 Aug 2007
DoD Guide for Achieving Reliability, Availability and Maintainability, Aug 2005
DoD Instruction 5000.02, Operation of the Defense Acquisition System, 08 Dec 2008
DoD Instruction 5154.19, Defense Logistics Studies Information Exchange (DLSIE), 13 Jul 1972
Department of Defense Reliability, Availability, Maintainability and Cost Rationale Report
Manual, 24 Jun 2009
DOT&E Operational Suitability Guide, Volume I – A Tutorial, 03 Feb 1990
DSMC, Glossary: Defense Acquisition Acronyms and Terms, 06 Nov 2009
Executive Order 12196: Occupational Safety and Health Programs for Federal Employee, 26
Feb 2008
Executive Order 13423: Strengthening Federal Environmental, Energy, and Transportation, 24
Jan 2007
International Organization for Standardization (ISO) 4802
JCIDS Manual, Joint Capabilities Integration and Development System, 10 January 2012
JITC website at http://jitc.fhu.disa.mil/
JP 1-02, Department of Defense Dictionary of Military and Associated Terms, 08 Nov 2012
Memorandum of Agreement on Multi-Service Operational Test & Evaluation (MOT&E) and
Operational Suitability Terminology and Definitions, 2012
MIL-HDBK-470A, Designing and Developing Maintainable Products and Systems, 04 Aug
1997
MIL-STD-882E, Standard Practice for System Safety, 11 May 2012
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                       69


MIL-STD-3022, Documentation of Verification, Validation, and Accreditation (VV&A) for
Models and Simulations, 28 Jan 2008
T.O. 00-5-1, AF Technical Order System, 15 Jan 2013
T.O. 00-5-3, AF Technical Order Life Cycle Management, 01 Jan 2013
T.O. 00-20-1, Aerospace Equipment Maintenance Inspection, Documentation, Policies, and
Procedures, 15 Jun 2011
T.O. 00-20-3, Maintenance Processing of Repairable Property and the Repair Cycle Asset
Control System, 02 Feb 2009
T.O. 00-35D-54, USAF Deficiency Reporting, Investigation, and Resolution, 15 Oct 2012
Adopted Forms
AF IMT 847, Recommendation for Change of Publication, 22 Sep 09
AFTO Form 22, Technical Manual (TM) Change Recommendation and Reply, 26 Jul 10
AFTO Form 27, Preliminary Technical Order (PTO) Publication Change Request, 10 Jul 09
AFTO Form 158, Technical Order Review Comment Sheet, 13 Aug 08

Abbreviations and Acronyms
3-M—maintenance and materiel management
AALPS—Automated Air Load Planning System
ACAT—acquisition category
ACCI—Air Combat Command Instruction
ACS—agile combat support
ADT—administrative delay time
AETC/SAS—Air Education and Training Command Studies and Analysis Squadron
AETF—Air Expeditionary Task Force
AFI—Air Force instruction
AFMAN—Air Force manual
AFMC—Air Force Materiel Command
AFOTEC—Air Force Operational Test and Evaluation Center
HO—AFOTEC History Office
AFOTECMAN—AFOTEC Manual
AFOTECPAM—AFOTEC pamphlet
AFPAM—Air Force Pamphlet
AFPD—Air Force policy directive
AFSC—Air Force specialty code
 70                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


AIS—automated information systems
ALC—air logistics center
ALDT—administrative and logistics delay time
Aa—achieved availability
Ai—inherent availability
Ao—operational availability
As—stockpile availability
ATE—automatic test equipment
ATO—Air Tasking Order
Aviation 3M—Aviation Maintenance and Material Management
BDR—battle damage repair
BIT—built-in test
BIT/FIT—built-in test/fault isolation test
BITE—built-in test equipment
BR—break rate
C4I—command, control, communications, computers and intelligence
CAMS—Core Automated Maintenance System
CAT—category
CBR—chemical, biological and radiological
CDD—capability development document
CF—critical failure
CFR—Code of Federal Regulations
C-I—compatibility and interoperability
CJCSI—Chairman Joint Chiefs of Staff Instruction
CLS—contractor logistics support
CM—corrective maintenance
CMT—corrective maintenance time
CND—cannot duplicate or could not duplicate
COI—critical operational issue
CONOPS—concept of operations
CPD—capability production document
DAG—Defense Acquisition Guidebook
AFOTECPAM 99-104 24 SEPTEMBER 2013                                        71


DAP—Department of Army Pamphlet
DART—deficiency analysis ranking technique
DE—downing event
DIFM—due in from maintenance
DLSIE—Defense Logistics Studies Information Exchange
DMAP—data management and analysis plan
Do—operational dependability
DoD—Department of Defense
DoDI—DoD instruction
DOT&E—Director, Operational Test and Evaluation
DR—deficiency report
DSMC—Defense Systems Management College
DT—developmental test
DT&E—developmental test and evaluation
DTIC—Defense Technical Information Center
DTP—detailed test procedures
E3/SM—Electromagnetic Environmental Effects and Spectrum Management
EMC—electromagnetic compatibility
EMI—electromagnetic interference
EOQ—economic order quantity
ESOH—environment, safety and occupational health
ESOHC—Environment, Safety and Occupational Health Council
ESOHCB—Environment, Safety and Occupational Health Certification Board
ESOH—MS - Environment, Safety and Occupational Health Management Survey
FAR—false alarm rate
FDE—force deployment evaluation
FH—flight hour
FH/LU—flying hours per life unit
FMC—fully mission capable
FOT&E—follow-on operational test and evaluation
FR—fix rate
FTA—fault tree analysis
 72                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


GIDEP—government-industry data exchange program
GIG—Global Information Grid
GO81—IMDS for Airlift
HCI—hardness critical item
HF—human factors
HFE—human factors engineering
HM/HS—hardness maintenance/hardness surveillance
HQ—headquarters
HSI—human system integration
HSP—health and safety plan
IA—Information Assurance
IAW—in accordance with
ICD—initial capabilities document
ICS—interim contractor support
ID—integrated diagnostics
ILS—integrated logistics support
IMDS—Integrated Maintenance Data System
IMT—information management tool
IOC—initial operational capability
IOT&E—initial operational test and evaluation
IRSP—in place readiness spares package
ISO—International Organization for Standardization
ISSL—initial spares support list
ITD—initial test design
ITT—integrated test team
IWSM—integrated weapon system management
JCALS—Joint Computer-Aided Acquisition and Logistics System
JITC—Joint Interoperability Test Command
JP—joint publication
JRMET—Joint Reliability and Maintainability Evaluation Team
KPP—key performance parameter
KSA—key system attribute
AFOTECPAM 99-104 24 SEPTEMBER 2013                    73


LCMP—life cycle management plan
LCOM—logistics composite model
LDT—logistics delay time
LMI—logistics management information
LOGMOD—Logistics Module
LRU—line-replaceable unit
LSA—logistics support analysis
LU—life unit
M&S—modeling and simulation
MAIS—Major Automated information System
MAJCOM—major command
MATLAB—matrix laboratory
MC—mission capable
MDA—milestone decision authority
MDAP—Major Defense Acquisition Program
MDT—mean downtime
M-demos—maintenance demonstration
MESL—minimum essential subsystem list
MIL—HDBK - military handbook
MIL—STD - military standard
MIPRB—Materiel Improvement Project Review Board
MISCAP—mission capability
MMH—maintenance man-hours
MMH/LU—maintenance man-hours per life unit
MOA—memorandum of agreement
MOE—measure of effectiveness
MOHBFA—mean operating hours between false alarms
MOS—measure of suitability
MOT&E—multi-service operational test and evaluation
MP—maintenance personnel
MPT—manpower, personnel, training
MRSP—mobility readiness spares package
 74                                           AFOTECPAM 99-104 24 SEPTEMBER 2013


MRT—mean repair time
MT—maintenance time
MTBCF—mean time between critical failures
MTBD—mean time between demands
MTBDE—mean time between downing events
MTBF—mean time between failures
MTBM—mean time between maintenance
MTBME—mean time between maintenance events
MTBOMF—mean time between operational mission failures
MTBR—mean time between removals
MTBSM—mean time between scheduled maintenance
MTBUM—mean time between unscheduled maintenance
MTTFL—mean time to fault locate
MTTR—mean time to repair
MTTRF—mean time to restore function
NAP—nuclear assessment plan
NEPA—National Environmental Policy Act
NMC—not mission capable
NMCB—not mission capable both (i.e., maintenance and supply)
NMCM—not mission capable maintenance
NMCS—not mission capable supply
NR KPP—net-ready key performance parameter
NRTS—not repairable this station
OA—operational assessment
OC—operational capability
OH—operating hours
OMF—operational mission failure
O&S—operations and support (usually used in relation to cost)
OPLAN—operation plan
OPR—office of primary responsibility
ORM—operational risk management
OSS&E—operational safety, suitability and effectiveness
AFOTECPAM 99-104 24 SEPTEMBER 2013                                        75


OT—operational test
OTA—operational test agency
OT&E—operational test and evaluation
OUA—operational utility assessment
OUE—operational utility evaluation
PBL—performance-based logistics
PESHE—Programmatic ESOH Evaluation
Pcnd—percent cannot duplicate
Pfa—percent BIT false alarms
Pfd—percent BIT fault detection
Pfi—percent BIT fault isolate
Prtok—percent retest okay
PH&S—packaging, handling and storage
PHS&T—packaging, handling, storage and transportation
PM—preventive maintenance or program manager (context sensitive)
PMC—partially mission capable
PMCB—partially mission capable both
PMCM—partially mission capable maintenance
PMCS—partially mission capable supply
PMT—preventive maintenance time
POC—point of contact
QOT&E—qualification operational test and evaluation
R&D—research and development
R&M—reliability and maintainability
RAM—reliability, availability and maintainability
RAPTOR—rapid availability prototyping for testing operational readiness
RBD—reliability block diagram
RDS—Records Disposition Schedule
RDT&E—research, development, test and evaluation
ReALOps—Reliability, Availability, and Logistics Operations
REMIS—Reliability and Maintainability Information System
RGA—Reliability Growth Analysis
 76                                         AFOTECPAM 99-104 24 SEPTEMBER 2013


RLA—repair-level analysis
Rm—mission reliability
RS—storage reliability
RTOK—retest okay
SATAF—site activation task force
SBSS—Standard Base Supply System
SE—support equipment
SEDS—Systems Effectiveness Data System
SEP—systems engineering plan
SGR—sortie generation rate
SII—special interest item
SMR—source, maintainability and recoverability
SPA—sorties per aircraft
SRU—shop-replaceable unit
SPO—system program office
SSWG—sortie surge weapons generation
ST—standby time
SUT—system under test
T&E—test and evaluation
TD—test director
TDSB—test data scoring board
TEMP—test and evaluation master plan
TM—technical manual
TO—tasking order
T.O.—technical order
TRP—test resource plan
TRR—test readiness review
UHF—ultrahigh frequency
UR—utilization (or usage) rate
USAF—United States Air Force
UT—uptime
UTC—unit type code
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              77


UTE—utilization rate
VV&A—verification, validation and accreditation
WSR—weapon system reliability

Terms
Abort—Failure to accomplish a mission for any reason, other than enemy action. It may occur
at any point from initiation of operation to destination. (JP 1-02)
Accreditation—The official determination that a model or simulation (or other test capability) is
acceptable for a specific purpose. (DODD 5000.59)
Acquisition—The conceptualization, initiation, design, development, test, contracting,
production, deployment, and logistics support, modification and disposal of weapon and other
systems, supplies, or services (including construction) to satisfy DoD needs, intended for use in
or in support of military missions. (Defense Systems Management College (DSMC) Glossary)
Acquisition Category (ACAT)— Acquisition categories determine the level of review, decision
authority, and applicable procedures. They facilitate decentralized decision making and
execution, and compliance with statutory imposed requirements. There are three ACATs based
on research, development, T&E (RDT&E) and/or procurement costs stated in fiscal year (FY)
2000 dollars. (Defense Systems Management College (DSMC) Glossary)
Acquisition Process—DoDI 5000.02, defines the acquisition process as Pre-System Acquisition,
Systems Acquisition, and Sustainment and Disposal. Normally, this process consists of discrete
logical phases separated by major decision points, called milestones. The phases span the life
cycle of a weapon system and provide a means of progressively translating broadly stated
mission needs into well-defined system-specific requirements.
Pre—System Acquisition—Pre-System Acquisition is composed of on-going in development of
user needs, science and technology, and concept development work specific to the development
of a material solution to an identified need. One path into systems acquisition begins with
examining alternative concepts to meet a stated mission need. This path begins with a decision
to enter concept and technology development at milestone A. The phase ends with the selection
of a system architecture and the completion of entrance criteria into milestone B and system
development and demonstration phase.
Systems Acquisition—Systems acquisition is the process of developing concepts into
producible and deployable products that provide capability to the warfighter. The concepts to
exploit in systems acquisition are based on analysis of alternative ways to meet the military need,
including commercial and non-developmental technologies and products and services
determined through market analysis. Systems Acquisition consists of two phases: system
development and demonstration initiated by a milestone B decision, and production and
deployment initiated by a milestone C decision.
The purpose of the system development and demonstration phase is to complete the
discovery process, develop a system, reduce program risk, ensure system supportability,
design for producibility, assure affordability and demonstrate system integration and
utility. Discovery and development are aided by the use of simulation—based acquisition
and test and evaluation and guided by a system acquisition strategy and TEMP. The purpose of
 78                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


milestone B is to authorize entry into system development and demonstration. Entrance into
system development and demonstration is dependent on three things: technology maturity
(including software), validated requirements and funding. Prior to entering system development
and demonstration, there will be requirements documents validated by the requirements
authority. The requirements documents contain operational performance requirements and
addressing cost for a proposed concept or system.
The objectives of the production and deployment phase are to achieve an operational
capability that satisfies mission needs. The purpose of milestone C is to authorize entry
into low—rate initial production for Major Defense Acquisition Programs (MDAP) and major
systems or into production or procurement for non-major systems that do not require low-rate
production. The production requirement of this phase does not apply to Major Automated
Information Systems (MAIS) or software-intensive systems with no developmental hardware
components. However, software has to prove its maturity level prior to deploying to the
operational environment. Once maturity has been proven, it is baselined and a methodical and
synchronized deployment plan is implemented to all applicable locations.
Sustainment and Disposal—The objectives of this activity are the execution of a program of
support that meets support performance requirements and sustainment of systems in the most
cost-effective manner for the life cycle of the system.
The sustainment program includes all elements necessary to maintain the readiness and
operational capability of weapon and other materiel systems. The scope of support varies
among programs but generally includes supply, maintenance, transportation, sustaining
engineering, data management, configuration management, manpower, personnel,
training, safety, occupational health, C4I and environmental management functions. This
activity also includes the execution of operational support plans. A follow—on operational
test and evaluation program that evaluates operational effectiveness, survivability, suitability and
interoperability, and that identifies deficiencies will be conducted, as appropriate.
       At the end of its useful life, a system must be demilitarized and disposed. The PM will
       address in the acquisition strategy demilitarization and disposal requirements. During
       demilitarization and disposal, materiel determined to require demilitarization is controlled
       and disposed in a way that minimizes DoD’s liability due to environmental, safety,
       security and occupational health issues.
Air Abort—A failure of an airborne aircraft so that it cannot effectively accomplish its primary
or alternate scheduled mission because of a reported malfunction.
Availability:—Availability (Launch, Space, Control, User)—A measure of the degree to
which the segment (launch, space, control, and user) is in an operable and committable state at
the start of a mission when the mission is called for at any (random) time. (AFPAM 63-128)
Availability (Materiel)—A measure of the percentage of the total inventory of a system
operationally capable (ready for tasking) of performing an assigned mission at a given time,
based on materiel condition. (JCIDS Manual)
Availability (Operational)—Ao is the probability that a system can be used for any specified
purpose when desired. It includes both the inherent R&M parameters and logistics support
effectiveness of the system that relates to the total time the system might be desired for use.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             79


Availability (Stock)—As is the probability a system or weapon’s specified resources are
available for use (not in the repair pipeline) over a storage life at a random point in time.
(AFPAM 63-128)
Bench Check—A workshop check for the condition, completeness, or working order of a piece
of equipment. (AFI 21-101, Aerospace Aircraft and Equipment Maintenance)
Cannibalization—The authorized removal of specific components from one item of Air Force
property for installation on another item of Air Force property to meet priority requirements with
the obligation of replacing the removed components. (AFI 21-101)
Cannibalization Rate—A measure of on-equipment cannibalization actions (removals)
performed to keep an end item in operationally ready condition. The rate may be expressed as
average cannibalization per sortie, per 1,000 flying hours, or other life unit. (AFI 21-101)
Cannot Duplicate (CND)—A maintenance event for which an operationally observed or
recorded malfunction for a system or subsystem cannot be duplicated or confirmed by
maintenance personnel. CND should be reported as rate per sorties, flying hours or operating
hours (OH). (AFPAM 63-128)
Code 3—a grounding condition for which the aircraft is unable to meet at least one of its
wartime missions.
Compatibility—Capability of two or more items or components of equipment or material to
exist or function in the same system or environment without mutual interference. (JP 1-02) The
capability of a system to be operated, maintained, and resupplied by persons wearing a full
complement of individual protective equipment, in all climates for which the system is designed,
and for the period specified in the capabilities documents.
Computer Resources Support—The facilities, hardware, software, documentation, manpower,
and personnel needed to operate and support embedded computer systems.
Concept of Operations (CONOPS)—Verbal or graphic statement, in broad outline, of a
commander’s assumptions or intent in regard to an operation or series of operations. The
CONOPS frequently is embodied in campaign plans and operation plans; in the latter case,
particularly when the plans cover a series of connected operations to be carried out
simultaneously or in succession. The concept is designed to give an overall picture of the
operation. (JP 1-02)
Contractor Logistics Support—Logistics support activities provided under contract to a using
command.
Corrective Maintenance—All actions performed as a result of a failure to restore an item to a
specified condition. Corrective maintenance can include any or all of the following steps:
localization, isolation, disassembly, interchange, reassemble, alignment and checkout. (AFPAM
63-128)
Critical Failure—See Failure.
Deficiency Report (DR)—A report used to identify, document, and track system deficiency and
enhancement data while a system is in advanced development, test and evaluation, or operational
transition. (T.O. 00-35D-54)
 80                                            AFOTECPAM 99-104 24 SEPTEMBER 2013


Dependability—The probability that a system is operable at any time during an interval that has
no preventative maintenance downtime.
Deployability—A function of system reliability, characteristics of required maintenance
equipment, processes that support the flow of required spares and support equipment, and the
maintenance concept. Deployability can be expressed as required airlift to support deployment
of initial and follow-on support elements, numbers of personnel required for setup and operation
of any equipment (air, power, etc.), and the amount of resupply. (AFPAM 63-128)
Depot—Level Maintenance—Maintenance performed on material requiring major overhaul or
rebuild of parts, assemblies, subassemblies, and end items, including the manufacture of parts,
modification, testing, and reclamation. Depot maintenance supports lower levels of maintenance
by providing technical assistance and performing that maintenance beyond their responsibility or
capability, providing stocks of serviceable equipment, and using more extensive facilities for
repair than are available in organizational- or field-level maintenance activities. (AFI 21-101)
Depot—Level Maintenance Support—Maintenance and modification support accomplished or
provided by an air logistics center (ALC). It includes organizational- and intermediate-level
maintenance or modification work that cannot be economically accomplished within the using
command’s total resources (and is so certified by the using command HQ); and, depot-level
maintenance or modification work that, because of the job complexity, requires special skills,
tools, equipment, or facilities that are available only at a depot-level facility. (AFI 21 101)
Depot Maintenance Facility—This is a military or contractor facility that performs depot-level
maintenance modification of aircraft/missiles. (AFI 21-102)
Developmental Test and Evaluation (DT&E)—DT&E is that T&E conducted to assist the
engineering design and development process and to verify attainment of technical performance
specifications and objectives. DT&E is normally accomplished or managed by the DoD
component’s systems, hardware/software integration, related software, and prototype or full-
scale engineering development models of the system. T&E of compatibility and C-I with
existing or planned equipment and systems are also included.
Diagnostics—The ability of integrated diagnostics (ID—automated, semiautomatic, and manual
techniques taken as a whole) to fault-detect and fault-isolate in a timely manner.
Documentation—Comprises operator and maintenance instructions, repair parts lists, and
support manuals, as well as manuals related to computer programs and system software.
Downtime per Sortie—For a specified period of time, the total time the system is not mission
capable, maintenance (NMCM), scheduled or unscheduled. Not mission capable, supply
(NMCS) or not mission capable both (NMCB), scheduled or unscheduled, in clock hours,
divided by the number of sorties.
End Item—A final combination of end products, component parts, and/or materials ready for
their intended use, e.g., aircraft, ships, tanks, mobile machine ship.
Engineering Change Proposal—The document for proposing design changes to an item,
facility, or part, delivered or to be delivered, which will require revision to contract
specifications, engineering drawings, or documents referenced that are approved or authorized
for applicable items under government contracts.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                            81


Environmental Effects—The effects of the natural environment on the system. For example,
corrosion is a natural environmental effect caused by weather and ocean conditions.
(“Memorandum of Agreement on Multi-Service Operational Test and Evaluation and
Operational Suitability Terminology and Definitions”)
Environmental Impacts—The system’s impact on the natural environment as a result of its
operational use, maintenance, transportation and storage. For example, impacts include pollution
(noise, air and water), threat to endangered species and threat to public health. (“Memorandum
of Agreement Multi-Service Operational Test and Evaluation and Operational Suitability
Terminology and Definition”)
Environment, Safety, and Health:—Environment—Used as a general reference, environment
includes the generic natural environment; e.g., weather, climate, ocean conditions, terrain, and
vegetation. Environment includes those conditions observed by the system during operational
use, standby, maintenance, transportation and storage.
Safety—Safety is freedom from conditions, which can cause death, injury, occupational illness,
or damage to or loss of equipment or property. Support equipment normally conducts formal
safety assessments. Safety considerations should be included in maintainability or logistics
supportability assessments.
Health—The overall condition (i.e., body and mind) of an organism at a given time.
Environment, Safety, and Occupational Health Certification Board (ESOHCB)—AFOTEC
board convened four weeks before TRR to ensure readiness for OT&E is not affected by safety
or environmental-related issues.
Evaluation—The review and analysis of qualitative and/or quantitative data obtained from
design review, hardware inspection, testing and/or operational usage of equipment.
Failure:—Standard Definition—The events, or inoperable state, in which any item or part of
an item does not, or would not, perform as previously specified.
Operational Mission Failure—A failure that could preclude successful completion of a
mission, and must be specifically defined for each system.
Software Failure—The termination of the ability of a functional unit to perform its required
function as a result of a software problem or an event in which a system or system component
does not perform a required function within specified limits. A failure may be produced when a
fault is encountered. (AFPAM 63-128)
Critical Failure—A failure, or combination of failures, that prevents an item from performing a
specified mission. A system degradation, indication of failure, or actual failure that prevents a
system from performing a specified mission. (AFPAM 63-128) Critical failures do not have to
occur during a mission or directly in the system; they merely must or could cause mission impact
(e.g. personnel, publications, etc.).
Failure Rate—The total number of failures within an item population divided by the total
number of life units expended by that population, during a particular measurement interval under
stated conditions.
 82                                               AFOTECPAM 99-104 24 SEPTEMBER 2013


False Alarm—A system-indicated malfunction that had no follow-on request for a CM action,
and, thus, cannot be validated. Different from a CND where degradation occurs, but the
degradation cannot be duplicated. (AFPAM 63-128)
False Alarm Rate (FAR)—The frequency of occurrence of false alarms, which is calculated by
dividing the total number of false alarms by the total number of life units (e.g., time, cycles, etc.).
Fix Rate—The percent of aircraft returning from a sortie with a critical failure resulting in an
NMC status and that are repaired and returned to MC status within a specified period of time (for
example 50% in 2 hours). (AFPAM 63-128)
Follow—on Operational Test and Evaluation (FOT&E)—Follow-on Operational Test &
Evaluation (FOT&E) continuation of IOT&E or QOT&E. FOT&E answers specific questions
about unresolved COIs and test issues, verifies the resolution of deficiencies determined to have
substantial or severe impact on mission operations, or completes areas not finished during the
I/QOT&E. Requirements for FOT&E are documented in an approved AFOTEC OT&E report
prior to the planning of the FOT&E.
Foreign Object Damage—Damage to or malfunction of an aircraft, missile, or drone caused by
an object that is alien to an area or system, being ingested by, or lodged in a mechanism. (AFI
21-101)
Global Information Grid (GIG)—The globally interconnected, end-to-end set of information
capabilities, associated processes, and personnel for collecting, processing, storing,
disseminating, and managing information on demand to warfighters, policy makers, and support
personnel.
Government—Industry Data Exchange Program (GIDEP)—An Army-, Navy-, Air Force-,
and NASA-sponsored program for the exchange of data among government agencies and
industry to reduce the costs of investigative efforts on parts and materials.
Ground Abort—Any aircraft confirmed by maintenance as operational and ready for flight that
fails to launch for any system malfunction/failure/reject.
Hazard—A condition that is prerequisite to a mishap. (MIL-STD 882D)
Hazard Probability—The aggregate probability of occurrence of the individual events
(conditions). (MIL-STD 882D)
Hazard Severity—An assessment of the consequences of the worst credible mishap that could
be caused by a specific hazard. (MIL-STD 882D)
Hazardous Materials—Any substances or materials that pose a threat to human health or the
environment typically because of their toxic, corrosive, ignitable, explosive, or chemically
reactive nature.
Human Factors Engineering (HFE)—The application of knowledge of a human’s capabilities
and limitations to the planning, design, development, and testing of aerospace systems,
equipment, and facilities to achieve optimum personnel safety, comfort, and effectiveness
compatible with systems requirements.
Human Systems Integration (HSI)—HSI optimizes the human part of the total system equation
by integrating HFE; manpower, personnel, training (MPT); health hazards; safety factors;
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              83


medical factors; personnel (or human) survivability factors; and habitability considerations into
the system acquisition process.
Human Factors (HF)—A body of scientific facts about human characteristics. The term covers
all biomedical and psychological considerations; it includes, but is not limited to, principles and
applications in the areas of human engineering, personnel selection, training, life support, job
performance aids and human performance evaluation.
Information Assurance (IA)—Information operations that protect and defend information and
information systems by ensuring their availability, integrity, authentication, confidentiality, and
non-repudiation. This includes providing for the restoration of information systems by
incorporating protection, detection, and reaction capabilities.
Initial Operational Capability (IOC)—The first attainment of the capability to employ
effectively a weapon, item of equipment, or system of approved specific characteristics, and
which is manned or operated by an adequately trained, equipped, and supported military unit or
force. (JP 1-02)
Initial Operational Test and Evaluation (IOT&E)—An independent and dedicated
operational test and evaluation conducted in as realistic an operational environment as possible
to estimate the prospective system’s overall operational capability determined by effectiveness,
suitability, and other operational considerations. In addition, OT&E provides information on
organization, personnel requirements, doctrine and tactics. It may also provide data to support or
verify material in operating instructions, publications and handbooks.
Initial Spares Support List (ISSL)—A list of spares and repair parts and quantities required for
organizational and field maintenance initial support of an end item for a given period of time.
Quantities established for ISSLs will be equal to initial base stockage objective.
In—Process Review—A review of a material development project conducted at critical points
in the development cycle for the purpose of evaluating the status of the project, accomplishing
effective coordination, and facilitating proper and timely decisions bearing on the future course
of the project.
Integrated Diagnostics—A structured approach to use the most effective combination of a
system’s automated, semiautomatic, and manual diagnostic resources in a total system
supportability approach that provides the required performance information to the appropriate
personnel when needed and the supporting mechanisms to efficiently isolate all faults to the
specific malfunctioning item. (AFPAM 63-128)
Built—in Test Equipment (BITE)—Any device permanently mounted in the prime equipment
and used for the express purpose of testing the prime equipment, either independently or in
association with external test equipment.
Built—in Test/Fault Isolation Test (BIT/FIT)—Can employ information from a wide variety
of sensors, especially using machine reasoning in onboard processors to interpret the indications.
Automatic Test Equipment (ATE)—Any automated device used for the express purpose of
testing prime equipment; usually external to the prime device (e.g., support equipment).
Integrated Logistics Support (ILS)—A disciplined, unified, and iterative approach to the
management and technical activities necessary to integrate support considerations into system
and equipment design; develop support requirements that are related consistently to readiness
 84                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


objectives, to design, and to each other; acquire the required support; and provide the required
support during the operational phase at minimum cost.
Integrated Logistics Support (ILS) Elements:—Maintenance Planning—The process
conducted to evolve and establish maintenance concepts and requirements for the lifetime of a
materiel system.
Manpower and Personnel—The identification and acquisition of military and civilian
personnel with the skills and grades required to operate and support a material system over its
lifetime at peacetime and wartime rates.
Supply Support—All management actions, procedures, and techniques used to determine
requirements to acquire, catalog, receive, store, transfer, issue, and dispose of secondary items.
This includes provisioning for initial support as well as replenishment supply support.
Support Equipment—All equipment (mobile and fixed) required to support the operation and
maintenance of a material system. This includes associated multi-use end items, ground
handling and maintenance equipment, tools, meteorology and calibration equipment, test
equipment, and automatic test equipment. It includes the acquisition of logistics support for the
support and test equipment itself.
Technical Data—Recorded information regardless of form or character of a scientific or
technical nature.
Training and Training Support—The processes, procedures, techniques, training devices and
equipment used to train civilian and active duty and reserve military personnel to operate and
support a materiel system.
Facilities—The permanent, semi-permanent or temporary real property assets required to
support the material system, including conducting studies to define types of facilities or facility
improvements, location, space needs, utilities, environmental requirements, real estate
requirements, and equipment.
Packaging, Handling, Storage, and Transportation—The resources, processes, procedures,
design considerations, and methods to ensure that all system, equipment, and support items are
preserved, packaged, handled, and transported properly, including environmental considerations,
equipment preservation requirements for short- and long-term storage, and transportability.
Design Interface—The relationship of logistics-related design parameters, such as R&M, to
readiness and support resource requirements. Logistics-related design parameters are expressed
in operational terms rather than inherent values and specifically related to system readiness
objectives and support cost of the material system.
Integrated Test and Evaluation (IT&E)—An efficient approach to T&E, executed with the
deliberate intent and planning to use specific test events and activities for both developmental
test and operational test analysis and reporting, when there are clear cost and/or schedule
advantages. The high cost or lack of sufficient test articles may provide an overall benefit for
DT&E and OT&E teams to share test resources and data. IT&E usually ends with a phase of
dedicated OT&E. AFOTEC always considers doing IT&E for all programs. The restriction for
contractor involvement in USC, Title 10 applies only to dedicated OT&E.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                 85


Interoperability—The ability of systems, units, or forces to provide services to and accept
services from other systems, units, or forces and to use the services so exchanged to enable them
to operate effectively together.
Joint Reliability and Maintainability Evaluation Team (JRMET)—established by the PM,
assists in the collection, analysis, verification, and categorization of reliability, maintainability,
and availability data. The JRMET may also review applicable Deficiency Reports (DRs) and
recommend whether or not they should be closed.
Levels of Protection—The degree of preservation, packaging and packing required to prevent
deterioration or damage to supplies and equipment because of the hazards to which they may be
subjected during shipment and storage.
Life—Cycle Cost—The total cost to the government of acquisition and ownership of the system
over its useful life. It includes the cost of development, acquisition, support, and where
applicable, disposal. (DSMC Glossary)
Life unit—A measure of use duration applicable to the item (such as operating hours, cycles,
distance, rounds fired, and attempts to operate). (DAG)
Line—Replaceable Unit (LRU)—An item that is normally removed and replaced as a single
unit to correct a deficiency or malfunction on a weapon or support system and item of
equipment. Such items have a distinctive stock number for which repairs may be locally
authorized to support the removal and replacement action. These items are repair cycle assets
subject to/due in from maintenance (DIFM) controls (T.O. 00-20-3) and may be disassembled
into separate components during shop processing. Components, shop-replaceable units (SRU),
may also be repair cycle assets subject to DIFM controls if they are processed separately and
spares are locally authorized and maintained to support intermediate-level repair of the LRU.
(AFI 21-101)
Logistics: —Logistics is the science of planning and carrying out the movement and
maintenance of forces. In its most comprehensive sense, those aspects of military operations that
deal with: 1) Design and development, acquisition, storage, movement, distribution,
maintenance, evacuation, and disposition of materiel, 2) Movement, evacuation, and
hospitalization of personnel, 3) Acquisition or construction, maintenance, operation, and
disposition of facilities, and 4) Acquisition or furnishing of services. (JP 1-02)
Logistics can also be defined as the functional fields of military operations concerned with: 1)
Material requirements, 2) Production planning and scheduling, 3) Acquisition, inventory
management, storage, maintenance, distribution and disposal of materiel, supplies, tools, and
equipment, 4) Transportation, telecommunications, petroleum, and other logistical services, 5)
Supply cataloging, standardization, and quality control, 6) Commercial and industrial activities
and facilities including industrial equipment, and 7) Vulnerability of resources to attack damage.
Logistics Management Information (LMI)—The selective application of scientific and
engineering efforts undertaken during the acquisition process, as part of the systems engineering
process, to assist in causing support considerations to influence design, defining support
requirements that are related optimally to design and to each other, acquiring the required
support, and providing the required support during the operational phase at minimum cost. LMI
was previously known as the logistics support analysis (LSA) process.
 86                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


Logistics Resources—The support personnel and materiel required by an item to ensure its
mission performance. It includes tools, test equipment, repair parts, facilities, TM and
administrative supply procedures necessary to ensure the availability of these resources when
needed.
Logistics Supportability—The degree to which the planned logistics support allows the system
to meet its availability and wartime usage requirements. Planned logistics support includes the
following: test, measurement, and diagnostic equipment; spare and repair parts; technical data;
support facilities; transportation requirements; training; manpower; and software. (DAG)
Low—Rate Initial Production—The production of a system in limited quantity to be used in
OT&E for verification of production engineering and design maturity and to establish a
production base.
Maintainability—The ability of an item to be retained in or restored to specified condition when
maintenance is performed by personnel having specified skill levels, using prescribed procedures
and resources, at each prescribed level of maintenance and repair. (AFPAM 63-128)
Maintenance—All actions necessary for retaining material in or restoring it to a serviceable
condition. Maintenance includes servicing, repair, modification, modernization, overhauls,
inspection, condition determination, corrosion control and initial provisioning of support items.
(AFI 21-103)
Maintenance Action—An element of a maintenance event. One or more tasks necessary to
retain an item in, or restore it to, a specified condition. (AFI 63-101, Acquisition and
Sustainment Life Cycle Management)
Maintenance Concept—A description of the planned general scheme for maintenance and
support of an item in the operational environment. (MIL-HDBK 470A)
Maintenance Downtime per Sortie—For a specified period of time, the total time the system is
NMCM and NMCB, scheduled, in clock hours, divided by the number of sorties.
Maintenance Event—One or more maintenance actions required to effect preventive and CM,
including troubleshooting, due to any type of failure or malfunction, scheduled maintenance, or
servicing as applicable. (AFPAM 63-128)
Maintenance Man—Hours per Life Unit (MMH/LU)—The maintenance hours required
divided by the appropriate life unit. (AFPAM 63-128)
Maintenance Personnel Per Operational Unit—(The user of this term needs to define the
operational unit.) The number of maintenance personnel that will be required to support an
operational unit (excluding depot level and other manpower that is excluded from maintenance
planning factors by AFI 38-201) under specified operating and maintenance concept. (AFPAM
63-128)
Maintenance Ratio—The cumulative number of maintenance work-hours during a given period
divided by the cumulative number of life units (e.g., OH).
Manpower Supportability—The identification and acquisition of military and civilian
personnel with the skills and grades required to operate and support a material system over its
lifetime at peacetime and wartime rates. (“Memorandum of Agreement on Multi-Service Test &
Evaluation and Operational Suitability Terminology and Definitions”)
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              87


Mature System—An operational system is considered mature when its R&M characteristics
cease to improve significantly with continued use. System, subsystems and components all
mature at varying rates and lengths of time. Unless otherwise specified, a system is considered
to have mature R&M characteristics two years after the IOC date.
Mean Downtime (MDT)—The average elapsed time, as a result of a downing event, required to
restore a system to full operating status includes maintenance, supply and administrative delay
times. Besides the inherent repair and maintainability characteristics, field conditions such as
tech-order availability and adequacy, support equipment capability and availability, supply
levels, manning, experience level and shift structure also affect downtimes.
Mean Repair Time (MRT)—MRT is the average on-equipment, off-equipment or both
corrective maintenance times. It includes all maintenance actions needed to correct a
malfunction, including preparing for test, troubleshooting, removing and replacing parts,
repairing, adjusting, reassembly, alignment, adjustment, and checkout. MRT does not include
maintenance, supply or administrative delays.
Mean Time Between Critical Failures (MTBCF)—MTBCF is the average time between
failures of mission-essential system functions. Critical failures do not have to occur during a
mission, they merely must or could cause mission impact.
Mean Time Between Demands (MTBD)—A measure of the system reliability parameter
related to demand for logistics support; it is the total number of system life units (e.g., flying
hours (FH) and sorties) divided by the total number of item demands on the supply system
during a stated period of time.
Mean Time Between Downing Events (MTBDE)—MTBDE is the average time between
events that bring the system down (e.g., critical or non-critical failures, preventive maintenance,
training, maintenance and supply response, administrative delays and actual on-equipment
repair).
Mean Time Between Failures (MTBF)—MTBF is a basic measure of reliability for repairable
items; it is the mean number of life units during which all parts of the item perform within their
specified limits during a particular measurement interval under stated conditions.
Mean Time Between Maintenance (MTBM)—The total life units (for example, OH, FHs or
rounds) divided by the total number of maintenance (base level) events for a specific period of
time.
Mean Time Between Maintenance (Induced)—The average time between the on-equipment
corrective events associated with malfunctions resulting from other than internal design and
manufacturing characteristics, for example, improper maintenance, operator error, foreign object
damage and failures caused by malfunction of associated equipment.
Mean Time Between Maintenance (Inherent)—The average time between the on-equipment
corrective events associated with malfunctions resulting from internal design and manufacturing
characteristics.
Mean Time Between Maintenance (No Defect)—The average time between the on-equipment
corrective events associated with equipment which has no confirmed malfunction, such as
removals which subsequently bench check satisfactory.
 88                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


Mean Time Between Maintenance (Preventive)—The average time between maintenance
events including removals, replacement or reinstallation associated with scheduled maintenance
or time changes.
Mean Time Between Operational Mission Failures (MTBOMF)—The total operating time
(e.g., driving time, flying time or system-on time) divided by the total number of OMFs.
Mean Time Between Removals (MTBR)—A measure of the system reliability parameter
related to demand for logistics support. The total number of system life units divided by the total
number of items removed from that system during a stated period of time. This term is defined
to exclude removals performed to facilitate other maintenance and removals for time compliance
T.O.s (product improvement).
Mean Time Between Scheduled Maintenance (MTBSM)—The total operating time divided
by the total number of scheduled maintenance events.
Mean Time Between Unscheduled Maintenance (MTBUM)—The total operating time
divided by the total number of incidents requiring unscheduled maintenance.
Mean Time To Failure—A basic measure of reliability for non-repairable items; it is the total
number of life units of an item divided by the total number of failures within that population
during a specified time interval under stated conditions.
Mean Time To Repair (MTTR)—(A contract term only) A basic measure of maintainability; it
is the sum of corrective maintenance times at any specific level of repair divided by the total
number of failures within an item repaired at that level during a particular interval under stated
conditions.
Mean Time to Restore Functions (MTTRF)—MTTRF is the average elapsed time, as a result
of a critical failure, required to restore a system to full operating status. It includes
administrative and logistics delay times associated with restoring function following a critical
failure.
Minimum Essential Subsystem List (MESL)—The MESL lists the minimum essential
subsystems needed to perform the intended missions. All intended mission profiles will have a
MESL. The MESL is used to judge the mission criticality of failures during testing. (AFPAM
63-128)
Micro—OMNIVORE—A data retrieval and analysis system.
Mishap—An unplanned event or series of events resulting in death, injury, occupational illness,
or damage to or loss of equipment or property, or damage to the environment. (MIL-STD 882D)
Mishap Risk Assessment—The process of characterizing hazards within risk areas and critical
technical processes, analyzing them for their potential mishap severity and probabilities of
occurrence, and prioritizing them for risk mitigation actions.
Mishap Severity—An assessment of the consequences of the most reasonable credible mishap
that could be caused by a specific hazard.
Mission Capable (MC)—The ability of a system to perform at least one of its assigned
peacetime or wartime missions. If no wartime mission is assigned, the system must be capable
of performing any one assigned mission (AFI 21-103 and AFPAM 63-128). MC rate is the
percentage of possessed time that a weapon system can perform its assigned missions mission
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             89


capable time divided by total possessed time. MC rate may be expressed as the sum of FMC and
PMC rates.
Mission Reliability—See Reliability.
Mobility Readiness Spares Package (MRSP)—An air transportable package of spares and
repair parts required to sustain planned wartime or contingency operations of a weapon system
for a specified period of time pending re-supply. MRSPs will include spares and repair parts for
aircraft, vehicles and other equipment, as appropriate. MRSPs are normally pre-positioned with
the using unit.
Not Mission Capable (NMC)—A status code which the system or equipment cannot perform
any of its primary missions. It can be followed by a reason code meaning maintenance (M),
supply (S) or both (B). (AFI 21-103)
Not Repairable This Station (NRTS)—A status condition determined during shop processing
of an item used to indicate that the item cannot be repaired at base level because of lack of
authorization, technical skills, parts, facilities, manpower, or any other causes. (T.O. 00-20-1)
Off—Equipment Maintenance—In-shop maintenance actions performed on removed
components, except complete aircraft engines.
On—Equipment Maintenance—Maintenance actions accomplished on a complete end item
such as aircraft, trainers, support equipment, communication-electronic-meteorology equipment,
complete round munitions and uninstalled aircraft engines.
Operating Command—The command or agency primarily responsible for the operational
employment of a system, subsystem, or item of equipment; it is also a participating command.
Operational Assessment (OA)—Analysis of progress toward operational effectiveness and
suitability made by an independent operational test activity, with user support as required, on
other than production systems. Additionally, AFOTEC assesses progress toward overall mission
capability. The focus of an operational assessment is on significant trends noted in development
efforts, programmatic voids, areas of risk, adequacy of requirements, and the ability of the
program to support adequate operational testing. Operational assessments may be made at any
time using technology demonstrators, prototypes, mockups, engineering development models or
simulations, but will not substitute for the independent OT&E necessary to support full
production decisions. An OA conducted before, or in support of, Milestone B is an early
operational assessment.
Operational Availability (AO)—AO is the probability that a system can be used for any
specified purpose when desired. It includes both the inherent R&M parameters and logistics
support effectiveness of the system that relates to the total time the system might be desired for
use.
Operational Dependability (DO)—DO is the probability that a system can be used to perform a
specified mission when desired. It includes both the inherent R&M parameters and logistics
support effectiveness of the system that relates to all time the system might be desired for
mission use and for which critical failures could occur. The difference between AO and DO is
that DO is used for systems that can be repaired during the mission.
Operational Effectiveness—The overall degree of mission accomplishment of a system used by
representative personnel in the context of the organization, doctrine, tactics, threat (including
 90                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


countermeasures and nuclear threats), and environment in the planned or operational
employment of the system. (AFPAM 63-128)
Operational Mission Failure—See Failure.
Operational Suitability—The degree to which a system can be placed satisfactorily in field use
with consideration given to availability, compatibility, transportability, interoperability,
reliability, wartime usage rates, maintainability, safety, human factors, manpower supportability,
logistics supportability, natural environmental effects and impacts, documentation, and training
requirements. (DAG)
Operational Task—An individual military operation that is accomplished in support of an
operational objective. (AFOTECMAN 99-101)
Operational Test and Evaluation (OT&E)— The field test, under realistic combat conditions,
of any item of (or key component of) weapons, equipment, or munitions for the purpose of
determining the effectiveness and suitability of the weapons, equipment or munitions for use in
combat by typical military users, and the evaluation of the results of such test. (10 USC Section
139)
Operational Utility Assessment (OUA)—OUAs are used to determine operational utility in
support of assessments conducted on innovation programs. An OUA is planned, conducted and
reported by adapting the OT&E construct to the technology being assessed.
Operational Utility Evaluations (OUE)—OUEs are evaluations conducted to demonstrate or
validate new operational concepts or capabilities, upgrade components, or expand the mission or
capabilities of existing or modified systems. OUEs are not used when IOT&E, QOT&E, or
Force Deployment Evaluation (FDE) are required or are more suitable.
Organizational—Level Maintenance—The maintenance and repair performed by the activity
level that uses the system’s equipment within the activity’s capability (DSMC Glossary).
Organizational maintenance normally consists of inspecting, servicing, lubricating, adjusting and
replacing parts, minor assemblies and subassemblies.
Partially Mission Capable (PMC)—The percentage of possessed time that a system is capable
of performing at least one but not all of its assigned wartime missions. PMC may be subdivided
into PMC maintenance (PMCM), PMCS and PMCB. (AFI 21-103)
Participating Command—A command or agency designated by HQ USAF to support and
advise the program manager during the execution of a program.
Possessed Hours—The total hours in a given period that assigned equipment is under the
operational control of the designated responsible organization.
Preventive Maintenance— All actions performed in an attempt to retain an item in a specified
condition by providing systematic inspection, detection, and prevention of incipient failures
(DAU Glossary).
Qualification Operational Test and Evaluation (QOT&E)—The operational testing
performed on programs instead of IOT&E for which there is no RDT&E-funded development
effort.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              91


Readiness—The ability of forces, units, or weapon systems or equipment to deliver the outputs
for which they were designed (includes the ability to deploy and employ without unacceptable
delays). (JP 1-02 and AFPAM 63-128)
Reliability—The ability of a system and its parts to perform its mission without failure,
degradation, or demand on the support system. (DAG)
Logistics Reliability—A measure of a system’s capability to operate as planned under the
defined operations and support (O&S) concepts using specified logistics resources (for example,
spares or manpower). (AFPAM 63-128)
Mission Reliability (Rm)—The probability that the system is operable and capable of
performing its required function for a stated mission duration or at a specified time into the
mission. Rm is based on the effects of system reliability during mission time only. Rm does not
take into account system maintainability.
Reliability Analysis Center—An official DoD contractor-operated center located at Rome Air
Development Center authorized to collect, analyze, and disseminate reliability data and
information on microcircuits, solid state devices, non-electronic parts and equipment, and
systems.
Repair—Level Analysis (RLA)—The basic decisions about: 1) Repair versus throwaway, and
2) The most desirable repair posture.
Risk—An expression of the possibility of a mishap in terms of hazard severity and hazard
probability. (MIL-STD 882D)
Safety—Freedom from conditions that can cause death, injury, occupational illness, damage to
or loss of equipment or property, or damage to the environment (DOT&E Operational Suitability
Guide, Volume I – A Tutorial).
Safety of Test—The application of safety and management principles, criteria, and techniques to
achieve acceptable risk, within the constraints of operational effectiveness and suitability, time,
and cost, throughout all phases of the testing to ensure personnel are safe from illness, injury,
and accidents.
System Safety—The application of engineering and management principles, criteria, and
techniques to achieve acceptable mishap risk, within the constraints of operational effectiveness
and suitability, time, and cost, throughout all phases of the system life cycle.
Shop—Replaceable Unit (SRU)—A module for an LRU which can be removed from the LRU
at an intermediate repair facility.
Software Failure—See Failure.
Sortie Generation Rate—The average number of sorties produced per aircraft in a defined
operating period. (AFPAM 63-128)
Suitability—See Operational Suitability.
Support Equipment—See ILS Elements.
Sustainability:—Wartime Sustainability—The ability to maintain the necessary level and
duration of operational activity to achieve military objectives. It is a function of providing and
 92                                              AFOTECPAM 99-104 24 SEPTEMBER 2013


maintaining levels of ready forces, material and consumables necessary to support the military
effort. (JP 1-02)
Peacetime Sustainability—The ability to maintain the necessary levels of forces, material, and
consumables to support the burden of ownership of the system.
System Program Office (SPO): —The organization comprised of technical and business
management and administrative personnel assigned full time to a system program director. The
office may be augmented with additional personnel from participating organizations.
The office of the program manager and the single point of contact with industry, government
agencies and other activities participating in the system acquisition process. (DSMC Glossary)
Technical Manual (TM)—A publication that contains instructions for the installation,
operation, maintenance, training, and support of weapon systems, weapon system components,
and support equipment. TM information may be presented in any form or characteristic,
including, but not limited to, hard copy, audio and visual displays, magnetic tape, discs, and
other electronic devices. A TM normally includes operational and maintenance instructions,
parts lists or parts breakdown, and related technical information or procedures exclusive of
administrative procedures. T.O.s that meet the criteria of this definition may also be classified as
TMs.
Technical Order (T.O.)—An Air Force publication that gives specific technical directives and
information with respect to the inspection, storage, operation, modification and maintenance of
given Air Force items and equipment.
Test and Evaluation (T&E): —The term “test” denotes any project or program designed to
obtain, verify and provide data for the evaluation of R&D other than laboratory experiments;
progress in accomplishing development objectives; performance and operational capability of
systems, subsystems, and components; and equipment items. The term “evaluation” denotes the
review and analysis of quantitative data produced during current or previous testing, data
obtained from test conducted by other government agencies and contractors, from operation and
commercial experience, or combinations thereof.
Process by which a system or components are compared against requirements and specifications
through testing. The results are evaluated to assess progress of design, performance,
supportability, etc. Three types of T&E occur during the acquisition cycle: Development
(DT&E), Operational (OT&E) and Production Acceptance (PAT&E). (DSMC Glossary)
Threshold—Minimum acceptable operational value for a system capability or characteristic
below which the utility of the system becomes questionable.
Training Requirements—The processes, procedures, techniques, training devices and
equipment used to train civilian and active duty and reserve military personnel to operate and
support a material system. This includes individual and crew training, new equipment training,
initial, formal and on-the-job training and logistics support planning for training equipment and
training device acquisitions and installations. (DOT&E Operational Suitability Guide, Volume I
– A Tutorial)
Transportability—The capability of material to be moved by towing, self-propulsion, or carrier
via any means, such as railways, highways, waterways, pipelines, oceans, and airways. (Full
consideration of available and projected transportation assets, mobility plans and schedules, and
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                           93


the impact of system equipment and support items on the strategic mobility of operating military
forces are required to achieve this capability.) (JP 1-02)
Unscheduled Maintenance—Corrective maintenance required by item conditions.
Utilization Rate—The average life units expended or missions attempted (launched and
airborne) per system or subsystem during a specific interval of time. (AFPAM 63-128)
Verification, Validation & Accreditation (VV&A)—1) Verification: The process of
determining that a model or simulation (or other test capability) implementation accurately
represents the developer’s conceptual description and specifications. For model and simulation,
verification also evaluates the extent to which the model and simulation has been developed
using sound and established software-engineering techniques. 2) Validation: The process of
determining a) the manner and degree to which a model and simulation (or other test capability)
is an accurate representation of the real-world from the perspective of the intended uses of the
model and simulation, and b) the confidence that should be placed on this assessment. 3)
Accreditation: An official determination that a model or simulation is acceptable for a specific
purpose, and is based on a five-step process: identify test issues; review validation
documentation; compare test capabilities and validation information with test issues; identify
potential shortfalls; and develop and execute strategy to address shortfalls (assess risk).
Wartime Usage Rates—The quantitative statement of the projected manner in which the system
is to be used in its intended wartime environment. (DOT&E Operational Suitability Guide,
Volume I – A Tutorial)
Weapon System Reliability (WSR)—The probability that a system will complete a specified
mission, given that the system was initially capable of performing that mission.
 94                                                AFOTECPAM 99-104 24 SEPTEMBER 2013


                                            Attachment 2
                                    RELIABILITY MEASURES

A2.1. Introduction. This attachment provides a discussion of reliability measures in general
and presents examples of system-specific measures.
A2.2. Time Between Failures.
      A2.2.1. Time between failures is the primary measure of reliability. The mean of many such
      times is the most commonly used metric of reliability known as the mean time between
      failures (MTBF). It is defined as the total functioning life of a population of an item during a
      specific measurement interval, divided by the total number of failures within the population
      during that interval. MTBF can be interpreted as the expected length of time a system will be
      operational between failures.
   A2.2.2. We need not restrict ourselves only to the mean to describe the typical time between
   failures. Also possible, and sometimes preferable, are a variety of other statistics (metrics)
   such as the median, maximum, and minimum times between failures. Just as the mean is not
   the only metric for describing the typical time between failures, time need not be the only
   measure of life between failures. Cycles, miles, events, or other measure-of-life units are
   also admissible and are sometimes more informative than clock time, permitting the time
   between failures measure to be tailored to the reliability requirements of a specific system.
   Some examples of this tailoring are: rounds between failures, miles between operational
   mission failure, time between unscheduled maintenance actions, cycles between downing
   events, sorties between any maintenance actions. In the most general sense, reliability is
   measured by life units (hours, cycles, miles, etc) between events. These events must be
   clearly defined in advance and might include failures, critical failures, operational mission
   failures, downing events, maintenance actions or others.
   A2.2.3. Failure Rate. Failure rate is defined as the number of failures of an item per
   measure-of-life unit (e.g., cycles, hours, miles or events as applicable). This metric is more
   difficult to visualize from an operational standpoint than the MTBF metric, but is a useful
   mathematical term which frequently appears in many engineering and statistical calculations.
   The failure rate is simply the reciprocal of the MTBF metric.
A2.3. Reliability Incident Classification. An understanding of the relationships existing
between the reliability aspects above and other terms is essential to the knowledgeable
application of these parameters.
   A2.3.1. Mission Failures—Mission failures are the loss of any of the system’s mission-
   essential functions. Along with system hardware/software failures and any SUT failure,
   operator errors and errors in publications that cause such a loss are included in this category.
   Mission failures are related to mission reliability measures because they prevent complete
   mission accomplishment.
   A2.3.2. System Failures—System failures are malfunctions that may or may not affect the
   system’s mission-essential functions, and they may or may not require spares for correction.
   A system failure generally requires unscheduled maintenance so system failures heavily
   influence maintenance-related reliability.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                95


   A2.3.3. Unscheduled Spares Demands—Unscheduled spares demands are used to evaluate
   supply-related reliability. All unscheduled spares demands require a response from the
   supply system, so they form the basis for evaluating supply-related reliability.
   A2.3.4. System or Mission Failures Requiring Spares—System or mission failures that
   require spares for correction are the most critical. Mission, maintenance and supply
   reliabilities are affected, and the system runs the risk of being held in a non-mission-ready
   status for an extended period of time by logistics delays.
A2.4. System-specific Example Measures.
   A2.4.1. Aircraft.
         A2.4.1.1. Mission Reliability (Rm). Rm is the probability that the system is operable and
         capable of performing its required function for a stated mission duration or at a specified
         time into the mission. Rm is based on the effects of system reliability during mission time
         only. Rm does not take into account system maintainability. For systems with failures
         distributed exponentially, Rm is defined as
         - (t / MTBCF)
Rm = e
where t is the average mission time and mean time between critical failures (MTBCF) is the
average time between failures of mission-essential system functions. Specific mission times
should be used to determine the Rm for each mission if the system is used under significantly
different mission lengths. NOTE: Exponential systems are systems whose times to failure
exhibit an exponential probability density function (i.e., systems that exhibit a constant failure
rate). In addition, MTBCF may be the same as mean time between operational mission failures
(MTBOMF) for some joint programs, however the distinction should be defined in the JRMET
Charter.
              A2.4.1.1.1. Weapon System Reliability (WSR). WSR is an expression of Rm often
              used in aircraft system testing. It is the probability of completing the mission (i.e.,
              peacetime and/or wartime) without critical failures. It is important to clearly define
              the mission into specific mission phases (i.e., take-off, ingress/cruise, release
              weapons/loiter, egress, and landing phases for an aircraft) to maximize the available
              data during test. Environmental profiles such as temperature, air density, humidity,
              vibration, shock or corrosive agents may be considered as factors delimiting Rm.
              Mission-critical systems should be identified for each mission phase. WSR is
              expressed as

WSR = successful missions
      attempted missions
              A2.4.1.1.2. Break Rate (BR). BR is the percentage of sorties from which an aircraft
              returns from an assigned mission with one or more inoperable (“code 3”) mission-
              essential systems that were previously operable. Break rate includes “code 3”
              conditions, such as ground and air aborts. BR is calculated with
               number of aircraft code 3 events during measurement period
BR (%) =                                                                  × 100
                          number of sorties flown during period
         A2.4.1.2. Logistics Reliability. Logistics reliability is the ability of a system to perform
         failure-free under specified operating conditions and time without demand on the
 96                                                  AFOTECPAM 99-104 24 SEPTEMBER 2013


       logistics support system. Logistics reliability addresses all incidents that require a
       response from the logistics support system. A measure of logistics reliability is the time
       between maintenance. A typical metric for aircraft systems is the mean time between
       maintenance (MTBM). Data collection systems allow tracking of standard MTBM
       parameters such as inherent malfunctions, induced malfunctions, no-defect events, total
       corrective events, preventative maintenance, removals or demands. MTBM is expressed
       as the average flying hours between scheduled and unscheduled maintenance events for
       the appropriate, selected parameter based on MAJCOM requirements:
                              flying hours
MTBM =         number of maintenance events of interest
   A2.4.2. C4I Systems.
       A2.4.2.1. Logistics Reliability. Logistics reliability captures the system’s frequency of
       maintenance under defined operational and support concepts, using specific logistics
       resources. A measure of logistics reliability is the time between maintenance. A typical
       metric of logistics reliability is the MTBM. It is the average time between all
       maintenance events, that is, both scheduled and unscheduled events for hardware and
       software:
              number of operating hours   
MTBM =
            number of maintenance events
This is equivalent to
               MTBUM x MTBSM
MTBUM = MTBUM + MTBSM
where mean time between unscheduled maintenance (MTBUM) and mean time between
scheduled maintenance (MTBSM) are most often defined as
                   number of operating hours
MTBM =
           number of unscheduled maintenance events
                  number of operating hours
MTBSM =
            number of scheduled maintenance events
A2.4.3. Munitions/Missile Systems
       A2.4.3.1. Storage Reliability (RS). For munitions/missiles, RS is defined as the
       probability the system will be removed from storage and pass BIT and visual inspection.
       RS is equivalent to the Navy’s operational availability for munitions/missiles. Typically,
       RS cannot be calculated during OT&E due to the immature logistics environment, test-
       unique events required to prepare weapons for test (such as telemetry kit installation), and
       non-representative storage and transportation procedures for OT weapons. However, RS
       could be modeled with data from surveillance programs of similar systems or from
       accelerated life testing.
       number of weapons removed from storage not experiencing critical failures prior to aircrew acceptance
RS =
                                       number of weapons evaluated

       A2.4.3.2. WSR. WSR is an expression of Rm often used in munitions/missiles testing. It
       is the probability of completing the mission (i.e., peacetime and/or wartime) without
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             97


     critical failures. It is important to clearly define the mission into specific mission phases
     to maximize the available data during test. For example, captive-carry, launch, free-flight
     cruise and terminal phases may be considered for a missile. This allows data from
     backup missiles, captive-carry tests, mission aborts, and other mission events to be
     included in the WSR database. WSR is expressed as
        successful missions
WSR =
        attempted missions
 98                                               AFOTECPAM 99-104 24 SEPTEMBER 2013


                                           Attachment 3
                                 AVAILABILITY MEASURES

A3.1. Introduction. This attachment provides a discussion of the difficulty of measuring
availability directly, general availability measures and system-specific availability measures.
A3.2. Discussion.
   A3.2.1. Availability, as defined in AFPAM 63-128, is the degree (expressed in terms of 1.0
   or 100 percent as the highest) to which one can expect an equipment or weapon system to
   work properly when it is required. In practice, this may include reliability as well as
   maintainability and logistics information. A system’s availability may suffer because it
   breaks frequently (reliability), a fix requires a substantial amount of time (maintainability) or
   parts may have to travel a great distance (logistics). In operational testing, we are not usually
   able to collect enough data to answer the availability question directly. Instead, we collect
   reliability, maintainability and logistics information which is entered into a model to arrive at
   a reasonable estimate of availability for a given operational scenario.
   A3.2.2. To illustrate the difficulty of measuring availability directly, consider MC Rate. MC
   Rate is determined by
         number of FMC hours + number of PMC hours
MC =
                         possessed hours
MC Rate is subject to the influence of other factors such as utilization rate. For example, if the
plane is FMC but isn’t flown, the MC Rate increases. In operational testing, it is difficult, if not
impossible, to determine MC Rate satisfactorily without using modeling and simulation. In
general, there will be too few test articles and the team will not have the aircraft for sufficient
time to collect enough information to answer the question with adequate confidence.
A3.2.3. It is best to collect as much data as possible during an operational test. You don’t want
to find out the analysis needed is impossible because you missed something simple. Collect
times and types of failures and repair times. Collect information on parts delays to the extent
possible. Collect information on the number of personnel required to restore functionality. This
information is common to IMDS and GO81. However, if it is not available in an existing
database, collect all data that may be required.
   A3.2.4. Analysis of this data depends on the time and money available. It is seldom simple
   and usually requires a great deal of discussion and research into the different methods
   available. A-2/9 can help determine the best method of analysis as well as analyze the data.
A3.3. General Availability Measures.
   A3.3.1. A0 provides the most realistic measure of availability of systems deployed and
   functioning in a combat environment.
         A3.3.1.1. A continuous model is appropriate for aircraft sitting on a ramp.
        uptime
A0 =
       total time
         A3.3.1.2. A discrete model (also referred to as Demand Availability) is appropriate for a
         satellite that is continuously orbiting and receives discrete demands for its services.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                                99


       number of times the system is available
A0 =
       number of times the system is required

   A3.3.2. Achieved Availability (Aa) is a measure primarily used during developmental testing
   when the system is not operating in its intended support environment.
                    operating time
Aa =
       operating time + total maintenance time

   A3.3.3. Inherent Availability (Ai) is useful in determining basic operational characteristics
   under controlled conditions such as in a contractor’s facility and usually should not be used
   to support an operational test. However, automated information systems (AIS) occasionally
   use this concept as a definition of Do.
                         operating time
Ai =
       operating time + total corrective maintenance time

   A3.3.4. Achieved Availability and Inherent Availability may be useful in OT&E since they
   focus on the system, whereas Operational Availability includes the effects of operating
   procedures. Achieved Availability and Inherent Availability also indicate the availability
   possible if logistical and administrative delays are not present.
A3.4. System-specific Example Measures.
   A3.4.1. Aircraft.
         A3.4.1.1. Availability. Usually the entire aircraft (including all its subsystems and
         support equipment) is assessed in terms of its availability. Subsystems are important only
         to the extent that they contribute to the mission and support of the aircraft. Subsystem
         mission criticality will be defined in a MESL. For example, if the avionics system is
         critical to the mission of the aircraft, the impact of the avionics system’s availability on
         the availability of the host aircraft should be assessed. If the avionics system is not
         critical to the mission of the aircraft, no availability assessment is required (nor is it
         desired). MOSs for availability are generally expressed as percentages, e.g., either the
         percentage of time a system is capable of performing its mission or the percentage of
         time a fleet is capable of performing its mission. The selection of MOSs for availability
         should be based on the terms found in AFPAM 63-128 and the requirements document.
         These MOSs are difficult to measure during IOT&E where many support elements are
         not representative of the operational environment. During a typical IOT&E, modeling
         may be required to estimate and project these measures. A-2/9 is the focal point for
         logistics modeling efforts. During FOT&E, with representative support elements in an
         operational organization, these MOSs may be measured directly.
         A3.4.1.2. Sortie Generation Rate (SGR). SGR is expressed as the number of sorties
         per aircraft per relevant period of time such as a day or month. It is calculated by
         averaging the sorties during the measurement period and dividing this figure by the
         average number of possessed aircraft (for peacetime) or authorized aircraft (for wartime).
         A sortie starts when the aircraft begins to move forward on take-off and ends when the
         aircraft returns to the surface and either the engines are stopped, the aircraft is on the
         surface for five minutes (whichever occurs first), or a change is made in the crew. SGR
         is calculated through direct measurement or from simulation results depending on the
 100                                                AFOTECPAM 99-104 24 SEPTEMBER 2013


       realism of the test environment. Because most aircraft can perform several missions, it
       may be necessary to calculate several SGRs to represent the full range of operational
       mission requirements.
          total number of sorties / total number of aircraft
SGR =
                          number of days
or
            number of ground aborts + number of air aborts
SGR = 1 -
             number of ground aborts + number of sorties
This measure typically applies to the total aircraft system and is not used for avionics systems
and simulators. However, the impact of a new avionics system on the host aircraft’s SGR may
be measured.
       A3.4.2.1. Ao. Ao is the probability that a system can be used for any specified purpose
       when desired. It includes both the inherent R&M parameters and logistics support
       effectiveness of the system that relates to the total time the system might be desired for
       use. The continuous model given in paragraph A4.3.1.2. for Ao is equivalent to
           MTBDE
Ao =
        MTBDE + MDT
            A3.4.2.1.1. MTBDE is the average time between events that bring the system down
            (e.g., critical or non-critical failures, scheduled maintenance and training). Besides
            the inherent repair and maintainability characteristics, field conditions such as tech-
            order availability and adequacy, support equipment capability and availability, supply
            levels, manning, experience level and shift structure also affect downtimes.
               number of operating hours
MTBDE =        number of downing events
            A3.4.2.1.2. MDT is the average elapsed time, as a result of a downing event, required
            to restore a system to full operating status. Downtime includes maintenance and
            supply response, administrative delays, and actual on-equipment repair.
          total downtime
MDT = number of downing events

       A3.4.2.2. Do. Operational Dependability is the probability that a system can be used to
       perform a specified mission when desired. It includes both the inherent reliability and
       maintainability parameters and logistics support effectiveness of the system that relates to
       all time the system might be desired for mission use and for which critical failures could
       occur.
        MTBCF
Do = MTBCF + MTTRF

            A3.4.2.2.1. MTBCF is the average time between failures of mission-essential system
            functions. Critical failures do not have to occur during a mission; they merely must
            or could cause mission impact.
        number of operating hours
MTBCF = number of critical failures
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              101


           A3.4.2.2.2. Mean time to restore functions (MTTRF) is the average elapsed time, as
           a result of a critical failure, required to restore a system to full operating status. It
           includes administrative and logistics delay times associated with restoring function
           following a critical failure.
            total critical restore time
MTTRF =
            number of critical failures
           A3.4.2.2.3. NOTE: Consider the question, “Does the system need to be performing
           its specified mission to count against Do?” The key is to think, not in terms of
           collecting data for Do, but collecting data for MTBCF. The answer is dependent on
           the definition of when critical failures can occur for the system. If CFs can occur
           only during the specified mission for that system, then yes. If CFs can occur during
           non-mission time, then no, the calculated value of DO may then approach that for A0.
           The definitions for Ao and Do are different in that Ao time includes all time the system
           may be called upon to workmission or not. Do time includes time the system may be
           called upon to perform its mission and time that a critical failure could occurto
           emphasize again, a CF need not occur during a mission to cause mission impact
           unless defined that way for the system.
       A3.4.2.3. Utilization Rate (UR). UR is the average life units (LU) used or missions
       attempted per system during a specified interval of clock time. There may be both
       peacetime and wartime rates.
     number of operating hours
UR = number of possessed hours

  A3.4.3. Munitions/Missile Systems.
       A3.4.3.1. Stockpile Availability (As). Missiles and munitions should be able to
       withstand long periods of storage with little maintenance and still perform with high
       reliability. As reflects the percentage of on-hand assets capable of performing the task.
       The number of munitions on-hand normally excludes on-hand assets that are
       disassembled for storage and/or testing. For example, a disassembled munitions is not
       normally available munitions; once assembled and checked out, it becomes available.
       Equivalent definitions are used for theater-level As and force-level As.
       number of available munitions
As =
        total munitions in inventory
       A3.4.3.2. Sortie Surge Weapons Generation (SSWG). Sortie surge weapons
       generation is the number of operable munitions/missiles that can be assembled, delivered
       and loaded to meet wartime sortie requirements within a specified surge period under
       defined surge conditions. Thus, the measure is a rate. The aircraft type; quantities of
       personnel, aircraft, support equipment, munitions; and time constraints are based on
       operational requirements and should be established by the using command. Sortie surge
       weapons generation can be calculated through direct measurement but is normally
       estimated using a simulation model because of the lack of assets to conduct a
       representative generation during OT&E.
102                                          AFOTECPAM 99-104 24 SEPTEMBER 2013


         number of weapons assembled, delivered, and loaded
SSWG =
                     specified surge time period
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                              103


                                            Attachment 4
                               MAINTAINABILITY MEASURES

A4.1. Introduction. This attachment provides a discussion of data                  collection   for
maintainability measures and general examples of maintainability measures.
A4.2. Discussion.
   A4.2.1. Maintainability is the ability of an item to be retained in or restored to a specified
   condition (mission capable or partial mission capable) when maintenance is performed by
   personnel having specified skill levels, using prescribed procedures and resources at each
   prescribed level of maintenance and repair. Maintainability measures are based on time and
   critical failures that render a system inoperable (non-mission capable) per event.
   A4.2.2. What data to collect:
        A4.2.2.1. Critical failures that render a system “non-mission capable.”
        A4.2.2.2. Non-critical failures that render a system “partially mission capable.”
        A4.2.2.3. System or subsystem repair time in hours per event.
        A4.2.2.4. System or subsystem operating hours.
   A4.2.3. How to collect data:
        A4.2.3.1. Note and record critical failures per system/subsystem.
        A4.2.3.2. Note and record time-to-restore system/subsystem to operational status
        (mission capable or partial mission capable) per event.
   A4.2.4. How to analyze data: Compare repair time per system/subsystem per event and note
   trend/outliers that may affect overall rating.
A4.3. General Example Measures.
   A4.3.1. Fix Rate (FR). Percentage of broken systems (aircraft, satellite, etc) returned to
   operational status in a certain amount of time. For fighter aircraft, measurements are made at
   the 4-hour and 8-hour points. For other aircraft, measurements are taken at the 12-hour
   point. A broken aircraft is an aircraft that lands with an overall status of Code 3 (a grounding
   condition for which the aircraft is unable to meet at least one of its wartime missions). FR
   includes direct maintenance time and downtime associated with administrative and logistics
   delays.
        number of aircraft fixed within “x” hours
 FR =      total number of broken aircraft
   A4.3.2. Time to Restore Function. The time required, as the result of a critical failure, to
   restore a system to full operating status. It includes administrative and logistics delay times
   associated with restoring function following a critical failure. A typical metric is the MTTRF
   and is defined as
        total critical restore time
MTTRF = number of critical failures
 104                                               AFOTECPAM 99-104 24 SEPTEMBER 2013


   A4.3.3. Repair Time. Repair time is the on- and off-equipment corrective maintenance
   times in an operational environment. It includes all maintenance actions needed to correct a
   malfunction, including preparing for test, troubleshooting, removing and replacing
   components, repairing, adjusting, re-assembly, alignment, adjustment, and checkout. Repair
   time does not include maintenance, supply or administrative delays. MRT is a typical metric
   of repair time. Note that MRT differs from the contractual term MTTR in that MRT
   measures maintenance activities that occur in the operational environment.
            number of corrective repair hours
MRT =
         number of corrective maintenance events
   A4.3.4. Downtime. The elapsed clock-time between loss of mission-capable status and
   restoration of the system to mission-capable status. This downtime includes maintenance
   and supply response administrative delays, and actual on-equipment repair. In addition to the
   inherent repair and maintainability characteristics, downtime is affected by technical order
   availability and adequacy, support equipment capability and availability, supply levels, and
   manning. MDT is a typical downtime metric used to measure the average elapsed time
   between losing MC status and restoring the system to at least PMC status.
            NMC time
MDT = number of downing events

   A4.3.5. Time Between Maintenance Events. The time between on-equipment, corrective
   events including inherent, induced, no-defect, and preventive maintenance actions. A typical
   metric is the mean time between maintenance events (MTBME). Divide the total number of
   life units (for example, operating hours, flight hours, rounds) by the total number of
   maintenance (base level) events for a specific period of time. A maintenance event is
   composed of one or more maintenance actions.
   A4.3.6. Time Between Removals. A measure of the system reliability parameter related to
   demand for logistic support. A typical metric is the mean time between removals (MTBR),
   which is the total number of system life units divided by the total number of items removed
   from that system during a stated period of time. This term is defined to exclude removals
   performed to facilitate other maintenance and removals for TCTO.
   A4.3.7. MMH/LU. For aircraft systems, this measure is usually based on FHs, i.e.,
   MMH/FH. Maintenance data collection systems allow tracking by inherent malfunctions,
   induced malfunctions, no-defect events, total corrective events, support general maintenance,
   preventative maintenance, product improvement and the sum of all categories.
A4.4. Integrated Diagnostics Measures. The following are a sampling of possible integrated
diagnostics measures. Consider others for the particular system under test. The percentage
measures could be stated as probabilities in requirements documents. Test measures and metrics
should match the requirements documents.
   A4.4.1. Percent BIT Fault Detection (Pfd). The proportion of the number of faults
   detected by the system BIT to the total number of faults experienced by the system,
   expressed as a percentage.
           number of correct detections
Pfd =                                      × 100
        total number of faults experienced
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             105


   A4.4.2. Percent BIT Fault Isolation (Pfi). The proportion of detected faults that were
   unambiguously isolated to a single replaceable unit or other rule identified in the
   procurement specification (e.g., to a group of 3 or less replaceable units), expressed as a
   percentage. Clearly define “fault isolation.”
          number of correct fault isolations
Pfi =                                          × 100
            number of correct detections
   A4.4.3. Percent BIT False Alarms (Pfa). The proportion of the number of faults that
   cannot be confirmed upon investigation to the total number of fault indications, expressed as
   a percentage. This measure is an indication of how frequently the diagnostics indicate a fault
   exists when in fact the system is functional. This area is particularly troublesome, since the
   system false alarms may be either improper indications of faults that do not exist or faults
   that did exist but were transient in nature. Identifying which situation exits is most difficult
   for complex systems.
          number of BIT faults that cannot be confirmed upon investigation
Pfa =                                                                        × 100
                       total number of BIT fault indications
   A4.4.4. Percent Cannot Duplicate (Pcnd). The proportion of fault detections that result in a
   CND to the total number of maintenance events (excluding manually generated CND),
   expressed as a percentage.
          number of fault detections that result in a CND
Pcnd =                                                    × 100
              total number of maintenance events
   A4.4.5. Percent Retest Okay (Prtok). The proportion of LRUs/SRU that retest okay
   (RTOK) at the intermediate level to the total number of LRUs/SRUs tested at that level,
   expressed as a percentage.
          total number of LRUs/SRUs that RTOK at the intermediate level
Prtok =                                                                 × 100
             total number of LRUs/SRUs tested at the intermediate level
   A4.4.6. Time to Fault Locate. The time required to locate faults. Mean time to fault locate
   (MTTFL) is a typical metric. MTTFL is the total amount of time required to locate faults
   divided by the total number of faults.
               total time to locate faults
MTTFL =          total number of faults
   A4.4.7. Time Between False Alarms. The time in life units (e.g., operating hours, flight
   hours, cycles) between indicated (detected) faults where no fault could be confirmed (i.e.,
   false alarm). Possible metrics for this measure include the minimum and mean times. An
   example metric is mean operating hours between false alarms (MOHBFA). The formula for
   MOHBFA is
                  total operating hours
 MOHBFA = total number of false alarms
Another alternative is a FAR, which is the reciprocal of a mean time and indicates the number of
false alarms per life unit.
           total number of false alarms
FAR =              total life units
106   AFOTECPAM 99-104 24 SEPTEMBER 2013
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                             107


                                         Attachment 5
                                  LOGISTICS MEASURES

A5.1. Introduction. This attachment provides a discussion of logistics measures considerations
and general logistics measures.
A5.2. Test Considerations for Integrated Logistics Support (ILS) Elements. Historically, an
evaluation of logistics supportability has been based on a qualitative assessment of each of the
elements of ILS. However, some of the elements are better addressed under a different
operational suitability area. Key elements that should be evaluated under logistics supportability
are supply support (spares and repair parts), support equipment for all levels of maintenance, and
planned support facilities. Comparison of operational test data with ILS planning factors is a
good approach to evaluating logistics supportability. Planning, test methodology, data
requirements and evaluation considerations for support equipment, supply support, facilities and
maintenance planning are below.
   A5.2.1. Support Equipment (SE).
       A5.2.1.1. Planning Considerations. SE can be divided into two classes: major and
       non-major. Major equipment usually includes such items as avionics systems, automatic
       test stations, or newly designed complex support equipment. Non-major equipment
       usually includes such items as non-powered support equipment, hand tools, and support
       equipment that is already in the inventory. Normally, only a few major types of
       equipment, such as avionics test stations, will have quantitative requirements listed in the
       maintenance concept or other program documentation.
       A5.2.1.2. Test Methodology Considerations. The SE methodology should parallel that
       used for the prime equipment. Because support equipment is part of the total system,
       collecting suitability data for RAM, etc., for support equipment applies as well. Selection
       of support equipment measures will depend on the complexity and purpose of the
       equipment.
       A5.2.1.3. Data Requirement Considerations. Data are collected using either
       automated or manual systems. IMDS, SEDS and Micro-OMNIVORE described in
       Chapter 13 all have the capability to capture RAM data on support equipment. Manual
       logs containing maintainer comments, completed questionnaires, test team developed
       forms may also help provide a comprehensive evaluation.
       A5.2.1.4. Evaluation Considerations. When evaluating support equipment, compare
       what is procured and available at each maintenance level to actual needs. Identify
       support equipment deficiencies requiring enhancement or optimization. A model may be
       used to simulate time demands on support equipment, e.g., to estimate failed item arrival
       rates, support equipment waiting times, and turnaround times. If the number of arrivals is
       low (i.e., reliability is high), a two-level maintenance capability is appropriate. If the
       number of arrivals is long and support equipment availability, inadequate, additional
       system quantities may be needed. The intent is to evaluate each facet of system operation
       and maintenance to verify that the proper type and quantity of support equipment are
       available at the right location.
   A5.2.2. Supply Support.
108                                             AFOTECPAM 99-104 24 SEPTEMBER 2013


      A5.2.2.1. Planning Considerations. Maintaining operational readiness under diverse
      conditions of military use depends directly on the right supplies being available when and
      where they are needed. Supply support can be divided into six basic activities:
         A5.2.2.1.1. Provide consumption and failure rate data to supply provider’s in order to
         update provisioning factors.
         A5.2.2.1.2. Compare test data with provisioning data to identify items that appear to
         be under- or over-provisioned.
         A5.2.2.1.3. Review ISSL, bench stock, MRSP, or in-place readiness spares package
         (IRSP) listings and compare them with test results to identify items that appear to be
         under or over expected requirements.
         A5.2.2.1.4. Review supply planning decisions and compare them with test
         experience. NOTE: This process is an ongoing and should continue throughout
         OT&E.
         A5.2.2.1.5. Measure the performance of the supply support system and identify
         deficient areas. Example measures are requisition fill rates, PMCS, and NMCS.
         A5.2.2.1.6. Review level of repair decisions.
      A5.2.2.2. Test Methodology Considerations. The test methodology is based either on
      an investigation of the supply support planned for critical supply items or on the total
      provisioning process. Consider factors such as complexity, cost, criticality and failure
      rates in selecting candidate supply items. As a rule, quantitative evaluation criteria are
      appropriate for supply only during FOT&E. Until then, the supply support system is
      either the responsibility of the contractor or is in the initial buildup stage for organic Air
      Force support. The methodology can also be based on obtaining primary evaluation
      criteria such as PMCB NMCB (both denotes maintenance and supply). Additional
      considerations are:
         A5.2.2.2.1. Compare component level reliability metrics (e.g., MTBM, MTBD) and
         base-level repair capability (e.g., NRTS) to those used in the provisioning process.
         Although this comparison may not in itself mean an item has been under- or over-
         provisioned, it will help identify candidates for further T&E. Such factors as
         anticipated reliability improvements, improved technical data and increased base-
         level repair capability should be considered before determining that a component is
         under- or over-provisioned.
         A5.2.2.2.2. Identify potential supply problems caused by top PMCS, NMCS and/or
         cannibalization items.
      A5.2.2.3. Data Requirement Considerations. Use data from the reliability and
      availability evaluation plus data from the provisioning agency. If the data for this
      requirement should come from the contractor, the test planner should ensure the
      requirements from which the data are obtained are included in the development contract.
      Other required data will include such items as the ISSL, MRSP, IRSP and bench stock
      listings. The provisioning activity or the prime ALC normally develops and provides
      these data.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                           109


     A5.2.2.4. Evaluation Considerations. When evaluating supply support, compare key
     supply parameters against criteria stated in the capabilities requirement document.
     Identify the components that are under-provisioned. Review and assess the MRSP, ISSL,
     IRSP and bench stock listing, to support mission requirements. Review and assess the
     RLA and the resulting source, maintainability and recovery (SMR) codes. If possible,
     conduct a random statistical sample of provisioned items to draw conclusions about the
     overall provisioning plans. Model the supply support capability to arrive at an optimum
     balance between a stock-out situation and the proper level of inventory. One way to
     accomplish this is to use the planned supply support capability in the model and note the
     effect on other system parameters such as availability. The other way is to use an
     unrestricted supply capability and determine the amount of supply used. Then, compare
     this amount to the planned supply support. NOTE: Stock-out conditions promote
     cannibalization of parts and/or the necessity of initiating high-priority orders from the
     supplier. Both options are costly and impact base operations, since using commands
     must now pay for this support. Supply support capability may differ between items of
     high value or cost and the more common spares. For high-value items, greater emphasis
     is placed on replacement factors and usage rates. For other items, the economic order
     principle applies. This principle equates the cost to order spares with the cost to hold to
     arrive at an optimum or economic order quantity.
  A5.2.3. Facilities.
     A5.2.3.1. Planning Considerations. Base maintenance facility plans on engineering,
     operational, and maintenance requirements.            The test team should monitor all
     maintenance activities to identify any facility requirement that has not been satisfied, use
     AFI 32-1024, Standard Facility Requirements as a guide. Close coordination with the
     prime ALC and using command is important. The test team should review all applicable
     facilities plans to ensure facilities requirements are properly stated. Such items as heavy
     maintenance docks, work areas, storage requirements, wash rack and test cell
     requirements are of interest.
     A5.2.3.2. Test Methodology Considerations. The test methodology for facilities may
     cover site activation activities by working with the site activation task force (SATAF).
     On programs not employing a SATAF, evaluators may work with the prime civil
     engineering activity responsible for facility survey and planning. Review the base and
     MAJCOM facilities program plans. AFI 32-1024, Standard Facility Requirements,
     computations may be used when applicable. Quantitative inputs for these computations
     should come from the results of the reliability, maintainability and manpower evaluation
     areas. Monitor and review periodically maintenance activities in light of facilities
     requirements to identify and report any unique requirements, new facilities, additions or
     modifications needed to support the system.
     A5.2.3.3. Data Requirement Considerations. Data requirements for facilities will
     typically include the contractor’s facilities program plan, base and MAJCOM facilities
     program plan, minutes of site activation conference, meetings and working groups, and
     results of facility evaluation questionnaire.
     A5.2.3.4. Evaluation Considerations. The facilities evaluation may include review of
     programmed facilities requirements in light of test experience and review of activities to
110                                           AFOTECPAM 99-104 24 SEPTEMBER 2013


      identify any unique, new or altered facilities requirements, which have not been
      previously identified or programmed.
  A5.2.4. Maintenance Planning.
      A5.2.4.1. Planning Considerations. Maintenance planning assesses the adequacy of
      each significant maintenance task required to support the weapon system. Specifically, it
      is the assessment of the planning for all the activity required to achieve, restore or
      maintain the operational capability of the system or equipment. MOSs for this area will
      be the subjective assessments (backed up by qualitative and quantitative data from the
      other evaluation areas) of the acceptability of maintenance and logistics planning to
      provide the required support.
      A5.2.4.2. Test Methodology Considerations. The test methodology is based on:
         A5.2.4.2.1. Comparison of logistics factors used to compute the RLA and SMR
         codes with conditions actually being experienced or projected for the mature
         environment.
         A5.2.4.2.2. Comparison of key programmed suitability performance parameters with
         those actually experienced or projected and planning actions being taken to adjust or
         accommodate differences. Consider such factors as RAM performance, ID
         capability, supply provisioning, technical data adequacy, and support equipment
         suitability.
         A5.2.4.2.3. Changes in the maintenance concepts and appropriate adjustments or
         changes in maintenance planning.
         A5.2.4.2.4. Review of all suitability problems and the adequacy of maintenance
         planning to overcome or compensate for those problems, when applicable.
      A5.2.4.3. Data Requirement Considerations. Data for the maintenance planning
      evaluation will typically come from the other suitability evaluation areas. If ICS will
      occur until the system reaches IOC, obtain and review the transition plan to organic
      maintenance.
      A5.2.4.4. Evaluation Considerations. Design the evaluation of maintenance planning
      to identify areas where maintenance planning is not acceptable to support the required
      level of mission performance and to make appropriate recommendations. The following
      are suggested areas:
         A5.2.4.4.1. The ability of the maintenance planning to result in the necessary actions
         and support to ensure the system or equipment attains required operational capability.
         A5.2.4.4.2. The realism of criteria for repair times, RAM characteristics, support
         equipment requirements, maintenance skills, and facilities requirements.
         A5.2.4.4.3. An evaluation of whether or not the most efficient and economical repair
         levels have been established. NOTE: AFOTEC is required to report upon the
         system’s suitability for two-level maintenance.
         A5.2.4.4.4. The scope and completeness of transition plans designed to facilitate
         transfer of logistics support from contract to organic capabilities.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                            111


          A5.2.4.4.5. For CLS, provisions for documentation, source code, and skills levels
          may be assessed to identify potential hardware or software problem areas that could
          affect system support, configuration management or mission performance.
A5.3. Data Collection Before and During Test.
   A5.3.1. Suitability evaluators should review logistics and readiness portions of the
   capabilities requirement document for consistency with the program office’s LCMP. In
   addition, there may be other plans such as the RAM plan, test and support equipment plan,
   supply support plan, technical data plan, facilities plan and computer resources plan. Also, if
   the system is under development by more than one contractor (or in competition), OT&E
   suitability evaluators should anticipate reviewing separate plans from each contractual effort.
   Collectively, these plans will indicate what data needs to be captured during test. Data
   should be simple to collect and maintained in logbooks or existing data systems.
   A5.3.2. M&S may be used to analyze the ability of the system to meet some operational
   requirements within the planned level of support. M&S is useful in overcoming limitations
   or differences between the planned operational support and support available during OT&E.
   Use of M&S should be explained in the TEMP, verified, validated and accredited prior to use
   for OT&E purposes.
   A5.3.3. Derive specific quantitative measures for HM/HS, CBR contamination and BDR
   from the requirements document, if possible. They are typically related to the timeliness of
   performing HM/HS, CBR contamination and/or BDR procedures. Other measures may
   relate to ratings on particular questions addressed on the HM/HS questionnaire and other
   questionnaires contained in the AFOTEC Questionnaire Guide.
A5.4. Data Analyses after Test.
   A5.4.1. Quantitative measures related to logistics supportability include:
      A5.4.1.1. Not Mission Capable Supply. The rate at which an item cannot perform any
      of its wartime missions due to lack of parts.
      A5.4.1.2. Downtime. This is the logistics supportability component of readiness
      (availability). (See Figures 2.1., 2.2. and 2.3. for depictions of downtime.) A typical
      metric of downtime is MDT. MDT equals the sum of the mean maintenance time, the
      mean ADT and the mean LDT. The delay times can include time required to hire and
      train personnel, provision for supply or repair parts, transport repair parts to a repair
      facility, etc.
      A5.4.1.3. Number of Spares Required. Self-explanatory.
      A5.4.1.4. Percentage of Assets in Local Supply. The number of assets in local supply
      divided by the number of assets required.
      A5.4.1.5. Fill Rates. The rate at which supply is filled. It may be limited to critical
      items of supply.
      A5.4.1.6. Availability (e. g., PMCS, NMCB). Indicates effects of supply support.
      A5.4.1.7. NRTS rates, condemnation rates, bench-check serviceable rates, CND rates,
      cannibalization rates, and their causes.
112                                           AFOTECPAM 99-104 24 SEPTEMBER 2013


      A5.4.1.8. Time delay for awaiting parts to repair components.
      A5.4.1.9. Average repair days (indicates the average number of days it takes to repair an
      item, excluding time waiting for parts).
      A5.4.1.10. MTBD.
 A5.4.2. Measures Based on Responses to Various Questionnaires. Logistics
 supportability questionnaires are contained in the AFOTEC Questionnaire Guide.
      A5.4.2.1. Support Equipment Attributes. The following are attributes to consider.
         A5.4.2.1.1. Support equipment reliability, maintainability, availability, HF, safety
         and compatibility.
         A5.4.2.1.2. Support equipment suitability for mobility, deployment and bare-base
         operations, as appropriate.
         A5.4.2.1.3. Ease or difficulty of operation.
         A5.4.2.1.4. Effectiveness in performing troubleshooting and diagnostic functions.
         A5.4.2.1.5. Susceptibility to damage, contamination or corrosion.
         A5.4.2.1.6. Quantity of units needed versus authorized.
         A5.4.2.1.7. Other ILS elements’ capability to support the support equipment such as
         technical data and supply support (for the support equipment).
      A5.4.2.2. Supply Support Attributes. The following are attributes to consider.
         A5.4.2.2.1. Economic Order Quantity (EOQ). EOQ is expressed as the square root
         of (2CpD/Ch), where Cp is the average ordering cost, Ch is the cost of carrying the
         item in inventory (usually equal to 25 percent of the item price), and D is the annual
         item demand.
         A5.4.2.2.2. Component logistics reliability and criticality.
         A5.4.2.2.3. ISSL, MRSP, IRSP and bench stock listing acceptability.
         A5.4.2.2.4. Inadequacies in technical data or support equipment, which impact
         supply support.
         A5.4.2.2.5. SMR or equivalent coding.
      A5.4.2.3. Facility Attributes. The following are attributes to consider.
         A5.4.2.3.1. Programmed and forecast UR.
         A5.4.2.3.2. Number of systems and units per squadron or wing.
         A5.4.2.3.3. Test measurement diagnostic equipment and support equipment
         authorizations per squadron or wing.
         A5.4.2.3.4. Wash rack, phase inspection dock, fuel cell and similar requirements.
         A5.4.2.3.5. Munitions storage requirements.
      A5.4.2.4. Maintenance Planning Attributes. The following are attributes to consider.
AFOTECPAM 99-104 24 SEPTEMBER 2013                                                          113


        A5.4.2.4.1. The ability to effectively and efficiently support the weapon system.
        A5.4.2.4.2. The suitability of repair-level decisions and the ID concept.
        A5.4.2.4.3. The use of organic, ICS CLS resources for organizational-, intermediate-,
        and depot-level hardware and/or software support.
        A5.4.2.4.4. Requirements for utilities, security, environmental control, war readiness
        materiel storage, forward operating location and deployment, and hazardous materials
        handling and disposal.
        A5.4.2.4.5. Maintenance manpower authorizations.
        A5.4.2.4.6. The validity of the assumptions upon which the maintenance
        concept/plan was based.
        A5.4.2.4.7. Review of any existing MAJCOM Logistics Composite Model (LCOM)
        studies.
     A5.4.2.5. The use of questionnaires to evaluate logistics supportability is encouraged.
     The AFOTEC Questionnaire Guide, contains the standard questionnaires for use by test
     teams. Automated tools are available from A-9I to capture questionnaire data and
     analyze them for test reporting purposes. A-9I will provide the test team with a copy of
     the automated questionnaire tools and will provide training in the use of the tools.
