BY ORDER OF THE                                             AIR FORCE PAMPHLET 63-128
SECRETARY OF THE AIR FORCE
                                                                                10 JULY 2014

                                                                                   Acquisition

                                              INTEGRATED LIFE CYCLE MANAGEMENT



              COMPLIANCE WITH THIS PUBLICATION IS MANDATORY

ACCESSIBILITY: Publications and forms are available on the e-Publishing website at
               www.e-Publishing.af.mil for downloading or ordering.

RELEASABILITY: There are no releasability restrictions on this publication.

OPR: SAF/AQXA                                                          Certified by: SAF/AQX
                                                                                   (Mr. Smart)
Supersedes:   AFPAM 63-128, 5 Oct 09                                                 Pages: 268


This publication complements Air Force Policy Directive (AFPD) 63-1/20-1, Integrated Life
Cycle Management and Air Force Instruction (AFI) 63-101/20-101 Integrated Life Cycle
Management. This Air Force Pamphlet (AFPAM) provides guidance and recommended
procedures for implementing Integrated Life Cycle Management (ILCM) for Air Force (AF)
personnel who develop, review, approve, or manage systems, subsystems, end-items and services
(referred to as programs throughout this document) procured under Department of Defense
Directive (DoDD) 5000.01, The Defense Acquisition System and DoD Instruction (DoDI)
5000.02, Operation of the Defense Acquisition System (collectively called the DoD 5000
acquisition series). Additional non-mandatory guidance on best practices, lessons learned, and
expectations is available in the Defense Acquisition Guidebook.

If there is any conflicting guidance between this publication and DoD 5000-series, CJCSI
3170.01, CJCSM 3170.01, or other AF Directive Publications, higher level guidance shall take
precedence. To ensure standardization, any organization supplementing this publication must
send the implementing publication to SAF/AQX for review and coordination before publishing.
This publication applies to all military and civilian AF personnel, including Air Force Reserve
Command (AFRC) units and the Air National Guard; other individuals or organizations as
required by binding agreement or obligation with the Department of the Air Force (DAF).For
nuclear systems or related components ensure the appropriate nuclear regulations are applied as
specified in AFI 63-101/20-101.

Ensure that all records created as a result of processes described in this publication are
maintained in accordance with Air Force Manual (AFMAN) 33-363, Management of Records,
and disposed of in accordance with Air Force Records Information Management System
(AFRIMS) Records Disposition Schedule (RDS).
 2                                                                                            AFPAM63-128 10 JULY 2014


This AFPAM should be periodically reviewed and updated to reflect changes in overarching
policy directives and incorporate suggested comments from the field. Refer recommended
changes and questions about this publication to SAF/AQXA using the AF Form 847,
Recommendation for Change of Publication; route AF Form 847s from the field through Major
Command (MAJCOM) publications/forms managers. Forward all comments regarding this
AFPAM to: usaf.pentagon.saf-aq.mbx.saf-aqxa-workflow@mail.mil.

SUMMARY OF CHANGES

This publication is substantially revised and must be reviewed completely. The document fully
revises the Acquisition Strategy, Acquisition Program Baseline (APB), and Item Unique
Identification (IUID) sections. This document eliminates the product support element section
since it is integrated into multiple Office of the Secretary of Defense (OSD) guidebooks on
product support. The document adds new sections on tailoring, best practices, program
termination, document development, acquisition program baselines, program integration, and
adds multiple appendices. The LCMP template has been removed.

Chapter 1—INTEGRATED LIFE CYCLE MANAGEMENT                                                                                                 9
       1.1.   Purpose and Applicability of AFPAM 63-128. .....................................................                             9
       1.2.   Program Execution Responsibilities. .....................................................................                    9
       1.3.   ILCM Purpose. .......................................................................................................       11
       1.4.   ILCM Framework. .................................................................................................           12
       1.5.   The ILCM Tenets. ..................................................................................................         12
       1.6.   ILCM Principles. ...................................................................................................        13

Chapter 2—LIFE CYCLE DOCUMENTATION DEVELOPMENT                                                                                            15
       2.1.   Introduction. ...........................................................................................................   15
       2.2.   Scoping Documentation. ........................................................................................             15
       2.3.   Preparing the Documentation. ...............................................................................                15
Figure 2.1.   Development Process. ............................................................................................           17
Table 2.1.    Standard Supporting Processes. .............................................................................                18
       2.4.   Stakeholders. ..........................................................................................................    18
       2.5.   Coordination and Approval. ...................................................................................              19
       2.6.   Regulatory Contracting Approval (RCA) Documents. ..........................................                                 20
       2.7.   Solicitation Release. ...............................................................................................       20
       2.8.   Preparing Acquisition Documentation. ..................................................................                     21

Chapter 3—PROGRAM INTEGRATION                                                                                                             23
       3.1.   Overview ................................................................................................................   23
AFPAM63-128 10 JULY 2014                                                                                                                    3


       3.2.   Program Integration Values ...................................................................................                23
Figure 3.1.   Program Integration. ..............................................................................................           24

Chapter 4—PRODUCT SUPPORT AND SUSTAINMENT                                                                                                   30
       4.1.   Background. ...........................................................................................................       30
       4.2.   Purpose. ..................................................................................................................   30
       4.3.   Applicability. .........................................................................................................      30
       4.4.   Content. ..................................................................................................................   30
       4.5.   Access. ...................................................................................................................   31
       4.6.   Configuration Control: ...........................................................................................            31
       4.7.   Points of Contact. ...................................................................................................        31

Chapter 5—SYSTEMS ENGINEERING                                                                                                               32
       5.1.   RESERVED. ..........................................................................................................          32

Chapter 6—HUMAN SYSTEMS INTEGRATION (HSI)                                                                                                   33
       6.1.   HSI Description. ....................................................................................................         33
       6.2.   HSI Responsibility. ................................................................................................          33
       6.3.   HSI in Capabilities-Based Planning. ......................................................................                    33
       6.4.   HSI in Materiel Development. ...............................................................................                  35
       6.5.   HSI in Operations & Support. ................................................................................                 37
       6.6.   HSI in Disposal. .....................................................................................................        37
       6.7.   Best Practice - Tools, Standards, and Samples. .....................................................                          37

Chapter 7—CONTRACTOR INCENTIVES                                                                                                             39
       7.1.   Purpose of Incentives. ............................................................................................           39
       7.2.   Contractor Motivations. .........................................................................................             39
       7.3.   What to Incentivize. ...............................................................................................          39
       7.4.   Contract Types. ......................................................................................................        40
Figure 7.1.   General Overview of Incentives, Contract Type and Risk. ...................................                                   41
       7.5.   Selecting an Incentive. ...........................................................................................           42
       7.6.   Follow Up/Execution. ............................................................................................             44
       7.7.   Considerations for Successful Implementation of Incentives. ...............................                                   44
       7.8.   Additional Information/Training. ..........................................................................                   45

Chapter 8—ITEM UNIQUE IDENTIFICATION IMPLEMENTATION PLANS                                                                                   46
       8.1.   Introduction. ...........................................................................................................     46
 4                                                                                                AFPAM63-128 10 JULY 2014


       8.2.       Applicability of Item Unique Identification Implementation Plans. ......................                                    47
Figure 8.1.       Determining if IUID is Required. ..........................................................................                 48
       8.3.       Preparing the IUID Implementation Plan. .............................................................                       50
       8.4.       Coordination Process for IUID Implementation Plans ..........................................                               51
       8.5.       Registration and Implementation. ..........................................................................                 51
       8.6.       Foreign Military Sales (FMS) IUID Requirements. ..............................................                              52

Chapter 9—PROGRAM REALIGNMENT                                                                                                                 54
       9.1.       Purpose and Overview. ..........................................................................................            54
       9.2.       Background and Framework. .................................................................................                 54
Table 9.1.        Program Realigment Assessment Criteria. ............................................................                        54
       9.3.       Program Realignment Process. ..............................................................................                 56

Chapter 10—FIELDING PROCEDURES                                                                                                                58
       10.1.      Purpose and Overview. ..........................................................................................            58
       10.2.      Background and Framework. .................................................................................                 58
       10.3.      Materiel Fielding Process Overview. .....................................................................                   58
       10.4.      Materiel Fielding Planning and Assessment Criteria. ............................................                            58
Table 10.1.       Materiel Fielding Planning and Assessment Criteria. ............................................                            58
       10.5.      Materiel Fielding Process. .....................................................................................            62
       10.6.      Technology Maturation and Risk Reduction Phase. ..............................................                              63
       10.7.      Milestone B Decision. ............................................................................................          63
       10.8.      Engineering and Manufacturing Development (EMD) Phase. ..............................                                       64
       10.9.      Milestone C Decision. ............................................................................................          64
       10.10. Production and Deployment Phase. .......................................................................                        65
       10.11. Types of Materiel Releases. ...................................................................................                 65
       10.12. Incremental Materiel Releases. ..............................................................................                   66
Figure 10.1.      Incremental Materiel Release Review Concept (Notional Example). ...................                                         66
       10.13. Special Cases. ........................................................................................................         66
       10.14. Additional Information. .........................................................................................               67

Chapter 11—PRODUCT AND SOFTWARE DATA ACROSS THE LIFE CYCLE                                                                                    68
       11.1.      Overview. ...............................................................................................................   68
       11.2.      Address Data in Program Documents. ...................................................................                      68
Table 11.1.       Data and Data Rights Consideration by Program Documents. ..............................                                     69
AFPAM63-128 10 JULY 2014                                                                                                                         5


       11.3.     Address Data in Program Phases. ..........................................................................                      71
Table 11.2.      Data and Data Rights Consideration by Life Cycle Phase. ....................................                                    71
       11.4.     Address Data in Design Reviews. ..........................................................................                      72
       11.5.     Determine Technical Data and Data Rights Needs. ...............................................                                 72
       11.6.     Address Data in Requests for Proposal (RFPs). ....................................................                              73
Figure 11.1.     How Key Sections of The RFP Relate To Each Other as They Address Data. .....                                                    74
Table 11.3.      Data and Data Rights Consideration RFP Section. ................................................                                74
       11.7.     Data Rights Assertions. ..........................................................................................              77
       11.8.     Integrated Data Environment (IDE). ......................................................................                       77
       11.9.     Independent Research and Development (IR&D). ................................................                                   78
       11.10. Inspection and Acceptance of Data. .......................................................................                         78
       11.11. Life Cycle Management of Data and Data Rights. ................................................                                    78

Chapter 12—LIFE CYCLE RISK MANAGEMENT                                                                                                            83
       12.1.     Overview of Life Cycle Risk Management. ..........................................................                              83
       12.2.     Five-Step Life Cycle Risk Management Process Model. ......................................                                      89
Figure 12.1.     AF LCRM Process. ................................................................................................               90
Figure 12.2.     LCRM Risk Matrix. ...............................................................................................               96
Table 12.1.      Likelihood Criteria. ................................................................................................           97
Table 12.2.      Standard AF Consequence Criteria – Performance. ..............................................                                  97
Table 12.3.      Standard AF Consequence Criteria – Schedule. ....................................................                               99
Table 12.4.      Standard AF Consequence Criteria – Cost. ...........................................................                           100
Figure 12.3.     Translation of MIL-STD-882 Risk Matrix to the OSD Risk Management Guide
                 Matrix. ....................................................................................................................   101
       12.3.     LCRM across the Life Cycle .................................................................................                   105

Chapter 13—ACQUISITION PROGRAM BASELINE (APB) PREPARATION AND
            GUIDANCE                                                                                                                            107
       13.1.     Introduction. ...........................................................................................................      107
       13.2.     Goals. .....................................................................................................................   107
       13.3.     Parameters - Thresholds and Objectives. ...............................................................                        107
       13.4.     Acquisition Program Baseline (APB) Content/Structure. ......................................                                   108
       13.5.     APB Preparation and Approval Process. ...............................................................                          120
Table 13.1.      APB Signature Authorities. ...................................................................................                 122
       13.6.     Obligation Restrictions. .........................................................................................             122
 6                                                                                                AFPAM63-128 10 JULY 2014


      13.7.      APB Updates. .........................................................................................................       123
      13.8.      Program Deviations. ..............................................................................................           124

Chapter 14—PROGRAM TAILORING                                                                                                                  129
      14.1.      Program Tailoring. .................................................................................................         129
      14.2.      Program Determination. .........................................................................................             130
      14.3.      Milestones. .............................................................................................................    130
      14.4.      Delegation of Authority. ........................................................................................            130
      14.5.      Regulatory vs. ........................................................................................................      130
      14.6.      Program Information. .............................................................................................           130
      14.7.      Integrated Documentation. .....................................................................................              130
      14.8.      MDD. .....................................................................................................................   131
      14.9.      Milestone Decisions. ..............................................................................................          131

Chapter 15—PRODUCT SPECIFIC INFORMATION AND BEST PRACTICES                                                                                    133
      15.1.      Information Technology (IT). ................................................................................                133
      15.2.      IT Budget. ..............................................................................................................    133
      15.3.      Requirements. ........................................................................................................       133
      15.4.      ACAT III Defense Business System (DBS) Determination and Requirements. ...                                                   134
      15.5.      Entry into the Acquisition System Capabilities that were acquired outside the
                 normal acquisition process have multiple ways to support entry into the
                 acquisition process. ................................................................................................        134
      15.6.      Acquisition Strategy. .............................................................................................          135
      15.7.      Documentation. ......................................................................................................        135
      15.8.      Open Systems Architecture. ...................................................................................               135
      15.9.      Integrated Testing. .................................................................................................        135
      15.10. Modifications. ........................................................................................................          137
      15.11. Standardized Processes. .........................................................................................                137
      15.12. IT Sustainment. ......................................................................................................           137
      15.13. Clinger Cohen Compliance (CCA). .......................................................................                          137
      15.14. IA Implementation. ................................................................................................              139
      15.15. Information Support Plan (ISP). ............................................................................                     139
Table 15.1.      Required Architecture Data by Document. ............................................................                         140
      15.16. Records Management. ...........................................................................................                  142
      15.17. Privacy. ..................................................................................................................      144
AFPAM63-128 10 JULY 2014                                                                                                                        7


       15.18. Chief Financial Officer (CFO) IT Compliance. .....................................................                               145
       15.19. Nuclear RESERVED. ............................................................................................                   149
       15.20. Centers of Excellence/Resources ...........................................................................                      149
       15.21. Space Systems. .......................................................................................................           150
       15.22. Quick Reaction Capability. ....................................................................................                  152

Chapter 16—PROGRAM STRUCTURE                                                                                                                   155
       16.1.      Increments. .............................................................................................................    155
Figure 16.1.      Merged Increments to Single Program. .................................................................                       158
Figure 16.2.      Direct Increment to Program. ................................................................................                159
Figure 16.3.      Increments to Multiple Programs. ..........................................................................                  159
       16.2.      Subprograms for MDAPS ......................................................................................                 159

Chapter 17—POLICY COORDINATION, REVIEW, AND WAIVERS                                                                                            161
       17.1.      Integrated Life Cycle Management Publication Coordination. .............................                                     161
       17.2.      Waivers. .................................................................................................................   161
       17.3.      Changes. .................................................................................................................   161

Attachment 1—GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION                                                                                 162

Attachment 2—HSI-RELATED STANDARDS, HANDBOOKS, AND DIDS; SAMPLE HSI
             CHECKLISTS; AND KEY HSI-RELATED TERMS                                                                                             194

Attachment 3—ITEM UNIQUE IDENTIFICATION IMPLEMENTATION PLAN
             TEMPLATE                                                                                                                          208

Attachment 4—GUIDELINES FOR ACQUISITION SUBJECT MATTER EXPERTS
             DURING DEVELOPMENT PLANNING                                                                                                       216

Attachment 5—EXAMPLE PROGRAM MANAGER CHARTERS                                                                                                  219

Attachment 6—SUSTAINMENT METRICS CALCULATIONS                                                                                                  221

Attachment 7—INDUSTRIAL PREPAREDNESS                                                                                                           244

Attachment 8—FORMAT FOR NEW START VALIDATION                                                                                                   245

Attachment 9—AIR FORCE DRAWING APPROVAL, RELEASE AND NUMBERING
             PRACTICES                                                                                                                         246

Attachment 10—STANDARDS AND MANUALS FOR ENGINEERING DRAWINGS AND
             RELATED DOCUMENTATION                                                                                                             249

Attachment 11—CONFIDENCE LEVEL CONSIDERATIONS                                                                                                  252

Attachment 12—PROGRAM TERMINATION TEMPLATES                                                                                                    254
8                                             AFPAM63-128 10 JULY 2014


Attachment 13—ACQUISITION PROGRAM BASELINE EXAMPLES                261

Attachment 14—ACQUISITION PROGRAM TECHNICAL CERTIFICATIONS
             SUMMARY                                               265
AFPAM63-128 10 JULY 2014                                                                       9



                                           Chapter 1

                      INTEGRATED LIFE CYCLE MANAGEMENT

1.1. Purpose and Applicability of AFPAM 63-128. The intent of this pamphlet is to provide a
single source for descriptive, vice directive, guidance. It expands upon the directive
requirements in AFI 63-101/20-101; it is not meant to be used as directive guidance but instead
conveys expectations, best practices, lessons learned and other descriptive information to help a
program manager (PM) initiate and expedite the development and delivery of systems. The
AFPAM also helps define the interrelationships between the PM and other functional areas
throughout the life cycle. All members of the acquisition and sustainment community should use
this guidance in partnership with higher level guidance previously cited, other AFPAMs, the
Defense Acquisition Guidebook, and the Acquisition Sustainment Took Kit (ASTK) to guide the
PM on the right path to program success. The terms “weapon” and “weapon system” are used in
this document to represent all systems managed through Integrated Life Cycle Management;
likewise the term “warfight” represents all users these systems.
1.2. Program Execution Responsibilities.
   1.2.1. Service Acquisition Executive (SAE). The term “service acquisition executive”
   means the civilian official within a military department who is designated as the service
   acquisition executive for purposes of regulations and procedures involving acquisition and
   sustainment for that military department. For purposes of defining SAE responsibilities, this
   includes life cycle management of systems and services processes from Materiel
   Development Decision (MDD) to weapon system retirement. This includes research,
   development, test, evaluation, production, delivery, and sustainment of new systems, or
   modifications and support of existing systems. For Acquisition Category (ACAT) ID or
   IAM programs, management responsibility flows directly, without intervention, from the
   Milestone Decision Authority (MDA) to the SAE to the Program Executive Officer (PEO) to
   the PM. For all other programs, management responsibility flows directly, without
   intervention, from the MDA to the PEO to the PM.
   1.2.2. Program Executive Officers. The Program Executive Officer is responsible for total
   life cycle management of activities in their assigned portfolios, ensures collaboration across
   the ILCM framework, and is the MDA for delegated ACAT II and III programs. The PEO
   conducts activities identified within the following core areas:
       1.2.2.1. Portfolio Oversight. A key role of the PEO is to collaborate and achieve balance
       between the acquisition, requirements, test, and resource communities for all programs in
       the portfolio and throughout the life cycle. This is supported by maintaining a continuous
       dialogue with the operational, test, lead, and implementing commands. The PEO seeks to
       establish an environment that promotes steady progress in process
       efficiency/effectiveness improvements and overall reduction in program total ownership
       costs across the portfolio. The PEO evaluates the portfolio to ensure it is aligned with
       integrated architectures, implements solutions to support the business and mission need,
       and looks for opportunities to optimize programs by identifying gaps and redundancies
       within the portfolio. The PEO continuously assesses portfolio health to identify programs
10                                                              AFPAM63-128 10 JULY 2014


     with programmatic issues and implement solutions based on best practices and lessons
     learned.
     1.2.2.2. Program Oversight. The PEO has an active role in the definition and
     development of program strategies in order to ensure operational needs are translated into
     affordable and executable programs while identifying and managing risk. As part of
     oversight, the PEO evaluates trades-offs made early in the life cycle and their impact on
     achieving affordability targets within current fiscal reality. The PEO also monitors the
     programs’ collaboration with the warfighter on technical feasibility, alignment of
     programs with overall AF priorities, and cycle time (evolutionary acquisition) to meet a
     changing threat. The PEO evaluates programs’ efforts to reduce cost as an integral
     component to overall strategy and execution throughout the life with an emphasis on
     reducing Operations and Support (O&S) costs. This will often include monitoring
     requirements baseline control creep and schedule (a critical requirement on par with
     performance and life cycle cost). The PEO looks to maximize competition throughout
     the life cycle and recommends appropriate resources and strategies early in the
     acquisition that provide the PM maximum flexibility across the life of the program. If
     competition is facilitated, prices are usually lowered, small business participation is often
     enhanced, and new technologies and innovation can benefit the warfighter. By
     continuously assessing individual programs as well as overall portfolio strategies, the
     PEO may be able to identify requirements, performance, and schedule adjustments that
     could result in benefits to the warfighter, program, or portfolio.
     1.2.2.3. Program Managers. As the direct report for most PMs of assigned programs, the
     PEO charters or assigns PMs with the appropriate responsibilities and authorities to
     execute to an agreed to baseline. The PEO is able to promote a more efficient and
     effective workforce by fostering an environment that relies on a PM’s individual
     judgment and creativity, eliminating unnecessary functions and management layers,
     concentrating core functions at appropriate levels, and consolidating related functions.
     By assigning PMs with the appropriate functional skills and experiences based upon
     risks, priorities, and utilization of flexible personnel management approaches, the PEO
     can minimize turbulence on critical programs and ensure tenure of critical personnel to
     enhance execution performance and accountability.
     1.2.2.4. Business Intelligence. Another important role of the PEO is to understand the
     portfolio from the contractor perspective by maintaining knowledge of prime and major
     subcontractor efforts within the portfolio. This knowledge often needs to be at the
     contractor business segment level or the facility level to maintain adequate insight into
     the corporate activity of primes/major subcontractors including corporate leadership,
     incentives, and financial health. Understanding their corporate counterparts forms the
     basis for bargaining for advantage and building common goals. The PEO also maintains
     awareness of other AF and Sister Service PEOs with program content within the same
     contractor/ business segment portfolio(s) in order to identify shared concerns, seek
     opportunities for leverage, develop informed positions of contractor performance of his
     portfolio, and help facilitate PM awareness and insight. Additionally, the PEO often
     seeks out the Defense Contract Management Agency (DCMA) and Defense Contract
     Audit Agency (DCAA) perception of contractor performance at department, service,
AFPAM63-128 10 JULY 2014                                                                       11


       PEO, and program levels in order to leverage DCMA/DCAA awareness of subcontractor
       performance gained by programs across the department.
   1.2.3. Program Manager. The PM is responsible for integrating all program activities and
   constraints into an affordable, executable, and effective program to deliver the required
   capability to the warfighter within the baseline cost, schedule, and performance parameters.
   The PM must balance the multiple sets of requirements levied upon the program including
   performance, schedule, cost, statue, compliance, oversight, and documentation requirements
   to achieve program objectives while minimizing risk. The PM must properly allocate and
   utilize program resources including time, budget, and manpower as they direct the path
   forward to complete the program. To be successful, the PM should be knowledgeable of all
   higher level guidance, organizational opportunities and constraints, financial conditions, user
   expectations, and technological opportunities and limitations that may impact the program.
   The PM is the connection between the user, oversight authorities, and the program
   management office to manage expectations and guide execution of the program. For more
   information, detailed PM requirements are covered in AFI 63-101/20-101, but some general
   rules and expectations are covered below.
       1.2.3.1. Communication. A PM must be able to communicate all aspects of the program
       to stakeholders. To do this, the PM should take a comprehensive look at all aspects of
       the program by communicating with all the people who touch the program including but
       not limited to operators, maintainers, functional experts, contractors, oversight
       authorities, and program office personnel. The PM must be able to take this knowledge
       and communicate with individuals and organizations that are totally unfamiliar with the
       program by simplifying and eliminating unique “jargon”. This should allow the PM to
       position the program in the best possible light and use established communication lines to
       all stakeholders.
       1.2.3.2. Trust. The PM must be able to generate trust both horizontally and
       vertically. This means that above all the PM must be transparent and forthcoming in
       everything said and done. The PM must guard against even the appearance of integrity
       lapses to maintain the trust and confidence of mission partners and stakeholders. The PM
       should act and make decisions with the best interests of the program in mind and must
       treat contractors, staff, and stakeholders in a forthright manner.
       1.2.3.3. Control. The PM controls the program through decision making supported by
       clear and complete knowledge of the program. The PM is action oriented and delegates
       decision making as appropriate; the PM needs to ensure that he does not become a
       bottleneck.
1.3. ILCM Purpose. The AF exists to fly, fight, and win; to achieve strategic, operational, and
tactical objectives; unhindered by time, distance, or geography. As the world’s premier global,
multi-dimensional maneuver force, the AF safeguards the United States and its interests by
maintaining technical air, space, and cyberspace superiority. This is accomplished by focused,
disciplined, and effective product support of legacy weapon systems and where or when
advantageous, recapitalization of the same with more efficient, capable, reliable, and
maintainable systems or components. Critically important is the assurance these new capabilities
are acquired and sustained without duplication and overlap in acquisition, procurement, and
sustainment. Synergy across the enterprise is imperative as the AF invests in the future through
 12                                                             AFPAM63-128 10 JULY 2014


recapitalization, which includes any combination of new development, refresh, and overhaul
through the ILCM process. The goals of ILCM are to recapitalize AF capabilities through
maximum acquisition cycle time efficiency, provide agile support that will optimize fielded
capabilities and the supply chain, minimize the logistics footprint, and reduce total ownership
cost.
1.4. ILCM Framework. ILCM is the overarching system of concepts, methods, and practices
used by the AF to effectively manage systems from need identification through final disposal and
should be applied to AF acquisition and sustainment activities. ILCM should be composed of
seamless and transparent governance and core and enabling processes to acquire and sustain
systems, subsystems, end-items, and services to satisfy validated needs. The framework
provides an overarching management structure that integrates across multiple dimensions,
systems, portfolios, and management levels in order to effectively influence and execute life
cycle decisions in response to capability shortfalls. The six ILCM tenets outlined below provide
the governing management principles necessary for the execution of the ILCM Framework.
1.5. The ILCM Tenets. The six tenets of ILCM are life cycle planning and integration;
expectation management; collaborative and continuous requirements management; life cycle
systems engineering; technology planning and insertion; and continual, integrated testing.
Enabling principles necessary for successful application of the ILCM tenets are listed.
   1.5.1. Life Cycle Planning and Integration. ILCM ensures the program is actively managed
   throughout its entire lifespan, from conception and requirements generation, to technology
   and product development and testing, and throughout manufacturing and field operations
   until the system or product is retired and disposed. Three major parallel management and
   execution structures support life cycle planning and integration: Capabilities Based
   Requirements Development, System Acquisition and Sustainment, and Capabilities Based
   Test and Evaluation. This execution framework provides a roadmap for the ILCM
   stakeholders and process owners to use in the integrated management of programs across
   their entire life cycle.
   1.5.2. Expectation Management. Expectation management establishes program credibility
   and accountability through formal, recurring communication among stakeholders and is the
   cornerstone of the ILCM process. Significant reasons to actively manage expectations are 1)
   developers, users, and sustainers often interpret requirements differently, 2) program changes
   occur throughout development and are not always documented with impacts to cost,
   schedule, performance, and risk which affect end-item deliverables, 3) different users may
   have different views of probability of success, and 4) expectations can drift apart over time
   through leadership/personnel changes.
   1.5.3. Collaborative and Continuous Requirements Management.                   Collaborative
   requirements development requires the user, acquirer, enterprise architect, developer, tester,
   and sustainer to operate as one team. Continuous management is monitoring and controlling
   the weapon system or services requirements baseline throughout the program life cycle.
   While the user is responsible for identifying the required capability, this must be
   accomplished in a collaborative environment with all stakeholders in order to understand and
   communicate the “art of the possible.” The Joint Capabilities Integration and Development
   System (JCIDS) process identified in CJCSI 3170.01, Joint Capabilities Integration and
AFPAM63-128 10 JULY 2014                                                                       13


   Development System, is closely integrated with the acquisition process and exists to identify,
   develop, and validate defense-related requirements.
   1.5.4. Life Cycle Systems Engineering. Life cycle systems engineering is the overarching
   process governing the transition from a stated capability need to an operationally effective
   and suitable system. Systems engineering addresses architecture, requirements development
   and management, design, systems and software security, technical management and control,
   and test and evaluation (T&E)/verification and validation (V&V). It is the integrating
   mechanism for balanced solutions. The systems engineering process begins early in concept
   definition and covers all efforts across all life cycle phases, to include sustainment and
   disposal.
   1.5.5. Technology Planning and Insertion. Technology planning and insertion is the timely
   maturation and incorporation of relevant technology throughout the program life cycle to
   ensure an operationally effective and suitable system. Technology planning and the
   assessment of technology readiness levels include consideration of such factors as reliability,
   producibility, testability, sustainability, and operational performance. Successful technology
   planning and insertion as part of program life cycle management results in higher fidelity
   time phased requirements with a more realistic schedule and improved cost estimates.
   1.5.6. Continual, Integrated Testing. Continual, integrated testing structures T&E to reduce
   the time it takes to field effective and suitable systems by providing qualitative and
   quantitative information to decision makers throughout the program’s life cycle. Integrated
   testing minimizes the distinction between contractor, developmental, and operational testing
   by implementing integrated testing techniques and objectives to the maximum extent
   possible. Key stakeholders share all information in open T&E databases, identify problems
   early, engage contractors to fix deficiencies sooner, and ensure systems are ready to enter
   dedicated operational testing and fielding with a high probability of success.
1.6. ILCM Principles. The seven guiding principles of Integrated Life Cycle Management are
balance, responsiveness, credibility, streamlined and efficient management, innovation,
collaboration, and affordability/reduced ownership costs. Stability is an enabling principle
which is not fundamental to ILCM but greatly enhances acquisition.
   1.6.1. Balance. The goal is to balance the basic elements of acquisition - cost, schedule, and
   performance as well as balance the remaining ILCM principles. The community needs to
   balance near term needs with long term needs, balance functional considerations, and balance
   resources.
   1.6.2. Responsiveness. Speed matters. The ILCM community needs to be responsive to our
   warfighers needs and also responsible for providing timely, accurate, and complete
   information to decision makers.
   1.6.3. Credibility. Credibility in the way the ILCM community does business is essential.
   The ILCM community must create and maintain realistic expectations by applying expertise
   for accurate and transparent communications. This is true between the program offices and
   the warfighters for whom the systems are being developed, and between the program
   manager (and the program team) and the Milestone Decision Authority and other senior
   acquisition officials. All the stakeholders involved in a program must know what is and is
   not achievable, and the potential risks involved.
14                                                               AFPAM63-128 10 JULY 2014


  1.6.4. Streamlined and efficient management. The ILCM community needs to develop and
  implement initiatives to streamline and improve our management strategies. Acquisition and
  life cycle strategies need to be flexible to fit the conditions of the particular weapons systems
  program. Specific process requirements have to 1) make sense in the context of the program
  and 2) contribute something worthwhile to the process. Ensuring compliance with law or
  preventing undue risk is value added.
  1.6.5. Innovation. Innovation in what the ILCM community does - the ILCM community
  needs to adapt the best practices they find both in and outside of government to their needs;
  however innovation does not mean adapting every different practice in the name of change.
  Innovation in what the ILCM community delivers - the ILCM community needs to find ways
  to incorporate technologies expeditiously, both into our weapons systems and into our
  acquisition, sustainment, and maintenance processes.
  1.6.6. Collaboration. Collaboration and teamwork is required from the very beginning (early
  capabilities development work) and needs to continue throughout the life cycle. The
  warfighting, requirements, scientific and technical, testing, sustaining, and development
  communities must work together. A program that has requirements that cannot be acquired
  or tested makes no sense. Furthermore, an acquisition strategy that ignores the needs of the
  warfighter or ignores operations and sustainment needs and costs is also illogical and
  detrimental to the overall mission.
  1.6.7. Affordability and Reduced Ownership Costs. The right systems to meet validated
  needs are those that are affordable over their expected life. The ILCM community also has
  to remember that the decisions made up front generally drive costs for a long time in the
  future. While it may more often be the case that it is better to make adjustments in design to
  account for future sustainment concerns, sometimes it makes sense to save the modifications
  for later. Also, the ILCM community needs to keep a total force view on priorities and
  funding –budgets should not be treated as an allowance that must be spent, nor play money
  that can be adjusted as needed. These are tax dollars, so the ILCM community needs to
  support affordability and reduced ownership initiatives to help the AF meet national security
  objectives.
AFPAM63-128 10 JULY 2014                                                                     15


                                          Chapter 2

                  LIFE CYCLE DOCUMENTATION DEVELOPMENT

2.1. Introduction. This section of the pamphlet does not replace or supersede regulatory or
statutory requirements found in other documents. United States Code (USC) Title 10; DoDD
5000.01, The Defense Acquisition System; DoDI 5000.02 Operation of the Defense Acquisition
System; Federal Acquisition Regulation (FAR) Part 7, as supplemented; AFI 63-101/20-101,
Integrated Life Cycle Management; AFI 65-501, Economic Analysis (if applicable), AFI 65-509,
Business Case Analysis (if applicable), and other guidance documents listed in this AFPAM are
used to develop the appropriate documentation content. This chapter presents key points helpful
in the preparation and coordination process for life cycle documentation.
   2.1.1. All new AF programs and existing programs requiring OSD oversight should prepare
   documentation consistent with the OSD approved templates (Acquisition Strategy (AS), Life
   Cycle Sustainment Plan (LCSP), Program Protection Plan (PPP), Systems Engineering Plan
   (SEP), etc.). For other existing programs, the MDA should determine how to capture the
   information requirements dictated by the new templates; it is an expectation that PMs should
   utilize the OSD templates when developing program documentation. Regardless of the
   format used to document the results, PMs should ensure the content of the plans meet all
   applicable statutory and regulatory requirements.
   2.1.2. OSD approved templates are documented in the Defense Acquisition Guidebook.
   Other tools, such as Acquisition Document Development and Management (ADDM),
   provide templates for the PM to use as a starting point in the development of programmatic
   documentation.
2.2. Scoping Documentation.
   2.2.1. Documentation should be initially written at a strategic level and updated with an
   increasing level of detail as the program matures. Documentation crafted at a strategic level
   provides the vehicle by which the Air Force Secretariat and OSD can provide overarching
   guidance, while maintaining empowerment of the implementation strategy to the PM and
   PEO.
   2.2.2. The discussion in the documentation should be limited to the information required to
   adequately describe the overall strategy and support the requested decision. With few
   exceptions, the summary information required to meet statutory requirements should be
   incorporated into the body of the documentation, and the detailed document referenced.
   Attachments should be minimized and be essential to support the program strategy (per
   Defense Acquisition Guidebook). When using attachments or embedded information to meet
   other DoDI 5000.02 requirements, clearly identify these sections/attachments as well as the
   additional or unique coordination requirements they drive. When determining the scope of
   the documentation, consider if using the documentation to address other requirements may
   have the unintended consequences of delaying approval.
2.3. Preparing the Documentation. The most effective approach to developing documentation
is through the use of Integrated Product Teams (IPTs).
16                                                             AFPAM63-128 10 JULY 2014


  2.3.1. The collaborative efforts of a multifunctional team results in well-written and useful
  documentation . In many respects, the process used to develop the documentation is as
  important as the documents themselves. All stakeholders must be active participants in this
  process. This is best accomplished through the establishment of a document IPT.
  2.3.2. The document IPT, led by the PM or another designated leader, should develop a
  proposed life cycle strategy as early in the program as possible (see Figure 2.1). Each
  program may have several document IPTs with each lead dependent on the functional area
  each document primarily addresses/impacts. The process begins by developing drafts of the
  documentation appropriate for the program phase. This process could take several months,
  based on program complexity and the number of stakeholders involved. It is important to
  record the activities, issues, agreements, and comments as well as the disposition of issues
  and comments in order to avoid revisiting them later in the development and approval
  process. This record can take the form of detailed meeting minutes, comment resolution
  matrix, or an IPT journal.
  2.3.3. In accordance with DoDI 5000.02 and AFI 63-101/20-101, the MDA may tailor
  regulatory program information requirements and acquisition process procedures contained
  within the respective regulations to achieve cost, schedule, and performance goals. Statutory
  requirements cannot be tailored, except as specified by statute. Tailoring decisions need to be
  documented and the ability to tailor does not remove the need to process waivers from
  guidance in accordance with documented procedures. Tailoring may include adjusting the
  level of detail, format, or timing of approval of different subject areas. Another way of
  tailoring is to consolidate multiple documents, for example consolidating the Acquisition
  Plan, AS, and LCSP into one overarching document/plan. In addition to cost, schedule, and
  performance considerations, examination of tailoring documentation should consider the
  complexity and scope of the program. Larger, more complex programs for example would
  likely benefit from keeping documentation such as the AS and LCSP separate due to the
  amount of information and overall planning effort required for each activity.
  2.3.4. The PM should consider if the documentation is planned for release to the public
  and/or contractors when preparing the document. Required content should not be avoided or
  talked around, but identified for review prior to release. If the documentation contains
  sensitive information (such as source selection, Scientific and Technical Information
  (STINFO), etc.) that should not be openly shared, the documentation should be reviewed
  and if necessary sections removed, prior to releasing to the public, contractors, or to other
  restricted personnel (such as foreign nationals). Removed sections should be clearly
  identified and a reference and/or Point of Contact (POC) provided.
  2.3.5. As a result of the collaborative effort of the IPT, each version should have the
  appropriate level of detail and emphasis. Accordingly, the MDA and SAE expectations
  should be that all relevant issues have been discussed and any alternative program
  management, acquisition, and sustainment strategies have been explored during the
  preparation process. The final version of the document should focus on the best alternative
  for the program.
  2.3.6. Updated documentation (or amended            annexes) should include a statement
  summarizing the changes.
AFPAM63-128 10 JULY 2014                                                                       17


Figure 2.1. Development Process.

    MDD/                Strategy                                        Panel
                        Planning                 Draft                 Review
     MS

                                                                  • Cadre of
 • Formal             • Planning best      • Documents              executives and
   decision to          done with IPT of     management,            senior advisors
   pursue a             stakeholders         business, test and     from functional
   material                                  sustainment            disciplines
   solution                                  strategies at        • Provides counsel
 • Entry point into                          strategic level        to PM and MDA
   the acquisition                         • Documents other        in development of
   framework                                 specific statutory     strategy
                                             requirements           dependent on
                                                                    document
     MS/                                      Coordination
     RFP                   Approved                                  Review and
                                                  and
    Release                Document                                    Revise
                                               Approval

 • Update for each                         • Coordinated with       • Draft refinement
   Milestone or as    • Becomes part of      stakeholders             to incorporate
   needed               program history    • Approved by MDA          Panel guidance
 • May require                               or document              and direction
   update after                              approval authority
   contract award


   2.3.7. When determining participants on the IPT, PMs should take into account the context
   of the program, the content and maturity, and the level of involvement of the functional areas
   and stakeholders. As a minimum, the PM should consider representatives for the following
   functional areas on the IPT: comptroller/finance, cost analysis, contracting, systems
   engineering (to include National Infrastructure asset requirements), human systems
   integration (HSI), small business, safety, Occupational Health, intelligence (consider
   information needs for both adversaries as well as friendly entities), legal, life cycle
   (acquisition and sustainment) logistics, acquisition and information protection (to address the
   Program Protection Plan and Information Assurance), small business, test, and the
   Acquisition Center of Excellence (ACE) (for advice and guidance). In addition,
   representatives from the lead command, the primary using Major Command (MAJCOM),
   and the sustainment organization should be invited. Depending on the context of the
   acquisition, PMs should consider adding a representative from the small business office. In
   most cases, the local representatives from these functional areas should be adequate to start
   the process, but Headquarters Air Force (HAF) functional involvement in the IPT may be
   needed as you approach the Acquisition Strategy Panel (ASP) (especially for Major Defense
   Acquisition Program (MDAP)/ Major Automated Information System (MAIS) programs).
   For assistance in preparation for the ASP, contact SAF/AQXC or your local ACE.
        2.3.7.1. The Program Manager should have a keen sense of how standard supporting
        processes are integral to the program. Standard supporting processes are those processes
        that are applied continuously across the life cycle from requirement generation, to
        concept development, to system development, to fielding, to sustainment,
 18                                                               AFPAM63-128 10 JULY 2014


       modification/modernization, and verification/validation. These are the working-level
       processes that produce the critical information that is the basis of decision-making. The
       level of standardization required varies by process, and is the determination of the
       process owner. Table 2.1 shows an initial list of those standard supporting processes.

Table 2.1. Standard Supporting Processes.

Integration and System Engineering

Life Cycle Risk Management

Cost Estimating

Acquisition Intelligence

Test and Evaluation

Product Support

Modeling, Simulation, and Analysis

Identification and Management of Requirements

Information Protection

Scheduling

Data Management

Configuration Management

       2.3.7.2. The functional area representatives identified in 2.3.7 represent most of the
       owners of the standard processes. The PM should identify any gaps in coverage of the
       standard processes and add representation to the IPT to ensure standard processes are
       integrated. Successful integration of these processes allow the Program Manager and the
       team to enter a program with a known level of risk, develop early cost estimates refined
       throughout the life cycle, manage their programs more effectively, and provide consistent
       advice and recommendations to decision makers based upon information generated
       through application of these processes.
2.4. Stakeholders.
   2.4.1. Each of the program’s stakeholders must be involved in the preparation process. A
   stakeholder is an individual or organizational entity (users, developers, acquirers,
   technologists, testers, budget analysts, sustainers, and industry) that is, or will be, associated
   with implementing and supporting the associated system, subsystem, or end-item capability
   requirements. This includes representation from all appropriate staff levels (Secretariat, Air
   Staff, OSD, etc), MAJCOMs, Direct Reporting Units (DRU) and Field Operating Agencies
   (FOA) as well as the local Center staff (including ACE). PMs should contact the Program
AFPAM63-128 10 JULY 2014                                                                         19


   Element Monitor (PEM) to identify HAF-level staff elements and agencies that should
   participate in the preparation and approval process. Particular consideration should be given
   to the functional areas (listed in the previous section on IPT membership), but the specific
   stakeholders are based on the individual program. In addition, representation from other
   participating services or governmental agencies should be involved as necessary to ensure
   interoperability and consistency with future joint CONOPS. Early and continuous
   involvement with both operational and sustainment organizations have proven to enhance a
   cooperative relationship and maximize the opportunity for a successful program. The level
   of representation from these organizations may vary according to the ACAT level of the
   program.
   2.4.2. The nature of the stakeholders’ involvement in the process depends primarily on the
   size and complexity of the program. Not all stakeholders will want to be involved from the
   very start. However, the PM should invite them to be part of the process as early as possible
   to ensure they understand that active, early participation of the right people is the real key to
   program success.
2.5. Coordination and Approval.
   2.5.1. The IPT, signature authorities, and local procedures recommend which offices
   coordinate on the package. All signature agencies should be represented on the IPT to ensure
   early involvement and thus minimize coordination time during the coordination phase.
   Signature agencies should be identified early in the acquisition planning process to ensure
   their participation is obtained.
   2.5.2. Step 1: Local/Internal Coordination.
      2.5.2.1. As a minimum, coordination with the following “local” organizations takes
      place: the competition advocate, procuring contract office, the legal office, the Small
      Business Office, the appropriate logistics complex/organization representative, functional
      representatives at the program level (product support manager (PSM), Chief Engineer,
      etc.), and the using/lead MAJCOM representative. Additionally, depending on the type
      of program, signatures may be required from the buying office, contracting official, or
      Senior Center Contracting Official. For programs in the sustainment phase, local
      coordination would also be with appropriate AFMC or AFSPC functional offices.
      Consult your center-level ACE for local coordination and approval procedures.
      2.5.2.2. Once the initial documentation is completed and considered ready for
      coordination, the PM should send it to the PEO staff (or as instructed per local
      procedures). The PEO staff should distribute it to local advisors for final review and
      comment. Once all comments are addressed to the satisfaction of the PM, the PM seeks
      PEO review and coordination. If Headquarters (HQ) Air Force (HAF) review is required
      (see below), the PM should prepare a HAF staff package for PEO signature.
   2.5.3. Step 2: HAF Staff Review/External Coordination.
      2.5.3.1. For ACAT I, IA, and non-delegated II programs requiring SAE approval,
      SAF/AQE, using the SAF/AQ workflow, is the single entry point for acquisition
      documentation staffing packages requiring SAF/AQ coordination or approval. The PEO
      and their staffs should complete preliminary coordination (to include any appropriate 4-
      letter Air Staff coordination, MAJCOM or other external organization coordination, and
 20                                                              AFPAM63-128 10 JULY 2014


       internal coordination below the PEO level) prior to sending packages to SAF/AQ
       Workflow. PEOs are authorized to streamline coordination and are urged to coordinate
       with the appropriate stakeholders and required organizations.
       2.5.3.2. Delegated ACAT II, ACAT III, and sustainment programs do not require HAF
       coordination unless requested to address interoperability issues, meet statutory
       requirements, or as needed to satisfy unique program considerations. If uncertain, the
       PM may request the PEM assess and provide recommendations regarding HAF
       coordination. Following local coordination and any required external coordination, the
       documentation should be submitted to the PEO or the designated approval authority for
       approval for delegated ACAT II and ACAT III programs.
       2.5.3.3. As part of the coordination process during this step, documentation is provided
       to HQ AFMC or HQ AFSPC for coordination purposes in order to support enterprise
       planning and allow AFMC/CC or AFSPC/CC (or their designee) the opportunity to make
       recommendations supporting milestone decisions. Note: If a program uses resources
       from another MAJCOM (e.g. a space program managed by AFMC resources), it should
       be provided to the resourcing command for information and planning purposes.
   2.5.4. Step 3: Other Statutory/Regulatory Approvals.
       2.5.4.1. Depending on the decisions requested or statutory information provided,
       additional signatures may be required. For example, in procurement of conventional
       ammunition cases the Single Manager for Conventional Ammunition (SMCA) in the
       Army is required to review and provide written concurrence with all DoD ASs.
   2.5.5. Step 4: Final Approval by the MDA.
       2.5.5.1. Dependent upon the MDA, PEO/SAE follows either local PEO or SAE
       coordination procedures. Minimum required approvals are indicated in AFI 63-101/20-
       101.
2.6. Regulatory Contracting Approval (RCA) Documents.
   2.6.1. RCAs are additional Secretariat approvals required for a variety of situations. These
   include, but are not limited to: Indemnification Requests, Special Termination Cost Clause
   Approvals, Source Selection Delegation Requests, Source Selection Plans, Fixed Price
   Determinations, Organizational Conflict of Interest Waivers, Truth in Negotiations Act
   Waivers, and Justification and Approvals. A program may need one or more RCAs
   concurrent with the AS approval in order to implement its acquisition strategy.
   2.6.2. Approval of some RCAs is dictated by statute and is often above SAF/AQ. Use of
   acquisition documentation does not change the preparation and submittal of the various
   RCAs. The approving official’s awareness of the overall management strategy should
   usually hasten the coordination and approval process of the RCA if processed in parallel.
2.7. Solicitation Release. The MDA must approve the AS prior to the Contracting Officer’s
release of the final solicitation (DoDI 5000.02, Air Force Supplement to the Federal Acquisition
Regulation [AFFARS] 5307.104(d)). Other approvals (e.g., approval of the source selection
plan in a competitive acquisition) may also be required prior to solicitation release. Consult all
current guidance prior to releasing the solicitation.
AFPAM63-128 10 JULY 2014                                                                      21


2.8. Preparing Acquisition Documentation. Note: Refer to DoDI 5000.02 and the FAR for a
complete listing of statutory, regulatory, and contract reporting information and milestone
requirements.
   2.8.1. The content should be a direct result of the unique circumstances of the program and
   the membership of the IPT. With the exception of Special Access Programs that are
   submitted through the appropriate channels, the entire documentation content should remain
   unclassified and also free of any source selection sensitive information. Incorporate
   classified or source selection sensitive information by reference.
   2.8.2. The format and content in the templates provide a guide to the PM to ensure all areas
   are considered, but can be tailored based on the phase, ACAT level, and type of program
   with concurrence from the MDA. All sections identified should be included; if a section is
   not applicable to the program, state “Not Applicable” and provide justification. This should
   communicate that the area was considered and not just overlooked. If something needs to be
   added that is unique to the program, add it, but try to retain the standard format. The use of
   the standard format helps streamline coordination by helping reviewers easily locate specific
   information.
   2.8.3. Referencing Other Documents. Rather than paste large sections from other
   documents, the document should reference supporting or related documents if additional
   detail is required. The following are examples of documents that may contain information
   that needs to be summarized or referenced (either all or in part).
       2.8.3.1. Acquisition Program Baseline (APB)
       2.8.3.2. Analysis of Alternatives (AOA)
       2.8.3.3. Aircraft Information Program (AIP) Management Plan
       2.8.3.4. Capability Development Document (CDD)
       2.8.3.5. Capability Production Document (CPD)
       2.8.3.6. Collaboration and Measurements Requirements Summary (CMRS)
       2.8.3.7. Concept of Operations (CONOPs)
       2.8.3.8. Cost Analysis Requirements Description (CARD)
       2.8.3.9. Information Support Plan (ISP) - (Per Per CJCSI 6212.01F, Net Ready Key
       Performance Parameter (NR KPP))
       2.8.3.10. Information System Initial Capabilities Document (IS-ICD)
       2.8.3.11. Initial Capabilities Document (ICD)
       2.8.3.12. Integrated Technology Roadmap
       2.8.3.13. Force Structure Analysis
       2.8.3.14. Life Cycle Cost Estimates (LCCE)
       2.8.3.15. Life Cycle Sustainment Plan (LCSP)
       2.8.3.16. Manpower Estimate Reports (MER)
22                                                      AFPAM63-128 10 JULY 2014


     2.8.3.17. Memorandums of Understanding (MOU)/Memorandums of Agreement (MOA)
     (for joint programs)
     2.8.3.18. Modeling & Simulation Support Plan
     2.8.3.19. Programmatic Environment, Safety, and Occupational Health (ESOH)
     Evaluation (PESHE)
     2.8.3.20. Program Protection Plan (PPP)
     2.8.3.21. Source of Repair Assignment (SORA) Package
     2.8.3.22. Source Selection Plan (SSP)
     2.8.3.23. Systems Engineering Plan (SEP)
     2.8.3.24. System Threat Analysis (STA)/System Threat Assessment Report
     (STAR)/Information Operations Capstone Threat Assessment/Capability Threat
     Assessments (CTA)
     2.8.3.25. Technology Readiness Assessments (TRA)
     2.8.3.26. Technology Transition Plan (TTP)
     2.8.3.27. Test and Evaluation Master Plan (TEMP)
AFPAM63-128 10 JULY 2014                                                                        23


                                           Chapter 3

                                PROGRAM INTEGRATION

3.1. Overview
   3.1.1. The goal of Program Integration is to synchronize and analyze the array of technical,
   cost, and schedule information so that the AF can communicate program concerns with a
   single and streamlined voice. Program Integration provides the AF with consistent insightful
   and synchronized recommendations, improved data reporting and analysis, and maximized
   utilization of resources. It is not only a source for more robust decision-support to PMs, it is
   also a bridge to a more collaborative relationship between HQ and the field. Therefore, the
   objective is to strengthen decision-making across AF Acquisition leadership.
   3.1.2. The integration of cost, schedule, and performance requires a single management
   process and/or organizational structure. Depending on the size and complexity of the
   program, different integration management approaches are necessary for each acquisition
   program. Each PEO and PM determine appropriate organizational structures/processes
   deemed appropriate to meet the needs of the respective programs, while simultaneously
   addressing the intent of this guidance.
   3.1.3. Each respective PEO /PM designates (as a minimum) a Program Integration Manager
   for all ACAT I programs. The PEO should consider a Program Integration Office at the PEO
   level to help service and advise on all programs within their portfolio. The PM can serve as
   the Program Integration Manager. Based on program needs, complexity, and development
   phase, PEOs and PMs establish and manage the appropriate Program Integration staff
   requirements as agreed between the PEO and PM and Program Integration Manager. Figure
   3.1 identifies program integration activities.
3.2. Program Integration Values
   3.2.1. Program Integration values functional diversity and expertise and the team-based
   approach. Program Integration Offices and processes allow programs to maintain a strategic
   perspective by monitoring and coordinating the answers to the following questions:
      3.2.1.1. Which performance factors have the greatest impact on the program?
      3.2.1.2. How are environmental and programmatic issues anticipated and risk mitigated?
      3.2.1.3. What is the best way to optimize resources within a program or across an
      investment portfolio?
      3.2.1.4. Are programs aligned with Service and DoD strategic objectives?
   3.2.2. By illuminating these analytical questions, Program Integration Managers, Program
   Managers, and other acquisition decision-makers are constantly comparing individual
   program needs and issues over its life cycle with those of the greater enterprise.
 24                                                               AFPAM63-128 10 JULY 2014


Figure 3.1. Program Integration.




   3.3.1. Program Integration Organizational Structure. Because no two acquisition programs
   are exactly alike, each Program Manager should consider the following questions when
   determining the size and scope of its Program Integration staff:
      3.3.1.1. What is the nature of the product or service? This should be the most significant
      determiner of program structure.
      3.3.1.2. How mature is the technology that will be included in the product?
      3.3.1.3. What will have to be done to mature that technology, and how much risk is
      involved?
      3.3.1.4. In addition to the technology that is included, how complicated will the design
      be?
      3.3.1.5. Is it like other designs that we have experience with, or is it novel?
      3.3.1.6. How difficult are the integration aspects of building and sustaining the product?
      3.3.1.7. Is the manufacturing technology mature, or will work have to be done to
      advance it prior to production? These questions on a large scale will begin the process of
      determining if a technology maturation and risk reduction phase is needed prior to the
      start of engineering and manufacturing development. They should also affect the duration
AFPAM63-128 10 JULY 2014                                                                     25


     of these phases, if used, and the number of test articles and types of testing that should
     have to be performed to verify the performance of the design.
     3.3.1.8. How urgently is the product needed?
     3.3.1.9. How prepared is industry to design and produce the product?
     3.3.1.10. How much uncertainty is there about the proper balance of cost and capability?
     3.3.1.11. What are the customer’s priorities for performance and life cycle sustainment?
     3.3.1.12. What resource constraints will affect program risk (not just financial resources,
     but also availability of competitors, time, and expertise in and out of government)?
     3.3.1.13. Is cost or schedule most important, and what are the best ways to control them
     on this program?
     3.3.1.14. What is the right balance of risk and incentives to provide to the contractors to
     get the results the government wants?
     3.3.1.15. What phase is the program in and which Program Integration activities require
     the most supervision and monitoring?
  3.3.2. Program Integration may include (but is not limited to) the activities and processes
  depicted in Figure 3.1. Listed below are some specific Program Integration activity and
  process descriptions:
     3.3.2.1. Audit Focal Point. This Audit Focal Point activity involves the assignment of an
     individual, either permanently or temporarily, to act as the liaison between program
     personnel and auditors. Responsibilities may include coordinating meetings and
     interviews between program personnel and auditors or acting as a repository for
     requested information and responses.
     3.3.2.2. Acquisition Program Baseline Management. Acquisition Program Baseline
     Management is the process of establishing, monitoring, and reporting program progress
     in achieving the Cost, Schedule, and Performance Objectives documented in the
     Acquisition Program Baseline (APB). . Specific functions might include:
         3.3.2.2.1. Serving the PM, PEO, and/or MDA for all matters of cost, schedule, and
         technical performance requirements in the management of all program baselines that
         exists for the program;
         3.3.2.2.2. Collecting data, measuring and monitoring performance of internal and
         external suppliers throughout the organization, including small business utilization
         and forecasting;
         3.3.2.2.3. Reporting program status internally to the PM/PEO and/or MDA and
         coordinating external communication and reporting;
         3.3.2.2.4. Planning and Management—Providing total life-cycle baseline
         management planning for the program/project and managing the implementation of
         that planning;
26                                                             AFPAM63-128 10 JULY 2014


        3.3.2.2.5. Identification—Establishing baseline information and documentation of
        functional and physical characteristics of each type of item (cost, technical, schedule,
        etc.);
        3.3.2.2.6. Documenting agreed-to baselines and changes to those configurations that
        occur over time;
        3.3.2.2.7. Change Management—Ensuring that changes to a baseline are properly
        identified, recorded, evaluated, approved or disapproved, and incorporated and
        verified, as appropriate;
        3.3.2.2.8. Status Accounting—Managing the capture and maintenance of product
        configuration information necessary to account for the configuration of a product
        throughout the product life cycle;
        3.3.2.2.9. Verification and Audit—Establishing that the performance and functional
        requirements defined in the baseline are achieved by the design or plan and that the
        design or plan is accurately documented in the specific associated baseline;
     3.3.2.3. Budget Management. Budget management is responsible for the PPBE
     (Planning, Programming, Budgeting, and Execution) process. This includes advocating
     for programs through the POM (Program Objective Memorandum) process feeding into
     the President’s Budget cycle, ensuring program “should cost” estimates are accurately
     reported, monitoring executable dollars through system reconciliation of all years
     funding, coordinating distributed funds, and conducting funds analysis. Budget
     Management provides oversight of official budgetary documents including the writing,
     coordinating, and monitoring of funding documents, and schedules fiscal year phasing
     based on programmatic analysis. Ultimately, Budget Management activities ensure funds
     availability, funds execution status, and funds reprogramming for contractual vehicles.
     3.3.2.4. Data Collection, Analysis, and Reporting. Data and reports collected are
     focused on measuring what we want to control at the enterprise level and at the program
     level. Therefore, data measured needs to be authoritative, accurate, complete, and
     analyzed at all levels without adjustment or spin.
     3.3.2.5. Cost Estimating and Analysis. The Cost Estimating and Analysis activity within
     Program Integration involves cost estimating, sufficiency reviews, should costs reviews,
     business case analyses, and proposal evaluations. Cost estimating and analysis activities
     may include the administration of cost, economic, financial, business case analysis
     policy, and guidance; the development of standards and templates; and the assurance of
     training and education for designated individuals throughout the program. In addition,
     cost estimating and analysis activities involve the participating in cost and technical data
     collection efforts, the maintenance of a historical cost database, the selection of
     appropriate cost estimating methods and model development projects, and the monitoring
     of program costs using earned value management (EVM).
     3.3.2.6. Strategic Communications (Strategic Communication and Data Information,
     Analysis, and Reporting). Strategic Communications activities ensure consistent
     messages are being conveyed to higher headquarters and external organizations. In order
     to sustain a program, the DoD must obtain approval and appropriations from Congress.
AFPAM63-128 10 JULY 2014                                                                        27


     To provide Congress with a complete and accurate picture of a program’s execution,
     program information must be able to answer the following:
        3.3.2.6.1. Is there the right amount of money in the specific budget year?
        3.3.2.6.2. How well are programs executing in terms of performance, cost, and
        schedule? How do we effectively and efficiently utilize tax payer dollars?
        3.3.2.6.3. What is the priority of this program versus other Federal macro-economic
        issues?
        3.3.2.6.4. Is your program required, and does the logic fit the overall service plan?
        3.3.2.6.5. How have you responded to Congressional Staffers?
        3.3.2.6.6. What do your J-Books say?
        3.3.2.6.7. Constraints to be aware of include:
            3.3.2.6.7.1. Time. Time is of the essence, and if program information is not
            available on time, decisions still will be made.
            3.3.2.6.7.2. External information such as GAO and CRS reports are used to
            inform Congress.
            3.3.2.6.7.3. Best responses include a brief explanation of problem, realistic
            Statuses of Program and identification of issues and proposed risk mitigations
            with specific timelines.
     3.3.2.7. Document Control Management.          Document Management includes the
     collection, recording, monitoring and reporting of all official program records. Such
     internal documentation of programs is very important for the long-term management of
     the program and associated supporting functions. The program integration effort requires
     a high level of collaboration and sharing among all functions. Documentation helps
     provide consistency, continuity, and an understanding of management decisions from
     both internal and external perspectives. Therefore, document management procedures
     must define:
        3.3.2.7.1. How a program approves documents (e.g., procedures, flow-charts, process
        maps, etc.) prior to use (e.g. signed-off paper versions, or added to your computer
        network via a password protected system);
        3.3.2.7.2. How a program updates and re-approves amended documents (recommend
        use of computer based systems);
        3.3.2.7.3. How a program identifies changes (e.g. by date or issue number, identify
        changes with different fonts or colors);
        3.3.2.7.4. How a program ensures that documents are available where they are
        needed;
        3.3.2.7.5. How a program controls documents of external origin;
        3.3.2.7.6. How a program prevents the inadvertent use of obsolete documents;
        obsolete-but-still-in-use is the single most common non-compliance.
28                                                             AFPAM63-128 10 JULY 2014


        3.3.2.7.7. How a program handles and maintains materiel according to the
        classification level.
     3.3.2.8. Earned Value Management (EVM). EVM involves the interpretation,
     implementation, compliance, oversight, and enforcement on all EVM-related matters in
     accordance with DoD EVM policy and guidance. EVM tasks establish processes to
     utilize EVM System (EVMS) outputs to support decision-making and accountability at
     all levels and supports the EVMS validation and surveillance processes in conformance
     with the Defense DCMA Standard Surveillance Operating Manual (SSOM). In addition,
     EVM activities involve the development and execution of procedures for oversight and
     enforcement actions for noncompliance with EVM policy.
     3.3.2.9. Financial Risk/Health Analysis. Financial Risk/Health Analysis involves
     conducting analyses of contractors’ financial health using standard finance and
     investment calculations to assess contractors’ motivations and ability to perform. This
     task includes the analysis of Forward Price Rate Agreements (FPRA) as published by
     DCMA for impact on program baseline and fiscal health of contractor. The analysis is
     used as part of the integrated risk analysis for overall impacts on the program
     3.3.2.10. Integrated Risk Analysis. An Integrated Risk Analysis (IRA) is a specific
     series of events that integrate the analysis of program risks associated with program cost,
     performance, and schedule dimensions.:
     3.3.2.11. Program Execution Reviews. Program Execution Review activities include the
     following:
        3.3.2.11.1. Developing and briefing budget execution plans;
        3.3.2.11.2. Comparing obligation and expenditure rates to OSD goals and analyzing
        any deviations; comparing obligation and expenditure rates to forecasted amounts and
        analyzing any deviations;
        3.3.2.11.3. Comparing the annual program office cost estimate to the approved
        program budget and developing a workaround plan for any projected shortfalls.
        3.3.2.11.4. Participating in Spring Execution Reviews/Investment Budget Reviews,
        PEO Reviews, and Program Management Reviews;
        3.3.2.11.5. Analyzing cost and schedule variances in Earned Value Management
        reports;
        3.3.2.11.6. Calculating an Estimated Cost at Completion (EAC) and
        projecting/budgeting for any cost overruns reflected in the EAC; 3.3.2.11.7.
        Reporting of small business initiatives and small business achievement at the prime
        and subcontract levels.
     3.3.2.12. Program Management Reviews (PMRs). PMRs facilitate external reviews for
     the program often including contractor support in such reviews.
     3.3.2.13. Program Integration Planning. Program Integration Planning tasks include the
     verification of the quality and compliance of all program documentation (Acquisition
     Strategy, Test and Evaluation Master Plan (TEMP), and LCSP) prior to making major
     programmatic decisions (e.g., Milestone Decisions). This requires the knowledge of all
AFPAM63-128 10 JULY 2014                                                                 29


     major functions within the program and their associated status as well as a keen
     understanding of organizational issues. This activity supports acquisition planning and
     oversight activities such as program assessments, Request for Proposal (RFP)
     development, source selections, and internal reviews and assessments.
     3.3.2.14. Schedule Management and Analysis. Schedule Management and Analysis
     ensures the program conforms to the developed program schedule and that necessary
     actions are taken to keep the program on schedule. Schedule Management activities
     include the monitoring of program progress against the program baseline (using the
     program schedule to measure progress). In addition, Schedule Management and Analysis
     includes:
        3.3.2.14.1. Support for implementation of requirements for the contractor’s
        Integrated Master Plan (IMP);
        3.3.2.14.2. Representation as the OPR for implementation of the contractor’s
        Integrated Master Schedule (IMS);
        3.3.2.14.3. Independent assessment of the IMS by analyzing and reporting on the
        IMS via analysis tools;
        3.3.2.14.4. Schedule Risk Assessments (SRAs) in support of program decisions and
        milestone events.3.3.3. Program Integration Staff Knowledge and Skills. Program
        Integration knowledge and experience takes time to cultivate. As such, a Program
        Integration Manager should have experience in Program Management as well as in
        other functional areas such as Financial Management, Contracting, Engineering, and
        Product Support. By having personnel with breadth and depth of experience, the AF
        ensures that a highly trained and experienced individual is synthesizing data and
        providing carefully analyzed recommendations to AF leadership.
 30                                                              AFPAM63-128 10 JULY 2014


                                           Chapter 4

                        PRODUCT SUPPORT AND SUSTAINMENT

4.1. Background. The Acquisition Sustainment Tool Kit (ASTK) was developed by acquisition
and sustainment professionals and subject matter experts from across the AF as part of the
Product Support Campaign Process Focus Team effort. It is designed for program and logistics
managers’ use but can also be used by other disciplines in the acquisition and sustainment
communities.
4.2. Purpose. The ASTK is designed to provide standardized, repeatable processes to ensure
product support is incorporated early in the planning stages and throughout the life cycle. The
application of this tool should enable the acquisition and sustainment communities to field
products and services with complete cradle to grave support that are affordable, reliable, and
sustainable. The ASTK’s approach to up front supportability planning directly contributes to the
AFPD 63-1/20-1, Integrated Life Cycle Management, direction of establishing an ILCM
approach to recapitalize AF capabilities through maximum acquisition cycle time efficiency.
This should in turn provide agile support that should optimize fielded capabilities and the supply
chain, minimize the logistics footprint, and reduce total ownership cost.
4.3. Applicability. The ASTK can be used by anyone performing daily acquisition and
sustainment tasks on any weapon system or commodity. Use of the ASTK should aid in the
development of operationally safe, suitable, and effective weapon systems and facilitate their
transition to sustainment.
   4.3.1. Formal training on the ASTK is available through the following AF Institute of
   Technology (AFIT) courses:
       4.3.1.1. AFIT LOG 131, Industrial Maintenance Management,
       4.3.1.2. AFIT LOG 499, Logistics Executive Development Seminar,
       4.3.1.3. AFIT SYS 281, Air Force Acquisition & Sustainment,
       4.3.1.4. AFIT SYS 400, Current Topics in Air Force Acquisition & Sustainment.
4.4. Content. The ASTK is a single body of acquisition logistics information, containing
checklists and links to DoD and AF directives, instructions, policies, and guides for acquisition
and sustainment procedures. Each process or task is listed under its applicable Department of
Defense (DoD) 5000 acquisition phase for easy reference and must be evaluated for program
application. All current Air Force Instructions, the Acquisition Process Architecture Team
(APAT) Model, and Independent Logistics Assessment Handbook were used to develop the
ASTK. The Tool Kit consists of:
   4.4.1. Acquisition Sustainment Processes Matrix – The Acquisition Sustainment Processes
   Matrix encompasses all programmatic aspects relevant to product supportability, logistics,
   and readiness at major acquisition milestones and other key decision points. It is a roadmap
   of separate logistics processes sequentially listed from pre-Milestone A through disposal.
   The processes matrix serves as a ready reference for ensuring product support is incorporated
   early in the planning stages.
AFPAM63-128 10 JULY 2014                                                                     31


   4.4.2. Acquisition Sustainment Checklists – The Acquisition Sustainment Checklists
   supplement the Processes Matrix and contain process descriptions, subtasks, and hyperlinks
   to supporting documentation for specific, complex tasks. They provide a starting point of the
   who, what, where, when, and how of the matching process embedded in the Processes
   Matrix.
   4.4.3. Acquisition Sustainment Kneepad Checklist – The Acquisition Sustainment Kneepad
   Checklist serves as a user guide to supplement the Processes Matrix and Checklists,
   providing greater detail on each process. All Acquisition Sustainment Checklists are
   attached to an appendix in the Kneepad Checklist for quick and easy reference.
4.5. Access. All processes and tasks within the Acquisition Sustainment Processes Matrix,
Checklists, and Kneepad Checklist are linked by the same Task Identification number, providing
for easy cross reference.
4.6. Configuration Control: The ASTK should continue to evolve to ensure AF logistics
support maintains a system life cycle focus. Responsibility for update and configuration control
of the ASTK rests with the Commander, Air Force Materiel Command (AFMC/CC). The ASTK
should be updated to maintain consistency with emerging policy changes as required; as a
minimum, the ASTK should be updated annually. Current ASTK configuration is identified by a
date included in all documents and file names of the tool kit materials. Proposed changes and
edits should be evaluated and incorporated in accordance with the ASTK Virtual Configuration
Control Process (VCCP). .
4.7. Points of Contact. Specific questions or comments on the ASTK should be addressed
through AFLCMC/LG.
32                                     AFPAM63-128 10 JULY 2014


                       Chapter 5

                 SYSTEMS ENGINEERING

5.1. RESERVED.
AFPAM63-128 10 JULY 2014                                                                       33


                                           Chapter 6

                         HUMAN SYSTEMS INTEGRATION (HSI)

6.1. HSI Description. Human Systems Integration (HSI) is the interdisciplinary technical and
management processes for integrating human considerations within and across all system
elements. Those human-centered elements, or “domains,” are manpower, personnel, training,
environment, safety, occupational health, human factors engineering, personnel survivability,
and habitability. The HSI domains embody all the dimensions of human characteristics that need
to be addressed to ensure systems can be operated and sustained in a manner that accomplishes
the DoDI 5000.02 goal to minimize total ownership costs while optimizing total system
performance. An optimal system design harmonizes system hardware/software with the physical
and cognitive abilities and limitations of humans, so the planned number of users, with the
planned knowledge, skill and abilities, can be trained to safely operate and maintain the system,
for the intended duration, in the intended environments, and survive. The expected results are
systems that reduce the potential for human error, increase operational availability, and improve
safety and performance. Best practice is to maintain relationships across the domains to
continuously address HSI issues, risks, and concerns and log, coordinate, track, and document
resolution decisions in applicable program documents. HSI is not a discipline or program in-
and-of-itself, but an integral part of the capability-based planning and materiel development
processes from conception to design to operation and maintenance. See the Defense Acquisition
Guidebook (DAG), Chapter 6, for more information on HSI.
6.2. HSI Responsibility. DoDD 5000.01, DoDI 5000.02, and AFI 63-101/20-101 place the
onus for HSI across the system’s life-cycle on the PM. However, there are offices,
organizations, and functional communities responsible for the HSI “domains” that contribute to
the PM’s task of optimizing the human contribution to total system performance. HSI
“practitioners” from these HSI domain functional communities provide expertise in professional
disciplines like Environmental Engineering, Engineering Psychology, Human Factors
Engineering, Occupational Medicine, Operations Research, Systems Safety, etc. These
functional communities have technical and management processes, specialty expertise,
standards, and guidance that contribute to the safety, personnel survivability, performance, and
health of the warfighter. To that end, all offices, organizations, and functional communities that
address human considerations throughout the life cycle of the system contribute to Human
Systems Integration and representatives from those communities are HSI practitioners.
6.3. HSI in Capabilities-Based Planning. HSI planning begins with concept development.
HSI is most effective when incorporated early in the planning phase of each new or enhanced
capability and should be considered in Advanced Technology Demonstrations (ATDs) and early
Modeling and Simulation (M&S) opportunities. Lessons learned from related developments or
deployed systems should be considered, entering each life cycle phase, and should be updated at
the end of each phase.
   6.3.1. Threat Assessments. Threat assessments address force protection and personnel
   survivability and are the basis for the CONOPS; i.e., how the warfighter intends to use the
   capability in the projected environments
34                                                              AFPAM63-128 10 JULY 2014


  6.3.2. Capability Based Assessments (CBAs). Outputs from the CBA include: an analysis of
  doctrine, organization, training, materiel, leadership policy and education, personnel,
  facilities, and policy (DOTMLPF-P) capability gaps, and courses of action (COAs) to
  mitigate the capability gaps. While “T” (training) and “P” (personnel) are direct elements of
  HSI, “D, O, M, L, F and -P” considerations often affect HSI. Therefore, DOTMLPF-P
  analyses need to include human considerations in all COAs.
  6.3.3. Requirements Development. As the capability solution matures, HSI considerations
  identified during the CBA process are reflected in the Joint DOTMLPF-P Change
  Recommendation (JDCR), Initial Capabilities Document (ICD), Capabilities Development
  Document (CDD), and Capabilities Production Document (CPD). Best practice is to provide
  language in the ICD, CDD, and CPD to ensure that human considerations are addressed
  adequately. However, avoid thinking in terms of “HSI requirements” which suggests that
  there should be unique requirements specifically-named “HSI.” Any requirement may
  highlight human considerations that logically result as part of good and effective capability-
  based requirements. The standardization documents in Attachment 2, Table A2.1 and the
  questions in Attachment 2, Tables A2.2 and A2.3, are useful resources for writing
  meaningful HSI-related requirements. The following sub-paragraphs highlight key areas for
  maintaining the HSI perspective in requirements documents.
     6.3.3.1. HSI implications from the CBA are found in four sections of the ICD.
         6.3.3.1.1. ICD Section 3 - Required Capability: Documents the capability required.
         This is where the human-related implications identified in the CBA are documented.
         6.3.3.1.2. ICD Section 4 - Capability Gaps and Overlaps or Redundancies: Describes
         the Capability Gaps: missions, tasks, and functions that cannot be performed or are
         unacceptably limited. The limitations of human performance are included in this
         section.
         6.3.3.1.3. ICD Section 6 - Ideas for Non-Materiel Approaches (DOTMLPF-P
         Analysis): Details the results of the DOTMLPF-P analysis. This provides a number
         of opportunities for HSI-related considerations to be introduced.
         6.3.3.1.4. ICD Section 7 - Final Recommendations: Describes materiel and non-
         materiel recommendations for responding to the capability gaps. HSI
         recommendations related to these approaches are documented in this section.
     6.3.3.2. HSI implications evolving from the ICD are addressed in the CDD/CPD. The
     CDD/CPD is written to define threshold and objective values for a single increment of
     the capability being developed. The primary objective of the CDD/CPD is to specify the
     operational performance criteria of the system being developed to deliver the required
     capability. The CDD/CPD considers and integrates the full range of joint materiel and
     DOTMLPF-P solutions. Documenting HSI-related requirements and attributes in the
     CDD/CPD is key to getting user/maintainer needs effectively translated into system
     specifications. There are three sections of a CDD/CPD where HSI can directly shape
     capability definition:
         6.3.3.2.1. CDD/CPD Section 6 - System Capabilities Required for the Increment.
         This section provides a description of each system attribute, identifies supporting
         rationale for the capability, and cites analytic references to support the specific needs
AFPAM63-128 10 JULY 2014                                                                     35


         for attribute threshold and objective values. This section contains the required system
         performance attributes (not physical design attributes) that have implications for HSI.
         These attributes are prioritized as Key Performance Parameters (KPP), Key System
         Attributes (KSA), or Additional Performance Attributes (APA). The JCIDS Manual
         provides guidance on required and selectively applied KPPs and KSAs.
         6.3.3.2.2. CDD/CPD Section 14 - Other DOTMLPF-P Considerations. This section
         describes any additional DOTMLPF-P implications associated with fielding the
         system that have not already been addressed in the CDD/CPD, to include: those
         approaches impacting CONOPS or plans within a combatant command’s area of
         responsibility, status highlights (strategy and timing) of the other DOTMLPF-P
         considerations, and implications for likely changes to any aspect of DOTMLPF-P.
         This section discusses HSI considerations that have a major impact on system
         effectiveness, suitability, and affordability. For example, HSI considerations include
         key logistics criteria (system reliability, maintainability, transportability, and
         supportability) to help minimize the system’s logistics footprint, enhance mobility,
         and reduce the total ownership cost. Additionally, any basing needs (forward and
         main operating bases, institutional training base, and depot requirements), specific
         facility, shelter (habitability), supporting infrastructure and environment, safety and
         occupational health (ESOH) asset requirements are addressed.
         6.3.3.2.3. CDD/CPD Section 15 - Other System Attributes (OSA). OSA Address
         other attributes of the proposed capability, including many that tend to be design,
         cost, and risk drivers. These attributes include ESOH and HSI considerations that
         have not been previously addressed embedded instrumentation (e.g., Human Factors
         Engineering), electronic attack, anti-tamper, information protection standards and
         information assurance (IA), and wartime reserve mode (WARM) requirements. In
         addition, HSI considerations such as conventional and initial nuclear weapons effects;
         chemical, biological, radiological and nuclear (CBRN) survivability; natural
         environmental factors (such as climatic, terrain, and oceanographic factors, and
         impact of the systems on the environment); and unplanned stimuli (such as fast cook-
         off, slow cook-off, bullet impact, fragment impact, sympathetic detonation, and shape
         charge jet) are discussed in this section. Considerations include applicable safety
         parameters, such as those related to system, nuclear, explosive, and flight safety as
         well as physical and operational security needs (manpower, personnel and training).
         6.3.3.2.4. CDD/CPD Section 15 does not include Thresholds and Objectives, because
         items in this section are “other system attributes,” not performance parameters. For
         Section 15, documenting HSI-related considerations or implications associated with
         particular system attributes is usually a sufficient level of detail for the CDD/CPD.
         However, these considerations need to be included in program costs.
6.4. HSI in Materiel Development. HSI planning in concept development becomes HSI
execution in materiel development.
   6.4.1. Development Planning (DP). HSI-related requirements need to be explicitly
   addressed in DP to ensure the human is included the trade space evaluation of emerging
   capability needs, system-of-systems assessments, risk drivers, and life cycle planning.
36                                                            AFPAM63-128 10 JULY 2014


  6.4.2. Analysis of Alternatives (AoA). The HSI perspective during the AoA is to evaluate
  alternative solutions to determine the alternative that maximizes human performance,
  minimizes HSI-related costs and supports safe and effective operations, maintenance, and
  support functions. Best Practice is to take mission tasks lists identified during the CBA,
  develop and translate them into assumptions, limitations, measures of effectiveness (MOE),
  and measures of performance (MOP) to evaluate solution alternatives. The AoA is also the
  forum to develop justifications for thresholds to be documented for use in later phases.
  6.4.3. Technical Reviews. HSI is a key focus area of technical reviews such as System
  Requirements Review (SRR), System Functional Review (SFR), Preliminary Design Review
  (PDR), and Critical Design Review (CDR). Statutory and regulatory phase-specific entrance
  criteria for programs are found in the tables of DoDI 5000.02, which also provides guidance
  for technical reviews of program progress. The DAG discusses exit criteria. Best practice is
  to have the HSI perspective represented in the activities to evaluate the technical reviews,
  exit criteria development, and risk assessment.
     6.4.3.1. DoDI 5000.02 requires PMs to report on the status of ESOH risks and
     acceptance decisions at technical reviews and clearly articulates that ESOH not only
     deals with the system impact on the environment, but includes the impact on the human.
     User concurrence and acceptance by appropriate decision authority must be documented,
     along with the associated risks, before accepting for serious- and high-risk items.
     6.4.3.2. All decisions made to address risk management or balance competing
     requirements that impact the HSI domains need to be documented in post-technical
     review reports and the Systems Engineering Plan (SEP).
  6.4.4. Systems Engineering Plan (SEP). DoDI 5000.02 states “The Program Manager will
  plan for Human Systems Integration… and will summarize HSI planning in the SEP.”
     6.4.4.1. The ODASD/SE SEP outline requires the following content in the table named
     “Mapping Key Design Considerations into Contracts”: 1) a descriptive summary of the
     HSI planning (best practice is to describe key HSI domain interactions and trade
     analyses); 2) identification of the cognizant official responsible for HSI in the program;
     and 3) any contractual requirements (by Contract Data Requirements Lists (CDRL) #)
     addressing HSI. The cognizant official responsible for HSI should also be identified in
     the “IPT Details” table of the SEP and/or the applicable IPT Charter. Reference
     paragraph 2.3.7 of this instruction for more details regarding HSI representation in IPTs.
     6.4.4.2. Alternatively, the PM’s plan for HSI can be a separate document. The PM can
     require an HSI plan from the contractor and/or write their own plan. Either way, there
     are three DIDs that provide an adequate structure and description of the content for HSI
     planning and reporting: DI-HFAC-81742 – Human Engineering Program Plan (HEPP),
     DI-HFAC-81743 – Human Systems Integration Program Plan (HSIPP), DI-HFAC-81833
     – Human Systems Integration Report (HSIR). If the HSI plan is a separate document,
     then that document should be referenced and hot-linked in the SEP table described in the
     paragraph above.
  6.4.5. Test and Evaluation. AFI 99-103 and AFMAN 63-119 address developmental test
  (DT), operational test (OT), and specialized testing performed by the Test and Evaluation
  community. The Test and Evaluation Strategy (TES) captures the approach to testing,
AFPAM63-128 10 JULY 2014                                                                        37


   including HSI items, prior to Milestone A. These human-centric test criteria are further
   developed in the Test and Evaluation Master Plan (TEMP), where they are reflected in
   critical operational issues (COIs), critical technical parameters (CTPs), objectives and
   thresholds. Eventually, test plans use MOEs, MOPs and MOSs to quantitatively evaluate the
   system’s capability; the human part of the system is either explicitly or implicitly included in
   this evaluation.
   6.4.6. Contracting. The HSI perspective should be represented when developing Requests
   for Proposals (RFPs), Statements of Work (SOW), CDRLs, and the System Requirements
   Document (SRD) to ensure that HSI-related requirements are adequately requested, derived
   and translated from the operational capabilities documents to system specifications. Best
   practice is to include HSI in the source selection evaluation criteria and capture HSI-related
   requirements, tasks, and deliverables on contract to ensure a human-focused engineering
   approach to system, equipment and facility design, development and test.                  The
   standardization documents in Attachment 2, Table A2.1 are useful resources for contracting
   activities.
6.5. HSI in Operations & Support. It is important to maintain the HSI perspective for
modifications and upgrades to fielded systems resulting from operational deficiency reports,
lessons learned, safety reports, etc. Modifications and upgrades, including Commercial Off-the-
Shelf (COTS) solutions, can have a significant impact on the human; e.g., hardware accessibility,
software upgrades that change the operator interfaces, changes in operator workload,
performance in extreme environments, changes to emergency operation and egress, changes to
visibility and anthropometrics, training procedures and training systems, maintainability, safety,
etc. AFI 63-131 describes the process to initiate a modification proposal to fielded systems using
the AF Form1067.
6.6. HSI in Disposal. It is important to maintain the HSI perspective during disposal of the
system; such as disassembly, detoxification, decontamination, disposal of hazardous waste, and
transportation to and from disposal site. See the DAG Chapter 6 for more information.
6.7. Best Practice - Tools, Standards, and Samples.
   6.7.1. The Acquisition Sustainment Tool Kit, section 1.13.1 addresses HSI. See Chapter 4
   of this publication for more information on the Acquisition Sustainment Tool Kit.
   6.7.2. The Defense Acquisition Program Support (DAPS) methodology includes a section on
   HSI. The DAPS methodology provides a standardized framework to assist program
   managers and decision makers assess readiness for milestone decision reviews. The DAPS
   methodology is available in the public domain, see: Defense Acquisition Program Support
   (DAPS) Methodology, 2008, Washington, D.C: Office of Deputy Under Secretary of
   Defense for Acquisition and Technology, Systems and Software Engineering.
   6.7.3. MIL-STD-882, DoD Standard Practice System Safety; MIL-STD-1472, DoD Design
   Criteria Standard Human Engineering; and MIL-STD-46855, DoD Standard Practice
   Human Engineering Requirements For Military Systems, Equipment, and Facilities, are three
   suitable military standards to address HSI.
   6.7.4. MIL-HDBK-338, Electronic Reliability Design; MIL-HDBK-470, Designing and
   Developing Maintainable Products and Systems; and MIL-HDBK-759, Human Engineering
   Design Guidelines, are three suitable military handbooks and guidebooks to address HSI.
38                                                           AFPAM63-128 10 JULY 2014


  6.7.5. The ASSIST web-site provides more DIDs, standards, specifications, etc., commonly
  used to address, plan, or manage HSI activities. Attachment 2 of this publication includes,
  Table A2.1, which contains a sample, non-exhaustive list of HSI-related standards and DIDs;
  Table A2.2, a sample phase-based HSI checklist, Table A2.3 a sample HSI domain-based
  checklist, and a list of key HSI-related terms.
AFPAM63-128 10 JULY 2014                                                                         39


                                           Chapter 7

                               CONTRACTOR INCENTIVES

7.1. Purpose of Incentives.
   7.1.1. One of the primary responsibilities of a PM is the development of an effective
   acquisition strategy to achieve cost, schedule, and performance objectives. This chapter
   provides insight into industry motivation, possible incentive tools/approaches, and a series of
   questions to guide the PM in developing incentives appropriate to his or her particular effort.
   Additional information can be found in the Award Fee Guide, the DoD and NASA Incentive
   Contracting Guides, and in the FAR.
7.2. Contractor Motivations. Money (or profit) is usually the first motivator considered, but
that is not the only motivation. Contractors are also concerned with:
   7.2.1. Company growth (new business, new products, increased market share);
   7.2.2. Enhanced public image and prestige;
   7.2.3. Opportunity for follow-on business;
   7.2.4. Cash flow and internal rate of return (IRR);
   7.2.5. Keeping available skills and capacity (keeping personnel on the payroll for future
   business);
   7.2.6. Intangibles - Intangibles include a number of psychological and sociological factors.
   Companies are run by “people” and their individual and group motivations play a basic role
   in how well an incentive works in a company.
7.3. What to Incentivize. The government normally incentivizes three factors: cost control,
technical performance, and schedule. However, in order for an incentive to be effective, the
contractor must perceive it is achievable—and tied to an appropriate motivator. It should be
noted that, per the FAR, no incentive contract may provide for performance or schedule
incentives without also providing a cost incentive (or constraint). Note: Regardless of
incentives, the contract should reflect a schedule that meets “mission needs.” Normally a
delivery date would not be put in the contract and then contractor asked to “beat” that date;
however, there may be situations where we evaluate the contractor on how efficiently they
respond to unusual circumstances that require an accelerated response (needed surge capability
for example).
   7.3.1. A cost incentive relates profit or fee directly to results achieved by the contractor.
   These incentives are normally based on a sharing formula between the Government and the
   contractor (i.e., fixed-price incentive (FPI) or cost plus incentive fee (CPIF) contracts) or the
   payment of a fee from an award fee pool within specified cost and schedule parameters. To
   be most effective the incentives should be quantitative, clearly related to the desired
   outcome, and within a reasonable range.
   7.3.2. Technical incentives have been used to motivate contractor superior technical
   performance. Emphasis can focus on design (improved reliability/maintainability, increased
 40                                                              AFPAM63-128 10 JULY 2014


   capability of a product, reduction in manufacturing time and/or equipment, or improvement
   in services (improved maintenance processes, reduced supply chain timelines)).
   7.3.3. Schedule incentives are used to incentivize the contractor to meet mission critical
   schedule requirements.
7.4. Contract Types.
   7.4.1. The government has a range of contract types available for its use. The spectrum
   covers firm fixed price (FFP) contracts that offer the most incentive to control cost to Cost
   Plus type contracts that provide for less motivation to control costs (risk shifts to the
   government). By including an incentive profit or fee, with emphasis on cost, the contractor
   has the opportunity to earn additional profit or fee by controlling costs.
   7.4.2. A determination and finding, signed by the head of the contracting activity, is
   completed for all incentive and award-fee contracts justifying that the use of this type of
   contract is in the best interest of the Government. Similarly, fixed price contracts for
   development of major systems must be approved by Acquisition, Technology, and Logistics
   (AT&L) IAW Defense Federal Acquisition Regulation Supplement (DFARS) 235.006.
   7.4.3. If a program’s primary incentive focus is on objectively verifiable cost, schedule, or
   performance criteria, then fixed-price incentive (firm target) (FPIF) or cost-plus incentive fee
   (CPIF) contract types are generally good choices. As a general rule, acquisition teams should
   consider the FPIF type first, then CPIF. The acquisition team should select a cost-plus award
   fee (CPAF) or fixed-price contract with award fee (FPAF) only upon determining that FPIF
   and CPIF type contracts are not appropriate.
   7.4.4. Award fee contracts may emphasize multiple aspects of a contractor’s performance in
   a wide variety of areas, such as quality, timeliness, technical ingenuity, overall management
   of the contract, and cost. Award fee incentive payments are tied to acquisition objectives
   with emphasis on cost, schedule, and technical performance.
      7.4.4.1. Subjective evaluation of the contractor’s performance can allow the government
      to use “judgment” such as “anticipation of problems” or “problem solving” to reward a
      contractor. Award fee type incentive arrangements may only be used when it is not
      possible to establish objective criteria to evaluate contractor performance; use of award
      fee requires documented justification and approval.
      7.4.4.2. Award Fee Plans should be structured to motivate excellent contractor
      performance. The Award Fee Plan sets forth the evaluation criteria for assessing
      contractor’s performance; how well the contractor performed determines the amount of
      the fee that may be paid. The plan should provide a structured approach to how
      contractor’s performance is evaluated (Award Fee Evaluation Board members, monitors,
      fee determining official (FDO) responsibilities, etc.); reference the AF Award Fee Guide
      for more information.
      7.4.4.3. An award fee is to be earned—each evaluation period the contractor starts at
      zero percent of potential award fee pool available for that period. Depending on
      performance, the contractor may earn up to 100% of the available award fee pool for that
      period.
AFPAM63-128 10 JULY 2014                                                                       41


      7.4.4.4. Use of an award fee incentive is labor intensive in that it requires a management
      structure to oversee and evaluate the contractor’s performance, which adds administrative
      costs and management effort to oversee the program. Therefore, award fee contracts are
      only recommended when the contract amount, performance period, and expected benefits
      warrant the additional administrative effort and when it is not possible to identify
      objective performance/ schedule measures. A cost benefit analysis is required (must be
      documented in the contract file) to justify the use of an award fee type arrangement.
   7.4.5. In those instances where objective criteria exist, and the Contracting Officer and
   Program Manager decide to also evaluate and incentivize subjective elements of performance
   by including award fee, the most appropriate contract type would be a multiple incentive type
   contract containing both incentive and award fee criteria (e.g., cost-plus-incentive/award fee,
   fixed; fixed-price-incentive/award fee) or a fixed price/award fee contract.
   7.4.6. 'Non-Monetary' Incentives. Award Term contract can be a useful incentive for
   recurring products or services. It allows a contractor, through superior performance, to earn
   additional work—the potential for future revenue can be a very effective incentive. Note:
   Award Term should be drafted to address funding availability if used for annually funded
   work.
   7.4.7. Figure 7.1 provides general guidance on types of contracts with fees and types of uses
   for them. Note: Actual requirements should be based on specific needs.

Figure 7.1. General Overview of Incentives, Contract Type and Risk.
 42                                                             AFPAM63-128 10 JULY 2014


7.5. Selecting an Incentive.
   7.5.1. The first question the PM should ask is, “Do I need an incentive?” The answer could
   very easily be no if the contract is short-term, or the contractor has a proven history of
   superior performance on similar efforts and can deliver the required product without an
   incentive. If the program has critical delivery dates, tight cost targets, technology issues,
   software risks, or challenging performance requirements, an incentive may be a good
   business decision. That being said, do not pick the tool first! Rather, decide what result you
   are seeking and then chose the incentive tool or combinations of tools to achieve the desired
   results.
   7.5.2. In constructing a business arrangement and incentive package, there is no substitute
   for planning, knowledge, and research. PMs must work with their Contracting Officers, as
   well as other program team members, when considering choices for contract incentives.
   7.5.3. Use market research, to include early one-on-one meetings with potential contractors,
   to gain information on product knowledge, technology status, industry practices and business
   arrangements. Contact SAF/AQXL for business intelligence on potential contractors or the
   business sector. The Defense Contract Management Agency (DCMA) can provide
   information on the company’s long-term objectives and current and anticipated business base
   and performance experiences.
   7.5.4. Take care to ensure that different incentives in the plan work together and do not
   conflict. For example, look at how the different criteria are related to ensure you are not
   incentivizing a contractor to focus solely on performance while ignoring cost and schedule.
   No incentive contract may provide for other incentives without also providing a cost
   incentive (or constraint).
   7.5.5. In order for an incentive to be effective, the contractor must perceive that it is
   achievable. Remember that a “reward” that cannot be gained is no reward. Conversely,
   incentives are not “gifts”. Incentives should be earned through performance. However, an
   incentive that does not speak to a contractor motivator may not be much of an incentive.
   Effective incentive arrangements must be large enough to motivate performance, and must
   provide a meaningful return to the contractor. Enhanced performance must add value to the
   mission. Reward must be commensurate with risk. Incentives must be worth the
   contractor’s investment. Overall, incentives should be challenging, but realistic and
   attainable.
   7.5.6. Motivational theory indicates that tying rewards to specific behaviors (or events) and
   choosing rewards that are paid “immediately” can be the most effective way to motivate. An
   incentive paid years from event completion will probably not motivate a company,
   particularly since companies (or at least the people) tend to have a short term focus
   (generally quarterly and/or yearly) and the personnel currently running the company will
   have moved on to other jobs or into retirement. However, in long term arrangements,
   immediate rewards may not be the best incentive either. A reward, nearer the end of a longer
   term arrangement (e.g. in subsequent options) may incentivize continued focus on “this
   work”, with potential for keeping the “A” team in place, instead of a shift in focus to find
   “new work”.
AFPAM63-128 10 JULY 2014                                                                   43


  7.5.7. The following series of questions can help guide the decision when selecting an
  incentive:
     7.5.7.1. What is important to the program—technical, schedule, cost—for program
     success? What are the key elements for success in the area deemed important to the
     program- technical talent, sub-tier supplier performance, etc.?
     7.5.7.2. What are the key program risks, and how can incentives help to mitigate risks
     and improve probability of success?
     7.5.7.3. Is the effort you want to incentivize realistic given the state of the art for
     technology?
     7.5.7.4. Are there objective criteria that can be used to measure how well the contractor
     is performing towards meeting incentive targets?
     7.5.7.5. Is it within the contractor’s control to meet the identified goals?
     7.5.7.6. Is incentive amount adequate to provide sufficient motivation? How is that
     known?
     7.5.7.7. What is the contractor’s environment?
        7.5.7.7.1. Where does this particular program fit within the contractor’s overall
        portfolio of work? Is it one of many contracts, or is it a major contract and plays a
        key role in the company’s/business segment’s profit future?
        7.5.7.7.2. What role do the employees play in the development of the product? Will
        (and how will) the incentive flow to employees? As an example, software is very
        manpower dependent—some contractors have been successful in identifying that a
        portion of the incentive will go directly to their program personnel. This gives the
        financial incentive to the people who are doing the work, encouraging "buy-in" at all
        levels throughout the contractor’s organization.
        7.5.7.7.3. Does the subcontractor play a critical role in program success? Will (and
        how will) the incentive flow to the subs? Will (and how will) the prime communicate
        with the sub? Understanding the prime’s relationship with their subcontractors may
        help determine how to use incentives to achieve the desired results. However, the
        government cannot dictate or negotiate subcontract types.
     7.5.7.8. What is the government environment? Is the budget extremely limited? Are
     there outside pressures for early delivery?
        7.5.7.8.1. Is there something in the contract administration process that will work
        against the incentive?
        7.5.7.8.2. Should a negative incentive be considered? In the event of non-
        performance, a negative incentive could require the contractor to return a portion of
        the fee paid (comptrollers of companies hate to write checks returning money to the
        government) or could have provisions for early termination of the contract or for
        contract options to not be exercised.
     7.5.7.9. If you decide you need an incentive, the next step is to work with your
     Contracting Officer, attorney advisor, and other team members as appropriate, to develop
 44                                                             AFPAM63-128 10 JULY 2014


       the necessary approval documentation for the use of an incentive type contract, crafting
       of the most effective incentive and creation of appropriate contract language. The
       Acquisition Centers of Excellence can be good sources of information and expertise for
       this.
7.6. Follow Up/Execution. Awarding the contract does not mean the efforts to implement a
successful incentive strategy are complete. The effort includes managing and administering the
program and contract, monitoring and evaluating the contractor’s progress against the identified
metrics, and providing feedback to the contractor on progress. Effective and timely
communication with the contractor will be key to the success of the incentive. If the incentive
has been selected and crafted correctly, the PM should be “on message” in communicating
regularly with the industry counterpart. What is said can impact the contractor’s efforts.
   7.6.1. Effective incentives will motivate the contractor toward program success. Program
   managers should remember that incentive programs are opportunities to assess contractor
   performance. Contractor performance is assessed in other ways such as program reviews,
   annual Contractor Performance Assessment Reports (CPARs), and Earned Value
   Management (EVM). If a contractor is earning full or substantial amounts of incentives,
   other program assessments should also be positive.            Discrepancies among various
   assessments of contractor and program performance can indicate there is a problem in
   contracting/acquisition strategy, incentive planning or evaluation alignment. The PM must
   understand and be able to explain the reason for the discrepancies. The PM should also
   consider whether the incentive program is properly driving the desired behaviors in the
   contractor.
7.7. Considerations for Successful Implementation of Incentives.
   7.7.1. There are examples of programs that have successfully used incentives. A clear
   understanding of critical mission needs combined with the development of sound evaluation
   criteria to measure contractor’s performance are key to a successful relationship that leads to
   excellent contractor performance. Competition, as well as positive and negative incentives,
   has been used to improve contractor performance. Consider that low award or incentive fees
   for poor performance may result in contractor management changes and improved
   performance.
   7.7.2. There are cases where incentives do not work. There are programs where the
   contractor does not have the capability to perform the work or an incentive was structured
   poorly and/or was not attainable—(e.g., an unachievable unit cost goal).
   7.7.3. It is important to realize when an incentive plan is working and when it is not
   working. Plan to collect metrics and documentation to determine whether criteria are being
   met; this can be supported with a well-crafted Quality Assurance Surveillance Plan. If the
   incentive plan is not working, work with the contracting officer, and consider changing it.
   Restructuring a contract after award is a very time consuming and expensive undertaking
   which is why it is critical to do the work up front before award.
   7.7.4. The Program Manager is responsible for developing the incentive strategy starting
   with determining the government needs. Be prepared to ask questions of the requirements
   community as well as the technical and business staffs to ensure incentivizing performance
   that is important to the government and worth the money. Be prepared to assist the
AFPAM63-128 10 JULY 2014                                                                      45


   contracting officer in negotiations with the contractor to achieve the government objective.
   Communicate openly with the contractor in an atmosphere of trust and mutual respect in
   order to achieve the goal. Stay away from complicated incentive approaches and use
   objective criteria whenever possible. Management is important—the contractor will focus on
   what the PM focuses on—make sure it relates to the incentives.
   7.7.5. When creating an incentive, consider a "war-game" approach. Look at how the
   incentive is planned to work, and then create several possible test cases to anticipate the
   potential trade-offs. As an example, the contractor seeks to increase reliability by 100 hours
   and as a result spends $10M extra dollars. Is it of value to the government and to the
   contractor for that trade off? Does the increase in reliability save well beyond the expended
   $10M? Is this an outcome you want and need to incentivize?
7.8. Additional Information/Training.
   7.8.1. Acquisition Centers of Excellence (ACE): For additional information on contract
   incentives, please contact your local ACE, contracting office, or SAF/AQXC.
   7.8.2. Training: See the Defense Acquisition University (DAU) website for training.
   7.8.3. For additional information reference the DoD Contracts Incentives Guide and the Air
   Force Award-Fee Guide.
 46                                                            AFPAM63-128 10 JULY 2014


                                          Chapter 8

           ITEM UNIQUE IDENTIFICATION IMPLEMENTATION PLANS

8.1. Introduction. This chapter is written to be used with AFI 63-101/20-101, Integrated Life
Cycle Management, DoDD 8320.03, Unique Identification (UID) Standards for a Net-Centric
Department of Defense, DoDI 8320.04, Item Unique Identification (IUID) Standards for
Tangible Person Property, DoDI 4151.19, Serialized Item Management (SIM) for Materiel
Maintenance DoD 4140.1-R, Supply Chain Material Management Regulation, Federal
Acquisition Regulation (FAR) Part 45, Government Property, Defense FAR Supplement
211.274, DoDI 5000.64, Accountability and Management of DoD Equipment and Other
Accountable Property, and DODI 4161.02 Accountability and Management of Government
Contract Property.
   8.1.1. Purpose of Item Unique Identification (IUID). IUID is a Department of Defense
   program requiring the marking and registering of assets that will enable easy access to
   information about DoD possessions in order to make acquisition, repair, and deployment of
   items faster and more efficient. Tangible assets are marked and associated with a set of data
   that is globally unique and unambiguous, ensures data integrity and data quality throughout
   the life of the component, and supports multi-faceted business applications and users.
   8.1.2. Terms.
       8.1.2.1. DoD Item Unique Identification – A system of marking items delivered to the
       DoD with unique item identifiers (UII) that have machine-readable data elements to
       distinguish an item from all like and unlike items. Items are marked with a Data Matrix,
       the contents of which are encoded in a syntax of International Organization for
       Standardization/International Electrotechnical Commission (ISO/IEC) 15434 and the
       semantics of ISO/IEC 15418 or the Air Transport Association Common Support Data
       Dictionary (ATA CSDD) for Text Element Identifiers (TEIs). The data matrix contents
       may be either a Unique Item Identifier (Construct#1 or Construct #2) or a DoD
       recognized IUID equivalent.
       8.1.2.2. Data Matrix – A two-dimensional matrix symbology containing dark and light
       square data modules based on ISO/IEC 16022. It has a finder pattern of two solid lines
       and two alternating dark and light lines on the perimeter of the symbol. Data matrix is
       used for item marking applications using a wide variety of printing and marking
       technologies. The Data Matrix ECC 200, which uses Reed-Solomon error correction, is
       the specified symbol for a UII when physically marked on an item.
       8.1.2.3. Enterprise Identifier – A code that is uniquely assigned to an enterprise (the
       manufacturer, vendor, etc.) responsible for assigning item unique identifiers to items.
       8.1.2.4. Item – A single hardware article or unit formed by a grouping of subassemblies,
       components, or constituent parts.
       8.1.2.5. Marking – The application of legible numbers, letters, labels, tags, symbols, or
       colors to ensure proper handling and identification during shipment and storage.
       8.1.2.6. Unique Item Identifier (UII) – A set of data elements marked on an item that is
       globally unique and unambiguous. For items that are serialized within the enterprise
AFPAM63-128 10 JULY 2014                                                                         47


       identifier, the UII data set includes the data elements of enterprise identifier and a unique
       serial number (Construct #1). For items that are serialized within the part, lot, or batch
       number within the enterprise identifier, the UII data set includes the data elements of
       enterprise identifier, the original part, lot, or batch number, and the serial number
       (Construct #2).
   8.1.3. Additional information and guidance including information specifically geared to
   PMs, depots, and industry can be found at the DoD IUID Toolkit.
8.2. Applicability of Item Unique Identification Implementation Plans. The program
manager (PM) is responsible for preparing an IUID implementation plan for all programs that
result in the delivery of tangible personal property items to the Department of Defense. Where
there is no designated Program Manager, such as in the case of re-procurements or contracted
MRO of common spares, follow the AFMC Automatic Identification Technology (AIT) Program
Office (AFMC/A4NA) guidance for delivery of replacement and repaired assets that are IUID
compliant. The PM should identify and address all items meeting the IUID criteria in the IUID
Implementation Plan as defined in the Defense Federal Acquisition Regulation Supplement
(DFARS) 211.274, Item Identification and Valuation Requirements. The implementation plan
should address cost, schedule, impacts on legacy assets in service and in inventory, existing
ongoing contracts, engineering drawing update strategy, budget requirements, and impacts to
foreign military sales. Plans should reflect coordination between program acquisition and
sustainment activities, and industry. An IUID Implementation plan involving IT/business
systems should include a strategy to integrate IUID data collection, storage, and transmission
across Information Systems (AIS/MAIS) using data syntax specified in DODI 8320.03 and MIL-
STD 130, Identification Marking of U.S. Military Property.
   8.2.1. The PM is required to prepare an initial IUID implementation plan. The approved
   IUID implementation plan is included in the SEP either as a link or an annex. For acquisition
   programs beyond milestone C, the IUID Implementation Plan is included as an Annex to the
   Life Cycle Sustainment Plan.
   8.2.2. To ensure IUID design consideration, the PEO/MDA approved IUID implementation
   plans are routed for review and approval as an annex to the SEP during SEP processing as
   described in AFI 63-1201. IUID implementation plans are updated for each Milestone (MS)
   review or when configuration changes drive new IUID marking and tracking requirements.
   (This is also required in the information support plan (ISP). 10 USC §2223, DoDI 5000.02,
   DoDI 4151.19)
   8.2.3. During post Initial Operational Capability (IOC) reviews, The PM ensures satisfactory
   progress toward completion of the program’s IUID Implementation. The PM initiates
   management actions to address progress issues/concerns until all items meeting IUID criteria
   have been uniquely identified and registered in the IUID registry.
   8.2.4. Implementation plans for ACAT programs with existing legacy items should work
   with the sustainment activities for those items and reference any existing or projected plans
   for IUID implementation.
   8.2.5. Program planning for Automatic Identification Technology (AIT) infrastructure
   requirements and/or Automated Information Systems (AIS) enhancements to enable IUID
 48                                                             AFPAM63-128 10 JULY 2014


   should occur only if the program is responsible for the management and/or maintenance of
   AIT and/or AIS.
      8.2.5.1. Plans should identify the items used by the program that meet the IUID criteria
      This includes items managed by the AF, other DoD Components and Agencies,
      Government agencies outside the DoD, or support contractors. Figure 8.1 provides a
      decision flowchart for determining if an item meets the criteria for IUID marking.

Figure 8.1. Determining if IUID is Required.




   8.2.6. The PM is responsible for determining IUID requirements for program assets meeting
   criteria defined in DFARS 211.274, Item Identification and Valuation Requirements:
      8.2.6.1. Items for which the Government's unit acquisition cost is $5,000 or more. For
      existing items already owned by the Government, this value should be construed as the
      acquisition value to replace the item.
      8.2.6.2. Items for which the Government's unit acquisition cost is less than $5,000, when
      identified by the managing or requiring activity as serially managed, mission essential,
      controlled inventory, or requiring permanent identification unless the terms and
      conditions of the contract state otherwise.
          8.2.6.2.1. Serially managed items (items the DoD elects to manage by means of its
          serial number). An item may be used that has been serialized by the manufacturer but
          is not designated by the DoD (usually the PM or Item Manager) to be uniquely
          tracked, controlled or managed in maintenance repair and/or supply by means of its
          serial number.
             8.2.6.2.1.1. Serial management includes requirements for unique item
             traceability. Unique item traceability is a requirement to establish the authenticity
AFPAM63-128 10 JULY 2014                                                                      49


            of an individual item or group of items at any time during their life. It is a
            requirement for the capability to link information about the item to it. The ability
            to discover life cycle information for an item is known as traceability and shall be
            enabled by IUID.
            8.2.6.2.1.2. Unique item level traceability is the requirement to trace life cycle
            management events related to acquisition, property accountability, storage,
            operation, maintenance, safety, physical security, retirement, and disposal by each
            individual item.
            8.2.6.2.1.3. Serially managed items include reparable items down to and
            including the sub-component reparable unit level, life-limited items, time
            controlled items, items requiring individual records, and items that require
            technical directive tracking at the part level.
        8.2.6.2.2. Mission essential is a measure of an item’s military worth in terms of how
        its failure (if a replacement is not immediately available) would affect the ability of a
        weapon system, end item, or organization to perform its intended functions. This
        determination, relative to UID, is made by the PM in coordination with the user.
        8.2.6.2.3. Controlled inventory are those items that are designated as having
        characteristics that require they be identified, accounted for, segregated, or handled in
        a special manner to ensure their safeguard and integrity. Includes classified items,
        sensitive items (such as precious metals, hazardous items, etc.), pilferable items (see
        DoD 4100.39-M, Federal Logistics Information System (FLIS) Procedures Manual,
        Vol 10, Table 61), and safety controlled items.
     8.2.6.3. Regardless of value, (a) any DoD serially managed subassembly, component, or
     part embedded within a delivered item and (b) the parent item that contains the
     subassembly, component, or part, and any warranted serialized item per DFARS 211.274.
     8.2.6.4. Nuclear Weapons-Related Materiel (NWRM). All individual NWRM items are
     accounted for and managed by serial number. These NWRM assets meet the
     requirements of DoDI 8320.04 and require IUID. Consistent with engineering analysis,
     individual NWRM items in the DoD Supply System should be marked with a machine
     readable UII or assigned a virtual UII. Coordinate IUID Implementation Plans involving
     nuclear critical components with AFSEC/SEW and AFNWC/NL. Refer to AFI 91-105,
     Critical Components for additional guidance.
     8.2.6.5. Government Furnished Property (GFP). The PM should ensure the PCO enters
     the DFARS clause 252.211-7007 and FAR 52.245-1, Government Property in new
     contracts that involve government furnished property in possession of the contractor,
     without regard to the availability of funding. Overarching requirements for GFP
     management are contained in FAR Part 45 and DoDI 8320.04. DoDI 8320.04:
        8.2.6.5.1. Establishes the DoD IUID register as the master data source for GFP.
        8.2.6.5.2. Requires the AF to identify and track GFP through the use of UIIs in
        transaction-derived data from electronic business transactions.
     8.2.6.6. Contractor Acquired Property (CAP). CAP assets are excluded initially from the
     IUID registry. CAP assets meeting the IUID criteria of DFARS 211.274-2 are marked
 50                                                              AFPAM63-128 10 JULY 2014


       and registered in the DoD IUID registry only upon delivery to DoD in accordance with
       the clause at DFARS 252.211-7003. Requiring activities update paragraph (c)(1)(ii) of
       the clause to insert the exhibit line item numbers of those items.
       8.2.6.7. Tooling for Major Defense Acquisition Programs (MDAP) unique tooling
       associated with the production of hardware for an MDAP is stored and preserved through
       the end of the service life of the related weapon system per FY 2009 P. L 110-417, Title
       VIII, Subtitle B, Section 815. Unique tooling designated for preservation is identified in
       the LCSP at MS C or prior to MS C in the Systems Engineering Plan and should be
       considered DoD serially managed and be IUID compliant.
       8.2.6.8. The PM may apply for exception to contractor application of IUID as described
       in DFARS2.11.274-2(b).
8.3. Preparing the IUID Implementation Plan. IUID Program Implementation Plans apply to
all programs that result in the delivery of tangible personal property items to the DoD. A
template for IUID implementation plans that includes detailed guidance for each section can be
found at Attachment 3. In general the plan should:
   8.3.1. Incorporate IUID requirements for all new end items meeting the IUID criteria
   identified in AFI 63-101/20-101 and DFARS Clause 211.274-2, Policy for Unique Item
   Identification. Identify the items and the plan to mark and register the items. Initial plans
   may not be able to include a detailed list of items that meet the criteria, but the PM should be
   able to identify expected categories and have a plan/schedule to ensure all items requiring
   marking are identified.
   8.3.2. Address IUID requirements for legacy items. Legacy items are DoD owned items that
   have been produced and deployed for use, or that have been produced and placed in
   inventory or storage pending issue for use. These assets are marked as the opportunity
   permits. Marking legacy assets follow strategies consistent with sustainment business
   process, priorities, and availability opportunities as prescribed by the AFMC Automatic
   Identification Technology (AIT) Program Office (AFMC/A4NA).
   8.3.3. Incorporate IUID requirements for all embedded items that meet the IUID criteria,
   including all serially managed embedded items.
   8.3.4. Apply IUID requirements to Contracted Logistics Support Agreements. Specify how
   DFARS clause 252.211-7003 is being applied to include marking and registering of spares,
   repaired items, and other items managed or procured under Contractor logistics Support
   (CLS).
   8.3.5. Identify IUID requirements for Foreign Military Sales and Security Assistance
   Programs.
   8.3.6. Support Performance Based Logistics objectives for total asset visibility, life cycle
   inventory management, and serialized item management.
   8.3.7. Integrate IUID in configuration and document management.
   8.3.8. Address organic manufacturing.
   8.3.9. Address Automatic Identification Technology (AIT) infrastructure requirements, to
   include:
AFPAM63-128 10 JULY 2014                                                                       51


       8.3.9.1. Maintenance and supply support,
       8.3.9.2. Organic manufacturing, and
       8.3.9.3. Deployable assets.
   8.3.10. Address compatibility with and impact to Automated Information Systems (AIS).
   Include:
       8.3.10.1. Program-specific information.
       8.3.10.2. Cross-program/cross-service information systems.
   8.3.11. Identify the capability requirements necessary to accommodate IUID data for the
   identified assets if the system is AIS used for the management of property.
   8.3.12. Be consistent with financial accounting and property management objectives.
8.4. Coordination Process for IUID Implementation Plans
   8.4.1. The PM prepares the plan collaborating with industry, sustainment, local ACE
   personnel, and the AFMC Automatic Identification Technology (AIT) Program Office
   (AFMC/A4NA) to ensure standardization and aid in identification of cross-cutting
   implementation activities.
   8.4.2. Prior to final PEO signature coordination, coordinate plans with AFMC/A4NA
   (SNT/IUID project office), SAF/AQXA, and AF/A4ID to ensure standardization and aid in
   identification of cross-cutting implementation activities.
   8.4.3. After obtaining PEO signature, ACAT I, IA, and non-delegated programs should work
   with their PEMs to obtain SAE coordination (ACAT ID/IAM) or approval (ACAT IC, IAC
   or non-delegated II). Following SAE signature, ACAT ID and IAM program plans should be
   forwarded by the PEM to OUSD (AT&L) or DoD CIO for approval.
       8.4.3.1. Changes such as updates to schedule, IUID item lists, and status updates do not
       require a re-coordination of the IUID Implementation Plan unless they drive a significant
       change in the approved strategies or resources required for implementation. Updates to
       existing plans do not need to change to revised templates or formats, but should be
       reviewed to ensure all new statutory or regulatory requirements are addressed during
       periodic SEP reviews.
8.5. Registration and Implementation. The marking and registration of the items in the IUID
registry is normally accomplished by the contractor through implementation of DFARS clause
252.211-7003, Item Identification and Valuation. However, it is still the responsibility of the PM
to ensure all items are marked and registered. This includes verification that the data submitted
to the IUID registry is accurate and usable for future asset management purposes. Some
consideration for the PM to ensure correct and complete registration and marking are:
   8.5.1. How the Contract Line Item Number (CLIN) and Contract Data Requirements List
   (CDRL) structure supports IUID. Separate CLINs or SubCLINs for items that require
   marking make it easier to determine value and to register at delivery. Requiring a CDRL to
   identify IUID items and/or embedded items provides a tracking mechanism and promotes
   early IUID planning by the contractor.
 52                                                             AFPAM63-128 10 JULY 2014


   8.5.2. When and how the contractor is going to mark items. Make sure the contractor is
   aware of the International Organization for Standardizations’ standard and related DFARS
   Clause inserted into the contract. Look for the activity in the Integrated Master
   Plan/Integrated Master Schedule and request status as a part of routine reporting.
      8.5.2.1. Understand how the contractor is going to register items. Preferred method is to
      use Wide Area Workflow (WAWF) and submit the IUID information at the time of
      acceptance (completion of the electronic Form DD250, Material Inspection and
      Receiving Report). The PM should ensure that the person responsible for accepting the
      delivery is aware of what should be in the IUID section of WAWF. Assign an IUID lead
      and make sure the lead is aware of common mistakes (like using a subcontract number
      instead of the government contract number or failure to follow the DoD standard.)
      8.5.2.2. Data Item Descriptions (DID). The DID templates at Attachment 3 offer
      standardized tools for executing the IUID Implementation Plans. The two templates
      describe the IUID Marking Plan and the IUID Marking Activity and Verification Report
      (quality control over the IUID mark). Reference the actual DID to ensure currency.
          8.5.2.2.1. IUID Marking Plan DID. This DID aids the contract officer, contractor,
          and government quality assurance personnel to define and understand the scope of
          meeting MIL-STD-130. It requires the contractor to deliver the marking details and
          UII management (UII Uniqueness and DoD registration approach) in a plan prior to
          the actual item marking.
          8.5.2.2.2. IUID Marking Activity and Verification Report DID. This DID aids the
          contract officer, contractor, DCMA, and government quality assurance personnel to
          understand the data elements associated with the IUID mark and to summarize the
          data as a deliverable. The mark verification information represents mark quality and
          allows program managers to verify the integrity of the data in the DoD registry and
          management system records.
8.6. Foreign Military Sales (FMS) IUID Requirements.
   8.6.1. Implementation Planning. The Product Support manager (PSM) may tailor IUID
   implementation planning on FMS assets per established FMS case direction. Such tailoring
   should be documented in the IUID Implementation Plan to include detailed justification for
   any waiver or exemption and planned course of action for FMS IUID requirements. Updated
   IUID Implementation plans should reflect impacts on cost, schedule, or availability/reliability
   and be approved by the PEO.
   8.6.2. Scope.     The scope for application of FMS IUID requirements includes
   acquisition/legacy programs with approved IUID implementation plans detailing specified
   course of action. The scope includes items being procured under new solicitations and on-
   going contracts, in operational use, inventory and/or undergoing depot maintenance, or
   overhaul by DoD repair activities. The PM ensures IUID Implementation planning for FMS
   Unique assets is consistent with AF and DoD guidance.
   8.6.3. Budget Requirements. PMs should ensure Planning, Programming, Budgeting, and
   Execution (PPBE) includes nonrecurring IUID cost.
   8.6.4. FMS Unique IUID Requirements.
AFPAM63-128 10 JULY 2014                                                                   53


     8.6.4.1. Follow on Operations and Support. Contracting officers should omit the IUID
     DFARS Clauses 252.211-7003 and 252.211-7007 from contracts where the requesting
     activity provides an approved IUID Implementation Plan that specifies assets will not
     migrate to DOD inventories and therefore do not require IUID marking or registration in
     the DOD IUID Registry. The requesting activity may also provide an approved LCSP
     which clearly reflects that DoD has no follow on role in the operations and support of
     deployed/fielded FMS assets
     8.6.4.2. FMS Asset Migration to Inventory Control Point (ICP). Upon receipt of an FMS
     asset that does not comply with IUID requirements, the ICP will suspend the asset in
     stock until DoD support requirements are verified with the Program Office. The ICP
     should confirm that the LCSP calls for a DOD repair, overhaul, etc and includes IUID
     marking and registration. The ICP will take action to comply with IUID requirements
     and recover all nonrecurring cost from the FMS Customer per DoDD 2140.2,
     Recoupment of Nonrecurring Costs (NC) on Sales of U.S. Items. If not, the ICP will
     obtain instruction from the program office on returning the asset to the FMS customer
     without action.
     8.6.4.3. Direct FMS Case/Contractor Logistics Support (CLS). After successful
     implementation of an FMS Case, the US Government may enter into a contract with an
     Original Equipment Manufacturer (OEM) to provide systems, spares and equipment
     directly to an FMS customer. FMS countries may also elect to procure systems, spares,
     and equipment directly from the OEM and not establish an FMS Case. Both of these
     situations constitute a direct contractual relationship between the sovereign country and
     the OEM. These systems, spares and equipment are outside the DoD inventory and not
     required to comply with IUID markings. The Program Manager should ensure the IUID
     Implementation Plan and LCSP clearly reflects that DoD has no follow on role in the
     operations and support of deployed/fielded FMS assets. Upon receipt of an FMS asset
     that does not comply with IUID requirements, the ICP will process the asset as described
     in paragraph 3.2 FMS Asset Migration to ICP.
     8.6.4.4. FMS Repair and Return. This unique FMS process provides serialized repair of
     specific country spares which are returned to the country of origin and maintained within
     the FMS processes and controls. The repair sources include direct Contractor Logistics
     Support, FMS Processed Repair/Return processed at the DoD sources, or through the
     FMS Parts Repair Ordering System (PROS). During the repair process, these assets
     tracked as property of sovereign countries and are earmarked for return to the respective
     FMS countries. They are not designed to enter the US inventory and therefore are
     exempted from the requirements of IUID.
     8.6.4.5. Non-Standard spares and equipment purchased through Parts Repair Ordering
     System (PROS). These assets are unique to the FMS country specific systems/end items
     and should not enter the US inventory. They are exempt from IUID requirements.
     8.6.4.6. Acquisition Advice Code “P”, Security Assistance Items. Spares and equipment
     with National Stock Numbers (NSNs) that are identified with Acquisition Advice Code
     “P” are no longer used in the US inventory and are exempt from IUID.
 54                                                             AFPAM63-128 10 JULY 2014


                                           Chapter 9

                                PROGRAM REALIGNMENT

9.1. Purpose and Overview. This chapter further explains the program realignment process
and responsibilities prescribed in AFI 63-101/20-101, Integrated Life Cycle Management. It
describes a collaborative process designed to ensure a seamless (within the ILCM community)
and transparent (to the user) workload transition if the location for executing AF systems and
acquisition programs is formally transferred (or split) between geographically separate locations
and/or MAJCOMs. This chapter outlines the process and criteria for assessing the readiness of a
given weapon system or acquisition program to transition, details the steps for accomplishing the
transfer action, and describes the roles and responsibilities of parties involved in the program
realignment process. Note: This chapter does not explicitly apply to the transfer of programs
between PEO portfolios (which are required to be coordinated through AFMC or AFSPC (as
appropriate) and approved by SAF/AQ per AFI 63-101). However, it could be used to aid in
preparation of a PEO transfer request.
9.2. Background and Framework. Executive management responsibilities for acquisition
programs remain with the PEO, and day-to-day responsibility for managing the development and
sustainment needs of the system throughout its life cycle remain with the PM, regardless of
program realignment. Workload may be realigned between locations to take advantage of
efficiencies and resource savings achieved by co-locating similar work.
   9.2.1. Workload for systems and acquisition programs is not transferred unless, at a
   minimum, the system, subsystem, component, or increment of capability has been certified as
   interoperable within its intended operational environment, has achieved IOC and Full Rate
   Production (FRP), and is logistically supportable per the user’s requirement, and the transfer
   can align the program office responsibilities to co-locate with the organization(s) responsible
   for the system’s/program’s depot maintenance and supply chain management.
   9.2.2. The PM is responsible for identifying and documenting if and when a program
   realignment or split is to occur (usually in the AS or LCSP). The PM must also work with all
   stakeholders, including resource personnel as appropriate to ensure transition requirements,
   activities, and timeframes associated with a proposed program realignment are fully
   coordinated and the gaining location has secured sufficient resources (manpower, funding,
   facilities, etc.) to accept workload so as not to impact mission success. Table 9.1 contains
   specific criteria to address when considering program realignment.

Table 9.1. Program Realigment Assessment Criteria.

      Assessment
                                                   Considerations
       Criteria
  System Technical     Has the system achieved IOC and been deemed interoperable in its
  Maturity             intended operational environment?
                       Have all deficiencies identified during developmental and operational
                       testing been satisfactorily resolved so that there are no remaining
                       substantial or severe impacts to the operational mission?
AFPAM63-128 10 JULY 2014                                                                       55


                     Will ongoing or planned system development activities (e.g., post-
                     production modifications) substantially change the system’s
                     performance characteristics or supportability requirements?
                     Are there any security issues for the program?
 System Production   Is the program nearing the end of, or has it completed, full-rate
 Status              production?1
                     Are additional production contracts planned or anticipated?
                     Have a significant percentage of production articles been delivered to
                     the operational command(s)?
 System              Have (or will) the relevant product support elements been (or be)
 Supportability      deemed ready 2 to support the system at the planned transfer point?
                     Has adequate technical data been obtained to support sustainment
                     activities?
 Program             Is the preponderance of system/program management effort being
 Management          expended on acquisition/product development tasks or
 Status3             sustainment/product support tasks? Has the production configuration
                     been baselined?
                     Is the system/program management environment stable? Are there
                     outstanding contractual issues?
                     Are significant system/program leadership changes underway or
                     envisioned?
 Program Funding     Is the preponderance of system/program funding being expended on
 Status3             acquisition/product development tasks or sustainment/product support
                     tasks?
                     Is significant investment still required to field or mature the planned
                     system sustainment infrastructure?
                     Are sufficient funds available or programmed to support the system as
                     planned after transfer?




 Assessment
 Criteria            Considerations
 56                                                               AFPAM63-128 10 JULY 2014


 External Program       Are any planned sustainment support service contracts projected to
 Factors                exceed $100M in total contract value? Have they been designated
                        “special interest” contracts by SAF/AQ?4
                        Would any other technology or product development programs be
                        negatively impacted if the system/program were transferred?
                        Are there any other internal or external special interests that may
                        preclude or be negatively impacted by system/program transfer?
 1
  In this context, “production” applies to the articles intended for employment by AF
 organizations. In some cases, a system’s production line may continue beyond the run for
 AF articles, e.g., foreign military sales.
 2
   The 12 Product Support Elements are further described in the DoD Product Support
 Manager Guidebook which provides specific evaluation criteria that may be useful in
 determining a system’s sustainment posture and readiness for transfer of management
 responsibilities.
 3
     Do not consider activities outside the scope of the AF program.
 4
  If the answer to either of these questions is “yes”, these services must be coordinated by the
 PEO for Combat and Mission Support (AFPEO/CM).


9.3. Program Realignment Process. The following description depicts the process by which
weapon system and program management program realignment occurs.
     9.3.1. Prepare Transition Support Plan (TSP).
        9.3.1.1. The transition process begins with the PM. Consistent with meeting a target date
        identified in acquisition documentation, the PM should develop a TSP to document the
        actions, responsibilities, and timelines necessary to transfer workload. If the transfer is
        scheduled to be concurrent with fielding, transfer planning should be accomplished as
        early as possible during the production and deployment phase (post-MS C). The exact
        timing depends on the specific needs and actions required to transfer the workload. The
        PM leads the TSP preparation effort, and is supported by other applicable functional
        organizations at the gaining location. The PM should also solicit support from the
        operating command(s) as necessary to develop the TSP. The TSP should be drafted with
        ample time for approval and completion of any other work required prior to transfer.
        9.3.1.2. The PM should develop a TSP to fit the system’s unique management
        environment and satisfy long-term requirements. While the TSP should focus on
        program realignment, it should reflect post-transfer organizational roles and
        responsibilities, manpower considerations, and funding requirements, residual system
        development and acquisition responsibilities, and system sustainment responsibilities for
        the operating command(s). The TSP should also incorporate any unresolved issues
        and/or action plans associated with the program realignment assessment considerations.
        9.3.1.3. The PM should coordinate the TSP as necessary to solicit comments and resolve
        any outstanding issues that may preclude a successful program realignment. If necessary,
AFPAM63-128 10 JULY 2014                                                                        57


     the PM should also forward any unresolved transfer issues to the PEO for resolution.
     Once the TSP is developed and all issues have been resolved, the PM should forward it to
     the, PEO, and AFMC and/or AFSPC for approval.
  9.3.2. Approve Transition Support Plan. The PEO, Center Commander, and AFMC/CC
  and/or AFSPC/CC must approve the TSP for program realignment to occur; approval of
  document is delegable. If either the PEO or the impacted MAJCOM(s) does not approve the
  TSP, it should be returned to the PM for continued development and resolution of issues. It
  is a best practice to have a fully approved TSP in place three years prior to the target transfer
  date. This ensures there is adequate time for all actions required of both the losing and
  gaining organization to be completed prior to transfer.
  9.3.3. Transition Support Plan Executive Review. The PEO and center commander(s) are
  signatories on the TSP. The AFMC/CC or AFSPC/CC are the final signatory on the TSP
  prior to forwarding the plan to SAF/AQ. If the SAE has concerns with the TSP, the concerns
  should be addressed and the TSP updated as necessary or as directed by this executive
  review. Once all issues have been resolved and the TSP has been finalized, the PM needs to
  coordinate any changes with the MDA, the gaining organization, and the appropriate
  MAJCOM (AFMC or AFPSC).
  9.3.4. Transition Workload. Once the TSP has been signed, transition activities prescribed
  in the TSP should flow per the timeline(s) contained in the plan. The PEO and PM should
  continue to manage and report on system/program activities. If a “show-stopper” occurs
  prior to the planned transition date, the PM should take the lead to resolve it.
 58                                                              AFPAM63-128 10 JULY 2014


                                           Chapter 10

                                  FIELDING PROCEDURES

10.1. Purpose and Overview. This chapter further explains the materiel fielding process and
responsibilities directed in AFI 63-101/20-101, Integrated Life Cycle Management. It describes
a collaborative process designed to ensure the seamless and transparent transition of AF materiel
from product development, modification, and manufacturing entities to operational users in the
field. This chapter provides planning criteria and considerations for developing materiel fielding
strategies and plans, and it describes a process for coordinating and conducting materiel
deliveries to operational units.
10.2. Background and Framework. The principal objective of every AF acquisition program
is to field an operationally effective and logistically supportable product1 to the organization(s)
that identified the need for the materiel. As the product is being developed and produced, PMs
must concurrently undertake activities to ensure the product makes a “smooth landing” at its
intended user’s operating location. This chapter is designed to help PMs in this regard, and to:
   10.2.1. Ensure sufficient planning is conducted in advance of anticipated materiel delivery
   dates, allowing both the materiel developer and the user(s) to identify, understand, and
   resolve issues associated with the materiel.
   10.2.2. Ensure sufficient time is available to develop the infrastructure necessary to operate
   and sustain the materiel, including the operations, maintenance, and mission support
   personnel who employ, repair, and support the materiel in the field.
10.3. Materiel Fielding Process Overview. The materiel fielding process can be characterized
as: supported and supporting commands collaboratively planning and executing the delivery
and beddown of an operationally effective and suitable platform or system, or a major system
modification/upgrade, from a total system capability perspective, that is sustainable over its
planned life cycle. This multi-dimensional process requires close and frequent coordination
among the acquisition, sustainment, and operational communities in order to field materiel that
meets users’ needs.
10.4. Materiel Fielding Planning and Assessment Criteria. The materiel fielding process
overlays a foundation of planning and analysis criteria that serves to frame potential issues and
enable the timely delivery of materiel to field organizations. The following materiel fielding
planning and assessment criteria may provide PMs with “conversation starters” that could lead to
the identification of potential materiel fielding issues for their program. They may also be used
as a framework for PMs to develop program specific materiel fielding strategies and plans for
their weapon systems or products.

Table 10.1. Materiel Fielding Planning and Assessment Criteria.

      Planning &
      Assessment                                   Considerations
       Criteria
  Materiel System/     Who are the principal participants involved in developing,
  Product Overview     manufacturing, delivering, operating, and sustaining the materiel to be
AFPAM63-128 10 JULY 2014                                                                        59


    Planning &
    Assessment                                    Considerations
     Criteria
                     fielded? Ex: AF; DoD; US Government; industry; foreign interests.
                     How will the materiel be used in the field? Ex: operational
                     employment concepts; deployment/forward basing scenarios; mission
                     frequency/utilization rates.
                     Are existing repair networks able to absorb new workload with existing
                     capability and capacity? (Infrastructure of Intermediate level repair
                     backshops can be assessed by reviewing capability and capacity data
                     available from the repair network manager.)
 Materiel Fielding   What actions must be accomplished prior to initiating delivery of the
 Methodology         materiel? Do these requirements change over time? Ex: product
                     acceptance testing; certification and accreditation; operational site
                     and/or depot activation tasks; interim contractor support agreements.
                     How will the materiel get from the manufacturing facility(ies) or
                     product acceptance site(s) to the user’s beddown location(s)? Who will
                     deliver the materiel and by what method? Ex: AF flight crews;
                     contractor personnel; commercial shipping company, standard base
                     supply.
                     Who will accept and inspect the materiel at the user’s beddown
                     location(s) or the original equipment manufacturer? Is government
                     acceptance, certification and accreditation, or other additional testing
                     required as part of acceptance by the user?
 Materiel Fielding   How many systems/products are to be delivered and at what interval?
 Schedule            Does this delivery schedule change over time?
                     Where/to what organization(s) will the materiel be delivered?
                     Does the user have a priority order for delivery of the materiel?
                     What are the impacts of delivery, integration, installation, and
                     acceptance schedule changes to the user? The PM? Ex: delays in
                     delivery require PM to find sufficient classified storage space.
 Materiel Support
 Concepts
 Sustainment         What sustainment concepts are associated with the materiel? Do these
 Concepts            concepts change over time? Ex: levels of maintenance (organizational,
                     intermediate, depot); sources of repair; sustainment partnering
                     relationships (government-government, government-contractor); use of
                     interim contractor support and/or contractor logistics support.
                     Are there any performance based logistics requirements that must be
                     met prior to delivering the materiel? Do these requirements change
60                                                          AFPAM63-128 10 JULY 2014


     Planning &
     Assessment                               Considerations
      Criteria
                  over time? (Example: Reliability, Availability Maintainability and
                  Supportability (RAMS), product performance agreements, etc.)
Manpower/         How many operations, maintenance, and mission support personnel
Personnel         will be needed to operate, sustain, and support the materiel? Who will
Requirements      provide them? What skill sets/certifications will they require? Will
                  these requirements change over time, or based on the number of
                  systems/products delivered?
                  Will contractor personnel operate or sustain the materiel? In combat
                  environments as well as at home station? Does their involvement
                  change over time?
Maintenance       Are there any standard processes that must be completed prior to
Planning and      fielding/acceptance? Ex. establishing system elements within
Management        Integrated Maintenance Data System (IMDS) for maintenance
                  reporting.
                  Have program interdependencies been recognized and documented?
                  Ex: components or sub-systems used in major platforms/systems with
                  separate funding or governance.
                  How many, and to what level or standard must the operations,
                  maintenance, and/or mission support personnel be trained prior to, or
Training
                  after the materiel is delivered?
                  Who will develop courseware materials and administer the requisite
                  training? When will the training be provided?
                  Are there any training support systems/devices (e.g., flight simulators)
                  that accompany the materiel? When must these systems be fielded in
                  relation to the planned materiel delivery schedule?
Technical         Are validated and verified technical manuals required prior to materiel
Publications      deliveries? What manuals (e.g., flight, maintenance) are necessary at
                  what point in time?
                  Are there any other forms of technical information or documentation
                  necessary to operate or sustain the materiel in the field? When must
                  these artifacts be delivered? Ex: engineering drawings; software
                  licenses/user guides.
Support           What types of, and how many pieces of support equipment must be
Equipment         delivered prior to, or along with the materiel? Ex: AF/DoD-common;
                  system peculiar; contractor-provided tools and test equipment.
                  Are there any other types of government-furnished and/or contractor-
                  furnished equipment or property that must be delivered prior to, or
                  along with the materiel?
AFPAM63-128 10 JULY 2014                                                                         61


    Planning &
    Assessment                                    Considerations
     Criteria
 Supply Support       What types of, and how many spare parts must be delivered prior to, or
                      along with the materiel? Ex: initial or replenishment spares,
                      deployment kits.
                      How and where will the spare parts be stored and delivered? Ex: at the
                      user’s beddown location; in organic AF/DoD depots; in a contractor-
                      operated spares site.
 Packaging,           Are there any unique PHS&T requirements associated with the
 Handling, Storage,   materiel? Must these products or capabilities be delivered prior to, or
 & Transportation     along with the materiel?
 (PHS&T)              How will “total asset visibility” requirements associated with the
                      materiel be assured?
 Computer          Are there any operations or logistics data collection, analysis, or
 Resources/Support production systems associated with the materiel? Must they be
                   delivered prior to or along with the materiel? Ex: mission planning
                   systems; command and control systems; logistics management
                   systems.Are there any AF or DoD electronic databases that need to be
                   modified or created in order to deliver and support the materiel? Ex:
                   SMART.
 Facilities &         What operations, maintenance, and mission support facilities are
 Environment          necessary to house, operate, and/or sustain the materiel at the user’s
                      operation location(s)? When must these facilities be provided in
                      relation to the materiel fielding schedule? Ex: aircraft hangars and
                      parking ramps; environmentally controlled storage or repair facilities;
                      hazardous material/ explosive storage areas.
                      Are the user’s existing facilities sufficient? Are modifications to
                      existing facilities necessary? Do new facilities need to be constructed?
                      When will these actions take place relative to the planned materiel
                      fielding schedule?
                      Will environmental impact assessments be performed, and/or
                      mitigation procedures undertaken prior to delivery of the materiel?
                      Are there any new, unique, or recurring environmental protection
                      requirements associated with the materiel?
                      Will Real Property Installed Equipment (RPIE) need to be in place
                      prior to materiel fielding? Will a support package for maintaining
                      (RPIE) be provided?
 Ancillary            Are there any systems or equipment this materiel must interface to, or
 Systems,             be interoperable with? (including communication or command and
 Equipment, &         control) If so, what is the fielding plan for these products? Are/will
 62                                                            AFPAM63-128 10 JULY 2014


    Planning &
    Assessment                                   Considerations
     Criteria
 Supplies            these systems be in the proper location(s) and quantities necessary to
                     provide the requisite operational or sustainment interfaces?
                     What other products are necessary to support operation and/or
                     sustainment of the materiel? Ex: petroleum, oil, lubricant products;
                     avionics systems or components; weapons; ammunition.
 Materiel Defect     How will materiel defects noted during or after delivery to the user be
 Reporting           reported? (i.e., IAW T.O. 00-35D-54-WA-1, USAF Deficiency
                     Reporting, Investigation, and Resolution). Who will process and
                     adjudicate these deficiency reports? Who will determine if these
                     materiel defects warrant stopping or slowing the materiel delivery
                     schedule?
 Materiel Safety &   Have the materiel’s safety hazards been identified and satisfactorily
 Occupational        mitigated? Are there any new or unique system safety-required
 Health              products associated with the materiel which must be delivered along
                     with the system/product?
                     Are there any potential adverse operational/occupational health risks
                     or other readiness impacts associated with the materiel? Have/will
                     these risks be satisfactorily resolved prior to delivery?
 Materiel Security   Are there any unique materiel controls or accountability procedures
                     that must accompany the product during and/or after delivery?
                     Are there any physical or electronic security requirements necessary to
                     store, handle, or limit access to the materiel?
 Materiel Post-      Who will maintain configuration control of the materiel during and
 Production          after delivery? Will this responsibility transfer at some point after
 Support             deliveries have begun or been completed?
                     Will/how will the materiel be modified or upgraded after delivery?
                     Who will perform this work? How will the upgraded materiel be
                     fielded?
 Materiel Disposal   Are there any existing systems or equipment, to include
                     communications and information network assets, to be retired or
                     relocated as a function of system/product deliveries? Are these
                     activities synchronized with the delivery of the new materiel?
                     When and how will the new materiel be disposed of after it has
                     completed its service life? Are there any de-militarization procedures,
                     electronic media sanitization procedures, product disposal equipment,
                     etc. that must accompany delivery of the materiel?
10.5. Materiel Fielding Process. Though every acquisition program will have unique materiel
fielding considerations and challenges, PMs should design their programs to satisfy the
AFPAM63-128 10 JULY 2014                                                                           63


following three overarching and fundamental requirements inherent to the materiel fielding
process.
   10.5.1. The need to develop comprehensive and coherent materiel fielding plans.
   10.5.2. The need to coordinate materiel fielding issues, action plans, responsibilities, and
   schedules with the materiel’s intended user(s).
   10.5.3. The need to conduct a thorough assessment and review of the materiel’s readiness for
   delivery to the user(s).
10.6. Technology Maturation and Risk Reduction Phase. The materiel fielding process could
begin in earnest at MS A, once the materiel solution(s) resulting from the Materiel Solution
Analysis (MSA) phase has been determined. Toward the end of the Technology Maturation and
Risk Reduction phase, the user should produce a Capability Development Document (CDD) and
maintenance or support CONOPS that should describe the user’s intended operational
employment concept(s), beddown considerations, and maintenance/support concept(s) for the
materiel being developed. In preparation for MS B, the PM uses the CDD as the basis for
developing an initial Materiel Fielding Plan (MFP) that serves as a “fielding roadmap” for the
upcoming Engineering and Manufacturing Development (EMD) phase and beyond. At MS B,
the MFP outlines the materiel fielding-related objectives and issues to be examined, as well as
any specific action plans, responsibilities, and timelines for materiel fielding-related activities to
be conducted during the EMD phase. While each program will have unique materiel fielding
objectives and challenges, during this phase, PMs should prepare a MFP to document how and
when they intend to explore the materiel fielding planning and assessment criteria discussed in
paragraph 10.4 with emphasis on long-lead issues such as:
   10.6.1. Potential materiel basing and employment scenarios at the user’s home station, and at
   forward/austere operating sites if so indicated in the user’s operational concept.
   10.6.2. Materiel support requirements at home station and in deployment scenarios,
   including the potential “logistics footprint” that may be necessary to support the materiel at
   forward or austere operating bases, potential sources of product support, and “50/50”
   considerations.
   10.6.3. The potential impact of technologies that may lessen the sustainment burden and
   logistics footprint for home station or deployed operations—for example: embedded
   diagnostics, automated failure reporting, and other similar maintenance enablers that might
   reduce the logistics tail associated with the materiel.
   10.6.4. Potential environmental impacts/issues, potential facility and infrastructure issues, or
   any other materiel fielding concern that may involve complex planning activities and/or
   lengthy remediation actions.
10.7. Milestone B Decision. In the MS B documentation, the PM summarizes the projected
materiel fielding methodologies and timelines, and discusses the materiel fielding-related
activities to be conducted during the EMD phase—for example: Site Activation Task Force
(SATAF) and Depot Maintenance Activation Working Group (DMAWG) activities,
responsibilities, and timelines. At the PM’s discretion and with MDA approval, the MFP may be
a stand-alone document, an annex to the program documentation, or embedded within MS B
documentation.
 64                                                               AFPAM63-128 10 JULY 2014


   10.7.1. Additionally, if appropriate, PMs may also recommend materiel fielding-related
   actions or decision criteria for inclusion in the MS B Acquisition Decision Memorandum
   (ADM).
10.8. Engineering and Manufacturing Development (EMD) Phase. During the EMD phase,
PMs may form a materiel fielding IPT to assist them with materiel fielding planning and related
activities. While such an IPT would typically consist of action officers and team leads, PMs
may, if the situation warrants, consider forming a General Officer Steering Group (GOSG) when
strategic level program issues might preclude successful deployment of the materiel to the field.
   10.8.1. If formed, materiel fielding IPTs should include representatives from the:
       10.8.1.1. Product development organization(s).
       10.8.1.2. Product sustainment organization(s).
       10.8.1.3. Using/operating command(s) including representatives from the National
       Guard Bureau (NGB) and HQ Air Force Reserve Command if applicable.
       10.8.1.4. Prime contractor(s) and key materiel vendors such as engine or avionics
       suppliers.
       10.8.1.5. Product training/training system providers, including government and
       contractor organizations.
       10.8.1.6. Product test organizations, including the lead developmental test organization
       (LDTO), operational test agency (OTA), and contractor test team members.
       10.8.1.7. If appropriate, PMs may use SATAF and/or DMAWG teams to fulfill the need
       for a materiel fielding IPT.
   10.8.2. By the end of the EMD phase, the PM updates the Materiel Fielding Plan (MFP) to
   detail the specific actions, timelines, and organizational responsibilities necessary to transfer
   the materiel from the product development or manufacturing entity to the operational user.
   The final MFP should build upon the initial MFP produced at MS B, and incorporate any
   new or modified requirements contained in the user’s Capability Production Document
   (CPD) and Maintenance or Support CONOPS that have bearing on materiel fielding matters.
   Specifically, the MFP should identify any materiel fielding-related actions necessary to
   satisfy initial user/cadre training needs, Initial Operational Capability (IOC) requirements,
   and Full Operational Capability (FOC) requirements. The MFP should also reflect materiel
   fielding-related recommendations from Developmental Test and Evaluation (DT&E)
   conducted by the LDTO, and any Operational Assessments (OA) or Military Utility
   Assessments (MUA) conducted by the OTA.
   10.8.3. The MFP should include any pertinent information contained in the lead MAJCOMs
   site activation plan for each site that is to receive the weapon system or product. The MFP
   should also incorporate considerations outlined in MAJCOM level guides and instructions.
10.9. Milestone C Decision. PMs should include a materiel fielding plan in acquisition
documentation for MS C.
   10.9.1. At their discretion, PMs may publish a stand-alone MFP, or embed the MFP in the
   acquisition documentation. If a stand-alone MFP is prepared, the PM should attach it as an
   annex to the MS C acquisition documentation.
AFPAM63-128 10 JULY 2014                                                                       65


   10.9.2. If appropriate, PMs may recommend materiel fielding-related actions or decision
   criteria for inclusion in the MS C ADM.
10.10. Production and Deployment Phase. During the production and deployment phase,
PM’s should focus on meeting the materiel delivery and acceptance requirements contained in
the acquisition documentation. For all Acquisition Category (ACAT) programs, once a PM is
satisfied the program has, or is on track to achieve these requirements, he/she should consider
conducting a Materiel Release Review (MRR) with the program’s Milestone Decision Authority
(MDA).
   10.10.1. The MRR is a review event that precedes delivery of the materiel to the operational
   user. The PM initiates the MRR process by completing a materiel release decision package
   and forwarding it to the MDA (SAE for ACAT ID and IAM). This package should
   nominally include:
       10.10.1.1. A “Materiel Fielding Decision” memorandum for MDA (SAE for ACAT ID
       and IAM) signature. This memorandum should formally document the MDA’s decision
       to authorize the materiel to be fielded. This memorandum may also convey any
       conditions, limitations, or restrictions the MDA wishes to place on fielding activities or
       timelines. Additionally, it may set conditions or establish responsibilities for subsequent
       materiel fielding actions.
       10.10.1.2. A copy of the PM’s MFP, either as a stand-alone document or as incorporated
       in the most recent acquisition documentation.
       10.10.1.3. Any reports, briefings, or other artifacts that may be necessary to support the
       PM’s assessment that materiel fielding requirements contained in the MFP have been
       achieved, or are on track to complete as required. These artifacts may include:
          10.10.1.3.1. Interim/final test results or other assessments that describe the
          system/product’s operational effectiveness and operational suitability as measured
          against the user’s KPPs and Key System Attributes (KSA).
          10.10.1.3.2. Materiel certifications or similar statements of assurance, such as system
          safety certifications, air worthiness certifications, weapon employment certifications,
          environmental impact certifications, information assurance certifications, and/or
          occupational health certifications. A non-exhaustive list of DoD program and
          system-level technical certifications is included at Attachment X.
          10.10.1.3.3. Materiel deficiency reports and corrective action plans.
   10.10.2. At his/her discretion, the MDA may convene a formal MRR meeting to discuss
   materiel fielding matters with the program management team and the user(s), or he/she may
   conduct a “paper MRR” if there are no significant issues with the materiel or its fielding
   plan.
10.11. Types of Materiel Releases. When deciding to release materiel to field units, the MDA
may consider authorizing materiel releases according to the following criteria:
   10.11.1. Full Release. A full release is warranted when the materiel delivery and acceptance
   criteria contained in the MFP can be met unconditionally, or with risk mitigation procedures
   that are acceptable to the using command(s). When designating a system or product for full
 66                                                                       AFPAM63-128 10 JULY 2014


   release, the MDA authorizes all subsequent materiel deliveries to proceed in accordance with
   the MFP and user agreements, without any further MDA notification or approval.
   10.11.2. Limited/Conditional Release. A limited/conditional release is warranted when the
   materiel delivery and acceptance criteria contained in the MFP can only be partially met, or
   met with restrictions that would prevent or limit some aspect of the user’s operations and/or
   maintenance concept. In this case, the MDA should authorize materiel deliveries to begin,
   but may limit the quantity of materiel to be delivered or slow down the planned materiel
   delivery schedule to accommodate materiel “get well” plans. Additionally, the MDA may
   establish additional reporting requirements and/or decision points that must be cleared before
   subsequent materiel deliveries can occur.
   10.11.3. Interim/Training Release. An interim/training release authorizes materiel deliveries
   for the purpose of conducting initial Air Education and Training Command (AETC) or unit
   training only. In this case, the materiel may be sufficiently effective and supportable for
   initial system/product training purposes, but not so for “real world” operations as described
   in the user’s CPD. In this case, materiel releases should only be authorized as necessary to
   support an AETC or user system/product training concept/plan.
10.12. Incremental Materiel Releases. PMs may choose to employ an incremental materiel
review concept for programs following an evolutionary acquisition strategy. In this case, MRRs
may be conducted for the product baseline and each subsequent increment, as depicted in the
following illustration. Incremental materiel releases may be of any type described in paragraph
10.11. above. For example, using the notional example depicted in Figure 10.1, the Baseline
MRR could result in an interim/training release, the Increment A MRR could be a
limited/conditional release, and the MRRs for Increment B and beyond could be full releases.

Figure 10.1. Incremental Materiel Release Review Concept (Notional Example).
        Materiel           Materiel             Materiel             Materiel
        Release            Release              Release              Release
        Review             Review               Review               Review
       (Baseline)          (Inc A)              (Inc B)              (Inc X)

  C
            PRODUCTION & DEPLOYMENT PHASE                   OPERATIONS & SUPPORT PHASE

        BASELINE
                          INCREMENT A
                                               INCREMENT B
                                                                     INCREMENT X




             Deliveries           Deliveries        Deliveries              Deliveries
              to User              to User           to User                 to User



10.13. Special Cases. Certain acquisition activities may require unique and innovative
approaches to the materiel fielding process. For example, the compressed acquisition timelines
associated with Quick Reaction Capability projects may require PMs to greatly accelerate
materiel fielding planning processes compared to traditional acquisition programs.) Joint
Capability Technology Demonstrations (JCTDs) may lead to the fielding of highly effective and
urgently desired operational capabilities, but at the expense of organic logistics sustainment
capabilities. Joint acquisition programs (e.g., Joint Strike Fighter) may require broader materiel
fielding planning and coordination to accommodate each participating Service’s unique
operational requirements, support concepts, or materiel fielding processes. In each of these
AFPAM63-128 10 JULY 2014                                                                  67


special cases though, PMs should nevertheless endeavor to meet the overarching materiel
fielding objectives described in this chapter.
10.14. Additional Information. For additional information on matters related to the materiel
fielding process, consult:
   10.14.1. AFI 10-501, Program Action Directives (PAD) Programming Plans (PPLAN),
   which contains guidance for Air Staff, major commands (MAJCOMs), and other
   organizations to prepare and manage PADs and PPLANs.
   10.14.2. AFI 10-503, Strategic Basing, which provides guidance for conducting site surveys
   and unit beddown procedures.
   10.14.3. AFI 16-403, Updating the USAF Program Installations, Units, and Priorities and
   Movement of Air Force Units, which assigns responsibility and authority for managing
   installations and units in the AF.
   10.14.4. AFI 32-9001, Acquisition of Real Property, which provides guidance for acquiring
   real property.
   10.14.5. AFI 32-9005, Real Property Accountability and Reporting, which provides
   guidance for maintaining real property records and reporting real property assets.
 68                                                             AFPAM63-128 10 JULY 2014


                                          Chapter 11

           PRODUCT AND SOFTWARE DATA ACROSS THE LIFE CYCLE

11.1. Overview. It is important to address product data and software data early in a system’s
life cycle in order to acquire that data cost-effectively and to enable a lifetime of competitive
sustainment and compliance with public law. Contractors create product and software data in
their development and production of a weapon system. This data can then be used by the AF to
review progress in development, upgrades, operation, and support of weapon systems. Data is
also used to provision for spares and to develop secondary sources of production.
   11.1.1. Product Data Acquisition (PDAQ) Resource Center. The PDAQ resource center
   provides specific guidance, tools, templates, and standard language to help programs to
   define product and software data requirements, and to acquire that data cost effectively.
   PDAQ is available on the AF Portal by searching for “pdaq”.
   11.1.2. As a recommended best practice, PMs should consider assigning an engineering data
   manager or data management specialist within the program office (referred to as “EDM”
   within this chapter). This individual should be the OPR for day-to-day execution of the
   program manager’s responsibilities to integrate data management into program strategies and
   documents; to ensure that appropriate data requirements are included in contract documents;
   to coordinate the review and acceptance of data delivered by contractors; and to act as
   liaison, on behalf of the Product Support Manager (PSM), with the AFLCMC organizations
   that are gatekeepers for standardized product data management systems (e.g. the Joint
   Engineering Data Management Information and Control System) that enable common
   government life cycle storage, maintenance, access, and control of digital product design
   data.
11.2. Address Data in Program Documents. The Technical Data Rights Strategy (TDRS) is
the key program document which addresses product and software data and data rights issues.
The TDRS is a section of the Acquisition Strategy. Data should also be addressed in the Life
Cycle Sustainment Plan, Systems Engineering Plan, Capabilities Development Document, and
Capabilities Production Document. Table 11.1 highlights product and software data and data
rights considerations for program documents. Note: Per DoDI 5000.02 TDRS is transitioning to
Intellectual Property Strategy (IPS). Future versions of this AFPAM will address IPS rather than
TDRS. Note: The PSM (or the designated EDM) should be involved in the development and
review of these program documents.
AFPAM63-128 10 JULY 2014                                                                          69


Table 11.1. Data and Data Rights Consideration by Program Documents.
Program Documents       Product and Software Data & Data Rights Considerations
                        The TDRS documents the strategy for meeting product life-cycle data
                        rights requirements and to support the overall competition strategy. Key
                        sections include:
                        Analysis of the data required to design, manufacture, and sustain the
                        system as well as to support re-competition for production, sustainment,
                        or upgrade.
                      How the program should provide for rights, access, or delivery of
                      technical data, and how the program should inspect, accept, and manage
                      data that the government requires for the life cycle. Include analysis of
Technical Data Rights data needs to implement the product support life cycle strategy
Strategy (TDRS),      including such areas as materiel management, training, Information
formerly the Data     Assurance protection, cataloging, open architecture, configuration
Management Strategy management, engineering, technology refreshment, and
(DMS) part of AS      maintenance/repair.
                        The business case analysis calculation that outlines the approach for
                        using open systems architectures and acquiring technical data rights.
                        The cost benefit analysis of including a priced contract option for the
                        future delivery of technical data and intellectual property rights not
                        acquired upon initial contract award.
                        Analysis of the risk that the contractor may assert limitations on the
                        government’s use and release of data, including Independent Research
                        and Development (IRAD)-funded data.
Acquisition Strategy    The TDRS is a key section of the AS.
(AS)
                        The LCSP should include an assessment for each product support
                        element (e.g., design interface, technical data management), compared
Life Cycle              to the data rights plan.
Sustainment Plan
                        When specific requirements cannot be satisfied (e.g., data rights, data
(LCSP)
                        deliverables), the impact on the life cycle should be noted (e.g., reduced
                        support competition, higher costs).
 70                                                             AFPAM63-128 10 JULY 2014


Program Documents     Product and Software Data & Data Rights Considerations
                      Expectations (e.g. design and documentation maturity) for program
                      reviews (e.g., Preliminary Design Review and Critical Design Review)
                      should be documented in the SEP.
                      Programs should describe in the SEP which artifacts (e.g., product and
Systems Engineering   software data) make up each technical baseline. These artifacts should
Plan (SEP)            be aligned with requirements in other documents (e.g., AS, RFP).
                      The SEP should provide a process diagram of how the program should
                      maintain configuration control of its baselines and when the program
                      should assume initial and full configuration control of its baselines.
                      CDDs/CPDs should include key logistics criteria in a paragraph “Rights
                      in Technical Data and Computer Software” that identifies product and
                      software data requirements. These requirements must be tailored per
                      program and address why that data is needed to enable the system’s
                      reliability, maintainability, operational availability, and supportability,
                      minimize its logistics footprint, enhance its mobility, and reduce the
                      total ownership cost.
                      The reasons for this paragraph include:
                      The inclusion of product and software data requirements in the
Capabilities          CDD/CPD will increase consistency between the RFP and the
Development           CDD/CPD.
Document (CDD) and    Requirements described in the CDD must be translated for evaluation in
Capabilities          a source selection in a clear and unambiguous way. Source selections
Production Document   are required to consider Government rights to technical data.
(CPD)
                      The Competition in Contracting Act requires the program office be able
                      to demonstrate that the requirements ultimately included in a RFP are
                      reasonably necessary for the AF to meet its minimum needs and not
                      restrict future competition.
                      The inclusion of requirements and their rationale into the CDD/CPD
                      will make it difficult for any successor program manager to relax such
                      requirements after the contract has been awarded. Each program
                      manager must uphold contractual requirements unless those
                      requirements are deleted from the CDD/CPD by the Vice Chairman of
                      the Joint Chiefs of Staff.
AFPAM63-128 10 JULY 2014                                                                       71


Program Documents       Product and Software Data & Data Rights Considerations
                        Describe in the SRD those technical and software data requirements
                        required for this system, including design requirements, system internal
                        data requirements, design constraints (e.g., data standards, programming
                        languages), and other documentation requirements. Data requirements
Systems Requirement     in the SRD should be used judiciously to avoid placing undue
Document (SRD)          constraints on the design team.
                        Note that complete data requirements are initially captured in the TDRS.
                        Detailed data requirements are specified in CDRLs in section C of the
                        RFP.

11.3. Address Data in Program Phases. Product and software data must be addressed
proactively throughout the weapon system life cycle, beginning with the Materiel Solution
Analysis and going through the Operations and Support phase. Table 11.2 highlights key
considerations for each phase below. The PSM (or the designated EDM) should be involved in
the development and review of these documents. Please note that considerations usually apply to
subsequent phases, but are omitted for clarity.

Table 11.2. Data and Data Rights Consideration by Life Cycle Phase.
Life Cycle Phase     Product and Software Data & Data Rights Considerations
Materiel Development Data is usually not addressed at the MDD, however, the data rights
Decision (MDD)       approach or strategy should be considered.
                        Product and software data requirements should be documented, and
                        funding identified to acquire and maintain this data.
                        Program documents (e.g., SEP) should adequately address data and data
                        rights.
Materiel Solution
Analysis (MSA)          Solicitation and contract documents (e.g., RFP, RFI, SOW, SOO, PWS,
                        contracts) should adequately address data and data rights.
                        Note that even in this phase, contracts from AF-funded prototypes
                        should include data rights and priced options for data deliverables.
                        Program documents (including the AS and CDD) should adequately
                        address data and data rights.
Technology              Product and software data reviewed at the PDR should be compliant
Maturation and Risk     with the contract requirements.
Reduction
                        Note that in this phase, contracts for AF-funded prototypes should
                        include data rights and priced options for data deliverables.
                        Program documents (including the CPD and LCSP) should adequately
Engineering &           address data and data rights.
Manufacturing
                        Product and software data reviewed at the CDR should be compliant
Development (EMD)
                        with the contract requirements.
 72                                                               AFPAM63-128 10 JULY 2014


Life Cycle Phase         Product and Software Data & Data Rights Considerations
                         Inspect delivered product and software data to verify that content and
                         markings are compliant with the contract.
                         Manage product and software data and ensure the data is under
Production &             configuration control.
Deployment (PD)          Begin delivery of digital product design data to standardized product
                         data management systems (e.g. the Joint Engineering Data Management
                         Information and Control System (JEDMICS)) for government storage,
                         maintenance, access, and control.
                         Manage product and software data and ensure the data supports end
                         users and systems which depend upon it.
Operations and           Maintain and update digital product design data in standardized product
Support (O&S)            data management systems.
                         Dispose/archive product and software data as part of the completion of
                         the program.

11.4. Address Data in Design Reviews. Product and software data problems are much easier
and less expensive to resolve if problems are identified early. These reviews, including the
Preliminary Design Review (PDR), the Critical Design Review (CDR), and in process reviews
broadly address design maturity, but they also provide the opportunity for the AF to review early
deliverables and in process data for compliance with the contract (e.g., format, marking).
Specific considerations during these reviews include:
   11.4.1. Ensure the PSM or the designated EDM participates in the review.
   11.4.2. Ensure product and software data deliverables are reviewed at the review for
   compliance with the contract.
   11.4.3. Review Contract Data Requirements Lists (CDRLs) and Supplier Data Requirements
   Lists (SDRLs) to ensure all product and software data under contract are being developed by
   the contractor and subcontractors.
11.5. Determine Technical Data and Data Rights Needs. There is a core set of data that is
usually required for the AF to review, upgrade, operate, and support weapon systems cost
effectively. Data is also used to provision for spares and to source secondary sources of
production. Yet, each program is different and may have different acquisition and support
strategies. It is important for programs to “do their homework” in order to identify data that is
required and should be acquired as well as data that may be required and should be priced. The
program should also understand data rights needs and compare those needs to data rights as
addressed in public law and the DFARS. Data deliverables without sufficient rights (or with
nonconforming markings) limit the usefulness of the data to the AF. Likewise, data rights
without sufficient data deliverables are of limited value. It is critical for the AF to address both
data deliverables and data rights to meet the needs of the warfighter.
AFPAM63-128 10 JULY 2014                                                                        73


   11.5.1. Data Needs. The AF needs product and software data at an affordable price, with
   sufficient rights, and in the right format. A starting point for most weapon systems is to
   acquire:
       11.5.1.1. Product definition data (drawings, models, and associated lists) as defined in
       "DI-SESS-81000D Product Drawings/Models and Associated Lists" and AF Drawing
       9579776 Product Data Specification.
       11.5.1.2. Technical Orders (TOs) as defined in "TO 00-5-3-WA-1 AF Technical Order
       Life Cycle Management" to meet Operation, Maintenance, Installation and Training
       (OMIT) requirements.
       11.5.1.3. Computer software (e.g., source code, executable code, binary libraries, etc.),
       software documentation (e.g., specifications, designs, test plans, test procedures, user
       manuals, installation manuals, etc.), software development environment (e.g., compilers,
       debuggers, source code management tools, etc.), and simulation software (e.g. test
       drivers/data/scripts, hardware models, environment models, etc.) and associated software
       licenses. Note: Source code without the appropriate versions of the operating system,
       assemblers, runtime libraries, build instructions, etc., may not be able to replicate the
       execution of the operational system.
   11.5.2. Data Rights Needs. The Government’s data rights to product data and software
   generally depend upon the extent to which the Government funded the development of the
   technology, whether the technology is commercial or noncommercial, and any negotiations
   for mutually agreeable “special” license agreements.
       11.5.2.1. The Government will generally receive Unlimited Rights (UR) to certain types
       of data regardless of development funding, such as form, fit, and function data (FFF), and
       data necessary for operation, maintenance, installation, and training (OMIT). The
       Government will generally receive Government Purpose Rights (GPR) for items
       developed with mixed (Government and private) funding. The Government will
       generally receive Limited Rights (LR) for items, components, or processes developed
       exclusively at private expense, and Restricted Rights (RR) for computer software
       developed exclusively at private expense. The government gets unlimited rights for
       technical data developed exclusively at the government’s expense and for software
       developed exclusively at the government’s expense.
   11.5.3. Program offices should assess claimed restrictions on the use of engineering data and
   the cost effectiveness of securing or obtaining unlimited rights or GPR rather than limited
   rights data. In addition, program offices should validate, monitor, and challenge (if
   necessary) contractor's assertions of data rights on engineering data throughout the life cycle.
   11.5.4. Tools. Use the PDAQ Decision Tree and the PDAQ Product and Software Data DID
   Selector to help develop initial data requirements. Use a Data Call to ensure that
   stakeholders have the opportunity to identify additional data required based on the unique
   characteristics of the specific weapon system.
11.6. Address Data in Requests for Proposal (RFPs). While it is important to properly
address product and software data in program strategies and other documents, the RFP is where
the AF communicates to offerors the specific data it intends to acquire. It is crucial that data be
addressed completely and consistently in all appropriate sections of the RFP (e.g., Section B, C,
 74                                                              AFPAM63-128 10 JULY 2014


H, I, J, K, L, and M) to communicate internal consistency and to eliminate inadvertent
omissions. Table 11.3 highlights considerations for individual sections of the RFP. Figure 11.1
shows the relationships of section of the RFP to each other with regard to data.

Figure 11.1. How Key of The RFP Relate To Each Other as They Address Data.




Table 11.3. Data and Data Rights Consideration RFP Section.
RFP Section             Product and Software Data & Data Rights Considerations
                        Consider creating separate Contract Line Item Numbers (CLINs) for the
                        data and data rights that will be used to sustain a program throughout its
                        life cycle. Separately priced data allows the Government to understand
                        what it is paying for and enhances its ability to make informed
                        decisions.
                        Specifically, RFPs should contain separate CLINs for the following data
                        items:
Section B – CLINS       Technical Data Package
                        Technical Orders (also known as Technical Manuals)
                        Computer Software
                        This section should also address:
                        Greater Rights for a specific CLIN
                        Priced Options
AFPAM63-128 10 JULY 2014                                                                        75


RFP Section            Product and Software Data & Data Rights Considerations
                       The following product data considerations should be taken when
                       developing the SOW. All data requested by the Government in the
                       CDRL must be clearly supported by an SOW task which describes
                       requirements for:
                       Technical Data Package (Engineering Data)
                       Technical Orders
                       Software
Section C – SOOs &     Engineering Data Guidance Conference
SOWs
                       In-Process Review (IPR) of the Technical Data Package
                       Relationship of Contractor with Subcontractors/Vendors
                       Engineering Data Updates and Revisions
                       The CDRL should define data requirements and should not
                       contractually allow for the automatic acceptance of data. Instead, the
                       CDRL should allow sufficient time for the responsible
                       organization/location to inspect and accept the data.
                       The following areas may have special contract requirements for
                       technical data and data rights:
                       Early and Often Technical Disclosure
                       Identification and Assertion of Restrictions on Commercial Technical
                       Data and Computer Software, including Open Source Software
Section H – Special
Contract Requirements Delivery and License Rights for Technical Data and Computer Software
                      Necessary for Organizational and Depot-Level Maintenance and
                      Training Systems
                       Special License Requirements
                       Warranty on Data and Software
                       The Federal Acquisition Regulations (FAR) and Defense Federal
                       Acquisition Regulation Supplement (DFARS) are the primary sources
                       of information regarding patent and intellectual property rights. The
                       appropriate clauses should be listed in section I of the RFP in order to
                       protect the Government's rights. The allocation of intellectual property
Section I – Contract
                       rights between a Government contractor and the Government is
Clauses
                       specified in standard DFARS contract clauses.
                       The Government should always pursue data rights to which it is entitled
                       at no additional cost. Since needs for the data may surface at a later
                       time, there is no supportable rationale for "giving up" rights for data to
                       which the Government is legally entitled.
 76                                                               AFPAM63-128 10 JULY 2014


RFP Section             Product and Software Data & Data Rights Considerations
                        Section J identifies a list of documents, exhibits, and other attachments.
                        Documents which might be identified in Section J include:
                        Statement of Work (SOW) or Statement of Objectives (SOO) or
                        Performance Work Statements (PWS)
                        System Requirements Document (SRD)
                      Integrated Data Environment (IDE) Concept of Operations (CONOPS)
Section J – List of   Contract Data Requirements Lists (CDRLs)
Documents, Exhibits,
and Other Attachments The CDRL is a list of authorized data requirements for a specific
                      procurement. It contains data requirements and delivery
                      information. The CDRL is the standard format for identifying potential
                      data requirements in a solicitation, and deliverable data requirements in
                      a contract. Each CDRL should have a DID associated with it. If certain
                      elements of data are not needed, the DID should be tailored downward
                      noting deletions in CDRL Block 16. The CDRL should only require
                      data specifically generated in a SOW work task.
                        DFARS clauses are intended to protect the Government from
                        restrictions on the use, release, or disclosure of data, especially on data
                        previously delivered to the Government.
Section K –
Representations &       252.227-7017 Identification and Assertion of Use, Release, or
Certifications          Disclosure Restrictions
                        252.227-7028 Technical Data or Computer Software Previously
                        Delivered to the Government
                        As it relates to product and software data and data rights, Section L
                        should include:
                        The rationale for requesting product and software data (e.g., funding
                        considerations, DFARS provisions, product support, competitive
Section L –             acquisition).
Instructions,
Conditions and          A statement on data rights (e.g., need for Offeror to assert when data
Notices to Offerors     will be delivered with less than Government Purpose Rights consistent
                        with DFARS 252.227-7017).
                        For software acquisitions, the requirement for a Software Architectural
                        Description (SAD) that identifies precisely where all software
                        applications should reside in its proposed architecture.
AFPAM63-128 10 JULY 2014                                                                        77


RFP Section              Product and Software Data & Data Rights Considerations
                         For most “best value” source selections, there is a spectrum of data and
                         data rights that may satisfy AF requirements for data. In order to weigh
                         responses properly, data rights in those source selections should not be
Section M –              listed in Section M as “pass/fail.”
Evaluation Factors
                         Rights beyond those provided by law (e.g., 10 USC § 2320 Rights in
                         Technical Data) cannot be an evaluation factor in “lowest price,
                         technically acceptable” source selections.

11.7. Data Rights Assertions. If data is adequately addressed in the RFP, and the winning
offeror is put on contract based in their winning proposal, the Government should acquire needed
data with sufficient data rights under the contract. However, if the offeror asserts rights to data
which the Government expected to secure through the “data rights assertions” process, the
Government must review those assertions to determine whether the assertions are appropriate.
   11.7.1. In the RFP, the AF should include DFARS 252.227-7017 to identify data that the
   offerors will deliver with less than unlimited rights.
   11.7.2. During source selection, the proposal evaluation team should review all proposed
   assertions against Government requirements and DFARS.
   11.7.3. Any DFARS 252.227-7013 & -7014 data rights assertion agreements reached during
   source selection are included in the contract as a Section J attachment.
   11.7.4. After contract award, the AF should only consider new assertions when based on
   new information or inadvertent omissions unless the inadvertent omissions would have
   materially affected the source selection decision.
11.8. Integrated Data Environment (IDE). An IDE is a tool for implementing digital data
operations in an acquisition program. Program managers should establish a data management
system within the IDE that allows every activity involved with the program to cost-effectively
create, store, access, manipulate, and exchange digital data.
   11.8.1. Develop a CONOPS. The program's vision for digital data operations for the life
   cycle should be captured in a program specific CONOPS that is derived from or supports the
   user’s CONOPS. It should describe how any contractor-provided solutions will support all
   life cycle activities, including interfacing with Government systems. The CONOPS is
   intended to guide contractors in proposing solutions to support the digital data operations
   vision. The document should be provided as an attachment in section J in the contract
   solicitation.
   11.8.2. During development of a weapon system, access to data may be preferred over
   delivery of data, though access to data may terminate at the end of a contract. Access
   without delivery may therefore be insufficient to satisfy long term data needs. During
   operation and support of a weapon system, delivery of data and hosting it on a government
   computer is usually preferred and a good management practice in order to maintain
   competition throughout the acquisition life cycle and to permit organic sustainment. The
   government should be aware that if a contract ends before data is delivered and the contract
   does not include a deferred ordering clause, the government may be left without needed data.
 78                                                             AFPAM63-128 10 JULY 2014


   In all phases of the acquisition life cycle, the government should account for long term data
   needs when establishing delivery requirements while also considering the likelihood the data
   will change.
   11.8.3. Insert Language and Clauses into the RFP. After the CONOPS has been developed
   and access and delivery planning has occurred, product and software data and data rights
   should be carefully addressed in the RFP.
   11.8.4. Consider including DI-MGMT-81453A, Data Accession list (DAL). The DAL
   specifies internally generated data and computer software generated by the contractor in
   compliance with the work effort describe in the Statement of Work (SOW). The DAL is an
   index of the generated data that is made available upon request. The DAL is an important
   item in any contract where the government has a potential need for data.
   11.8.5. Data Vulnerability. RESERVED
11.9. Independent Research and Development (IR&D). IR&D is initiated and conducted by
defense contractors independent of DoD control and without direct DoD funding. It includes
basic and applied research, development, and systems and concept formulation studies. IR&D
funded capability is attractive to program managers because it usually results in reduced
acquisition costs, reduced risk, and reduced timelines. However, IR&D funded capability could
result in higher life cycle costs if data rights are not addressed appropriately.
   11.9.1. Benefits of IR&D. IR&D is intended to strengthen the defense industrial base and
   the technology base of the U.S., enhance the industrial competitiveness of the U.S., and
   promote the development of technologies identified as critical. Often, IR&D will help a
   program reduce acquisition costs, reduce development risk, and enable faster time to
   deployment of military systems and capability.
   11.9.2. Risks & Cautions with IR&D. The government may reimburse (indirectly fund)
   development costs of a system or capability through IR&D, but not get the data rights
   typically received when the government directly funds development. Without data rights, the
   government may have limited flexibility for upgrades, spare parts, or operations and support,
   resulting in higher life cycle costs.
11.10. Inspection and Acceptance of Data. Provisions within the contract need to identify
inspection and acceptance requirements for data. This is usually done on the CDRL.
   11.10.1. Data received as part of a contract is sometimes accepted without adequately being
   inspected for contract compliance (e.g. completeness, quality, format, markings). Data in this
   case is not dependable for operations and support or other reasons for which it was acquired.
   11.10.2. PDAQ provides an inspection and acceptance checklist that should be followed to
   ensure that technical data and software provided to the AF by contractors during the
   acquisition life cycle meets contract requirements and is complete, accurate, and includes the
   appropriate rights.
11.11. Life Cycle Management of Data and Data Rights. This section provides additional
guidance and best practices for program offices to execute the requirements at AFI 63-101,
paragraph 6.12.4.2. This AFI paragraph requires program offices to provide digital product
design data to a DoD standardized product data management system (e.g. the JEDMICS for
AFPAM63-128 10 JULY 2014                                                                      79


common government storage, maintenance, access, and control. Program offices maintain
updated digital product design data in the standardized system throughout O&S.
   11.11.1. Use guidelines in AFI 63-101, this document, the PDAQ website, MIL-STD-31000
   DoD Standard Practice Technical Data Packages, AFMCI 63-402, MIL-HDBK-245,
   Preparation of Statement of Work (SOW), MIL-HDBK-288 Military Handbook Review and
   Acceptance of Engineering Drawing Packages and DoD 5010.12M, Procedures for
   Acquisition and Management of Technical Data, to incorporate requirements for engineering
   data into Requests for Proposal (RFP), Statement of Objectives (SOO), Statement of Work
   (SOW) and contracts.
   11.11.2. Preparing, Approving, Maintaining and Updating Digital Product Design Data for
   Government Storage, Access, and Control. Note: For the purposes of this Pamphlet, the
   term “product design data” consists of the information routinely delivered as a part of the
   Technical Data Package. See MIL-STD-31000, Figure 1. It includes manually and
   computer generated engineering drawings, unincorporated change documentation, data
   bases, models, and associated lists. Air Force activities engaged in the generation and
   maintenance of engineering drawings and associated documentation implement the guidance
   of applicable non-Government standards (see Attachment 10). Commercially available
   generic drawing requirements manuals, based primarily on these standards (also listed at
   Attachment 10), may be used for the preparation and revision of engineering drawings and
   associated documentation.
      11.11.2.1. IAW AFI 63-101, all engineering drawings generated should be compatible
      for retention within a DoD standardized product data management system such as
      JEDMICS government storage, maintenance, access, and control. If a prime contractor
      central repository is used instead of a government maintained and control facility,
      appropriate access for government personnel must be ensured through specified inclusion
      in the contract.
      11.11.2.2. ISO 10303, Standard for Exchange of Product model data (STEP), AP239,
      Product Life Cycle Support, defines a neutral format that allows product data to be shared
      among heterogeneous systems. PMs for new aerospace system designs and major
      modifications should acquire engineering data that conforms to ISO 10303-239 and the
      MIL-STD/DID in which the blue-highlighted requirements are incorporated unless either
      the PEO or System Program Manager approves a waiver. Use of ISO 10303-239 for
      legacy systems, based on a positive business case analysis, is encouraged.
      11.11.2.3. Verification, Approval, and Authorization. Air Force design activities should
      have an effective manual or electronic verification, approval, and authorization process or
      system. Authorized individuals enter their names in the appropriate locations to indicate
      that engineering drawings and engineering orders conform to all applicable requirements.
      Department of Defense Form 2617, Engineering Release Record, or equivalent local
      protocol, is used to document these actions. See Attachment 9 of this document for
      recommended implementation practices.
      11.11.2.4. Formatting.
80                                                           AFPAM63-128 10 JULY 2014


        11.11.2.4.1. AF-unique engineering drawing layouts in a variety of formats (formerly
        AF Forms 1651 through 1656A) are available as .dxf files for direct use with current
        Computer Aided Design (CAD) systems generating engineering drawings.
        11.11.2.4.2. Larger drawing sizes may be generated as prescribed in ASME Y14.1,
        Decimal Inch Drawing Sheet Size and Format, using the current AF Form 1656
        format criteria.
        11.11.2.4.3. When AF-unique engineering drawing layouts are used to prepare multi-
        sheet drawings, abbreviated title block formats of may be used for continuation
        sheets. For larger sizes used to prepare multi-sheet drawings, continuation sheets
        may be generated as prescribed in ASME Y14.1.
        11.11.2.4.4. AFMC standardized parts list format (formerly AF Form 1658), data list
        format (formerly AF Form 1659), and index list format (formerly AF Form 1660)
        may be generated in a CAD system provided the final format output meets current
        ASME requirements as applicable. They should duplicate the AFMC-approved form
        or include the following exception notice in the lower right-hand corner of the form
        margin or border: "Exception to AFMC template for CAD generation approved by
        HQ AFMC/A4UE (date of approval) (name of software and vendor)."
     11.11.2.5. Drawing Numbers. To enable the management, traceability, and accessibility
     of drawings in the standardized product data management system, Air Force generated or
     acquired drawings must use assigned numbers provided by HQ AFMC/A4UE.
     AFMC/A4UE may also provide deviations. See Attachment 9 for additional practices on
     drawing numbers.
     11.11.2.6. Drawing Revision Practices. ASME Y14.35M, Revision of Engineering
     Drawings and Associated Documents, prescribes standard practices for revising
     engineering drawings and associated lists.
     11.11.2.7. Engineering Orders (EO). EOs document information that affect the content
     or status of an Air Force drawing or the products defined on contractor drawings. EOs
     may be approved only by the chief/lead engineer(s) authorized by the responsible
     Configuration Control Authority. EOs may be initiated by anyone.
        11.11.2.7.1. EO Formats (formerly AF Forms 3925, 3926, and 3927). When
        inclusion of non-text data is required, AFMC standardized EO formats may be
        generated in a CAD system provided the final form output meets current ASME
        requirements as applicable. They should duplicate the AFMC-approved format or
        include the following exception notice in the lower right-hand corner of the form
        margin or border: "Exception to AFMC template for CAD generation approved by
        HQ AFMC/A4UE (date of approval) (name of software and vendor)."
        11.11.2.7.2. Change Notice Engineering Order (CNEO). Chief/lead engineers use
        CNEO’s to inform drawing users that a revision has been made. It describes and
        records changes incorporated during the revision action and precludes extensive
        revision descriptions in the drawing revision block, and provides documentation of all
        engineering and configuration management approvals. The EO identifier should be
        annotated in the revision history block of the revised drawing.
AFPAM63-128 10 JULY 2014                                                                      81


        11.11.2.7.3. EOs should be permanently maintained with the affected drawing to
        provide a history of Air Force required drawing changes.
        11.11.2.7.4. Advance Engineering Change Orders (AECO). AECO’s authorize
        drawing changes before revising the affected drawing original. Released AECO’s are
        considered an integral part of the drawing and represent changes that will be
        incorporated only on Air Force drawings and other original drawings acquired by the
        Air Force through a design activity transfer, at the next drawing revision.
           11.11.2.7.4.1. AECO Use. AECO’s are used only when the schedule for
           revising, releasing, and distributing the affected drawing does not allow time to
           incorporate the change.
           11.11.2.7.4.2. AECO Incorporation. AECOs should be incorporated into the
           affected drawing at the earliest opportunity.
        11.11.2.7.5. Advance Engineering Supplemental Orders (AESO).                AESO’s
        supplement drawings of another design activity, government or contractor, to
        document and control Air Force required departures from the established product
        baseline. AESO’s supplement the product baseline drawings of other design
        activities for modification or follow-on production of materiel. However, AESO’s
        should be avoided to prevent configuration management, control, and data conflicts
        between the drawing original and Air Force “supplemented” copies. The AESO must
        accompany the affected drawing to correctly implement the supplemental
        requirements until incorporated through formal revision action by the Current Design
        Activity (CDA).
           11.11.2.7.5.1. AESO Incorporation. The affected drawing CDA is the only
           activity authorized to incorporate the contents of an AESO on the drawing
           original. The affected drawing CDA must be tasked to incorporate the AESO
           through contract action for contractor activities and other appropriate means for
           Government activities. Note: As an alternative, acquiring the drawings through
           design activity transfer action should be considered when the CDA is reluctant,
           for whatever reason, to incorporate an AESO.
           11.11.2.7.5.2. AESO Limitation. An AESO:
               11.11.2.7.5.2.1. Does not constitute a revision to the drawing.
               11.11.2.7.5.2.2. Does not establish new, or alter existing, item identification.
               11.11.2.7.5.2.3. Does not alter drawing identification in any manner,
               including the drawing title.
               11.11.2.7.5.2.4. Is valid only for the drawing revision level against which it is
               prepared. Further revision of the drawing by the CDA, whether or not the
               subject AESO is incorporated, invalidates the AESO.
               11.11.2.7.5.2.5. Must be permanently associated with the drawing until such
               time as that drawing is revised.
           11.11.2.7.5.3. AESO Impact. When generating an AESO against the drawing of
           another design activity, the preparing official determines the impact of proposed
82                                                            AFPAM63-128 10 JULY 2014


            changes to related drawings. An AESO is prepared for affected drawings to
            document required changes resulting from the initial proposed AESO.
            11.11.2.7.5.4. AESO Alternatives. As an alternative to generating an AESO, the
            use of an Altered Item Drawing or Modification Drawing as described in ASME
            Y14.24, Types and Applications of Engineering Drawings, may be considered.
            Both of these drawing types generate a new identification for the affected item
            necessitating revision of related documentation such as next higher assemblies,
            technical manuals, and cataloging, provisioning, and configuration management
            records.
        11.11.2.7.6. Change History. All applicable outstanding approved EOs should be
        incorporated when the drawing is revised. All incorporated AECOs should be
        maintained with the drawing to provide drawing change history.
        11.11.2.7.7. Deviations. EO’s may be used to authorize and document deviations to
        items or processes defined on previously released engineering drawings and
        associated lists. This information allows engineers to substitute materials and
        processes for one time deviation from dimensions, tolerances, finishes, processes, and
        so on, for specific applications.
     11.11.2.8. Design Activity. The original design activity is the owner of the drawing until
     design responsibility is transferred to another activity. The design activity whose name
     appears in the title block is the original design activity. The original drawing design
     activity identification should never be changed. When design responsibility changes, a
     transfer of design activity responsibility should be accomplished by adding the current
     design activity to the drawing in accordance with ASME Y14.100 paragraph 6.5.2.1 and
     ASME Y14.35.
        11.11.2.8.1. Design Activity Transfer. Original engineering drawings transferred to
        an Air Force design activity are revised to show the current design activity per the
        guidance of ASME Y14.35M. Variations in placement of the CDA identification on
        the drawing may be required due to drawing formats and content. The ODA
        Commercial and Government Entity (CAGE) code and drawing number is retained
        without change or alteration and the current design activity legend added as
        prescribed by ASME Y14.100, Attachment D.
        11.11.2.8.2. Design Activity Transfer Documentation. Transfer of design activity
        requires documentation of agreement between the losing and gaining design
        activities. This documentation is retained by both the gaining and losing design
        activities. Documentation of subsequent transfer to another design activity should
        also be retained.
        11.11.2.8.3. Design Activity Transfer Notification. Notify AFMC/A4UE upon
        completion of a design activity transfer action. Notification includes a list of the
        drawing numbers transferred (first and last numbers if an uninterrupted sequence), the
        sending and receiving activities, and date of transfer completion.
AFPAM63-128 10 JULY 2014                                                                     83


                                         Chapter 12

                          LIFE CYCLE RISK MANAGEMENT

12.1. Overview of Life Cycle Risk Management.
   12.1.1. Introduction. Without prescribing a particular methodology, DoDI 5000.02 requires
   programs to assess and manage cost, schedule, and performance risks. AFI 63-101/20-101
   identifies the minimum standardized attributes for any AF program’s risk management effort.
   “Life Cycle Risk Management” (LCRM) is the AF term for the standardized risk
   management approach. This Chapter provides additional guidance on implementing LCRM
   across the AF integrated life cycle management and oversight enterprise.     Note: LCRM
   differs from operational risk management addressed in AFI 90-802, Risk Management.
      12.1.1.1. LCRM is not a new or stand-alone risk management process. LCRM leverages
      the existing, well accepted risk management methodologies already used by industry and
      DoD. These accepted methodologies are included in the Defense Acquisition Guidebook
      and are treated in expanded detail in the Risk Management Guide for DoD Acquisition.
      In addition, they are taught as a basic part of DoD and AF Acquisition and Sustainment
      training and education.
      12.1.1.2. LCRM builds on these established risk management methodologies and
      provides the framework for standardizing the implementation of risk management across
      the AF integrated life cycle management enterprise.
      12.1.1.3. LCRM is not a separate, stand-alone risk management activity. When properly
      implemented, LCRM uses inputs from and provides outputs to most other program
      planning, analysis, management, and oversight activities.
   12.1.2. LCRM Concept. At its core, effective program management and oversight is risk
   management: the proactive management of future uncertainties to ensure that program cost,
   schedule, and performance objectives are achieved in every phase of the life cycle. To be
   effective, LCRM must be conducted throughout the life cycle at all levels, in a proactive
   rather than reactive mode by an integrated team.
      12.1.2.1. To be effective, risk management must be performed continuously across the
      integrated life cycle management framework. LCRM risk information must be preserved
      and “handed off” between life cycle phases. Risk management must be conducted jointly
      with the prime contractor team. Consider including contractual language in the request
      for proposal for programs seeking to ensure joint risk management participation during
      program execution. For examples of contractual language, see AFLCMC Process for Risk
      and Issue Management.
      12.1.2.2. LCRM is the process used by decision makers at all levels of program
      management and oversight to identify, analyze, and then reduce, offset or accept risks.
      LCRM provides leaders and staff with a consistent and systematic methodology to
      identify, assess, and choose the preferred course of action for any given situation. To be
      effective, LCRM must become a fully integrated element of planning, executing, and
      overseeing a program.
84                                                             AFPAM63-128 10 JULY 2014


     12.1.2.3. LCRM must be proactive rather than reactive. When those with a stake in a
     program do not consistently and rigorously use risk management principles and practices,
     what could have been manageable, potential risks become unexpected current problems
     (i.e. issues), driving program management and leaders to react with a more limited range
     of options that frequently have bigger adverse impacts on program cost, schedule, and
     performance.
     12.1.2.4. The risk management process is owned by program management; however,
     financial management and engineering play leading roles. Risk management has often
     been conceived as an exclusively engineering and technical activity, functionally driven
     by the engineering community. LCRM is not an exclusively technical activity. It is an
     integrated approach to managing all of the programs cost, schedule, and performance
     risks. That is why within each program office, LCRM must be executed by cross-
     functional teams that could include cost analysts, contracting officers, acquisition
     intelligence analysts, sustainment planners, schedulers, sub-system managers, and other
     specialists in addition to engineering.
  12.1.3. LCRM Definitions.
     12.1.3.1. Risk. A risk is a future event that, if it occurs, may cause a negative outcome
     or an execution failure in a program within defined performance, schedule, and cost
     constraints. The likelihood and consequence may be estimated for a risk, in contrast to a
     concern where one or both terms are unknown and cannot be estimated. Risks should
     only be identified if their likelihood and consequence are plausible and credible. For AF
     purposes, the likelihood, as reflected in the probability of occurrence for risks, should be
     between 5-99 percent. The range for risk management likelihood reflects that the lower
     limit bounds risks as greater than 5 percent certain (less than 5 percent is insignificant)
     and the upper limit bounds risks as less than 100 percent certain. A risk must have all of
     the following three components: 1) it is a future event, 2) it has a likelihood, as assessed
     at the present time, of that future event occurring, and 3) it has a negative consequence.
     Note: Programs can also manage issues and opportunities but are not a part of risk
     management. If a risk comes to fruition, it should be categorized as an issue. Risks can be
     defined/presented as: .05 < Likelihood < .99, Consequence > 0, Timeframe: future.
        12.1.3.1.1. Risk Handling. Risk handling is the preferred and more encompassing
        term to recognize that there are potentially multiple options to manage risks. These
        options include accepting, monitoring, transferring, mitigating (or controlling), and
        avoiding risks. These options are defined in Section 12.2.5.1. Note: The LCRM
        process model expands the title of this step from “Risk Mitigation Planning” in the
        Risk Management Guide for DoD Acquisition to “Risk Handling Planning and
        Implementation” to recognize that most of these options address handling risk in a
        manner other than mitigating (i.e. eliminating or reducing) it. This also emphasizes
        that in some cases it may be appropriate to “handle” a risk through acceptance or
        transferring the risk, for example, rather than mitigation actions that may prove more
        costly.
     12.1.3.2. Concern. A concern is a potential future event for which the cross-functional
     LCRM team does not have sufficient information to quantify a likelihood or
     consequence. An example of a concern is “Congress may not fund the full program, and
AFPAM63-128 10 JULY 2014                                                                       85


     the amount of funding is unclear.” A concern should be periodically monitored and
     reevaluated for likelihood and/or consequence. Once likelihood and consequence can be
     quantified by the team, a CONCERN becomes a RISK. The “concern list” should be a
     short and/or temporary list. Most potential future negative events can be evaluated and
     managed as risks: Likelihood = unknown, Consequence = unknown, Timeframe: future.
     12.1.3.3. Issue. An issue is a negative event that has occurred (came to fruition), is
     occurring (happening in present time), or is certain to happen in the future (100 percent
     probability of occurring) and has a detrimental impact on at least one dimension of
     consequence. (performance, schedule, cost). An example of an issue is “satellite
     availability will drop below the required threshold value in 2 years.” Issues can be
     defined as: Likelihood = 1.00, Consequence > 0, Timeframe: past, present or future.
         12.1.3.3.1. If the negative event, while not a risk, has occurred or is occurring, the
         program should be actively addressing the detrimental effects. If the negative event,
         while not a risk, is in the future and has 100 percent probability of occurrence, the
         program should plan measures to address the effects for when it comes to fruition.
         12.1.3.3.2. An issue should be periodically monitored and re-evaluated for
         consequence. If the probability changes to below 100 percent, an issue may need to
         be re-categorized as a risk. Note: An issue is not a risk and does not belong on the
         LCRM 5x5 matrix because the probability does not fall between 5-99 percent and the
         “If” risk statement portion cannot be developed.
     12.1.3.4. All risks should be visible up to the Service Acquisition Executive
     (SAE)/Milestone Decision Authority (MDA) level. Moderate and high risks tend to get
     the most attention; however, evaluate low risks to determine if they have a compounding
     effect greater than a single moderate or high risk impact. If so, these low risks require the
     same visibility as moderate and high risks. There are different perspectives for
     interpreting risks at different management levels. The consequence tables provide a
     uniform rating structure applicable to all management levels.
  12.1.4. Roles and Responsibilities. LCRM is a key enabler of risk-based program
  management and decision-making. Roles and responsibilities at all levels must be executed
  in order for LCRM to be effective. Under LCRM, standardized risk reporting about each
  program follows the hierarchy of Program Manager (PM), Program Executive Officer (PEO),
  and Milestone Decision Authority. How leadership uses that information is critical to the
  successful LCRM adoption and to the launching and sustainment of high confidence
  programs.
     12.1.4.1. Program Managers. Program managers are responsible for:
         12.1.4.1.1. Establishing and monitoring the program’s LCRM effort.
         12.1.4.1.2. Approving the content of the program’s Risk Management Plan (RMP).
         12.1.4.1.3. Constituting cross-functional risk management integrated product teams
         (IPTs).
     12.1.4.2. Cross-Functional Risk Management IPTs. Cross-functional risk management
     IPTs are responsible for the day-to-day execution of LCRM within program offices.
86                                                           AFPAM63-128 10 JULY 2014


        12.1.4.2.1. Risk management touches on all aspects of program management to
        include cost, schedule, performance, technical, product data access, technology
        protection, information assurance, production, sustainment, logistics planning, and
        other appropriate areas. Effective LCRM efforts must have representation, as
        necessary, from all program specialties, however, it is recognized that membership
        should need to be scaled to program size and life-cycle phase.
        12.1.4.2.2. The RMP documents the program’s cross-functional risk management
        IPT membership and responsibilities.
        12.1.4.2.3. The IPT ensures that the program does not limit risk management to
        contractor activities, but also includes those risks inherent to the Government (e.g.
        Government Furnished Equipment [GFE], external interfaces to other programs, etc.).
        12.1.4.2.4. The IPT should also ensure that risk management efforts interface with
        cost analysis, schedule analysis, requirements analysis, systems engineering, and
        system safety efforts.
        12.1.4.2.5. The IPT ensures historical risk information is documented and maintained
        for “lessons learned” and trend analysis purposes.
        12.1.4.2.6. The following contractor and Government role equivalents are suggested
        for participation in the IPT: program management, engineering, logistics, financial
        management, scheduling, cost analysis, test, independent subject matter experts (i.e.
        software, avionics, airworthiness, systems engineering), and advisors, and risk
        manager. The management level of the actual participants should be dependent on
        the level of the risk.
     12.1.4.3. Risk Training. For successful risk management, all functionals of the AF team
     should receive, at a minimum, basic risk management process training. Key program
     personnel with program management or analysis responsibilities should receive training
     on risk management tools and databases they use relative to their roles. The functionals
     should be familiar with contractor and stakeholder risk management processes and tools.
     Many performance risks are identified and managed by contractors and stakeholders.
     Risk management training is available from a variety of Government sources, including
     but not limited to the Defense Acquisition University (DAU), Air Force Institute of
     Technology (AFIT), and Life Cycle Management Center (LCMC).
  12.1.5. Key Elements of LCRM.
     12.1.5.1. Risk Management Guide for DoD Acquisition is the basic guidance for
     executing risk management. This Chapter provides additions and clarifications to the
     basic guidance in the DoD guide. The DoD guide and this Chapter should be used
     together.
     12.1.5.2. Five-step LCRM Process. LCRM is executed throughout the life cycle in a
     continuous and iterative five-step process. These steps differ slightly from the steps
     identified in the DoD guide. Additional AF-specific guidance on each of these steps is
     included in Section 12.2 of this Chapter. The five steps are:
        12.1.5.2.1. Risk Management Planning (not considered a separate step in the DoD
        guide).
AFPAM63-128 10 JULY 2014                                                                    87


        12.1.5.2.2. Risk Identification.
        12.1.5.2.3. Risk Analysis.
        12.1.5.2.4. Risk Handling Planning and Implementation (to include documentation of
        interim risk acceptance).
        12.1.5.2.5. Risk Tracking (to include the formal documentation of final/residual risk
        acceptance).
     12.1.5.3. Risk Management Plan (RMP). The RMP describes the strategy by which the
     program should coordinate and integrate its risk management efforts, and should be
     continually matured throughout the program’s life cycle. It does not need to be a stand-
     alone document. It can be incorporated into other appropriate planning document, and
     must be linked to risk management activities described in other planning documents (e.g.
     source selection plan, Systems Engineering Plan, etc).
     12.1.5.4. Cross-functional Risk Management IPTs. As discussed previously, cross-
     functional risk management IPTs are critical to the successful execution of LCRM.
     12.1.5.5. Standardization. To ensure consistent and rigorous LCRM execution and
     reporting, all programs are required per AFI 63-101/20-101to use the standard LCRM
     5x5 matrix, likelihood criteria, and consequence criteria to analyze program risks. All
     moderate and high risks must be presented using the standard LCRM 5x5 matrix as a part
     of program, technical, and Milestone decision reviews. Realizing that every risk may
     have multiple consequences (performance, cost, and schedule) which should be analyzed,
     the matrix should depict the consequence with the most severe impact. Risk
     handling/mitigation plans are prepared for moderate and high risks. Formal decisions to
     proceed (e.g. Milestone Decisions, Acquisition Strategy Panels, etc.) constitute approval
     of a program’s current risk analysis and handling/mitigation plans. Formal acceptance of
     moderate and high residual risks (after all handling/mitigation actions have been
     completed) is included in approval documentation (e.g. the Acquisition Decision
     Memorandum). The use of the matrix and these criteria is discussed in more detail later.
     12.1.5.6. Products include qualified risks identified as high, moderate, or low based upon
     the LCRM 5x5 matrix output; risks quantified in terms of performance, schedule, and
     cost; and risk handling/mitigation plans (including waterfall charts). Additional products
     may be available if a risk analysis simulation is performed. These include a cumulative
     distribution function (“S-curve) representing confidence levels for schedule and cost. The
     outputs from a Schedule Risk Assessment (SRA) should be included for trend analysis,
     used to better manage the schedule, and used for Integrated Baseline Reviews (IBRs) and
     other top-level reviews, where applicable. A Cost Risk Assessment (CRA) provides
     better risk impact estimates in terms of cost based upon subject matter expert (SME)
     inputs. The outputs from a CRA should be included in Program Office Estimates (POEs).
     12.1.5.7. LCRM Database.          PMs are responsible for tracking all risks and
     handling/mitigation activities in a database that archives risk management across each
     program’s life cycle. This is especially important to support the seamless transition of
     risk management between life cycle phases, responsible organizations, and prime
     contractors. When a program transitions to the Operations and Maintenance (O&M)
     phase, the LCRM database can form the basis for the sustainment program’s risk
88                                                            AFPAM63-128 10 JULY 2014


     management efforts. The AF Enterprise-wide Risk Management System (an AF-tailored
     version of the COTS software “Active Risk Manager (ARM)”) is the current standard AF
     tool to manage and track program risks across the life cycle.
  12.1.6. LCRM Relationships to Other Program Management and Risk Management
  Activities.
     12.1.6.1. Section 12.3 of this chapter describes in detail how LCRM uses inputs from
     and provides outputs to most other program planning, analysis, management, and
     oversight activities. LCRM differs from and connects to other existing program risk
     management efforts in the following ways.
        12.1.6.1.1. System Safety/Mission Assurance. Mission assurance and system safety
        risks are assessed and managed using methodologies separate from LCRM. Manage
        system safety risks by applying MIL-STD-882, the DoD Standard Practice for
        System Safety. All high and serious system safety risks must also be translated and
        presented IAW AFI 63-101/20-101 at all program, technical, and Milestone decision
        reviews or to support other key decision points. The LCRM 5x5 should display
        integrated system safety, cost, schedule, and performance risks; this is important
        because the handling/mitigation of system safety risks can often increase cost,
        schedule, and performance risks, and vice versa.
        12.1.6.1.2. AFI 90-802, Risk Management. The tenets of AF-wide operational risk
        management covered in AFI 90-802 are based on the same general principles as
        LCRM. However, the key elements of LCRM discussed in this chapter have been
        tailored specifically for life cycle management programs. When a system is fielded,
        some of the program’s LCRM risk information may be useful to the risk
        identification efforts of operators and maintainers. Similarly, operator and maintainer
        risk management activities can identify risks that should be integrated into the
        program’s LCRM efforts.
        12.1.6.1.3. Risk-Based Source Selection.          Risk-based Source Selection is
        accomplished IAW the FAR, DFARS, and AFFARS. LCRM risk information must
        be used as inputs to source selection activities.
        12.1.6.1.4. Technology Readiness Assessments (TRAs) and Manufacturing
        Readiness Assessments (MRAs). TRAs and MRAs are not, by themselves, risk
        management processes, nor do the results represent risks. For example, TRA and
        MRA results do not incorporate consequence of occurrence, and are only partially
        related to the probability of occurrence term. Instead, TRAs and MRAs are tools for
        identifying triggers that may become risks and providing inputs with regard to how
        well these items are managed over time. Results from TRAs and MRAs should be
        examined as inputs to the risk identification process as appropriate. However,
        because technology readiness level values and similar measurements for
        manufacturing and other categories are related only to the risk likelihood portion and
        not related to the consequence of occurrence, an item with a low Technology
        Readiness Level (TRL) or related value is not necessarily a risk.
        12.1.6.1.5. Programs developing systems/platforms/components that will interface
        with any Information Technology (IT) network, data links, and/or classified radio
AFPAM63-128 10 JULY 2014                                                                       89


          network passing data should review AFI 33-210, Air Force Certification and
          Accreditation (C&A) Program (AFCAP), and comply, if applicable.
12.2. Five-Step Life Cycle Risk Management Process Model.
   12.2.1. Introduction
      12.2.1.1. Proper risk management of any activity, whether tied to AF acquisition or any
      other endeavor, uses basic and universally recognized steps to execute the process. These
      steps involve: planning, identifying risks, analyzing those risks to determine their
      importance, determining how to handle/mitigate those risks, implementing
      handling/mitigation actions and tracking to determine if the handling/mitigation actions
      are effective.
      12.2.1.2. These steps should be conducted continuously throughout the acquisition life-
      cycle in an iterative manner. Identifying a risk and taking handling/mitigation actions
      should not end the process regarding that risk; teams need to continue to track
      handling/mitigation actions, determine if the root cause still remains (i.e. repeat risk
      identification), analyze the likelihood and consequence to determine remaining potential
      programmatic impacts, revise risk handling/mitigations plans if needed, implement those
      plans, and then return to tracking again. Until a risk is eliminated or sufficiently reduced
      to an acceptable level, this process is repeated.
      12.2.1.3. The AF LCRM process model reflects these continuous and iterative steps as
      illustrated in Figure 12.1. As stated earlier, the Risk Management Guide for DoD
      Acquisition serves as the basic guidance for risk management and should be used with
      this chapter. There are slight differences between the AF LCRM process model and the
      process model presented in the DoD guide. Much of the content of the DoD guide is
      identical for the AF LCRM process, though, and this chapter does not repeat that content.
      The remainder of this section addresses the five steps in the AF LCRM process model
      and those differences and focuses on additions, clarifications, and points of emphasis
      only. Note: As handling efforts are realized, the risk parameters change and the risk
      needs to be re-characterized by revisiting step 2. Once a risk is managed to an acceptable
      level, the risk is closed-out as determined by the Risk Manager/Program Office
      responsible party/PEO.
 90                                                             AFPAM63-128 10 JULY 2014


Figure 12.1. AF LCRM Process.




   12.2.2. Risk Management Planning—Step 1
      12.2.2.1. Risk management planning is the foundation of the LCRM process and key to
      successful program execution. It links a program’s risk management effort to life cycle
      planning by answering “who, what, where, when, and how” risk management should be
      performed. The product of risk management planning is a Risk Management Plan.
      12.2.2.2. Risk Management Plan (RMP).
         12.2.2.2.1. The PM prepares a Risk Management Plan as summarized in the Risk
         Management Guide for DoD Acquisition.
         12.2.2.2.2. The RMP explains the strategy by which the program should coordinate
         and integrate its LCRM effort. The RMP is a strategic document providing an
         overarching plan. Risk handling/mitigation plans are separately developed to address
         individual risks and are tactical in nature. Risk handling/mitigation plans are typically
         contained in the AF standard risk tool and not contained in the RMP.
         12.2.2.2.3. As previously stated, the RMP does not need to be a stand-alone
         document. It is recommended that the RMP be incorporated into appropriate
         planning documents, and linked to risk management activities described in other
         planning documents (e.g. IMS, SEP, Acquisition Strategy) as necessary. PEOs may
AFPAM63-128 10 JULY 2014                                                                  91


        develop organizational RMPs addressing strategy common across the organization
        with Program unique strategy addressed within planning document.
        12.2.2.2.4. RMP Content. The Risk Management Guide for DoD Acquisition
        provides an example format summary for the RMP. The RMP should also describe a
        database for PM and IPT use in tracking risks, handling/mitigation actions, and
        decisions regarding risks.
           12.2.2.2.4.1. The database is intended to provide a mechanism for archiving
           LCRM activities across each program’s life cycle to support transition of risk
           management between life cycle phases, responsible organizations, and
           contractors. It also provides a source for “lessons learned” that can be passed to
           subsequent programs.
        12.2.2.2.5. To further assist with RMP development, consider the following:
           12.2.2.2.5.1. Does it explain the purpose, scope, ground rules and assumptions,
           processes, success criteria, and constraints pertaining to the program LCRM
           process?
           12.2.2.2.5.2. Does it describe how the LCRM process integrates and relates to
           other life cycle activities?
           12.2.2.2.5.3. Does it include and discuss potential sources (design/engineering,
           manufacturing, support, technology, cost, budget, schedule, etc.) in sufficient
           detail that this information can be used to assist in risk identification?
           12.2.2.2.5.4. Does it explain LCRM roles and responsibilities and the cross-
           functional IPT; describe customer and supplier interactions with respect to
           LCRM? In addition, who determines what is a risk, concern, or issue? Who are
           the responsible parties for risk handling and reporting progress?
           12.2.2.2.5.5. Does it address how team members will be trained to apply LCRM?
           12.2.2.2.5.6. Does it describe frequency, methods, tools, and metrics?
           12.2.2.2.5.7. Does it include a process for identification of risk acceptance
           criteria?
           12.2.2.2.5.8. Does it describe how and when risk information is aggregated and
           communicated both internally to the program and throughout the execution chain?
           12.2.2.2.5.9. Does it specify the format and data elements for tracking risks;
           document how the list will be maintained; how configuration control will be
           maintained; who it will be shared with; and how often it will be
           reviewed/updated?
           12.2.2.2.5.10. Does it address a strategy for developing a contingency plan (e.g.,
           requirement de-scoping) if sufficient risk handling cannot be accomplished?
        12.2.2.2.6. RMP Updates. As a strategic document, the RMP should still be updated
        periodically and matured throughout each program’s life cycle as it crosses phases at
        Milestones. Other events that may lead to RMP updates include:
           12.2.2.2.6.1. Changes in acquisition strategy, program re-baselining, or support
92                                                             AFPAM63-128 10 JULY 2014


            strategy,
            12.2.2.2.6.2. Preparation for a milestone decision,
            12.2.2.2.6.3. Significant changes in success criteria, program architecture, or
            design,
            12.2.2.2.6.4. Results and findings from event-based technical reviews, and
            12.2.2.2.6.5. Program Objective Memorandum (POM) submissions.
  12.2.3. Risk Identification—Step 2
     12.2.3.1. With the RMP in place, the next step in the LCRM process initiates execution
     of the plan: Risk Identification.
     12.2.3.2. Risk Identification is the action of examining a program or project to determine
     “What can go wrong?”
     12.2.3.3. Risk Identification should be performed continuously and by all program
     personnel.
     12.2.3.4. Risk identification focuses on identifying the “root cause” (when possible)
     contributing to the uncertainty. An If-Then statement is developed for each risk to
     describe the risk, consequence, and impact where: “If” is the potential negative event or
     “root cause”, and “Then” provides the results or consequences (possible outcomes of the
     negative event). (Note: DoD refer to the potential future event as a “root cause” to
     distinguish it from the consequence or impact. The AF typically refers to it as the
     “risk”.)
         12.2.3.4.1. An example of a risk statement: “IF long lead components are not
         received in time to produce the flight controller by 5/17/13 for software integration
         testing, THEN program will lose access to testing and ground flight resources,
         leading to a delay in flight control system completion by four months and increasing
         cost by $156,000.”
     12.2.3.5. Risk Identification Sequence. It is best performed by decomposing the
     program or project into the lowest level of activities, elements, or processes reasonable
     for that phase of the life cycle, and then asking “What can go wrong?” and “Why?” (to
     determine root cause when possible).
         12.2.3.5.1. Decompose the program or project using the Work Breakdown Schedule
         (WBS), Integrated Master Schedule (IMS) or Integrated Master Plan (IMP), sub-
         processes, key requirements, or other means of identifying discrete efforts for the
         program or project.
         12.2.3.5.2. Examine each discrete effort in terms of risk sources or areas,
         12.2.3.5.3. Determine what could potentially go wrong, and then
         12.2.3.5.4. When possible, use appropriate methods to identify a root cause(s), such
         as observation or digging deeper into the WBS/IMS/etc. until a root cause is
         determined.
AFPAM63-128 10 JULY 2014                                                                     93


     12.2.3.6. For each risk identified, clearly assign ownership and responsibility tied to the
     program or project structure (IPT) and linked to discrete efforts within the WBS/IMS/etc.
     Risks may affect more than one IPT, thus, IPTs should communicate with one another to
     understand how they interact.
     12.2.3.7. Specific WBSs or activities in the IMS that relate to risks should be identified
     in the risk register and schedule.
     12.2.3.8. Sources of Risk. The Risk Management Guide for DoD Acquisition includes a
     list of typical risk sources. Additional sources of risk for consideration are:
        12.2.3.8.1. Changes in Government and contractor leadership and key personnel,
        12.2.3.8.2. Changes in assigned or planned resources,
        12.2.3.8.3. Transition activities between life cycle phases and/or organizations,
        12.2.3.8.4. Concurrency with other interrelated programs (dependent on either input
        or output).
        12.2.3.8.5. Information Assurance and cyber security considerations.
     12.2.3.9. Risk Identification Methods. One or more top-level and one or more lower-
     level risk identification approaches should be used in performing a comprehensive risk
     identification. Using only top or lower level approaches increase the chance that potential
     risks are not identified because the evaluation is often performed in a non-comprehensive
     and/or unstructured manner.
        12.2.3.9.1. Top-Level Risk Identification Approaches. Examples of top-level
        approaches include, but are not limited to using the WBS, key requirements, key
        processes, and risk categories to reveal risk triggers.
        12.2.3.9.2. Lower-Level Risk Identification Approaches. Examples of lower-level
        approaches include, but are not limited to: affinity, brainstorming, cause/effect
        diagrams, checklists, critical and near critical path, expert opinion, failure analysis,
        influence diagrams, lessons learned from analogous programs (contact the local
        Acquisition Center of Excellence (ACE)), Logistics Health Assessments (LHAs),
        metrics (e.g., System Metric and Reporting Tool (SMART)), models (e.g., Systems
        Engineering Assessment Model (SEAM)), trigger questions, and triggers from risk
        scales.
        12.2.3.9.3. Risk Identification: Integration & Ilities (RI3). RI3, an Air Force Smart
        Operations for the 21st Century (AFSO-21) best practice, is an integrated method
        involving both top-level (key processes) and lower-level (trigger questions)
        approaches for identifying technology risks in pre-Milestone A and later activities.
        The methodology (including key processes and trigger questions) are incorporated in
        the Risk Identification: Integration & Ilities (RI3) Guidebook, Dec 2008 available
        from the Acquisition Community Connection of the Defense Acquisition University.
        Note: The “Ilities” and “ility” in the guidebook refer to characteristics of a unit or a
        technical discipline that is typically associated with the support, operation, and
        maintenance of said unit. This can also include items that do not end with “ility,”
        such as integration, training, and human factors. Reference the RI3 Guidebook for
        additional information.
94                                                             AFPAM63-128 10 JULY 2014


        12.2.3.9.4. Other Considerations.
            12.2.3.9.4.1. Additional attention should be given to risks that occur at relatively
            high WBS levels (e.g., WBS level 1 and 2) and those affecting various types of
            integration (e.g., hardware/hardware, hardware/software, software/software,
            component through system level, and systems-of-systems level) because the risk
            might be more complex than when initially evaluated, have interdependencies
            with other risks, and involve ownership sharing.
        12.2.3.9.5. Other assessment activities.
     12.2.3.10. Risk Identification Output. The product of risk identification is a list of risks
     and their corresponding root causes (when known). Where practical, risks should be
     related to the specific tasks in the current IMS and applicable WBS element.
        12.2.3.10.1. All risks, root causes, and the resulting outcomes should be documented
        and tracked in a database as described earlier.
  12.2.4. Risk Analysis—Step 3.
     12.2.4.1. Risk Analysis assesses the degree of likelihood and impact of risks to the
     program or project.
     12.2.4.2. Risk Analysis is the process of refining each risk in terms of its quantifiable
     likelihood and quantifiable consequences, always assuming the risk occurs. Analysis
     determines “What is the magnitude of the risk?” Each risk is evaluated independently. In
     some cases common attributes across risks may lead to a new risk with an equal if not
     higher likelihood and/or consequence rating (e.g., in the case of resource constraints).
        12.2.4.2.1. Likelihood is an estimation of probability that the risk will occur.
        12.2.4.2.2. Consequence of occurrence is an evaluation of the worst credible
        potential impact to performance, schedule, and cost if the risk occurs based on the
        most likely scenario. The maximum value of these three dimensions is selected
        without performing any mathematical operations. .”
        12.2.4.2.3. Plotting the likelihood and consequence assessments on the LCRM 5x5
        matrix (Figure 12.2) provides a relative priority of risks based on potential impact to
        a program or project.
     12.2.4.3. Risk Analysis should be performed continuously as new risks are identified, but
     should also be re-accomplished periodically to assess if the likelihood and/or
     consequence have changed for a previously identified risk “(e.g., as risk
     handling/mitigation plan activities are implemented or other events occur).
     12.2.4.4. Risk Analysis Sequence. Three basic activities are involved in Risk Analysis:
        12.2.4.4.1. Assign a probability or likelihood of occurrence for the risk using Table
        12.1.
        12.2.4.4.2. Assess the consequences for each risk in terms of cost, schedule and
        performance impact using the criteria in Tables 12.2 through 12.4. Note the
        consequence score for each of the three impacts (performance, schedule, and cost),
AFPAM63-128 10 JULY 2014                                                                       95


        and select the most severe (highest) consequence associated with a risk to place on
        the matrix for program reviews.
        12.2.4.4.3. Plot the likelihood and consequence for each risk on the LCRM 5x5
        matrix (Figure 12.2) to depict its potential magnitude and relationship to other risks.
        If a likelihood or consequence cannot be reasonably assessed, then it should not be
        reported as a risk on the LCRM 5x5 matrix. It may be separately reported as a
        “concern” and monitored for change and/or determination of likelihood and
        consequence.
        Note: Risks should not be confused with issues. A risk, whether or not previously
        identified, that has occurred or will occur with certainty is an issue (i.e. problem).
     12.2.4.5. The likelihood and consequence scales given in Tables 12.1 through 12.4 are
     required to be used by AF personnel (AFI 63-101/20-101). Methods of analyzing risk
     include, but are not limited to, the following:
        12.2.4.5.1. Individual or group expert judgment.
        12.2.4.5.2. Analysis of historical data.
        12.2.4.5.3. Uncertainty analysis of cost, schedule, and performance projections.
        Uncertainty is the indefiniteness about the outcome of a situation. Uncertainty is
        assessed in estimate models for the purpose of estimating the risk (likelihood) that a
        specific limitation is exceeded.
        12.2.4.5.4. Probabilistic Risk Assessments associated with performance, schedule,
        and cost to the program. Always assume the risks come to fruition for simulations.
            12.2.4.5.4.1. Performance Risk Assessment (PRA). A PRA is a process that uses
            statistical techniques to quantify the performance impact of the modeled item.
            PRAs are used to evaluate a wide variety of potential complex risks, including but
            not limited to predictions of: dynamic stability of control systems, missile
            accuracy, satellite gap analysis vs. time, and timing closure on application specific
            integrated circuits (ASICs). Each PRA may have a different model structure and
            resulting output, depending upon the engineering discipline.
            12.2.4.5.4.2. Schedule Risk Assessment (SRA). A SRA is a process that uses
            statistical techniques to quantify the schedule impact of technical, programmatic,
            and other risks on specified project or program milestones and other key dates to a
            targeted confidence level. This analysis focuses on critical path, near-critical path,
            and medium and high risk activities, as well as less critical activities, since any
            activity may potentially affect the program’s completion date.
            12.2.4.5.4.3. Cost Risk Assessment (CRA). A CRA is a process that uses
            statistical techniques to capture the cost impacts of technical risks, schedule risks,
            estimating error (statistical error that exists in parametric estimating
            methodologies), and pure cost risk (such as inflation and consumer price
            variables).
        12.2.4.5.5. Fault Tree Analysis and Failure Modes and Effects Analysis.
        12.2.4.5.6. Comparison to similar systems or programs.
 96                                                                   AFPAM63-128 10 JULY 2014


              12.2.4.6. LCRM 5x5 Risk Matrix. The LCRM 5x5 matrix (Figure 12.2) is adopted from
              the Risk Management Guide for DoD Acquisition as the standard for displaying AF
              programmatic risks. Programs are not allowed to modify the matrix scale or color coding
              (e.g., no 5x8 or 4x6 matrix, no color changes to individual blocks of the 5x5 matrix).
                 12.2.4.6.1. The LCRM likelihood criteria and consequence criteria are more specific
                 than criteria in the DoD guide to assist program offices and decision makers with
                 improved consistency in risk assessments and reporting.
                      12.2.4.6.1.1. AFI 63-101 requires PMs to use the cost, performance, and schedule
                      consequence dimensions (Tables 12.2, 12.3, and 12.4), at a minimum. PMs may
                      develop additional consequence criteria, if needed.
                 12.2.4.6.2. A risk is considered “high” when the intersecting point of the likelihood
                 and consequence of a risk on the matrix falls in a red square, “moderate” when falling
                 in a yellow square, and “low” when falling into a green square.
                 12.2.4.6.3. The LCRM 5x5 matrix is in the ASP/Air Force Review Board (AFRB)
                 mandatory templates maintained by SAF/AQ for SAE briefings.
                 12.2.4.6.4. Mission assurance and system safety risks are managed using separate
                 methodologies (for system safety risks apply MIL-STD-882, DoD Standard Practice
                 for System Safety).

Figure 12.2. LCRM Risk Matrix.

                  5


                  4
 Likelihood




                  3


                  2


                  1


                            1       2        3        4        5



                                Consequence
AFPAM63-128 10 JULY 2014                                                                        97



Table 12.1. Likelihood Criteria.

                                                                           PROBABILITY
                                                                               OF
         LEVEL                         LIKELIHOOD                          OCCURRENCE

            5                           Near Certainty                         81%-99 %

            4                           Highly Likely                          61%-80%

            3                               Likely                             41%-60%

            2                          Low Likelihood                          21%-40%

            1                             Not Likely                            5%-20%


Table 12.2. Standard AF Consequence Criteria – Performance.

Level                  Standard AF Consequence Criteria - Performance

        Minimal consequence to technical performance or supportability but no overall
  1     impact to the program success. A successful outcome is not dependent on this issue;
        the technical performance goals or technical design margins will still be met.

        Minor reduction in technical performance or supportability, can be tolerated with
  2     little impact on program success. Technical performance will be below the goal or
        technical design margins will be reduced, but within acceptable limits.

        Moderate shortfall in technical performance or supportability with limited impact
        on program success. Technical performance will be below the goal, but approaching
  3
        unacceptable limits; or, technical design margins are significantly reduced and
        jeopardize achieving the system performance threshold values.

        Significant degradation in technical performance or major shortfall in supportability
        with a moderate impact on program success. Technical performance is unacceptably
  4
        below the goal; or, no technical design margins available and system performance
        will be below threshold values.

  5     Severe degradation in technical performance or supportability; will jeopardize
        program success; or will cause one of the triggers listed below (Note 1)
Note 1: Any root cause that, when evaluated by the cross-functional team, has a likelihood
of generating one of the following consequences is rated at Consequence Level 5 in
Performance:
 98                                                             AFPAM63-128 10 JULY 2014


Will not meet Key Performance Parameter (KPP) Threshold
Critical Technology Element (CTE) will not be at Technology Readiness Level (TRL) 4 at
MS/ A
CTE will not be at TRL 6 at MS/ B
CTE will not be at TRL 7 at MS/ C
CTE will not be at TRL 8 at the Full-rate Production Decision point
Manufacturing Readiness Level (MRL)* will not be at 8 by MS C
MRL* will not be at 9 by Full-rate Production Decision point
System availability threshold will not be met


* MRLs will be calculated in accordance with the DoD Manufacturing Readiness
Assessment Deskbook.
AFPAM63-128 10 JULY 2014                                                                                 99


Table 12.3. Standard AF Consequence Criteria – Schedule.

   Level                         Standard AF Consequence Criteria - Schedule


      1         Negligible program or project schedule slip


                Schedule slip, but:
                Able to meet milestone dates (e.g. A, B, and C) and other key dates (e.g. CDR,
      2         FRP, FOC)
                Does not significantly decrease program total float and
                Does not impact the critical path to program or project completion date
                Schedule slip that requires closely monitoring the schedule due to the following:
                Impacting the ability, but still able to meet milestone dates (e.g. A, B, and C)
      3         and/or other key dates (e.g. CDR, FRP, FOC)
                Significantly decreasing program total float
                Impacting the critical path to program or project completion date
                Schedule slip that requires schedule changes due to the following:*
                Significantly impacting the ability to meet milestone dates (e.g. A, B, and C)
      4         and/or other key dates (e.g. CDR, FRP, FOC)
                Significantly impacting the ability to meet the program or project completion
                date
                Schedule slip that requires a major schedule re-baselining due to the following:*
      5         Failing to meet milestone dates (e.g. A, B, and C) and/or other key dates (e.g.
                CDR, FRP, FOC)
                Failing to meet the program or project completion date
* Exhibit awareness to exceeding Nunn-McCurdy threshold breach for schedule.
Note: Impact varies based on 1) The schedule slip relative to the remaining duration in the program or
major milestones; amount of remaining time to work-around the impact; 2) The impact of the slip with
respect to key resources.
 100                                                       AFPAM63-128 10 JULY 2014


Table 12.4. Standard AF Consequence Criteria – Cost.
                           Standard AF Consequence Criteria – Cost
  Level
                             (A-B Refers to Milestone designation)

           For A-B Programs: <1% increase from MS A or last approved Development or
       1   Production cost estimate.
           For Post-B and Other Programs: <1% increase from MS A or last approved
           Development or Production cost estimate.

           For A-B Programs: 1% to <3% increase from MS A or last approved
       2   Development or Production cost estimate.
           For Post-B and Other Programs: 1% to <3% increase from MS A or last
           approved Development or Production cost estimate.

           For A-B Programs: 3% to <5% increase from MS A or last approved
           Development or Production cost estimate.
       3   For Post-B and Other Programs: 3% to <5% increase in Development or >1.5%
           increase to Program Acquisition Unit Cost (PAUC) or Average Unit
           Procurement Cost (APUC) from last approved baseline estimate or >3%
           increase to PAUC or APUC from original baseline. (1/10 of Nunn-McCurdy
           ‘significant’ breach).

           For A-B Programs: 5% to <10% increase from MS A or last approved
           Development or Production cost estimate.
       4
           For Post-B and Other Programs: 5% to <10% increase in Development or >3%
           increase to PAUC or APUC from last approved baseline estimate or >6%
           increase to PAUC or APUC from original baseline. (1/5 of Nunn-McCurdy
           ‘significant’ breach).

           For A-B Programs: >10% increase from MS A or last approved Development or
       5   Production cost estimate.
           For Post-B and Other Programs: >10% increase in Development or >5%
           increase to PAUC or APUC from last approved baseline estimate or >10%
           increase to PAUC or APUC from original baseline. (1/3 of Nunn-McCurdy
           ‘significant’ breach).
AFPAM63-128 10 JULY 2014                                                                    101


Figure 12.3. Translation of MIL-STD-882 Risk Matrix to the OSD Risk Management
Guide Matrix.




      12.2.4.7. Risk Interdependencies. During Risk Identification and Risk Analysis, it is
      important that PMs and IPTs address and communicate risk interdependencies and the
      ability of a given risk to affect other risks and potentially create new risks. Risks and
      handling activities may affect other risks within the same program or project, or affect
      other (external) programs (in the same AF portfolio, in another AF portfolio, or in a
      portfolio of another Service). Identifying, documenting, and communicating risks across
      these relationships is especially critical when associated with a Systems of Systems or a
      Family of Systems. PMs and IPTs should also find that prime contractor involvement is
      key to this area of communication success.
      12.2.4.8. Risk Analysis Output. The following are typical outputs expected from a risk
      analysis: risk ID, LCRM category, and root cause (when known), risk interdependency
      identification, risk level (from LCRM 5x5 matrix reflecting the likelihood and maximum
      of the three consequences for each risk), risk historical information, sub process results
      (e.g., PRA, SRA and CRA with desired percentile confidence level), aggregated Risk
      Report
102                                                             AFPAM63-128 10 JULY 2014


         12.2.4.8.1. All moderate and high rated risks are presented on the matrix as a part of
         program, technical, and Milestone decisions. In addition, low risks that have a
         compounding effect equal to a single moderate or high risk are presented on the
         matrix.
         12.2.4.8.2. Briefing charts should follow the ASP/AFRB template provided by either
         the SAF/AQ Oversight Secretariat for ACAT I/IA or non-delegated ACAT II
         programs or the Center level oversight POC for ACAT II and III programs.
         12.2.4.8.3. Analysis results are documented and tracked in a risk management
         database as described earlier.
  12.2.5. Risk Handling/Mitigation Planning and Implementation—Step 4.
      12.2.5.1. Risk handling planning and implementation is the process that identifies,
      evaluates, selects options then develops and implements approaches to reduce risk to an
      acceptable level given program constraints and objectives. This includes the specifics on
      what should be done, when it should be accomplished, who is responsible, associated
      available resources, etc. After risks are identified and analyzed, the next step is
      determining and documenting an appropriate action for each risk. Note: Risk handling is
      the preferred and more encompassing term to recognize that there are potentially multiple
      options to manage risks than simply mitigating the risk (mitigation or control option).
      Options for addressing risks include:
         12.2.5.1.1. Accept. Risk acceptance is a conscious decision to accept the associated
         level of a risk, without monitoring, transferring, mitigating, or avoiding the risk.
         12.2.5.1.2. Monitor. Take no immediate action but watch for changes. Recognize
         what is monitored (i.e., Technical Performance Measures (TPMs) and production
         rates) and the threshold or trigger event that initiates additional handling actions.
         12.2.5.1.3. Transfer. Shift the responsibility elsewhere. Risk transfer may reallocate
         risk from one part of the system or interface to another, thereby reducing the overall
         system risk, or redistributing risks between the Government and the prime contractor
         or within Government organizations, or among members of the contractor team, or
         from one IPT to another. In some cases, the transfer option is a form of risk sharing
         and not risk cancellation on the part of the Government. The transfer option can result
         in the reduction of likelihood and/or consequence of occurrence.
         12.2.5.1.4. Mitigate (Control). Apply resources aimed at reducing the risk to an
         acceptable level by reducing the likelihood and/or consequence of the risk.
         12.2.5.1.5. Avoid. Avoidance eliminates the sources of high and/or moderate risks
         and replaces them with a lower risk solution to reduce the likelihood and/or
         consequence of the risk. Risk avoidance involves a change in the concept, design,
         requirements, specifications, and/or practices that can remove the potential root cause
         and/or reduce the risk to an acceptable level.
      12.2.5.2. A risk handling strategy, composed of a risk handling option and an
      implementation approach, is developed and implemented for all moderate and high risks
      and selected low risks. All risk handling options (accept, monitor, transfer, mitigate, and
      avoid) are evaluated with regard to performance, schedule, cost and risk tradeoffs
AFPAM63-128 10 JULY 2014                                                                     103


     performed, and the “best” option selected for each risk. Given the option chosen, an
     implementation approach is then selected for each risk, again based upon evaluating
     performance, schedule, cost, and risk tradeoffs. It is also possible that a risk handling
     strategy may include a combination (or hybrid) of some or all risk handling options, and
     is not limited to a single option.
        12.2.5.2.1. Multiple risk handling strategies can be developed on an as-needed basis
        and performed in parallel for the same risk, or be implemented contingent on
        intermediate progress associated with the primary risk handling strategy.
     12.2.5.3. Decisions to proceed (e.g., Milestone Decision Authority, Acquisition Strategy
     Panels, etc.) constitute approval of the current risk assessments and handling plans for all
     moderate and high risks. Therefore, it is imperative that the Milestone Decision Authority
     be aware of all moderate and high risks, their respective handling/mitigation plans, and
     the current implementation status of each plan.
        12.2.5.3.1. For programs or projects with a high number of moderate risks in addition
        to high risks that cannot be briefed due to time constraints, the Decision Authority
        may elect to rely on staff independent of the program/project office for
        recommendations regarding approval of Risk Handling Plans for moderate risks. As
        previously stated, though, all moderate and high risks must be presented to decision
        makers on the LCRM 5x5 matrix to ensure their awareness of these programmatic
        risks during program, technical, Milestone reviews.
     12.2.5.4. Risk Handling/Mitigation Plan Content. The Risk Management Guide for DoD
     Acquisition provides the recommended content for risk handling/mitigation plans. In
     general, risk handling/mitigation plans describe actions to eliminate or reduce the
     identified risks, as well as risk measures, indicators, metrics, and trigger levels used to
     track the risks and the effectiveness of their handling actions. These plans also include
     the cost and schedule information required for implementation.
     12.2.5.5. Risk Handling/Mitigation Planning Output. The outcome of this step is
     documentation of decisions. and a risk handling/mitigation plan to
     accept/monitor/transfer/mitigate/ avoid all moderate or high rated risk impact (and
     selected low rated risks) to reduce the risk to an acceptable level.
        12.2.5.5.1. Acceptances and planning actions must be documented in a database as
        described earlier.
     12.2.5.6. Risk Handling Implementation. The final risk handling step is to allocate the
     resources needed to implement each developed and approved risk handling strategy via
     its risk handling/mitigation plan and implement the plan. It is essential that the risk
     handling/mitigation plan be resourced and implemented, or else risks may not be reduced
     to an acceptable level.
        12.2.5.6.1. When a risk handling plan is implemented, a successor risk handling
        activity should not be started until its preceding activity has been successfully
        completed. Working activities out of sequence may lead to missteps, possible rework,
        and decisions which can lead to an inefficient application of resources.
  12.2.6. Risk Tracking—Step 5.
104                                                              AFPAM63-128 10 JULY 2014


      12.2.6.1. Risk tracking is the process that systematically evaluates and tracks the
      performance of risk handling actions against established metrics. Risk tracking is a
      proactive technique to observe the results of risk handling and provide feedback to the
      prior risk management process steps.
      12.2.6.2. Risk tracking provides information to draw conclusions about the effectiveness
      of the risk handling/mitigation plan and its actions.
      12.2.6.3. Risk tracking involves collecting, updating, organizing, and analyzing risk data
      and reporting risk trends. By monitoring risk handling/mitigation plan implementation at
      specific intervals, the resulting information can be fed back to update risk
      handling/mitigation plans as necessary, re-analyze existing risks (to determine if
      particular risks have decreased, remained the same, or increased over time), identify new
      facets of an existing risk (or new risks), and update risk planning considerations, such as
      risk categories, and ground rules and assumptions. Historical risk data should be saved in
      the risk management database to permit evaluating how risks change over time.
         12.2.6.3.1. Metrics included in the risk handling/mitigation plan are evaluated for
         each risk at the same point in time. Ideally, this includes metrics for performance,
         schedule, cost, and risk tradeoffs to provide the program or task manager with a
         multi-dimensional view of how the risk has changed with time.
         12.2.6.3.2. Risk and metrics information collected can serve as a trigger to initiate an
         additional risk handling/mitigation plan if the original implemented risk
         handling/mitigation plan proves to be inadequate.
         12.2.6.3.3. If reported trends collectively indicate a different strategy for risk
         management is appropriate, changes to the overall Risk Management Plan may be
         necessary.
      12.2.6.4. The frequency to evaluate tracking results and triggers should be such that
      adequate time remains to react to adverse trends.
      12.2.6.5. Following risk handling implementation, decisions to accept, avoid, monitor, or
      transfer, rather than further mitigate, any moderate or high rated risk impacts are included
      in approval documentation (e.g., Acquisition Decision Memorandum).
         12.2.6.5.1. Acceptance of system safety risks must comply with DoDI 5000.02 that
         dictates specific levels of approval regardless of the ACAT level.
         12.2.6.5.2. Review of processes applicable to Joint weapon system development
         must comply with the requirements of DoDI 5000.69, DoD Joint Services Weapon
         and Laser System Safety Review Processes.
      12.2.6.6. Updated information from risk tracking can significantly impact programmatic
      decisions and plans, and must be communicated within the program/project team and to
      decision makers. This should be accomplished via existing channels (e.g. reviews,
      decision points, etc.) without creating a separate risk reporting process for staff oversight.
      This should be outlined in the Risk Management Plan, using the existing risk
      management reporting process.
      12.2.6.7. Risk information is documented and tracked for each risk in a database that
      includes the information below. In addition, the same risk information should be recorded
AFPAM63-128 10 JULY 2014                                                                      105


      each time a risk parameter changes to maintain historical data. Risk activities that require
      additional resources, should be considered for inclusion on the program’s master
      schedule.
          12.2.6.7.1. Risk ID (reference AQXC web portal for Risk ID nomenclature guidance)
          12.2.6.7.2. Risk Description
          12.2.6.7.3. Risk Origination Date
          12.2.6.7.4. Risk Owner
          12.2.6.7.5. WBS (if applicable)
          12.2.6.7.6. IMS Activity ID (if applicable)
          12.2.6.7.7. Risk Rating (Level: high, moderate, and low)
          12.2.6.7.8. Likelihood of the Risk Occurring (levels 1-5)
          12.2.6.7.9. Performance, Schedule, and Cost Consequences of the Risk Occurring
          (levels 1-5)
          12.2.6.7.10. Risk Handling Plan Option ID (if applicable)
          12.2.6.7.11. Burndown or waterfall chart (optional) depicting projected and actual
          risk burndown associated with risk handling activities over time
12.3. LCRM across the Life Cycle
   12.3.1. Risk management is the proactive form of acquisition life cycle management. To
   remain proactive, risk management must be performed continuously across the acquisition
   framework. It must be a core responsibility of every team member performed on a daily
   basis. A program team that only assesses risks or determines status of mitigation actions just
   prior to milestone reviews, or only to meet periodic reporting requirements will likely
   discover that these risks have migrated into issues (i.e., current problems); driving the
   program team into a reactive mode with a subsequent loss of control.
   12.3.2. As previously stated, the LCRM process is continuous and iterative. This continuous
   process feeds into all key aspects of acquisition management from the formal entry point into
   the acquisition process, the Materiel Development Decision (MDD) that begins the Materiel
   Solution Analysis (MSA) phase, through the Operations & Support phase into final
   disposition. Conducted properly, risk management drives down risks and uncertainty as a
   team progresses through the acquisition framework to a point of mitigation and/or acceptance
   of risks.
   12.3.3. Viewing the “big picture,” operational risks generate capability requirements that
   often lead to materiel solutions to satisfy these requirements. During the MSA phase, cost,
   schedule and performance risks associated with each potential materiel solution must be
   identified and assessed to determine the best strategy for the Technology Maturation & Risk
   Reduction (TM&RR) phase. The primary purpose of the (TM&RR) phase is to reduce
   technology risk (i.e. mitigate the risks identified during the MSA) and to assess
   manufacturing and integration risks before proceeding into EMD. Mitigation actions for
   these manufacturing and integration risks are then initiated during EMD and completed
   (verified) in Low-Rate Initial Production (LRIP) and Initial Operational Test & Evaluation
106                                                           AFPAM63-128 10 JULY 2014


  (IOT&E). Upon successful completion, a decision to proceed into Full-Rate Production can
  be made with considerably lower risk remaining for Production and Deployment as well as
  for Operations and Support. Even from the initial assessment of potential materiel solutions
  during the MSA phase, it is crucial that the team look as far forward as possible to identify
  risks across the life cycle out through sustainment and eventual disposal activities.
AFPAM63-128 10 JULY 2014                                                                       107


                                          Chapter 13

   ACQUISITION PROGRAM BASELINE (APB) PREPARATION AND GUIDANCE

13.1. Introduction. The purpose of this chapter is to provide guidance to Program Managers in
developing their Acquisition Program Baseline (APB) for all AF acquisition programs. To
comply with 10 USC §2435, 10 USC §2220, and DoD Instruction (DoDI) 5000.02, every
program manager documents program goals prior to program initiation. The APB satisfies this
requirement. The APB serves as the contract between the program manager and the Milestone
Decision Authority (MDA) documenting the objective and threshold value for key performance,
schedule and cost parameters as the program is expected to be developed, produced, and/or
deployed. The APB also forms the basis for complying with some reporting requirements such
as the Selected Acquisition Report (SAR), Defense Acquisition Executive Summary (DAES),
Major Automated Information System (MAIS) Quarterly Report, MAIS Annual Report , and
Monthly Acquisition Report (MAR).
13.2. Goals. Every acquisition program establishes program goals (via thresholds and
objectives) for cost, schedule, and performance parameters that describe the program over its life
cycle. The difference between the threshold and objective represents the potential trade space –
the area in which the PM has to work in to achieve successful closeout of the parameter.
Anything beyond the trade space must be approved by the MDA.
13.3. Parameters - Thresholds and Objectives. Program goals consist of the performance,
schedule and cost parameters, each with an objective value and a threshold value. The default
value for objectives in AF requirements documents is the threshold value (i.e., T = O).
   13.3.1. Thresholds. Threshold values represent the acceptable limits to the parameter values
   that, in the user's judgment and consistent with should cost/will cost objectives, still provide
   the needed capability. For performance, a threshold represents either a minimum or
   maximum acceptable value, however for schedule and cost parameters, thresholds would
   normally represent maximum allowable values. The failure to attain program thresholds may
   degrade system performance, delay the program (possibly impacting related programs or
   systems), or make the program too costly. The failure to attain program thresholds, therefore,
   places the overall affordability of the program and/or the capability provided by the system
   into question.
   13.3.2. Objectives. The objective value represents an incremental, operationally meaningful,
   time-critical, and cost-effective improvement to the threshold value. The objective value is an
   operationally significant increment above the threshold that represents a desired operational
   goal associated with a performance attribute beyond which any gain in utility does not
   warrant additional expenditure.       An objective value should be developed only when
   absolutely necessary and is the same as the threshold when an operationally significant
   increment above the threshold is not significant or useful.
   13.3.3. Missing Values. The above guidelines notwithstanding, if no threshold is specified,
   the PM may propose an appropriate threshold value that targets affordability and control of
   cost growth, subject to MDA and user approval. In those situations where an objective value
   is required, the objective value should be analytically justified in terms of operational risk
   and impacts to program cost and schedule. For each parameter, if no objective is specified,
 108                                                            AFPAM63-128 10 JULY 2014


   the threshold value should also serve as the objective value. There may also be occasions
   where “To Be Determined (TBD)” may apply for both Objective and Threshold values.
   They typically apply to Schedule parameters when the parameter is contingent upon some
   other event or decision. These cases are very uncommon and the TBD values require MDA
   authorization. One exception to this is the MAIS schedule Full Deployment (FD) parameter.
   Per statute, this is not defined until Full Deployment Decision (FDD). The dates are defined
   in the FDD ADM.
   13.3.4. Trade Space. Maximizing PM and contractor flexibility to make cost/schedule/
   performance trade-offs is essential to achieving program objectives. Trade-offs—within the
   objective-to-threshold “trade space”—should not require higher-level permission, but should
   require coordination with the lead operating command. The lead operating command should
   strictly limit the number of threshold and objective items in requirements documents and
   acquisition program baselines (APBs). Performance threshold values should represent true
   minimums, with requirements stated in terms of capabilities rather than as technical solutions
   and specifications. Cost threshold values should represent true maximums. Cost objectives
   should be used as a management tool.
   13.3.5. Subprograms/Increments. The capability requirement document should specify the
   key performance parameters (KPPs) per subprogram or increment. When a MDAP program
   uses subprogram(s), each subprogram should have a set of parameters with objective and
   threshold values specific to the subprogram. A MAIS Increment is an acquisition program,
   thus has its own requirements, documentation (including APB), and decision points.
13.4. Acquisition Program Baseline (APB) Content/Structure. Every acquisition program
needs to prepare an APB prior to program initiation – typically Milestone B. The baseline
should include sufficient parameters to describe the cost estimate (referred to as the “Baseline
Estimate” in 10 USC 2433), schedule, performance, supportability and any other significant
factor of a major defense acquisition program, major automated information system, or
designated major subprogram. The PM derives the APB from the users' performance
requirements, schedule requirements, and best estimate of total program cost, which are
referenced in the Acquisition Decision Memorandum (ADM).
   13.4.1. APB Content. Acquisition Program Baseline parameter values should represent the
   program as it is expected to be developed, produced and/or deployed, and funded. The
   baseline should only contain those parameters that, if thresholds are not met, will require the
   Milestone Decision Authority to re-evaluate the program and consider alternative program
   concepts or design approaches. The number of performance parameters should be limited to
   provide maximum trade space.
   13.4.2. APB Types.
       13.4.2.1. MDAP APBs. In the case of MDAP programs, there are two baselines per
       program – the Original Baseline and the Current Baseline. Each subprogram also has
       these two baselines.
           13.4.2.1.1. The Original Baseline is established at program inception, typically
           Milestone B. Program inception can also be when a post- MS B ACAT II or III
           program increases in scope (cost and funding) sufficient to cause it to be declared an
AFPAM63-128 10 JULY 2014                                                                       109


         ACAT I. The Original Baseline can be changed only following a critical Nunn-
         McCurdy Unit Cost Breach certification, resulting in a revised Original Baseline.
            13.4.2.1.1.1. The statutory and regulatory requirements for each milestone,
            including MS B, are contained in the Tables in DoDI 5000.02. Additional non-
            mandatory guidance on best practices, lessons learned, and expectations is
            available in the Defense Acquisition Guidebook.
            13.4.2.1.1.2. Requirements for MS B include completion of Preliminary Design
            Review (PDR), Independent Cost Estimate, recommended Service Cost Position
            (or Component Cost Estimate as referenced in DODI 5000.02), an approved
            requirements document, and technology that has been demonstrated in a relevant
            environment. Findings from the PDR should be considered when developing the
            Original Baseline.
         13.4.2.1.2. The Current Baseline is initially established at program inception and
         matches the Original Baseline at that time. Circumstances authorizing changes are
         limited and revision to the current baseline is not automatically authorized if there is a
         change to cost, schedule, or performance parameters. DoDI 5000.02 lays out the
         limited circumstances as a basis to authorize a revision to the Current Baseline. They
         may be considered if changes are the result of:
            13.4.2.1.2.1. A major program restructure that is fully funded and approved by
            the MDA; or,
            13.4.2.1.2.2. Program deviations (breaches), if the breach is primarily the result
            of external causes beyond the control of the program manager. A good example
            of this is a breach resulting from content being added to the program (i.e.
            additional capability or units).
            13.4.2.1.2.3. A Milestone or full rate production decision.           Typically, this
            involves an update to implement the MS C decision.
            13.4.2.1.2.4. Multiple revisions to the Current APB should not be authorized, and
            at no time should a revision to the Current APB be authorized if it is proposed
            merely to avoid a reportable breach.
     13.4.2.2. All Other Programs. There is only the single baseline for all other programs
     (ACAT IAM, ACAT IAC, ACAT II, and ACAT III programs). This equates to the
     Current Baseline referenced for MDAP programs, but the “Current Baseline”
     nomenclature is not used.
     13.4.2.3. Technology Projects. Technology Projects are defined in DoDI 5000.2. The
     MDA makes the determination whether a Technology Project requires a baseline, and if
     so, what parameters should be included. The APB module in SMART may be used for a
     Technology Project baseline. The MDA should consider cost, schedule and scope (in
     lieu of performance because there should not be KPPs) goals for such an effort in
     establishing a project baseline.
     13.4.2.4. The PM should retain all approved baselines for historical purposes.
  13.4.3. Parameters.
110                                                           AFPAM63-128 10 JULY 2014


      13.4.3.1. Performance.
         13.4.3.1.1. The sponsor of a capability requirements document provides a threshold
         and an objective value for each Key Performance Parameter (KPP) that describes an
         aspect of a system or capability to be developed or acquired. The total number of
         performance parameters should be the minimum number needed to characterize the
         major drivers of operational performance, supportability, and interoperability (10
         USC §2435). KPPs are those performance parameters whose thresholds, if not met,
         would require an evaluation by the MDA to consider alternative acquisition
         approaches, or possible program termination. These KPPs must be verifiable by
         means of developmental and operational testing.
             13.4.3.1.1.1. An AF Form 1067, Modification Proposal, may also be used as the
             basis for the APB performance section for improvement modifications using
             investment appropriations.       As described in AFI 63-131, Modification
             Management, the AF Form 1067 may be used to initiate a modification proposal
             to fielded systems and equipment as described in AFI 10-601, Operational
             Capability Requirements Development.           It captures a description of the
             modification requirement and the technical aspects of the materiel solution that
             satisfies the requirement. This is limited to ACAT III size efforts (less than 10%
             of ACAT II dollar threshold). The AF Form 1067 is used for modifications for
             both sustainment modifications and new capability. Refer to AFI 63-131 for
             additional guidance.
         13.4.3.1.2. All validated Key Performance Parameters should be inserted verbatim
         into the performance section of the APB. For performance parameters, “threshold”
         will mean the minimum or maximum acceptable value that, in the user's judgment, is
         necessary to satisfy the need. If performance threshold values are not achieved,
         program performance may be seriously degraded, and the utility of the system may
         become questionable.
         13.4.3.1.3. The PM may propose additional parameters (usually derived parameters)
         to be tracked using the APB, subject to approval by the MDA. Additionally, the
         MDA may add additional performance parameters to track other than the validated
         KPPs in order to ensure adequate oversight. It should be make clear that additional
         parameters are not KPPs.
         13.4.3.1.4. The number and specifics of performance parameters may change over
         time. Early in a program, the APB should reflect broadly defined, operational-level
         measures of effectiveness or measures of effectiveness or measures of performance to
         describe needed capabilities. As a program matures, system-level requirements
         become better defined. This maturity is reflected in updated requirements documents
         and Milestone APBs.
         13.4.3.1.5. For Evolutionary Acquisitions, the capability requirements document
         should break out the KPPs by increment.
             13.4.3.1.5.1. Document which increment(s) will satisfy which KPPs. This is
             required in order to build the cost estimate and the test requirements for each
             block/increment.
AFPAM63-128 10 JULY 2014                                                                    111


            13.4.3.1.5.2. Specify the minimum performance parameters, including those that
            define the core capability and target which increment will provide the core
            capability.
            13.4.3.1.5.3. For MAIS increments, the KPPs included in the performance
            section of the APB will be only those associated with the increment being
            baselined.
     13.4.3.2. Schedule.
        13.4.3.2.1. Schedule parameters should minimally include dates for program
        initiation, major program decision points, and key events. If schedule threshold
        values are not achieved, the program timing may no longer meet the user’s needs, and
        the utility of the system may become questionable. Ensure schedule parameters are
        clearly within the scope of authority and responsibility of the PM. Events outside PM
        control should be avoided as schedule milestones, or defined in a way consistent with
        PM authority and responsibility.
        13.4.3.2.2. The program summaries in the capability requirements document
        describe the overall program strategy for reaching full capability, and the timing of
        the delivery of each increment. In general, the schedule parameters usually include
        the following key events:
            13.4.3.2.2.1. Program initiation (MS B).
            13.4.3.2.2.2. Design Reviews (SRR, PDR, CDR, etc.).
            13.4.3.2.2.3. Major Milestones (MS C, FRP, First Article Delivery, etc.).
            13.4.3.2.2.4. Key test milestones (DT&E Start and/or Complete, IOT&E Start
            and/or Complete). It may be duplicative to include both test start and complete
            especially if the completion of a test is a pre-requisite for a follow-on key event.
            In these cases, including just the key event may be sufficient. An example is
            DT&E Completion may not be needed if MS C is included because completion of
            DT&E is a pre-requisite for a MS C. Any slip in DT&E will likely impact the MS
            C date. In this case, just listing DT&E Start should be sufficient.
            13.4.3.2.2.5. IOC equivalent (i.e., Required Assets Available (RAA)). For
            MDAPs, OSD tracks IOC as part of a defense acquisition cycle time span metric
            for overall acquisition program reporting to the Congress. The PM should
            consider using equivalent milestones, containing things within the scope of their
            authority and responsibility. As an example, Required Assets Available (RAA)
            has been used as an equivalent to IOC (and is acceptable for the OSD metric).
            This is defined as what the PM must deliver to support IOC (i.e. 18 aircraft plus
            trainer, tech pubs and initial spares). If IOC is required, there should be a full
            description of the exit criteria for this milestone included as a footnote in the
            Schedule section of the APB. It should be written so it can be understood and
            implemented by persons not associated with program inception. This should also
            be documented in the Program Management Agreement for ACAT I and II
            programs. Note: The operator, or customer, retains the authority to declare IOC
            and FOC. IOC and FOC include things outside the authority and responsibility of
112                                                        AFPAM63-128 10 JULY 2014


         the PM such as training and readiness of the operators.
         13.4.3.2.2.6. FOC is not a required milestone for APBs. It is also a milestone
         outside the scope of a PM’s authority and responsibility. If it should be required,
         follow the guidelines laid out for inclusion of IOC.
         13.4.3.2.2.7. In all cases, the PM may propose, for MDA approval, other specific,
         critical, system events, as necessary.
      13.4.3.2.3. MDAP programs. The following are mandatory milestones for MDAP
      programs: MS A (if applicable), B, C, Full Rate Production, and IOC (or IOC
      Equivalent). Consider milestone(s) to cover at least one test event (either start or
      completion), and design events (PDR and CDR). It is highly advisable to not use a
      milestone for an event the PM does not retain both authority and responsibility. For
      instance, one might use “Available for Launch” instead of “First Launch” for a
      satellite.
      13.4.3.2.4. MAIS programs. The following are mandatory milestones and decision
      points for MAIS programs:
         13.4.3.2.4.1. Commencement of Five Year Period to achieve Full Deployment
         Decision (previously known as “Funds First Obligated” or “FFO” date). Statute
         provides that the commencement of the five year period occurs at Milestone A or,
         if there was no Milestone A, the date when the preferred alternative is selected.
         The period of time during which program activity is delayed as a result of a bid
         protest is excluded from the calculation of the five year period.
         13.4.3.2.4.2. Materiel Development Decision (MDD).
         13.4.3.2.4.3. MS A, B, and C. At their discretion the MDA may tailor out MS A,
         MS B, or MS C, with an appropriate, concise footnote explaining the tailoring.
         13.4.3.2.4.4. Full Deployment Decision (ref FY10 NDAA Sec 841)
         13.4.3.2.4.5. Full Deployment (FD). 10 USC Ch 144A provides that FD is the
         fielding of an increment in accordance with the terms of the Full Deployment
         Decision (FDD). Consequently, FD is not defined until the FDD, and the dates
         for FD are “TBD” until defined in the FDD ADM.
         13.4.3.2.4.6. The FY10 NDAA Sec 841 rescinded the requirement to use IOC
         and FOC as mandatory schedule milestones. However, it did add the requirement
         for programs to use Full Deployment Decision (FDD) and Full Deployment (FD)
         as mandatory schedule milestones. These milestone decisions are made by
         someone within the program’s acquisition chain.
      13.4.3.2.5. ACAT II and III Programs: ACAT II and III weapon system programs
      should consider milestones that meet the intent of the MAIS requirements and
      reference AFI 63-101 when identifying decision points and milestones.
      13.4.3.2.6. Objective Dates: The objective value corresponds to the approved
      program schedule, executable within the resources identified in the Cost section
      (Objective value) and approved by the MDA.
AFPAM63-128 10 JULY 2014                                                                   113


        13.4.3.2.7. Threshold Dates: The APB default threshold value is the Objective date
        plus 6 months for Acquisition Category (ACAT) I programs. ACAT II and III
        programs should use the 6 month span. The span for any program can be increased or
        decreased based on the risk associated with the individual milestone. The PM may
        propose, with justification, an appropriate threshold date different from the default
        date to optimize program trade space or commensurate with program risk, subject to
        MDA and user approval.
        13.4.3.2.8. To Be Determined (TBD) in lieu of specific Dates for objective and/or
        threshold values may be appropriate in limited circumstances.
            13.4.3.2.8.1. If the dates cannot be determined due to circumstances beyond
            control of the PM – for instance GFE/I or dependency on another system for
            which availability cannot be determined.
            13.4.3.2.8.2. For MAIS programs, the Full Deployment (FD) milestone dates
            should be “TBD” until after the FDD milestone is met. Criteria to meet FD are
            established in the ADM documenting FDD.
     13.4.3.3. Cost. The APB incorporates the MDA-approved cost estimate, as documented
     in the ADM, in the Objective Cost values. It should not include costs that are not part of
     the program approved by the MDA. A Program Element (PE) does not define an
     acquisition program; program cost may include funding from more than one PE (i.e.,
     multiple PEs make up funding for the program), or a portion of a PE (i.e. one PE may
     support multiple programs). Additionally, the PE may contain funding extending beyond
     the completion date of the program. This typically is for support in the O&S phase of the
     system. As the program progresses, the PM typically updates the APB based on ADM
     direction at Milestone decision(s). If the budget is not sufficient to support the cost
     requirements, the PM proposes a plan to the MDA on how funding shortfalls will be
     resolved. The MDA may recommend requirements tradeoffs or approve the plan as is.
        13.4.3.3.1. Cost Estimate: The cost of the program should reflect realistic estimates
        of the total program, including a thorough assessment of cost uncertainty and risk.
        The PM is responsible for developing a reasonable cost estimate for their program. To
        successfully estimate the costs, PMs and their staffs should work closely with the
        center Financial Management Cost Staff and the Air Force Cost Analysis Agency
        (AFCAA). In many cases, the cost estimate is one of the pacing items in preparation
        for a milestone decision, on average taking 7 months for a MDAP Independent Cost
        Estimate (ICE) by Office of the Secretary of Defense Cost Assessment and Program
        Evaluation (OSD (CAPE)) or AFCAA. (See AFPD 65-5, Cost and Economics, and
        AFI, 65-508, Cost Analysis Guidance and Procedures, for additional direction and
        guidance.)
            13.4.3.3.1.1. For MDAP and MAIS programs, the Deputy Assistant Secretary
            (DAS) of the Air Force for Cost and Economics (SAF/FMC) approves and
            recommends a Service Cost Position (SCP) for consideration by the Service
            Acquisition Executive (SAE). The SAE typically accepts the recommended SCP,
            but may designate an alternative position as the official AF SCP. The SCP
            represents the AF's official cost estimate. Typically, the recommended SCP is
            developed through the Air Force Cost Assessment and Program Evaluation
114                                                       AFPAM63-128 10 JULY 2014


         process. This process is a product of the combined efforts of all stakeholder cost
         organizations designed to be collaborative and include a broad range of functional
         input. Recommended SCPs should be established for all Milestone (A, B, C and
         FRP or FDD review) decisions for ACAT IC, ID, IAM, and IAC programs (as
         well as pre-MDAPs expected to be designated ACAT Is). Recommended SCPs
         should also be established or updated whenever a program Acquisition Program
         Baseline (APB) is established or updated. SAF/FMC must provide OSD (CAPE a
         memorandum documenting the recommended SCP. SAF/FM and SAF/AQ must
         provide a memorandum, with AF/A8 concurrence, certifying the program is fully
         funded to the SCP. This full-funding memorandum includes the SAF/FMC
         recommended SCP memorandum as an attachment. If the SAE chooses a cost
         position different from the SAF/FMC recommended SCP, this memorandum must
         also document the alternative SCP.
         13.4.3.3.1.2. For “non-select” ACAT II and III programs, the MDA will make the
         determination as to what type of estimate or level of review of the POE is to be
         done. Typically, the Product Center FMC will conduct a review of an ACAT III
         program POE. For an ACAT II program, they may do their own independent
         estimate.
         13.4.3.3.1.3. The OSD CAPE has the authority to review any cost estimate based
         on statutory language. An issue at this point is whether the OSD CAPE has to
         review and concur with all AF MDAP and MAIS estimates prior to MS A and
         MS B decisions (including ACAT IC and ACAT IAC programs for which the
         SAE is the MDA and MAIS). The OSD CAPE conducts Independent Cost
         Estimates or Cost Analysis for ACAT ID, ACAT IC, ACAT IAC and ACAT
         IAM programs. In most cases, AFCAA is delegated the responsibility to conduct
         the Independent Cost Estimate for ACAT IC, ACAT IAM, and ACAT IAC
         programs. Two statutes provide direction regarding when ICEs are required. 10
         USC §2434 requires an ICE for ACAT I programs prior to initiating development
         (Milestone (MS) B) and production (MS C). Statute requires MDAP and MAIS
         ICEs be accomplished by OSD (CAPE) for all programs where USD (AT&L) is
         the milestone decision authority in advance of (1) 10 USC. §2366a and §2366b
         required certifications (MDAP MS A and MS B decision points), (2) Low Rate
         Initial Production (LRIP) and FRP decisions (MS C), and (3) other certifications
         and reports required under 10 USC §2433a (i.e., MDAP unit cost breaches, also
         known as Nunn-McCurdy breaches) and 10 USC §2445c(f) (i.e.,
         Significant/Critical Changes for MAIS programs).
      13.4.3.3.2. MDAP Cost Baselines. There are two cost baselines associated with all
      MDAP APBs (they are described in 10 USC §2435, Baseline Description). They
      correlate to the Original Baseline and the Current Baseline, and both estimates are
      assessed against the Nunn-McCurdy Unit Cost Breaches criteria.
         13.4.3.3.2.1. Original Baseline Estimate is the cost baseline established with the
         initial APB (Original Baseline) at program inception, typically Milestone B. It
         documents the program cost as it was originally approved. There are only Nunn-
         McCurdy breaches against the Original Baseline Estimate. This cost baseline
         only changes following a Nunn-McCurdy Critical Breach certification (10 USC
AFPAM63-128 10 JULY 2014                                                                 115


           §2435); it is then referred to as a Revised Original Baseline Estimate.
           13.4.3.3.2.2. Current Baseline Estimate is the cost estimate component of the
           Current Baseline. The Original Baseline Estimate should initially match the
           Current Baseline Estimate until a Current APB update is proposed and approved.
           A Current Baseline update results in an updated Current Baseline Estimate. APB
           and Nunn-McCurdy breaches are against the Current Baseline Estimate.
        13.4.3.3.3. Non-MDAP Baselines. Unlike MDAP programs, there is only one
        baseline for ACAT IA (MAIS), ACAT II, and ACAT III programs. It is analogous to
        the Current Baseline. Differences between the different ACATs in this paragraph
        may include:
           13.4.3.3.3.1. Unit Costs (PAUC and APUC) do not normally apply to MAIS,
           AIS, and IT programs.
           13.4.3.3.3.2. Working Capital Funds may be a breachable element for MAIS,
           AIS, and IT programs.
           13.4.3.3.3.3. Total Development Cost and Total Life Cycle Cost (breachable
           elements of the Original Estimate for MAIS programs) are not breachable APB
           totals.
           13.4.3.3.3.4. As otherwise modified/tailored by the MDA.
        13.4.3.3.4. Cost Baseline Preparation. The Acquisition Program Baseline should
        contain cost parameters (objectives and thresholds) for major elements of program
        life-cycle costs. The cost parameters are prepared in both base-year and then-year
        dollars.
           13.4.3.3.4.1. Total Acquisition Cost Elements: Total Acquisition Cost elements
           represent the total program costs to develop and acquire the program as defined in
           the program scope as documented in the ADM. This encompasses the entire
           acquisition phase, program acquisition costs are comprised of cum-to-date,
           current fiscal year, Future Years Defense Program (FYDP), and beyond FYDP
           costs. For MAIS programs, the sum equates to “Total Development Cost” which
           is included in the MAIS Original Estimate and is breachable under MAIS
           Significant/Critical Change requirements. These elements include: Research,
           development, test, and evaluation costs; Procurement costs; Military construction
           costs; Acquisition-related Operations and Maintenance costs (O&M costs that
           procure acquisition items, or support the production, test and/or deployment
           phase, if any); and Working Capital Funds which typically applies to AIS
           programs.)
           13.4.3.3.4.2. Total Operations and Support: Operations and Support costs are
           those costs that occur during the O&S phase of a system, and comprise the costs
           to operate, maintain, sustain, dispose of, and improve all delivered units. These
           costs can be in any appropriation/ any source. O&S costs can oftentimes occur in
           the same year as R&D and Investment costs.
           13.4.3.3.4.3. Total Life Cycle Cost (derived by totaling Total Acquisition and
           Total Operations and Support costs). This sum does not have an APB Threshold
116                                                            AFPAM63-128 10 JULY 2014


             value and is not breachable. For MAIS programs, the sum equates to “Total
             Development Cost” which is included in the MAIS Original Estimate and is
             breachable under MAIS Significant/Critical Change requirements.
             13.4.3.3.4.4. Total RDT&E quantity (fully configured development units). Fully
             configured is defined to mean it satisfies the requirements capability document. It
             does not have to be production representative. Objective and Threshold values do
             not apply to this element. This item does not usually apply to AIS or IT systems.
             13.4.3.3.4.5. Total system procurement quantity (production units). Objective
             and Threshold values do not apply to this element. This item does not usually
             apply to AIS or IT systems.
             13.4.3.3.4.6. Average Procurement Unit Cost (APUC): APUC isderived and
             defined as total procurement cost divided by total procurement quantity and
             usually does not usually apply to AIS or IT systems.
             13.4.3.3.4.7. Program Acquisition Unit Cost (PAUC): PAUC is derived and
             defined as the total of all acquisition-related appropriations divided by the total
             quantity of fully configured end items. This item does not usually apply to AIS or
             IT systems.
             13.4.3.3.4.8. Any other cost objectives established by the Milestone Decision
             Authority.
             13.4.3.3.4.9. Objective values are based on the cost estimate approved by the
             MDA and documented in the ADM. Objective values are shown in both base-
             year and then-year dollars. Then year dollars should be provided for each cost
             element for information purposes only.
             13.4.3.3.4.10. The default threshold value for cost elements is 10% over the
             objective value. The span can be increased or decreased based on the risk
             associated with the individual milestone. The PM may propose with justification
             an appropriate threshold value different from the default value to optimize
             program trade space or commensurate with program risk, subject to Milestone
             Decision Authority (MDA) and user approval. Threshold values are only done in
             base year dollars. Base year selected typically reflects the year of program
             initiation. A base year may change to reflect the year of the Milestone C decision,
             or the year of the estimate supporting a Critical Nunn-McCurdy Unit Cost Breach
             certification. A base year change must be approved by the MDA.
  13.4.4. System Modifications. Modifications can occur throughout the life of a system,
  both in and prior to the Operations and Support (O&S) phase. The O&S phase begins after
  the initial production or fielding decision and is based on an MDA-approved Life-Cycle
  Sustainment Plan.        During the O&S phase, changes planned to be made to
  completed/delivered units are treated as O&S Cost. The O&S phase may include any
  appropriation, including investment appropriations (RDT&E and Procurement). The
  approved cost estimate for O&S used in the initial APB should include a factor for mods over
  the life of the system. Typically, changes made to maintain the existing capability are funded
  via the O&M appropriation while changes made to improve or upgrade the system are funded
  with investment appropriations. The discussion below addresses the issue of how to
AFPAM63-128 10 JULY 2014                                                                   117


  categorize what sometimes amounts to a significant amount of investment appropriation that
  is planned and programmed for improvement/upgrade mods that occur before and after units
  have been delivered by the contractor, and accepted and placed in service by the
  Government. Mods in the O&S phase using investment appropriation(s) are a separate
  acquisition program (usually ACAT III).
     13.4.4.1. Development. Mod development is treated as Research, Development, Test,
     and Evaluation Acquisition Cost until the APB is retired. After that point, the RDT&E
     done in support of a change is treated as O&S Cost.
     13.4.4.2. Procurement.
         13.4.4.2.1. If the mod is to a unit not yet delivered (it is cut in on the production
         line), then the mod is counted as Procurement Cost.
         13.4.4.2.2. If the improvement/upgrade mod is to be to a unit already delivered and
         accepted, then the mod is counted as O&S Cost. Note: The discussion above on
         Development and Procurement costs does not apply to planned increments or
         upgrades for which there is sufficient scope and definition to call it a subprogram
         (MDAP) or additional increment (MAIS). This would include requirements,
         schedule, an approved cost estimate, acquisition strategy and the Milestone B
         documentation and an MDA decision (Milestone B) to initiate it.
  13.4.5. Footnotes: Footnotes are used in the Performance, Schedule, and Cost sections for
  explanation and documentation. They should be kept brief; elaborate or lengthy paragraphs
  distract from the note. They are not used to document acquisition strategy, program
  achievements, future planned actions, assumptions, or should cost vice will cost differences.
     13.4.5.1. Examples of footnotes used for documentation include: the date of the source
     of requirements (usually CDD or CPD, current version and date), the criteria the PM
     must meet for IOC, additional info needed to characterize a performance parameter,
     clarification of the scope of the program or relationship to or reliance on other
     program(s), and the number of FMS units.
         13.4.5.1.1. It is recommended a footnote cite the confidence level used for the cost
         estimate, and the source/date of the cost estimate. For MAIS and MDAP programs,
         Defense Acquisition Management Information Retrieval (DAMIR) should require it
         but not automatically display it.
         13.4.5.1.2. It is recommended a footnote cite the basis for the Funds First Obligated
         date for MAIS programs. It would also be good practice for ACAT III AIS programs
         to document their FFO date (in their APB or elsewhere) in case they ever “grow” into
         a MAIS program.
     13.4.5.2. Footnote examples for explanation include the quantity and planned delivery
     date of different variants for different customers. It is considered a good practice for
     MAIS programs to identify the source and date of the cost estimate and the associated
     cost confidence level.
  13.4.6. MDAP Issues.
     13.4.6.1. Subprograms. The Fiscal Year 2009 National Defense Authorization Act added
     a new section to 10 USC §2430a that permits the Secretary of Defense (delegated to the
118                                                            AFPAM63-128 10 JULY 2014


      Under Secretary of Defense for Acquisition, Technology and Logistics) to designate
      subprograms within a Major Defense Acquisition Program (MDAP).
         13.4.6.1.1. The (USD)AT&L Memo dated 23 Jun                     2009, Designation of
         Subprograms for Major Defense Acquisition Programs, specifies there are two
         primary instances when establishing subprograms within an MDAP may be
         advisable: 1) when an MDAP requires the delivery of two or more categories of end
         items that differ significantly in form and function, and 2) when there are major
         components of a program that are dissimilar and therefore cannot be combined in a
         rational way to produce a unit cost that is representative of the program. If either of
         these two conditions is met, subprograms may be established for baseline
         development and reporting purposes.
         13.4.6.1.2. An update to 10 USC §2430a provides for the designation of subprograms
         if an MDAP to purchase satellites requires the delivery of satellites in two or more
         increments or blocks.
         13.4.6.1.3. The law stipulates that when one subprogram is designated within an
         MDAP, all remaining elements (increments or components) of the program should
         also be appropriately organized into one or more other subprograms.
         13.4.6.1.4. The decision whether to establish subprograms for an MDAP requires
         careful analysis and must be made on a case-by-case basis. Structuring an MDAP
         with subprograms should reflect the way the program is being managed, and
         represent the most efficient and informative way to convey information about a
         program to senior defense acquisition officials as well as to the Congress.
            13.4.6.1.4.1. One factor to consider is a critical Nunn-McCurdy breach to any
            subprogram requires a certification that encompasses all subprograms. If a
            specific subprogram has a critical unit cost breach, the Nunn-McCurdy
            certification is at the overall program level.
         13.4.6.1.5. The law requires that the Congressional defense committees be notified in
         writing of any proposed subprogram designation not less than 30 days before the date
         such designation takes effect.
            13.4.6.1.5.1. The approval of an APB reflecting subprogram designation should
            be considered the date that subprogram designation takes effect; therefore,
            notification to Congress by the Defense Acquisition Executive (DAE) must occur
            not less than 30 days before a subprogram APB is approved. Accordingly, the
            SAE must notify the Director, Acquisition Resources and Analysis of all proposed
            APBs that reflect new or revised subprogram designation at least 60 days before
            the proposed APB is submitted to the MDA for approval.
            13.4.6.1.5.2. If a subprogram breakout is required by law, the 30-day
            Congressional defense subcommittee notification is not required.
         13.4.6.1.6. Selected Acquisition Report (SAR), Defense Acquisition Executive
         Summary (DAES), and the Unit Cost Report (UCR) all require reporting at the
         subprogram level.
  13.4.7. MAIS Issues.
AFPAM63-128 10 JULY 2014                                                                   119


     13.4.7.1. Incremental Delivery.
         13.4.7.1.1. MAIS programs typically follow an evolutionary acquisition approach
         that delivers capability in increments, recognizing, up front, the need for future
         capability improvements. For 10 USC Chapter 144A purposes, a MAIS program
         equals the sum of all its Increments. An Increment is a set of capabilities that when
         fielded provide a useful warfighting or business capability even if future Increments
         are not developed and fielded. A MAIS program Increment is a separate acquisition
         program. The objective is to balance needs and available capability with resources,
         and to put capability into the hands of the user quickly. The success of the strategy
         depends on phased definition of capability needs and system requirements, and the
         maturation of technologies that lead to disciplined development and production of
         systems that provide increasing capability over time.
         13.4.7.1.2. A MAIS system may be broken down into MAIS Increment(s). Each
         MAIS Increment is treated as a separate MAIS program and normally receives a
         schedule, performance and cost Acquisition Program Baseline that is approved at a
         Milestone B. This approved baseline is used as the basis for a 10 USC Chapter 144A
         Original Estimate that is reported to the congressional defense committees in the
         MAIS Annual Report. Once established, the Original Estimate can only be changed
         following a Critical Change Report that is submitted to Congress. An APB baseline
         may be updated per DoDI 5000.02 but such an update does not change the Original
         Estimate.
     13.4.7.2. Original Estimate. MAIS programs also have a separate baseline called an
     Original Estimate that is similar to the MDAP Original Baseline. It is established in the
     initial MAIS Annual Report (MAR) to Congress and is used as the baseline for
     subsequent MAIS Annual reporting to Congress. By policy, the Original Estimate is
     based on the Objective schedule and cost data, and the Threshold Key Performance
     Parameters copied from the Milestone B APB. The Original Estimate also serves as the
     basis for MAIS Significant/Critical Change determination. The Original Estimate’s Total
     Acquisition Cost and Total Life Cycle Cost are the bases to determine Significant/Critical
     Change in the area of cost, but are not the basis of an APB breach. An Original Estimate
     can only be changed after a Critical Change Process is completed.
  13.4.8. Special Interest Programs. Per the DAG Chapter 10, a program, or a technology
  project that should result in a program, has special interest if it has one or more of the
  following factors: technological complexity; Congressional interest; a large commitment of
  resources; the program is critical to achievement of a capability or set of capabilities; the
  program is part of a system of systems; or the program is a joint program. Generally, the
  level of funding, desired oversight and/or reporting should determine the MDA and whether
  or not the program is designated a "Special Interest" program. Programs that already meet the
  dollar thresholds for an MDAP, Major System, or MAIS program cannot also be designated
  Special Interest programs.
     13.4.8.1. ACAT I Special Interest Program. If the DAE desires oversight of a program
     that falls below MDAP dollar thresholds, and deems that statutory reporting associated
     with MDAPs is not needed, the program may be designated a Special Interest Program.
     If the DAE retains MDA, the program is an ACAT ID Special Interest program. If the
 120                                                           AFPAM63-128 10 JULY 2014


       DAE delegates MDA to the Component Head or CAE, then the program is an ACAT IC
       Special Interest program. The SAE may also designate Special Interest programs that are
       ACAT II or below. For such Special Interest programs, the reporting requirements are
       tailored to meet the specific oversight needs.
       13.4.8.2. ACAT IA Special Interest Program. If the DAE (or delegated MDA within
       OSD) desires oversight of an AIS program, but deems that the statutory reporting
       associated with MAIS programs is not needed, the program is designated a "Special
       Interest" program. If MDA remains within OSD (DAE or DAE delegated MDA within
       OSD), the program is an ACAT IAM Special Interest program but not a MAIS program.
       If MDA is delegated by the DAE to the Component Head or CAE, then the program is an
       ACAT IAC Special Interest program, again not a MAIS program. For such Special
       Interest programs, the reporting requirements are tailored to meet the specific oversight
       needs.
   13.4.9. ACAT II Issues. RESERVED.
   13.4.10. ACAT III Issues. RESERVED.
   13.4.11. Technology Projects Issues. RESERVED.
13.5. APB Preparation and Approval Process. In general, the PM prepares the APB at
program initiation; and revises the APB at subsequent milestone reviews, program major
restructurings, or unrecoverable program deviations. The Program Manager, Program Executive
Officer (PEO), and the Service Acquisition Executive (SAE), as appropriate, concur in the
proposed APB and sign the cover sheet signifying approval. The proposed APB is submitted to
the MDA for approval. The MDAs approval date becomes the date of the current APB.
   13.5.1. Format and Preparation
       13.5.1.1. The PM, in coordination with the user/sponsor, prepares the APB for program
       initiation. The MDA is the approval authority for the APB. Templates/samples depicting
       format, content, and hints for APB preparation can be found at Attachment 13.
       13.5.1.2. ACAT I programs (MDAP and MAIS) and ACAT I Special Interest programs
       should prepare and submit their APBs via the DAMIR system at
       https://ebiz.acq.osd.mil/damir.
          13.5.1.2.1. Classified APB Information. Use of DAMIR also applies for collateral
          SECRET ACAT I program APBs. As of December 2010 Selected Acquisition
          Report (SAR) season, the SECRET parameters and values (typically Performance)
          are entered into SIPR DAMIR. Programs with SECRET parameters are required to
          create their Unclassified and Classified APB simultaneously utilizing the NIPR and
          SIPR versions of DAMIR. For each section in the Unclassified DAMIR APB that
          contains classified Information, a checkbox is provided for the program to mark
          [Classified Data Exists Checkbox]. By checking this information, the program office
          is indicating to DAMIR that a classified APB needs to be created. The program
          office should enter all unclassified information into the Schedule and Performance
          sections of the NIPR DAMIR system and then go to the
          httaps://damir.acqs.osd.pentagon.smil.mil/damir SIPR DAMIR site. Within the SIPR
          DAMIR system, the program office should be able to add/delete/edit all classified
AFPAM63-128 10 JULY 2014                                                                     121


         APB information. Unclassified information should be available for viewing purposes
         only. The full APB report (to include classified and unclassified information is
         available for review and printing within the SIPR DAMIR Purview Program view.
     13.5.1.3. ACAT II and III Programs: All other AF programs should prepare their APBs
     using the APB module in the System Metric And Reporting Tool (SMART) system. The
     scope, parameters and definitions are the same as used with ACAT I programs. A
     SMART APB User’s Guide has been developed for assistance. As with ACAT I APBs,
     the MDA has the authority to add or subtract from the template content.
  13.5.2. Program Funding. A key criterion for APB approval is that the program be fully-
  funded to the Objective Cost values. Full-funding means the dollars and manpower needed
  for all current and future efforts to carry out the acquisition strategy are in the President’s
  Budget Program (cumulative to date, FYDP, and beyond FYDP). The Program Objective
  Memorandum or Budget Estimate Submission (BES) lock positions are not sufficient to
  support the full-funding criteria. If a program is not fully-funded, the issue should be raised
  early to the appropriate PEO or SAE staffs. One approach to address this problem with
  ACAT I programs is to obtain a full funding commitment letter signed by SAF/FM and
  SAF/AQ for submittal with the APB approval request to the MDA for their consideration.
  13.5.3. APB Coordination/Approval.
     13.5.3.1. ACAT I and IA. The PM and PEO sign the cover signature page before
     forwarding for SAE approval. Coordination should include Lead User/Operating
     Command and affected agencies/departments with requirements/funding in the program
     (i.e., joint programs). Internal coordination processes should be defined at the
     PEO/Center level but may include FM, legal, ACE, and others on the Center or PEO
     staff.
         13.5.3.1.1. The SAE signs the cover signature page either as the MDA for ACAT IC
         or ACAT IAC programs, or as coordinating as SAE before forwarding it on to the
         DAE as MDA for ACAT ID or ACAT IAM programs. Coordination for SAE
         signature includes SAF/AQX and the Capability Directorate. Coordination may
         include legal (SAF/GCQ), FM, and others on the headquarters staff as the case
         dictates. The PEM is the focal point for coordination within the headquarters. In the
         case of an ACAT ID or ACAT IAM program, the PEM is the AF focal point after it is
         forwarded to OSD for coordination and then signature by the DAE.
         13.5.3.1.2. ACAT I and IA program managers should review the proposed APB with
         the PEM and SAF/AQX prior to signature by the PEO and committing it to SAF/AQ
         coordination/signature (ideally, discussions on program structuring should occur in
         conjunction with development/review of Acquisition Strategy). If necessary, and
         with the participation of the Program Office, the OSD SME may also be brought into
         “trusted agent” discussions to further vet the APB. A SAF/GCQ representative may
         also be called in if there appears to be any legal issues. This helps to ensure issues
         peculiar to the program or more complex than can be addressed in a comment page
         are identified and resolved prior to the start of formal coordination.
     13.5.3.2. ACAT II: The PM and PEO sign the cover signature page before forwarding
     for SAE approval. Coordination must include Lead User/Operating Command and
 122                                                           AFPAM63-128 10 JULY 2014


       affected agencies/departments with requirements/funding in the program (i.e., Joint
       programs). Internal coordination processes should be defined at the PEO/Center level but
       may include FM, legal, ACE, and others on the Center or PEO staff. The SAE signs the
       cover signature page as the MDA. If the SAE signs the APB cover page, then
       coordination for SAE must include SAF/AQX. Coordination may include legal, FM and
       others on the headquarters staff as the case dictates. The PEM is the focal point for
       coordination within the headquarters.
           13.5.3.2.1. Where the SAE has delegated MDA authority to the PEO there is no need
           for the PEO to forward the APB to the SAE. Delegated ACAT II program
           coordination processes should be defined at the PEO/Center level but may include
           FM, legal, ACE, and others on the Center or PEO staff.
       13.5.3.3. ACAT III, Technology Projects, and other Activities. The PM and MDA (or
       similar decision authority if not ACAT) need to sign the cover signature page.
       Coordination must include Lead User/Operating Command and affected
       agencies/departments with requirements/funding in the program. Internal coordination
       processes should be defined at the PEO/Center level but may include FM, legal, ACE,
       and others on the Center or PEO staff.
       13.5.3.4. Signature Summary. See Table 13.1. to identify required signatures for APBs
       prepared for programs at each ACAT level.

Table 13.1. APB Signature Authorities.

       Signature                           Acquisition Category
                    ID/IAM       IC/IAC   Non-Delegated     Delegated (to      Delegated (to
                                          ACAT II           PEO) ACAT II       PEO) ACAT
                                                                               III
    Program            X           X             X                 X                  X
    Manager
    PEO                X           X             X                 X                  X
    SAE                X           X             X
    DAE                X


13.6. Obligation Restrictions.
   13.6.1. ACAT I Programs:
       13.6.1.1. Program Initiation. Per 10 USC §2435, the Department of Defense (DoD) may
       not obligate funds for Major Defense Acquisition Programs after entry into EMD without
       an MDA-approved baseline unless the Under Secretary of Defense for Acquisition,
       Technology, and Logistics (USD(AT&L)) specifically approves the obligation. DoDI
       5000.02 extends this policy to ACAT IA programs. Obligation authority in these types of
       cases is specified via an ADM by the DAE. Note: If a program “grows” from ACAT II
AFPAM63-128 10 JULY 2014                                                                        123


      to ACAT I, it must comply with ACAT I requirements. This includes either a MDA
      approved baseline or specific obligation authority from the AT&L to continue obligation
      of program funds. Failure to do this can result in an Anti-Deficiency Act violation.
      13.6.1.2. Nunn-McCurdy and Significant/Critical Changes.
          13.6.1.2.1. Nunn-McCurdy. The prohibition on obligations until the submission of
          the SAR for significant breaches, and the certification for critical breaches, will affect
          all major contracts of the program, including, if appropriate, all subprogram(s).
             13.6.1.2.1.1. Following a critical Nunn-McCurdy breach resulting in the
             recession of the most recent Milestone approval, a new Milestone approval is
             required before taking any contract action to enter a new contract, exercise an
             option under an existing contract, or otherwise extend the scope of an existing
             contract under the program. An exception is given to the extent determined
             necessary by the Milestone Decision Authority, on a non-delegable basis, to
             ensure that the program can be restructured as intended by the Secretary without
             unnecessarily wasting resources.
          13.6.1.2.2. MAIS Significant/Critical Change. If the Senior Official determines that a
          Critical Change has occurred based upon a Quarterly Report, the PM should not
          obligate funds for a major contract during the period in which the Critical Change
          Report (CCR) is being prepared. If the CCR is not submitted to the congressional
          defense committees within the 60-day period following the determination,
          “appropriated funds may not be obligated for any major contract under the program."
          For 10 USC Chapter 144A purposes, the term "major contract" is defined as any
          contract under the program that is not a firm fixed price contract whose target cost
          exceeds $17M (FY00 constant dollars); or if no contract exceeds $17M (FY00
          constant dollars), then the largest contract under the program. The prohibition on the
          obligation of funds should cease to apply on the date on which which a Critical
          Change Report has been submitted in accordance with 10 USC Chapter 144A.
   13.6.2. ACAT II and III Programs. There are no provisions in law or OSD policy that
   prohibits obligations without an MDA approved APB.
   13.6.3. Technology Projects. RESERVED
13.7. APB Updates.
   13.7.1. ACAT I Programs (MDAP). DoDI 5000.02 specifies the Current APB should be
   updated at Milestone Decisions and at full rate production. For Milestone Decisions, the
   Development APB follows MS B, the Production APB update follows MS C. Other
   opportunities to change the current APB are limited – only due to: 1) the result of a major
   program restructure that is fully funded and approved by the MDA, or 2) the result of
   program deviation(s), if the breach is primarily the result of external causes beyond the
   control of the program manager (these are also referred to as programmatic breaches). This
   should be addressed by the PM in their communication to the MDA regarding causes and
   proposed future actions within 30 days of the notifying the MDA of a breach. A change is
   not automatically authorized due to changes to cost, schedule and/or performance. An APB
   update is subject to approval by the MDA. The changes discussed here affect the Current
   APB only.
 124                                                           AFPAM63-128 10 JULY 2014


       13.7.1.1. Following a critical Nunn-McCurdy Unit Cost Breach, both the Current and
       Original Baselines should be revised resulting in a “revised” Original APB and a new
       Current APB that reflect the Nunn-McCurdy certification approved by the MDA. At that
       time, as at program inception, both the Original and Current Baselines should match.
       13.7.1.2. Following a significant Nunn-McCurdy Unit Cost Breach, only the Current
       APB may be revised, subject to approval by the MDA.
   13.7.2. ACAT IA Programs (MAIS). For ACAT IA programs, the APB is approved at
   Milestone B, and updated as needed at Milestone C and the Full Deployment Decision. The
   APB is also updated following a Critical Change. Per 10 USC Chapter 144A, a MAIS
   Original Estimate (baseline) must be established and submitted to Congress. By policy the
   Original Estimate is the Objective schedule and cost data, and the Threshold Key
   Performance Parameters copied from the Milestone B APB, but by law the Original Estimate
   may only be revised through the Critical Change process. Following a Critical Change
   Report a revised APB forms the basis for the revised Original Estimate.
   13.7.3. ACAT II Programs. The APB is updated based on the rule-set for ACAT I (MDAP)
   Current APBs. If the MDA is the SAE, the process should follow that established for ACAT
   I programs. If the program MDA has been delegated from the SAE to the PEO, the update
   process should follow that established by the PEO.
   13.7.4. ACAT III Programs. The APB is updated based on the rule-set for ACAT I (MDAP)
   Current APBs. Individual ACAT III MDAs may establish their own internal update process.
   13.7.5. Technology Projects. If a Technology Project baseline is required by the MDA, an
   update is required based on criteria established by the MDA
13.8. Program Deviations.
   13.8.1. Terms.
       13.8.1.1. Breach. An APB deviation/breach occurs when the PM has reason to believe
       that the Current Estimate for the program indicates that a performance, schedule, or cost
       Threshold value will not be achieved.
       13.8.1.2. Current Estimate. The Current Estimate is the latest estimate of program
       acquisition cost and quantity, schedule milestone dates, performance characteristic
       values, and critical technical parameters of the approved program (i.e., the approved
       program as reflected in the currently approved APB, ADM, or in any other document
       containing a more current decision of the MDA or other approval authority).
          13.8.1.2.1. Cost. The Current Estimate for cost parameters is defined as the
          President’s Budget program plus or minus fact-of-life/known changes. The
          President’s Budget program is defined to be the total of: Cum-to-date, FYDP and
          beyond FYDP costs through completion of the scope in the APB. The beyond FYDP
          costs are based on a cost estimate. The most common fact-of-life changes include
          Below Threshold Reprogramming (BTR) and Above Threshold Reprograming
          (ATR). Another fact-of-life change may be an unrecoverable cost increase (most
          often caused by a contract overrun) recognized by the MDA (or the SAE in cases of a
          Nunn-McCurdy breach determination) – this too should be supported by a cost
          estimate and a commitment to fund the overrun. Program Objective Memorandum
AFPAM63-128 10 JULY 2014                                                                    125


         (POM) positions associated with a change in scope in the program alone are not
         considered fact-of-life changes and are not sufficient to support a cost breach
         determination. However, the POM may constitute support for a breach determination
         if associated with funding a recognized overrun. The Current Estimate for Cost must
         done in base-year dollars (to be compared to the Cost Threshold value in BY $).
         13.8.1.2.2. Schedule. The Current Estimate for schedule parameters is defined as the
         program manager’s best judgment/projection of schedule milestone dates that will be
         attained relative to the APB parameters. A schedule breach occurs when the PM
         projects it, not when the threshold date actually arrives.
         13.8.1.2.3. Performance: The Current Estimate for performance parameters is
         defined as the program’s manager’s best judgment/projection of performance
         characteristics values that will be attained relative to the APB parameters.
  13.8.2. Program Deviations (Breaches):
     13.8.2.1. Breach Categories. APB breaches can be categorized as one of two types,
     Programmatic or Fact-of-Life.
         13.8.2.1.1. Programmatic breach. Factors outside the PM’s management control
         (external factors) can cause a program deviation. These breaches, referred to as
         “programmatic breaches” may be the result of guidance from above the program
         office level. Several factors can cause these breaches such as doctrinal revisions,
         program restructuring, requirements changes, or budget-related quantity and dollar
         changes. Even “good-news” events such as funding increases to support a quantity
         increase can cause the Current Estimate to fall outside the threshold and cause an
         APB breach.
         13.8.2.1.2. Fact-of-Life breach. A fact-of-life (FOL) breach is an internal cost-
         growth, management, or technical problem leading to a breach of the program’s
         performance, schedule, and/or cost parameters.
     13.8.2.2. APB Breaches.
         13.8.2.2.1. APB Cost Breaches. An APB cost breach occurs when the Current
         Estimate exceeds the APB Threshold value(s). If cost threshold values are exceeded,
         the program may be too costly, and the affordability of the system may become
         questionable. Base year dollars are used to calculate cost growth (effects of inflation
         are removed and give true value of changes over time) which forms the basis to
         calculate a breach. APB Breachable elements include:
            13.8.2.2.1.1. RDT&E;
            13.8.2.2.1.2. Procurement;
            13.8.2.2.1.3. Military construction costs;
            13.8.2.2.1.4. Acquisition O&M;
            13.8.2.2.1.5. Working Capital Fund (if applicable);
            13.8.2.2.1.6. O&S;
            13.8.2.2.1.7. Total Life Cycle Cost (MAIS only)
126                                                          AFPAM63-128 10 JULY 2014


             13.8.2.2.1.8. APUC (if applicable);
             13.8.2.2.1.9. PAUC (if applicable);
             13.8.2.2.1.10. Any other parameter directed by the MDA.
         13.8.2.2.2. APB Schedule Breaches. An APB breach occurs when the PM projects a
         Milestone Threshold date will not be met. The breach occurs when the PM projects a
         Current Estimate exceeds the Milestone Threshold date; it is not when the Threshold
         date is actually exceeded.
         13.8.2.2.3. APB Performance Breaches. An APB breach occurs when the PM
         projects performance values which do not meet the threshold values.
      13.8.2.3. Other Breaches:
         13.8.2.3.1. Nunn-McCurdy Cost Breaches. A MDAP program can incur a Nunn-
         McCurdy Unit Cost Breach (defined in Title 10, Section 2433) when the APUC
         and/or PAUC exceeds their Original Baseline Estimate by 30% (Significant) or 50%
         (Critical; or their Current Baseline Estimate by 15% (Significant) or 25% (Critical).
         13.8.2.3.2. Significant or Critical Cost Breaches. A MAIS program can incur a
         Significant or Critical Cost Change when the Current Estimate for Acquisition Cost,
         and/or for Life Cycle Costs, exceeds their MAIS Original Estimate Objective value
         by 15% (Significant) or 25% (Critical).
         13.8.2.3.3. Selected Acquisition Report (SAR) Schedule Breach. For MDAP
         programs, a SAR Schedule Breach occurs when the Quarterly DAES Current
         Estimate is more than 6 months beyond the Current Estimate date reported in the last
         SAR report to Congress.
         13.8.2.3.4. MAIS Annual Report Schedule Breach. A MAIS program can incur a
         Significant or Critical Schedule Change when the PM projects a Current Estimate that
         exceeds a MAIS Annual Original Estimate date by more than 6 but less than 12
         months (Significant), and 12 months or more (Critical). There is an additional
         Critical Change breach if the time between date of Milestone A or if no Milestone A
         the identification of the preferred alternative and FDD (normally the date of the FDD
         ADM) exceeds 5 years. This breach is not reported until the 5 year date is actually
         exceeded.
  13.8.3. APB Deviation (Breach) Notification.
      13.8.3.1. ACAT I and IA. As soon as an unrecoverable deviation/breach to the APB
      occurs, the PM immediately notifies the MDA that a deviation has occurred or will occur
      based on available information in accordance with DoDI 5000.02. This is usually done
      via submission of a Program Deviation Report. Notification may also occur via the
      Quarterly Defense Acquisition Executive Summary (DAES) report – Current Estimate
      values which exceed the Threshold values are a breach. In almost all cases, this should
      not be the first time the MDA is made aware of the program issue(s) and risk(s). The PM
      should have been keeping the PEO informed of concerns and risks as well as actions to
      mitigate them though formal periodic acquisition reporting and/or other
      communication(s).
AFPAM63-128 10 JULY 2014                                                                     127


     13.8.3.2. Program Deviation Report (PDR). The PDR should at a minimum contain the
     name of the program, the date of determination, a description of the breach, to include
     identification of what the breach is, and a short description of the cause. It may include a
     description of the way ahead. The PDR should not be longer than one page. For MDAP
     Nunn-McCurdy breaches, a unit cost report must be attached. The unit cost report may
     be generated from DAMIR.
     13.8.3.3. The MDA has authority beyond that of a PM to address potential breaches and
     may require additional information such as an independent cost estimate to assess current
     status and potential actions prior to a breach determination by the PM. They should also
     use this information in forming a solid basis to support a breach determination.
        13.8.3.3.1. Process Following a Breach Notification - ACAT I (MDAP) and IA
        (MAIS) Programs:
            13.8.3.3.1.1. The PM is required to notify the MDA of the reason for the program
            deviation and the actions that need to be taken to bring the program back within
            the baseline parameters (if this information was not included with the original
            notification) within 30 days of the occurrence of the program deviation
            notification (PDR).
            13.8.3.3.1.2. Within 90 days of the occurrence of the program deviation, the
            program should (1) be back within APB parameters, (2) a proposed APB update
            (changing only those parameters that breached) should have been submitted for
            approval, or (3) an Overarching Integrated Product Team (OIPT) or equivalent
            Component-level program review should be held to review the program and the
            program managers proposed baseline revisions and make recommendations to the
            Milestone Decision Authority regarding the parameters that were breaches. The
            MDA should decide, based on criteria in 10 USC §2433 and §2435 (MDAP-
            only), and DoDI 5000.02, whether it is appropriate to approve a revision to the
            APB.
            13.8.3.3.1.3. If one of the three actions listed in the previous paragraph has not
            occurred within 90 days of the program deviation, the USD(AT&L) for ACAT ID
            programs and ACAT IAM programs, or the SAE, for ACAT IC and/or ACAT
            IAC programs, should require a formal program review to determine program
            status.
            13.8.3.3.1.4. While APB breach reporting is regulatory, the breach should be
            identified in the SAR for MDAP and the MAIS Annual Report for MAIS
            programs and thus reported to Congress. The SAR and MAIS Annual Report are
            statutory.
     13.8.3.4. ACAT II and III. As soon as an unrecoverable deviation/breach to the APB
     occurs, the PM should immediately notify the MDA that a deviation has occurred or will
     occur based on available information. This is done via a Program Deviation Report.
        13.8.3.4.1. ACAT II programs, should provide a PDR up to the SAE. If the SAE has
        delegated MDA responsibility to the PEO, the program should provide a PDR up to
        the PEO.
128                                                         AFPAM63-128 10 JULY 2014


         13.8.3.4.2. ACAT III programs, the program should notify up through the designated
         MDA via a PDR.
      13.8.3.5. Technology Projects. For Technology Projects, which have a project baseline,
      the PM should immediately notify the decision authority that a deviation has occurred.
AFPAM63-128 10 JULY 2014                                                                      129


                                          Chapter 14

                                  PROGRAM TAILORING

14.1. Program Tailoring. The purpose of program tailoring is to streamline the acquisition
program to the maximum extent possible, consistent with risk, to provide new systems to
operational commanders as fast as possible. Tailoring is authorized within DoDI 5000.02,
section 1.b. as follows: “Authorizes Milestone Decision Authorities (MDAs) to tailor the
regulatory requirements and acquisition procedures in this instruction to more efficiently achieve
program objectives, consistent with statutory requirements and Reference (a) [DoDD 5000.01].”
In addition, DoDI 5000.02 states; “MDAs should tailor regulatory procedures in the document
consistent with sound business practice and the risks associated with the product being
acquired.” All MDAs should promote maximum flexibility in tailoring programs under their
oversight and document all information tailoring decisions. It is important to note that the
authority to tailor lies with the MDA, but the responsibility to present a sound, reasoned, and
achievable approach lies with the PM.
   14.1.1. Tailoring Recommendation. PMs should be assertive in the development of tailoring
   recommendations, as long as they have good justification, and can balance the risks in line
   with the user's priorities. The PM should propose a tailored execution, management,
   program information, documentation, and oversight structure for the program in the
   Acquisition Strategy (AS). The PM proposal should consider program size, complexity,
   system service-life, total force structure, and associated risk.
   14.1.2. Tailoring Review. MDA tailoring determinations should be reexamined at each
   program decision point against the current program conditions and execution strategy.
   14.1.3. Tailoring Documentation and Approval. Tailoring recommendations, justification,
   and approval should be captured in the appropriate documentation including the Acquisition
   Strategy (AS). The MDA should approve in writing in an ADM a tailored execution,
   management, program information, documentation, and oversight structure. Upon approval,
   all deviations from the program’s documented tailoring plan require MDA approval.
   14.1.4. Tailoring versus Waiving. Tailoring is the ability to integrate, consolidate,
   incorporate, and streamline documentation to meet the intent of the requirement in the most
   efficient and effective manner possible. Waiving a requirement (e.g.; statute, policy,
   document) is different than tailoring, waiving a document is stating that the document does
   not apply and the intent will not be fulfilled.
       14.1.4.1. Tailoring. The manner in which certain core areas are addressed in a particular
       program. Tailoring provides the ability to integrate, consolidate, incorporate, and
       streamline strategies, oversight, reviews, decision levels, documentation and information.
       The purpose is to streamline the acquisition program to the maximum extent possible,
       consistent with risk, in order to most efficiently and effectively deliver a capability.
       Tailoring is only allowed within the scope of the applicable statute or regulation.
       Tailoring must be documented, including the supporting rationale and citation to the
       applicable statute or regulation. Tailoring authority is at the MDA level.
       14.1.4.2. Waiver. A waiver is an expressed or written statement to relinquish or provide
       exceptions to specific statutory or regulatory requirement. A waiver is employed when
 130                                                            AFPAM63-128 10 JULY 2014


       complying with directive publication adversely affects the mission or national security
       due to a unique situation. Waiver processes for documentation are incorporated into the
       functional document and may require approvals above that of the MDA tailoring
       authority. Waiver authorities remain with the publication owner.
14.2. Program Determination. ACAT III has no funding floor and encompasses all acquisition
programs not included within ACAT I, IA or II. Mods in the O&S phase using investment
appropriation(s) are a separate acquisition program. Refer to AFI 63-101/20-101 for the
specifics of the content of the Acquisition Master List (AML).
14.3. Milestones. MDAs make the determination where to enter the acquisition process based
upon the technical maturity of the product being acquired. The program is still responsible to
ensure the work required for the previous phases is completed. This includes the technical
planning and certifications. The determination of entry into the acquisition system is made
during MDD and documented in an ADM.
14.4. Delegation of Authority. A major component of tailoring is ensuring the program is
delegated to the appropriate level of authority. This must be done consistent with DoDD
5000.01, DoDI 5000.02, HAF MD 1-10, AFPD 63-1/20-1, and AFI 63-101/20-101. ACAT III
programs are automatically delegated to the PEO through AFPD 63-1/20-1; the PEO/MDA
delegation should be documented in the initial ADM and reported through SMART and the
AML. During program execution, the MDA retains overall responsibility for the program, but
the MDA has the authority to delegate within his organization the actual execution of work. This
means that, unless explicitly stated in policy, when the MDA is the signatory of a document/
requirement, the MDA has the authority to delegate the execution and approval of the work
while retaining overall responsibility.
14.5. Regulatory vs. Statutory. A simple way to remember this in context of program
tailoring is that regulatory information has the ability to be tailored, whereas statutory
information may not be tailored except in rare cases and only with the approval of Congress or
by the SECDEF using Rapid Acquisition Authority. The tables in DoDI 5000.02 differentiate
between regulatory and statutory information requirements and must be carefully reviewed for
applicability. Regulatory information can be tailored according to the guidelines set forth in
controlling documentation. For example, refer to AFI 99-103, Capabilities Based Test and
Evaluation, for applicability of the Test and Evaluation Master Plan (TEMP) to ACAT III
programs.
14.6. Program Information. Tailor to: (1) combine program information and documents with
similar information and approval authorities; (2) establish a common reference for basic system
and program information; and (3) eliminate non-applicable information. MDAs and PMs should
be aware that there are statutory and regulatory requirements that cannot be tailored out of a
program’s milestone information requirements. Failure to comply with these requirements
should preclude the successful completion of applicable milestone reviews. PMs should be
aware that there is no program information required beyond that contained in statute, directives,
and instructions, and additional information required by the MDA. Non-directive guidance is
not a requirement.
14.7. Integrated Documentation. The concept which allowed for streamlining the number of
program documents by consolidating the AS, Acquisition Plan, TEMP, Systems Engineering
Plan (SEP), Life Cycle Sustainment Plan (LCSP), and other program documentation into a single
AFPAM63-128 10 JULY 2014                                                                         131


document is still a viable tailoring alternative. The MDA retains the ability to tailor and make
the final determination of what information is contained in each document. For AF programs
delegated to the SAE and below, the MDA can approve a tailored AS combining the AS,
Acquisition Plan, LCSP, TEMP, SEP, PPP, and other documentation requirements if it is
appropriate for the program and properly documented. MDAs may still keep all program
information in one document with attachments/annexes or a virtual information reservoir to
ensure an integrated life cycle approach, but the formats should comply with established
templates (OSD templates) to ensure standardization of information being provided. The PM
should identify the documents being integrated and document this as part of the tailoring
strategy.
   14.7.1. It is consistent with policy to fully leverage the authority on ACAT IC, ACAT ID,
   ACAT IAC, ACAT IAM, ACAT II, and ACAT III programs, as provided in policy, to tailor
   the regulatory information program information to fit the particular conditions of the
   program. Case studies have shown that programs can achieve a 40-60% reduction in
   documentation by incorporating required information into a consolidated document instead
   of stand-alone documents that duplicate general program information.
   14.7.2. Instead of waiving the document, the PM should present the “thought process” in the
   acquisition strategy to show that the document has been considered and addressed but due to
   the unique nature of the program certain aspects or content of the document do not apply to
   the program. For example, if the program is an ACAT III program the PM may choose to
   incorporate its test planning into the acquisition strategy versus a TEMP, this does not mean
   that the PM is waiving the requirement but they are fulfilling the intent of the requirement by
   addressing it and consolidating the information into another document. Another example is
   the program may choose to utilize an organizational SEP and document program unique SEP
   requirements in the Acquisition Strategy or in an annex to the SEP.
   14.7.3. The PM should reference DoDI 5000.02, AFI 63-101/20-101, this document, the
   Defense Acquisition Guidebook, and functional guidance to determine the requirements and
   ability to reduce documentation.
14.8. MDD. The formal entry review into the acquisition life cycle is the first opportunity for
program tailoring. At this point, the program should be targeted towards a specific authority and
level. The PM (responsible acquisition organization in coordination with responsible Lead
Command) should propose and document the expectations for the next milestone review and the
entry point into the acquisition life cycle. This should be based upon technical maturity, cost
maturity, and other supporting considerations. All programmatic information including next
milestone review, documentation required for next milestone, and other program expectations
should be documented in an ADM. If the program is recommending entry into the acquisition
process at a later MS (e.g., MS B or C) it needs to ensure that all intent of the requirements of the
preceding MSs has been fulfilled. This means that if the program enters at MS B it must fulfill
the intent of the requirements of MS A.
14.9. Milestone Decisions. PMs should present the tailoring/streamlining approach in the
acquisition strategy of the program and get approval at MDD or another MS decision. Programs
can enter at almost any milestone if they can present a case that shows that they have completed
the work for the prior MS decisions and phases. If the capability being acquired is mature, they
can streamline the acquisition process by entering the acquisition framework at a MS that is
 132                                                         AFPAM63-128 10 JULY 2014


more appropriate to the work being conducted for the program. If the program is COTS, it may
be more appropriate to enter at MS B or C dependent upon risk. This should be documented in
the ADM to ensure that program expectations are communicated and documented.
AFPAM63-128 10 JULY 2014                                                                      133


                                          Chapter 15

            PRODUCT SPECIFIC INFORMATION AND BEST PRACTICES

15.1. Information Technology (IT). IT programs, as with all programs, should be tailored to
the maximum extent possible consistent with risk. This includes integrating processes and
documentation to the maximum extent possible. Items of note specific to IT include: if the IT
program is using investment and/or RDT&E money and is not a MAIS, then it is an ACAT III
program; an IT program can be an ACAT program and a Defense Business System and; if the
program is a commodity buy, tech refresh, or sustainment activity it should be managed
according to the applicable laws, regulations, and policy concerning the specific activity.
Guidance in this section is not mandatory guidance but lists considerations, best practices and
direction while managing a program and is primarily geared toward IT programs that do not
meet the MAIS thresholds as specified in DoDI 5000.02. Program Managers should consider the
type of system they are acquiring when applying best practices.
15.2. IT Budget. All IT programs have associated costs. All resource information for IT
programs is reported in Enterprise Information Technology Data Repository (EITDR). This
resource information serves two purposes: (1) determines the life-cycle cost of a program and (2)
is reported to DoD CIO for inclusion in the department’s IT Budget submission to the
President’s Budget (PB). The IT Budget is not in itself a separate budget; it is more of a report.
The IT Budget submission reflects what dollars within the overall AF budget is dedicated to IT
spending. SAF/CIO A6 provides specific AF guidance with its Budget Estimate Submission
(BES) and PB Submission Guidance.
15.3. Requirements. IT program requirements, as with all requirements, should be clear,
concise, unambiguous, testable, and provide traceability. The program should employ a rigorous
configuration control process that controls the composition of the baselines. IT requirements can
be developed and approved within multiple approval chains. Regardless of the size of the IT
effort, requirements should be documented in writing. As the IT effort transitions into an IT
program, the program should eventually be asked to document the original requirement in
multiple databases and program reviews. Dependent upon the type of IT required, IT
requirements should be done in accordance with the JCIDS process, Defense Business System
process, or modification process. Included below is a summary of different ways that IT
requirements can be approved before the IT required is approved.
   15.3.1. JCIDS. CJCSI 3170 details the requirements for IT systems to follow the JCIDS
   process. CJCSI 3170 does not require that IT systems under $15 million in post MS B cost
   follow the JCIDS process, but still requires that the sponsor manage the requirements,
   approve the JCIDS documentation and comply with acquisition requirements. To ensure that
   IT programs have flexibility and to implement the IT Box model, IS-ICDs are used to
   document capability requirements and associated capability gaps where the intended solution
   approach involves research, development, and acquisition of applications system software,
   and the projected software development costs exceed $15M. It is not intended to be used for
   software embedded as a subset of a capability solution developed under other validated
   documents. All hardware associated with an IS ICD is COTS/GOTS/NDI and hardware
   development is restricted to that necessary for system integration, system enhancements, and
   hardware refresh due to obsolescence. Follow-on IS-CDDs, CDDs and CPDs are not
 134                                                            AFPAM63-128 10 JULY 2014


   required for IS using an IS-ICD. An IS-ICD can be used for all Automated Information
   Systems that exceed $15M in development costs and are not designated an MDAP.
   Reference AFI 10-601 for more detail.
   15.3.2. Defense Business Systems. Follow the rules set in place by the Investment Review
   Board (IRB)/Defense Business Council (DBC) and AFI 33-141, Air Force Information
   Technology Portfolio Management and IT Investment. Receiving IRB approval of the
   problem statement is equivalent to receiving JROC approval of a requirements document.
   DBC certification allows a program to expend funds. Receiving IRB approval and DBC
   certification still requires the capability to be acquired using the above guidance with the
   MDA making acquisition execution decisions.
   15.3.3. Base Level IT. RESERVED
   15.3.4. IT Service. IT Service is the performance of any work related to IT and the
   operation of IT, including National Security Systems (NSS). This includes outsourced IT-
   based business processes, outsourced IT, and outsourced information functions. If a program
   is under this definition, it is required to follow process as specified in DoDI 5000.02 and AFI
   63-138, Acquisition of Services.
   15.3.5. Urgent Operational Need (UON). A warfighter submits the UON to Lead
   Command, AF/A5R and SAF/AQX. Lead Command develops a validation recommendation.
   The AFROC validates the UON in accordance with AFI 10-601. This capability should be
   fielded in accordance as a Quick Reaction Capability (QRC) and undergo a Capability
   Transition Review (CTR) to determine the future disposition of the capability. The QRC
   process can also field Joint Urgent Operational Needs (JUONS).
   15.3.6. Modification. A modification requirement is normally documented with an AF form
   1067 in accordance with AFI 63-131.
15.4. ACAT III Defense Business System (DBS) Determination and Requirements. If an IT
system falls under the definition of a DBS, it should follow the additional requirements for DBS
found in DoDI 5000.02 Enclosure 12. DBS is an information system, other than a national
security system, operated by, for, or on behalf of the Department of Defense, including financial
systems, mixed systems, financial data feeder systems, and information technology and
information assurance infrastructure, used to support business activities, such as acquisition,
financial management, logistics, strategic planning and budgeting, installations and environment,
and human resource management. DBS programs are required to follow DoDI 5000.02
acquisition process which includes MDA as well as IRB/DBC responsibilities. DBS programs
are ACAT programs and must comply with the requirements of AFI 63-101/20-101. ACAT III
programs are automatically delegated to the PEO level and should be managed under the
appropriate PEO portfolio.
15.5. Entry into the Acquisition System Capabilities that were acquired outside the
normal acquisition process have multiple ways to support entry into the acquisition
process. Capabilities without adequate requirement documentation may be determined to be out
of scope, duplicative, or not needed and may not be funded. Entry into the acquisition phase can
be handled multiple ways:
   15.5.1. Portfolio Assignment. To identify appropriate PEO portfolio utilize the Portfolio
   Assignment process documented in AFI 63-101/20-101.
AFPAM63-128 10 JULY 2014                                                                       135


   15.5.2. Requirement Documentation. Document the requirement according to section above
   and related guidance and enter into acquisition phase through the JCIDS process, DBS
   process, or Cyberspace Infrastructure Planning System (CIPS) process.
   15.5.3. Integration with an existing program. Work with an existing program to include as a
   modification.
15.6. Acquisition Strategy. The Acquisition Strategy for an IT program should account for all
considerations contained within the AS template. This does not mean that the PM has to develop
a plan or strategy to comply with each consideration but show that each consideration has been
evaluated and appropriate justification for compliance, non-compliance, or applicability has been
presented. In addition, the PM should document the plan for integration of activities, processes,
and documentation. It is essential that the PM document this to ensure that expectations are
managed and agreements are formalized and documented. The PM should look for ways to
consolidate documents to ease review requirements and maintain configuration control.
15.7. Documentation. The PM should present a plan to integrate documentation and determine
applicability of regulatory documentation to the program. For example, if the program is a
software program it is more than likely that a corrosion prevention plan is not applicable to the
program. For items similar to this the justification should be presented in the Acquisition
Strategy and approval gained during the coordination of the strategy.
15.8. Open Systems Architecture. RESERVED
15.9. Integrated Testing. The PM should implement an integrated testing program to ensure
that test data and artifacts are used to satisfy multiple requirements. The program DT&E,
OT&E, IA, Interoperability, and other required testing should be integrated to share test events,
reuse the data, and manage test conditions to ensure the applicability of test data to fulfill
different requirements. Some best practices to follow when developing test plans for IT
solutions include:
   15.9.1. Component Build. The Component Build follows the DoDI 5000.02 activities.
   During Component Build (done one component at a time), the software components are built
   to provide the materiel solution. Organize, combine, validate, and regression test the software
   components to form the complete system. Organize and combine test materials to form a
   complete test package for component integration testing. Test the combined components and
   continue efforts to validate regression test conditions carried forward from earlier phases.
   Testing conducted during component build should continue to test system performance,
   security, and interoperability capabilities. Test planning for certification should continue for
   these three areas.
   15.9.2. System Build and Test. During the System Build and Test, verify that all
   components are integrated to function properly, and the system has been integrated with the
   supporting infrastructure. Stub-test the system interfaces before entering the system
   integration test. Leverage the test materials used during Component Build Testing, and any
   additional data needed to test system integration functions that would otherwise go untested,
   to the maximum extent possible. This test package consists primarily of transactions to test
   the functionality of the system, thus making comprehensive test results available for review
   and analysis by functional test personnel. Additionally, this same information is used to
   establish simulated operational conditions needed to test other functions (e.g., various types
136                                                             AFPAM63-128 10 JULY 2014


  of recovery), interoperability testing, and other capabilities that are dependent on activity
  being generated by the system. Through test automation, complete systems tests can be
  conducted and validated in a darkroom environment, thus making possible regression and
  compatibility testing at a level and within time constraints otherwise not attainable (e.g.,
  technical compliance network order (TCNO) validation by using applications, application
  compatibility with infrastructure changes, regression testing of sustainment activity). In
  addition to the System Build and Test specified above, also accomplish initial
  interoperability testing. Prior to this time, stub testing was used to validate the system
  interfaces; however, testing at this point should consist of the actual end-to-end
  interoperability test when feasible.
  15.9.3. Performance Evaluation Test. The Performance Evaluation Test evaluates factors
  like response time and capacity. Stress the system by executing a real or simulated load and
  compare performance as the load varies. It is important to create tests that are repeatable
  under a variety of possible field configurations.
  15.9.4. System Operability Test. Conduct the System Operability Test in a laboratory or a
  location that is isolated from the base network that is functionally equivalent and
  appropriately scaled to the target operational environment. If the operational test is conducted
  in an operational information environment or with live data, then the AF-DAA will have to
  provide an Interim Authorization to Test and ATC. If the IS has an appointed DAA or Lead
  DAA then the AF-DAA or authorized authorities based on MAC level should provide the
  ATC in conjunction with the IATT. Certify security, interoperability, performance, and
  functionality. The testing conducted is as near as possible to actual operating conditions.
  Testing begins by validating the test environment (hardware and software), loading and
  installing the release, and performing other activities needed to ready the environment for
  testing. Conduct the systems test using essentially the same test materials as used for system
  integration testing. This should be an automated test, with automated validation, and is
  intended primarily to support a realistic assessment of the system from a functional
  perspective. In addition, train functional personnel and provide hands-on time to test any
  functional conditions that remain outstanding and to build the confidence needed to support
  functional certification. Conduct integrated tests to support security, performance, and
  interoperability certification. Throughout the phase, identify and document defects, and
  return the system to the applicable phase for resolution. After correction, validate and
  regression test as part of their acceptance.
  15.9.5. Operational Test & Evaluation (OT&E). OT&E determines system operational
  effectiveness and operational suitability, and the operational impacts of fielding or employing
  a system across the full spectrum of military operations. OT&E also looks at doctrine,
  operational concepts, system performance, procedures, tactics, training, organization,
  personnel, logistics support elements, intelligence support elements, and materiel issues.
  Conduct OT&E in a realistic operational environment under actual operating conditions with
  representative users supporting the test. This testing determines if operational requirements
  are satisfied, and assesses system impacts to peacetime and combat operations. OT&E
  identifies and helps resolve deficiencies as early as possible, identifies the need for
  enhancements, and looks at changes in system configuration that alter system performance.
  The PM should look to structure early testing as much as possible to allow reuse of data and
  results for the next iteration of testing.
AFPAM63-128 10 JULY 2014                                                                         137


15.10. Modifications. RESERVED
15.11. Standardized Processes. Programs should explore grouping similar programs under
standardized processes. This concept should be explored for processes such as configuration
management, systems engineering, risk management, etc… Small programs can benefit from
standardized processes at the portfolio level and reduce documentation and process requirements.
   15.11.1. Systems Engineering. PMs should explore standardizing Systems Engineering
   processes within the organization by approaching the systems engineering technical and
   technical management processes in a consistent manner. Program supplements to the
   standard plans should only address aspects unique to the program, such as integrated product
   team structure and membership and which stakeholders control the various configuration
   baselines (allocated, functional, or product.). The management “processes” and
   organizational suite of tools and methods should be standard. Standardizing like systems
   should allow for cost and time savings by utilizing standard tools and reducing
   documentation
   15.11.2. Configuration Management.          The process by which a program maintains
   configuration control of its configuration items should be standardized. Two like systems
   should not have vastly differing ways of identifying configuration items, maintaining
   configuration control, conducting configuration reviews (e.g. CCBs), configuration
   databases, etc.Differences may arise due to the support strategy and the level of the baseline
   that the program controls (allocated, functional, etc. but this is implementation level activities
   not related to the standardized process.
15.12. IT Sustainment. IT sustainment begins when the initial increment reaches the Full
Deployment Decision. Programs in sustainment should follow procedures outlined during
development for the management of the program operations and support. This means that during
development of the increment, the program office has to define their configuration management
process, the deficiency reporting process, the patch process, the process to comply with security
vulnerabilities, process to maintain the IA certifications, etc. Programs should program for the
right mix of money to ensure that they have the appropriate color of money to maintain
capability versus investment funding to add capability. Defense Business Systems in
sustainment need to conform to the DBC certification process. In addition, when the program
modifies the system to add additional capability and utilizes investment funding they have to
follow the procedures in AFI 63-131 and AFI 63-101/20-101 to ensure that modifications that
add capability are managed in accordance with policy.
15.13. Clinger Cohen Compliance (CCA). CCA compliance and reporting applies to the
acquisition, management, operation, and closure of all AF IT investments, as well as to all
programs that acquire IT. This includes NSS, Space and non-space weapon systems and IT
systems acquisition programs, defense business systems, infrastructure, and intelligence systems.
CCA compliance should be an ongoing concern throughout the life cycle of an IT investment or
program acquiring IT. Depending on its type, a program should prepare and submit either a
CCA Compliance Report or CCA Compliance Table to confirm its compliance. The report or
table should be submitted with documentation that supports the program’s compliance with the
11 CCA compliance elements. Unless otherwise stated below, all CCA submissions should be to
the AF CIO CCA POC for review and confirmation. The guidelines for CCA reporting and
 138                                                           AFPAM63-128 10 JULY 2014


approval are summarized in AFMAN 33-407, Air Force Clinger-Cohen Act (CCA) Compliance
Guide.
   15.13.1. CCA Compliance Report. The report incorporates the CCA Compliance Table
   (Table 8 located in Enclosure 5 of DoDI 5000.02) and a corresponding narrative that
   describes the program’s compliance with CCA. The report should be no longer than 20
   pages and be accompanied by a transmittal e-mail to the AF CIO. A template for the CCA
   Compliance Report and preparation instructions are contained in AFMAN 33-407.
       15.13.1.1. ACAT I, ACAT II, and selected ACAT III programs prepare and submit a
       CCA Compliance Report, with supporting documents, to confirm compliance with the
       AF CIO before all milestone reviews and major contract awards.
       15.13.1.2. All MAIS and MDAP programs will also be confirmed by the DoD CIO.
   15.13.2. CCA Compliance Table. The table is the CCA Compliance Table from DoDI
   5000.02 (Table 9, Enclosure 1). The table contains a list of the 11 CCA compliance elements
   and a column for listing the source documents that demonstrate the program’s compliance
   with CCA. Preparation instructions for the CCA Compliance Table are contained in
   AFMAN 33-407.
       15.13.2.1. ACAT III defense business or financial systems business systems must
       prepare and submit a CCA Compliance Table, with supporting documents, to confirm
       compliance with the AF CIO before all milestone reviews, major contract awards, or
       NDAA certification and re-certification reviews.
       15.13.2.2. ACAT III programs that are business systems (under 1 million dollars per
       Defense Business System Management Committee (DBSMC) certification guidelines)
       must prepare and submit a CCA Compliance Table, with supporting documents, to
       confirm compliance with their MAJCOM or functional levels before NDAA certification
       or re-certification reviews.
   15.13.3. Sustainment Programs that are providing no new capability, are only required to be
   registered in EITDR. If a sustainment program undergoes a modification to provide a new
   capability, it must submit a CCA Compliance Table for approval.
   15.13.4. CCA Compliance Best Practices. Best Practices are listed below:
       15.13.4.1. CCA reporting utilizes many documents prepared earlier in the program life
       cycle as proof of compliance. For example, these items include those listed in Table 9,
       such as the ICD, CDD, AoA, EA, ISP, Information Assurance Strategy, APB,
       Acquisition Strategy, etc. If one of those documents is not available to prove compliance
       with a CCA element, a PM may utilize other documentation, actions, and or events that
       fulfill the intent of compliance with the CCA element. The item must be a primary
       source document that is coordinated, approved, and official. For Defense Business
       Systems, AFMAN 33-407 Attachment 8 provides an alternative table showing DBS
       program documentation that can be provided to support a CCA compliance submission.
       15.13.4.2. The CCA compliance process can be accomplished concurrently with the
       coordination of other documents. For example, the PM may conduct the CCA
       compliance process if the ISP is in coordination at the AF level. Begin working with
AFPAM63-128 10 JULY 2014                                                                     139


       SAF/A6PP early to ensure that reporting expectations are communicated throughout the
       process.
       15.13.4.3. It is recommended that the PM take into consideration the time required to
       obtain CCA confirmation of compliance when developing the project schedule, preparing
       program documentation, and approaching program milestones to ensure that obtaining
       CCA confirmation from the AF CIO’s office does not negatively impact program
       schedules. PMs are encouraged to submit drafts of the CCA Compliance Report/Table
       and supporting documentation to SAF/A6PP at least four months before the milestone
       review is scheduled to allow sufficient time for multiple review and revisions.
       Submissions may be made by e-mail or CD to the SAF/A6PP.
       15.13.4.4. Extra attention should be given to scheduling preparation of documents that
       directly support CCA compliance, such as the IAS and ISP. These two documents often
       take much longer to prepare than the four months allocated for a CCA compliance
       review, so an appropriate amount of time should be set aside ahead of time to ensure their
       completion before submission. Although the IAS may be submitted for review at the
       same time as the CCA Compliance Report/Table, the ISP should be submitted earlier as
       the ISP review process takes longer than the IAS or CCA compliance review process.
       Also, a program should be registered in EITDR before the CCA Compliance
       Report/Table is submitted for review. If a document is not available to prove compliance
       with a CCA element of the package, a PM may utilize other documentation, actions, and
       or events that fulfill the intent of compliance with the CCA element. The item must be a
       primary source document that is coordinated, approved, and official.
15.14. IA Implementation. Information Assurance should be an integral activity integrated
with acquisition, systems engineering reviews, DT/OT, and milestone entrance/exit criteria.
Information Assurance activities should be integrated throughout the life cycle of the program.
IA expertise should be represented on appropriate IPTs including the Integrated Test Team. IA
is implemented through the Risk Management Framework documented in DoDI 8510.01.
15.15. Information Support Plan (ISP). An Information Support Plan is a technical document
required for all Information Technology and National Security Systems (IT/NSS) that exchange
information of any type to other systems (e.g. not a standalone system or application); this
includes Commercial off-the-shelf (COTS) and Government off-the-shelf (GOTS) systems. The
ISP provides a means to identify and resolve implementation issues related to an acquisition
program's IT and NSS information infrastructure support and information interface requirements.
It identifies IT and information (including intelligence) needs, dependencies, and interfaces for
programs in all acquisition and non-acquisition categories, focusing on net-readiness,
interoperability, information supportability, and information sufficiency concerns. The ISP
documents the program's interoperability, data exchange, and support requirements, and it
addresses the interoperability and supportability shortfalls for the program of record and
proposes shortfall mitigation plans (as applicable). The ISP is the authoritative program
technical document used by AF and Joint Staff levels to determine a program’s eligibility for
Interoperability and Supportability (I&S) Certification.
   15.15.1. I&S Certification . I&S Certification is required for all IT and NSS, regardless of
   ACAT, with a goal of validating interoperability of all fielded systems. I&S assessment and
   certification of Net-Ready KPP compliance is granted after satisfactory Service-level and
 140                                                            AFPAM63-128 10 JULY 2014


   Joint review and approval. Establishing and maintaining Interoperability and Supportability
   in a DoD system is a continuous process that must be managed throughout the life cycle of
   the system. The ISP is the vehicle that facilitates this effort.
   15.15.2. Fielded Systems/Legacy System Modifications. Fielded Systems that exchange
   information of any type to other systems (e.g. not a stand-alone system or application) that
   did not develop an ISP or a Command, Control, Communications, and Computers
   Intelligence Support Plan (C4ISP) during the acquisition phase are required to develop a
   system level ISP if they undergo a major modification or change. A major modification
   occurs when any of the following criteria are met: initiating a changed/new capability to
   exchange information or data when the modification exceeds 10% of ACAT II minimum
   thresholds, $14 Million RDT&E, or $66 million Procurement in FY 2000 dollars. Or, when
   the modification results in a change to the JCIDS documents, or when the Net-Ready
   (Interoperability) KPP is changed or a new one developed. This includes any changes to the
   internal or external data exchange interfaces.
   15.15.3. Architectural Views. Architecture data is portrayed in various architectural views
   within the Information Support Plan. The architectural views within an ISP are driven by the
   JCIDS requirement for an NR-KPP as described in CJCSI 6212.01. The data should be used
   to complete the program analysis, and to develop operational test quality data. Analysis of
   the qualitative and quantitative sufficiency of C4ISP support (e.g., hardware, software,
   processes, etc.) should be accomplished in terms of the operational/functional capabilities
   that are being enabled. The ISP architecture views required for use in the Analysis section of
   the ISP are reflected in Table 15.1. (Taken from CJCSI6212.01F Enclosure B. Note: See
   Attachment 1 of this document for acronyms.)

Table 15.1. Required Architecture Data by Document.
AFPAM63-128 10 JULY 2014                                                                       141


  15.15.4. The Tailored ISP (TISP) option is available only to ACAT II, III and non-ACAT
  activities that meet specific criteria, and only by permission of Joint Staff/J6. The Joint Staff
  should "tailor" the ISP and advise the program office which artifacts need to be created,
  along with the time limitation for submitting the TISP for assessment. Program Managers
  may submit requests to develop the TISP to the AF Interoperability Office who should
  review, validate, and forward the request to OSD (NII) for approval. Detailed guidance and
  template are available on the DAU Acquisition Community Connection website.
  15.15.5. Information Support Plan Development Tools. The current version of the DoD and
  Joint approved Information Support Plan development tool should be used to create all ISPs.
  Information Support Plans created manually or using the Enhanced ISP (EISP) format should
  be transferred to the most current enterprise format at the first update of the document. For
  document requirements, guidance in developing the ISP, and download of the ISP
  development template (EISP), refer to the AF ISP Guide, CJCSI 6212.01or DoDI 4630.8.,
  Procedures for Interoperability and Supportability of Information Technology (IT) and
  National Security Systems (NSS).
  15.15.6. Best practices for development of the ISP primarily involve use of the Enhanced
  ISP development template (EISP). The EISP is approved for use on all AF networks, and is
  listed on the AF Approved Products List (Local policies may require network management
  assistance for download to desktops). AF programs should use the EISP template for
  generation of ISPs until the enterprise service version (ESV) becomes available. The EISP
  allows data entry through a formatted template that focuses the user on the program’s
  processes and critical information dependencies. The EISP consists of three Sections:
  Program Analysis, Program Overview, and Analysis Section which includes Process
  Analysis*, Net-Centric Assessment, Information Assurance Status, and Radio Frequency
  Spectrum Needs. (*Program Issues and Risks are an automatically generated product of the
  Process Analysis). Use of the EISP template should allow seamless distribution and
  coordination of program data among the AF, DoD and Joint communities for assessment and
  reference archiving, and should allow uninterrupted transition to the enterprise service
  version (web-based) development capability when fielded.
  15.15.7. ISP Staffing and Assessment. Information Support Plans are required at the
  following intervals of the Integrated Defense AT&L Life Cycle Management System, with a
  goal of granting I&S Certification for new and modified systems: a) All program
  documentation, including the Information Support Plan, that affects a program’s Request for
  Proposal (RFP) should be developed prior to permission being given to write or release the
  RFP; b) Milestone B (Initial ISP); c) Critical Design Review (Revised ISP) unless waived;
  d) Milestone C (ISP of Record). The ISP should first be submitted to the AF staffing system
  for assessment by AF subject matter experts for functional and operational compliance and
  supportability. Upon approval, a letter should be provided to the PMO authorizing
  submission of the documents to the Joint Staff review system for evaluation for
  Interoperability and Supportability Certification. I&S Certifications are granted for four
  years from the date of the approval letter, or until modifications are made to the external
  interface and data exchange capabilities of the system.
  15.15.8. ISP Waivers may be available to programs that meet the criteria cited in DoDI
  4630.8, Procedures for Interoperability and Supportability of Information Technology (IT)
  and National Security Systems (NSS), and CJCSI 6212.01. The requirement for an ISP may
 142                                                             AFPAM63-128 10 JULY 2014


   be waived when the requirement for JCIDS documentation has been waived. The categories
   of ISP waivers include: Systems in Acquisition, Legacy Waiver (Permanent), Legacy
   Waiver (Four Year), and Interim Certification to Operate (ICTO). Waiver requests must be
   submitted through the AF Interoperability Office to the appropriate OSD waiver approval
   authority. Guidance for waiver requests is located on the AF ISP website.
15.16. Records Management. If an IT investment has data that is used for the business of the
AF, the data is generally considered to be records. Records created and received in the AF, are
Federal records and are required by law to have a properly approved scheduled disposition, i.e., a
plan approved by the National Archives and Records Administration (NARA) on what should be
done with the records when the records no longer have business value to the AF. Hence, AF IT
investments with data must be scheduled for disposition and the data managed in accordance
with its approved disposition. National Security and classified systems are not exempt for the
scheduling requirement. IT infrastructure is exempt. Evidence that an IT investment is
compliant with Records Management is demonstrated when the data in the IT investment is
properly scheduled with an approved records disposition; the records disposition cited in Item 7
on AF Form 1341, Electronic Record Inventory, is validated by the AF Records Office; and
confirmation is received that the cited records disposition can be executed in the IT investment.
If an applicable disposition from a “Table and Rule” (one or more) is not available from the Air
Force Records Disposition Schedule or from a “General Record Schedule Item” from the NARA
General Records Schedules, the Program Manager of the IT investment should follow the
guidance on developing a disposition schedule by completing and coordinating an AF Form 525,
Recommended Records Disposition, in accordance with AFMAN 33-363, Management of
Records (Chapter 6). Guidance for preparing the AF Form 1341, Electronic Record Inventory, is
located on the AF Records Management website. Instructions on completing the AF Form 525
are found in Chapter 11 of AFI 33-364, Records Disposition - Procedures and Responsibilities.
Records Management considerations should be integrated throughout the life cycle of the
program and not considered as an afterthought.
   15.16.1. Milestone A. The PM should submit the AF Form 1341 (survey of the data and
   system characteristics of the IT investment) or its forthcoming equivalent in EITDR. The
   intent is to begin identifying the types of data (records) to be captured in the IT investment
   and to commence evaluating the expected usefulness of the data to the AF, e.g., 1 year, 3
   years, 25 years, etc., and to start determining how to dispose the data at the end of the
   retention period.
   15.16.2. Milestone B. The PM should finalize the AF Form 1341 and confirm the types of
   data (records) that should be collected. If a valid Table and Rule or GRS Item (and what is
   GRS) is not identified, the PM should complete and coordinate the AF Form 525.
   15.16.3. Milestone C. The PM should have submitted the AF Form 1341 and if needed AF
   Form 525 to the AF Records Office via the AF Records Management hierarchy (Command
   Records Manager or Agency Records Manager). The PM should factor 6 months before the
   operational date to ensure time for NARA to approve the proposed records disposition
   schedule for the IT investment’s data.
   15.16.4. Full Rate Production/Deployment. The PM should have the AF Form 1341 and, if
   needed, the AF Form 525 completed and approved by the AF Records Office prior to the
   operational date of the IT investment. If the IT investment should have personally
AFPAM63-128 10 JULY 2014                                                                     143


  identifiable information (PII) that is retrieved by name or personal identifier, a Privacy Act
  System of Records Notice (SORN) must be prepared and published in the Federal Register.
  (See AFI 33-332, The Air Force Privacy and Civil Liberties Program) The records
  disposition on the AF Form 1341 and, if needed, the AF Form 525, should match the records
  disposition found in the Retention and Disposal section of the SORN.
  15.16.5. Sustainment. The PM is required to manage the data in the IT investment in
  accordance with AFMAN 33-363 and dispose the data in accordance with its NARA-
  approved records disposition. An alteration to the approved records disposition in light of
  changing business needs is accomplished through the completion and coordination of an
  updated AF Form 525.
  15.16.6. Disposal/Decommission. While the hardware and software of the IT investment
  may readily be disposed of, the same is not necessarily true for the data within the IT
  investment. The data can only be disposed in accordance with its NARA-approved
  disposition. Activities that should occur include:
     15.16.6.1. If the data from the decommissioned IT investment is being transferred in
     whole to a succeeding IT investment, the PM should provide to the AF Records Office an
     addendum to the approved AF Form 1341 regarding where the data should reside and
     what is the succeeding IT investment.
     15.16.6.2. If the data from the decommissioned IT investment is not being transferred in
     whole to a succeeding IT investment, such data must be kept on a storage media until its
     approved records disposition transpires. If the data has a temporary disposition, the AF is
     responsible for ensuring its accessibility due to evolving computing and storage
     technologies (e.g., storing the data in a flat file). If the data has a permanent retention,
     the data can be pre-accessioned to the National Archives and Records Administration,
     which should be responsible for ensuring its accessibility. Just because the IT investment
     is being decommissioned does not alleviate the AF from its responsibility in providing
     data due to Freedom of Information Act (FOIA) requests, litigation holds, and electronic
     discovery (e.g. e-Discovery).
     15.16.6.3. Data that is also PII and is retrieved from the decommissioned IT system by
     name or personal identifier should require an amendment or deletion to its Privacy Act
     System of Records Notice per paragraph C6.5. of DoD 5400.11-R, Department of
     Defense Privacy Program. The Program Manager should work with the base or
     MAJCOM Privacy Office on the SORN per AFI 33-332.
     15.16.6.4. Archive System in EITDR.
     15.16.6.5. References:
         15.16.6.5.1. Federal Records Act of 1950
         15.16.6.5.2. e-Government Act of 2002
         15.16.6.5.3. 18 USC 2071 (Fines, imprisonment regarding concealment, removal, or
         mutilation of records)
         15.16.6.5.4. Office of Management and Budget (OMB) Circular A-130 - paragraph
         8a(1)(k)
 144                                                           AFPAM63-128 10 JULY 2014


          15.16.6.5.5. National Archives and Records Administration guidance – 36 CFR
          1236, 36 CFR 1236.26, NARA Bulletin 2010-02
          15.16.6.5.6. DoD Records Management policy - paragraph 5.3.6. of DoDD 5015.2,
          DoD Records Management Program; the Defense Acquisition Guidebook; section 6
          of DoDI 5000.02, Operation of the Defense Acquisition System
          15.16.6.5.7. AF Records Management policy – Chapter 6 and paragraphs 6.3.1 and
          6.5 of AFMAN 33-363, Management of Records; Chapter 11 of AFI 33-364, Records
          Disposition - Procedures and Responsibilities, paragraph 3.107.1.1. of AFI 63-
          101/20-101.
15.17. Privacy. The PM should ensure information assurance controls are implemented to
protect personally identifiable information (PII). The PM should ensure a Privacy Impact
Assessments (PIA) has been approved on IT Systems that maintain, use, store, and/or
disseminate PII and a System of Records Notices (SORN) is published in the federal register
when information in retrieved by an individual’s name, SSN, or a unique identifier for a system
to be in compliance with the E-Government and Privacy Act.
   15.17.1. SSN Usage. The PM should submit a Memorandum of Justification signed by the
   first Senior Executive Staff (SES) or General Officer (GO) address to DoD Privacy Office
   for approval prior to any collection of SSN in accordance with AFI 33-332, The Air Force
   Privacy and Civil Liberties Program.
   15.17.2. General Public. The PM should insure that all forms uploaded into the IT system
   have an approved OMB Control Number when collecting personal information on ten or
   more in the general public within a 12 month period. The OMB Control Number is required
   to be annotated in the PIA. Refer to DoD 8910.1-M, Department of Defense Procedures for
   Management of Information Requirements, for definition of general public and guidance of
   OMB Control Number.
   15.17.3. PIA.
       15.17.3.1. PM should submit PIA to their privacy officer for review and signature.
       15.17.3.2. PM’s privacy officer should forward PIA to the AF Privacy Officer at
       usaf.pentagon.saf-cio-a6.mbx.af-privacy@mail.mil to obtain AF IA and CIO
       signature.
       15.17.3.3. AF Privacy Officer should forward approved PIA to PM, privacy officer and a
       copy to DoD CIO on IT systems that collect information on the General Public.
       15.17.3.4. AF Privacy Officer should update PIA information in EITDR.
       15.17.3.5. AF Privacy Officer should post approved PIA on the AF Privacy public
       website http://www.privacy.af.mil/.
       15.17.3.6. PM should review PIA(s) 90 days prior to the expiration date of the systems’
       Authority to Operate (ATO); send updated PIA(s) to usaf.pentagon.saf-cio-a6.mbx.af-
       privacy@mail.mil
       15.17.3.7. PM should notify the AF Privacy Officer usaf.pentagon.saf-cio-a6.mbx.af-
       privacy@mail.mil when the system is being decommissioned to removed PIA from the
       AF Privacy public website
AFPAM63-128 10 JULY 2014                                                                   145


   15.17.4. SORN.
       15.17.4.1. PM should research the Defense Privacy Notice websites for an existing
       SORN that should support the IT system prior to adding a new SORN.
       15.17.4.2. PM should forward SORN to their privacy officer for review
       15.17.4.3. MAJCOM privacy officer should forward SORN to the AF Privacy Officer
       usaf.pentagon.saf-cio-a6.mbx.af-privacy@mail.mil
       15.17.4.4. AF Privacy Officer should forward SORN to the DoD Privacy Office for
       review and submission to the Federal Register office for publishing.
       15.17.4.5. DoD Privacy Office should notify the AF Privacy Officer when the SORN has
       been published to the Federal Register.
       15.17.4.6. AF Privacy Officer should notify the MAJCOM Privacy Officer when the
       SORN has been published.
       15.17.4.7. AF Privacy Officer should update EITDR.
       15.17.4.8. PM should review SORN(s) 90 days prior to expiration of the systems ATO.
   15.17.5. Activities at Milestone A (or approximately 25% Solution) include:
       15.17.5.1. Identifying types of PII data being collected identified
       15.17.5.2. Drafting PIA/SORN
   15.17.6. Activities at Milestone B (or approximately 85% Solution) include:
       15.17.6.1. PIA/SORN submitted for approval (SSN Justification Memo submitted if
       required)
       15.17.6.2. Privacy question in EITDR answered as applicable
   15.17.7. Activities at Milestone C (or approximately 95% Solution)
       15.17.7.1. PIA/SORN approved
       15.17.7.2. Privacy question in EITDR completed
   15.17.8. At Full Rate Production/Deployment (100% Solution) the PM should be privacy
   compliant.
   15.17.9. Disposal/Decommission. Data that is personally identifiable information and is
   retrieved from the decommissioned IT system by name or personal identifier should require
   an amendment or deletion to its Privacy Act System of Records Notice per paragraph C6.5.
   of DoD 5400.11-R, Department of Defense Privacy Program. The Program Manager should
   work with the privacy officer on the SORN.
   15.17.10. Archive System in EITDR
15.18. Chief Financial Officer (CFO) IT Compliance. The purpose of the CFO Act
Compliance is five-fold: Bring more effective general and financial management practices to the
Federal Government; Improve systems of accounting, financial management, and internal
controls in each agency of the Federal Government; Assure the issuance of reliable financial
information; Provide for the production of complete, reliable, timely, and consistent financial
 146                                                             AFPAM63-128 10 JULY 2014


information for use by the executive branch of the Government and the Congress in the
financing, management, and evaluation of Federal programs; and Deter fraud, waste, and abuse
of Government resources.
   15.18.1. Public Law. The following Federal Laws are pertinent to CFO compliance:
       15.18.1.1. Chief Financial Officers (CFO) Act of 1990 (P.L. 101-576) - Requires all
       federal agencies, including the AF, to prepare annual financial statements that conform
       with generally accepted accounting practices and are certified by the department or
       agency inspector general or auditor general. (Title III - Sections 303 & 304)
       15.18.1.2. Government Performance and Results Act (GPRA) of 1993 (P.L. 103-62)
       provides for the establishment of strategic planning and performance measurement in the
       federal government.
       15.18.1.3. Government Management Reform Act (GMRA) of 1994 (S. 2170 (103rd))
       requires agency-wide audited financial statements for all agencies covered by the CFO
       Act.
       15.18.1.4. Federal Financial Management Improvement Act (FFMIA) of 1996 (P.L. 104-
       208) provides for consistent accounting by an agency from one fiscal year to the next,
       and uniform accounting standards throughout the federal government. Requires federal
       financial management systems to support full disclosure of federal financial data,
       including the full costs of federal programs and activities, to the citizens, the Congress,
       the President, and agency management, so that programs and activities can be considered
       based on their full costs and merits.
   15.18.2. DoD Policy/Guidance. At present, there is only one DoD guidance document
   governing CFO Compliance - OUSD(C) Financial Improvement and Audit Readiness
   (FIAR) Guidance. It is updated annually and provides instructions for implementing a
   consistent, Department-wide plan toward achieving the DoD’s financial improvement and
   audit readiness objectives. It also defines the department's goals, strategy and methodology to
   becoming audit ready. Moreover, it details the roles and responsibilities of reporting entities
   (i.e., IT program offices) and service providers, as well as the processes they should use to
   achieve audit readiness.
       15.18.2.1. Roles and Responsibilities.
          15.18.2.1.1. Reporting Entities (MAJCOMs/HAF Functional Offices) execute FIAR
          Plan and Financial Improvement Plans (FIPs), perform the Discovery & Evaluation
          tasks, test and strengthen internal controls, and correct deficiencies, design and
          implement control activities to limit the risk of material misstatements by meeting the
          key control objectives, and support account balances with sufficient and appropriate
          audit evidence (documentation).
          15.18.2.1.2. Service Providers are responsible for providing a description of their
          controls that may represent or affect their customer reporting entities’ control
          environment, risk assessment, control activities, information systems, and monitoring
          activities. The description of controls should be presented at a level of detail that
          provides reporting entity auditors with sufficient information to assess the risks of
AFPAM63-128 10 JULY 2014                                                                 147


        material misstatement. See Section 3.B.4 in the FIAR Guidance for a complete list of
        the internal controls to be described.
        15.18.2.1.3. Service Providers and Reporting Entities provide access to subject
        matter experts or contractors supporting those organizations in agreed upon
        timeframes. Agree on rules for the creation, completion, and retention of supporting
        documentation for service provider-affected financial transactions. This includes, by
        business process and transaction type, defining which organization should retain
        specific documents, and establishing the retention period for the documents.
     15.18.2.2. FIAR Requirements:
        15.18.2.2.1. Evaluation and Discovery. Management documents its business and
        financial environment, defines and prioritizes its processes into assessable units,
        assesses risks and tests controls, evaluates supporting documentation, identifies
        weaknesses and deficiencies, and defines its audit readiness environment.
        15.18.2.2.2. Corrective Action. Management develops and executes CAPs that
        include implementation of the audit ready environment, solutions to resolve
        deficiencies and weaknesses, identification of resources required and committed, and
        tests and strengthens internal controls Requirements.
        15.18.2.2.3. Evaluation. Management evaluates corrective action effectiveness
        through testing and determines whether it is ready to assert audit readiness.
        15.18.2.2.4. Assertion. Management prepares documentation and asserts audit
        readiness to the OUSD(C) and DoD Office of Inspector General (OIG).
        15.18.2.2.5. Sustainment. Management maintains audit readiness through risk based
        periodic testing of internal controls utilizing the OMB Circular A-123, Appendix A,
        processes and procedures, and resolves any identified weaknesses timely (e.g., before
        the next annual reporting cycle).
        15.18.2.2.6. Validation. OUSD(C) and DoD OIG review management’s assertion,
        and auditors perform an examination on audit readiness assertion.
        15.18.2.2.7. Audit. Reporting entity engages an auditor and supports the audit of
        assessable unit or financial statements.
        15.18.2.2.8. Requirements for Service Providers illustrated in Figure 30, Section
        3.B.2 of the FIAR Guidance.
  15.18.3. CFO Compliance Criteria. A system is CFO IT compliant after it has undergone a
  review of IT controls IAW with the Federal Information System Controls Audit Manual
  (FISCAM); undergone a review of CFO IT Compliance questions/answer in EITDR; and
  been issued a CFO IT Compliance Certification Letter signed by SAF/FMPA. Additionally:
     15.18.3.1. Management must have proof that the information reflected in the financial
     statements comes from reliable data sources that conform to Federal system and
     accounting standards;
     15.18.3.2. An external auditor is able to trace information contained in AF financial
     statements back to the original authorized transaction; and
148                                                             AFPAM63-128 10 JULY 2014


      15.18.3.3. Confidence regarding the reliability of data associated with the management
      controls and accounting processes in a compliant system must be supported by
      documented evidence.
  15.18.4. Document Requirements for Program Management Offices. Fulfillment of
  documentation requirements are accomplished by providing information in in EITDR
  (EITDR question numbers shown in parentheses):
      15.18.4.1. Results of DoD Information Assurance Certification and Accreditation
      Process (DIACAP), Federal Information Security Management Act (FISMA), and
      Federal Information System Controls Audit Manual (FISCAM) controls (EITDR
      Question CFO3).
      15.18.4.2. Link to the most recent document that describes and indicates the control of
      data processing, and the management and reporting of the system (CFO4).
      15.18.4.3. Link to the document that identifies and describes how error conditions are
      handled (CFO5).
      15.18.4.4. Link to most current user access control list (not older than 30 days) (S38a).
      15.18.4.5. Link to system root Access Control List (ACL) (S38b).
      15.18.4.6. Link to the IA Control Interconnection Documentation (DCID-1) (CFO6).
      15.18.4.7. Link to the Security Technical Implementation Guide & Configuration
      Management Diagrams (DCCS-1) (CFO7).
      15.18.4.8. Link to SV-1, “Systems Interface Description” (interconnectivity diagram)
      (A130).
      15.18.4.9. Link to the document that indicates how the transfer of data to/from source
      system is authorized/controlled (A8a).
      15.18.4.10. Link to the System Identification Profile (SIP) (S63).
      15.18.4.11. Link to the Information Support Plan (ISP) (S64a).
      15.18.4.12. Link to most current POA&M (S209a).
      15.18.4.13. CFO Compliance Process Report from EITDR. Before you create the report,
      be sure you have answered EITDR questions A130, A7, A8, A8a, CFO8, and C40 in the
      CFO IT Compliance Filter. Also, attach to the email a copy of the System Identification
      Profile (SIP) if it is not readily available in eMASS, the AF C&A workflow tool.
  15.18.5. Review/Approval Process.
      15.18.5.1. Program Office answers CFO IT Compliance questions in EITDR, generates
      CFO IT Compliance Report, and submits report to SAF/FMPS.
      15.18.5.2. SAF/FMPS:
         15.18.5.2.1. Reviews/validates documented IT General and Application Controls
         submitted by the Program Management Office (PMO).
         15.18.5.2.2. Verifies DIACAP/FISMA IA controls have been validated by the AF
         Certifying Authority.
AFPAM63-128 10 JULY 2014                                                                  149


          15.18.5.2.3. Reviews/validates architecture documents/controls.
          15.18.5.2.4. Submits CFO IT compliance assertion package to SAF/FMPA.
      15.18.5.3. SAF/FMPA (Audit Readiness):
          15.18.5.3.1. Reviews CFO IT compliance assertion package.
          15.18.5.3.2. Aligns IT system to processes being asserted and provides timeline for
          audit of the system by the AFAA.
          15.18.5.3.3. Prepares CFO IT Compliance Memo (addressed to SAF/FMP & AFAA;
          copy to SAF/FMPS) and posts to FIP SharePoint site.
      15.18.5.4. SAF/FMPS provides copy of CFO IT Compliance memo to PMO.
      15.18.5.5. SAF/FMPS provides link to signed CFO IT Compliance memo in EITDR.
      15.18.5.6. CFO IT compliance certification should be reviewed annually as part of the
      SAF/A6 Investment Review process or as part of the MAJCOM/HAF Functional Review
      process.
      15.18.5.7. System audits should be coordinated with Program Managers through the
      SAF/FMPA office. Upon completion of audits aligned to the AF Financial Improvement
      Plan (FIP), systems should be certified as CFO Compliant.
   15.18.6. Milestone A (25% Solution)
      15.18.6.1. Determine whether system should be a financial or financial feeder (mixed)
      system.
      15.18.6.2. Determine whether system should directly or indirectly feed data that impacts
      the AF financial statements to another system.
   15.18.7. Milestone B (85% Solution)
      15.18.7.1. Answer CFO IT Compliance questions in the EITDR, as applicable.
      15.18.7.2. Generate CFO IT Compliance Report and submit to SAF/FMPS for review.
   15.18.8. Milestone C (95% Solution). CFO IT Compliance Report reviewed/approved.
   15.18.9. Full Rate Production/Deployment (100% Solution). The IT system should be CFO
   IT Compliant and the PMO should have a CFO IT Compliance memo on file signed by
   SAF/FMP and/or the AFAA.
   15.18.10. Disposal/Decommission. Follow applicable AF policies for disposition and
   decommissioning of an information system. Be sure to notify the appropriate IT Portfolio
   Management Office.
   15.18.11. Archive System in EITDR.
15.19. Nuclear RESERVED
15.20. Centers of Excellence/Resources
   15.20.1. Software Engineering Institute COTS Integration. In support of IT programs, the
   PM should consider the use of the Software Engineering Institute (SEI) developed
   Evolutionary Process for Integrating COTS-Based Systems (EPIC). EPIC has been used
 150                                                              AFPAM63-128 10 JULY 2014


   successfully in both the AF and the commercial world and currently represents a best practice
   for COTS or GOTS based programs. EPIC is a structured, disciplined process that is highly
   tailorable depending on the project. It sets forth descriptions of the various phases of
   activities, specific exit criteria, phase activities, and supporting activities. It still requires
   judgment to affect tailoring, but the combination of phase descriptions, exit criteria, phase
   and supporting activities enables a PM to arrive at a reasonable conclusion as to which
   activities need to be accomplished in depth and which activities can be streamlined.
   Additional information can be found at: http://www.sei.cmu.edu/reports/02tr009.pdf.
   15.20.2. Application Software Assurance Center of Excellence (ASACoE) – The mission of
   the ASACoE is to: (1) Foster security into the software development life cycle (SDLC) and
   software acquisitions through techniques, tools, and education; (2) Leverage information
   technology, through the deployment of practices and automated tools, to support and improve
   AF software development processes; (3) Take advantage of software assurance state-of-the-
   art information technology and industry best practices; and (4) Shield and defend
   applications against potential attacks. ASACoE provides a resource for the PM to
   accomplish software development and testing for applications and software.
   15.20.3. Air Force Network Integration Center (AFNIC) is the AF's Certification Authority
   within the Air Force Certification and Accreditation Program (AFCAP). AFNIC also
   provides a one-stop-shop for AF-level IA policy. The newest addition, AF Certification and
   Accreditation Program (AFCAP) Primer, provides a good overview of the AFCAP which is
   beneficial to anyone attempting to learn the C&A process from the bottom up.
15.21. Space Systems. Program Managers should consider the following best practices for the
type of system they are acquiring. This is not mandatory guidance but lists considerations, best
practices and direction while managing a program.
   15.21.1. Independent Program Assessment (IPA). The primary purpose of the IPA is to
   provide independent, objective advice to the MDA about the readiness of the program to
   proceed into the next phase of acquisition. The IPA was created as “the type of high-level
   review and analysis that the DoD Space MDA would do personally if time were available”.
   An IPA consists of a review of the program's plans for entering the next acquisition phase
   (and the rest of the life cycle), assessing the risks associated with those plans, and providing
   recommendations for improving those plans. The IPA Lead completes the assessment by
   delivering the IPA briefing to the MDA. Following this, the MDA should make decisions at
   or after the Defense Acquisition Board (DAB) or other assessment review meetings, and
   documents decisions in the Acquisition Decision Memorandum (ADM) for the program.
       15.21.1.1. IPA Scope. IPAs focus on an integrated view of the entire program, i.e., not
       only on the technical aspects of the program(s), but include all of the programmatic (i.e.
       cost, schedule, etc.) considerations crucial to measuring program success and improving
       program execution. IPAs can be expanded in scope to review, not only the individual
       program, but also provide an additional integrated review of a DoD capability it supports.
       This technique can be instrumental in highlighting the lack of synchronization on several
       multi-Service ACAT I programs and provide synthesized, corrective-action
       recommendations to assure an improved delivery of capability to the warfighter.
       15.21.1.2. IPA Scheduling and Funding.
AFPAM63-128 10 JULY 2014                                                                   151


        15.21.1.2.1. Conducted over a course of three to four weeks
        15.21.1.2.2. Begins about two months prior to each milestone
        15.21.1.2.3. Fully funded by the SPO, yet is wholly independent of their influence
     15.21.1.3. IPA Team and Support.
        15.21.1.3.1. The Milestone and IPA process begins when the program manager
        requests, through acquisition channels, a milestone date from the MDA. The MDA
        establishes the milestone date and appoints an IPA Lead.
        15.21.1.3.2. The IPA Lead may be a government employee, who does not have a
        conflict of interest associated with the program. The IPA Lead is normally a retired
        flag officer or civilian equivalent, who has been a program manager of a space-related
        program.
        15.21.1.3.3. The IPA Lead selects the IPA team members. To avoid conflict of
        interest and to maintain independence, the IPA Lead and IPA team members cannot
        be drawn from the staff of the program manager or the SPO director. Best practice
        over the last five years has been the use of retired military members or civilian
        executives from defense contractors with broad program execution experience, as
        core members of the IPA team. Typically, six or seven of the 20 team members are
        from this group. The remaining team members are government and Federally Funded
        Research and Development Centers (FFRDC) subject matter experts.
     15.21.1.4. IPA Criteria consist of well-defined assessment criteria and optional questions
     for the program manager to use in preparing for the IPA and for the use of the IPA team
     in making their assessments, findings, and recommendations. Future assessment should
     be structured around the Acquisition Strategy outlines. In addition, the MDA may
     choose to add special emphasis criteria before the review, to target areas of particular
     concern or interest. Based upon lessons learned from many past IPAs, Aerospace
     Corporation has prepared an IPA and Milestone Evaluation Guide (Aerospace Report
     TOR-2011(8591)-6, available to government employees by request through the Director,
     SMC ACE).
     15.21.1.5. IPA Key Values.
        15.21.1.5.1. Independence with no political influence
        15.21.1.5.2. Consistent criteria for every review (including requirements,
        architecture, acquisition strategy, schedule, risk, technology maturity, systems
        engineering, cost, resource management, program protection, test and evaluation, data
        management, sustainment, and others as directed by the MDA)
        15.21.1.5.3. Integration across all aspects of the program or capability
        15.21.1.5.4. Identification of risk
        15.21.1.5.5. Corrective recommendations to accompany every negative finding
        15.21.1.5.6. IPA Lead/team coaching/mentoring of the Program Manager and staff
        15.21.1.5.7. Consistency of IPA team members from one IPA to the next on the same
        program
 152                                                           AFPAM63-128 10 JULY 2014


       15.21.1.6. Program Office Support to IPA
           15.21.1.6.1. Program manager provides documentation addressing legal and
           regulatory requirements
           15.21.1.6.2. Program manager provides locally generated plans and documents
           which support the program's activities for the next acquisition phase and future
           phases
           15.21.1.6.3. Program manager provides briefings on subjects, grouped into
           assessment areas. Briefings should be interactive with IPA members
           15.21.1.6.4. Assessment areas are consistent with content of the AS and address the
           following areas; Acquisition Approach (including Top-Level Strategy), Source
           Documentation (including past direction and Congressional input), Capability Need
           (including AoA, Preferred Alternative, Enabling Concept, and Technical
           Requirements Document), Top-Level Integrated Schedule (including IMS),
           Interdependency and Interoperability (including Architectures), Risk and Risk
           Management, Technology Maturation, System Engineering Approach (including SEP
           and PESHE), Industrial Capability and Manufacturing Readiness, Business Strategy,
           Resource Management (including program office staffing, Cost drivers, POE/SCP,
           FYDP, and EVM), Program Protection Planning (including CPI, PPP, and IA
           Strategy), Test and Evaluation (Developmental and Operational), Data Management
           Strategy, Life Cycle Sustainment Plan, Clinger Cohen Act, Other, and
           Recommendations.
           15.21.1.6.5. Assessment areas and the scope, content, and agenda of the IPA can be
           tailored to the needs of the IPA Lead and team
       15.21.1.7. Final Recommendation. The IPA Lead is responsible for the final
       recommendations to the MDA and also provides recommendations to the program
       manager.
           15.21.1.7.1. The IPA Lead takes input from all IPA team members and summarizes
           the key risks, findings, and recommendations in the briefing. For each finding, the
           team provides a backup slide which describes the finding, observations and relevant
           facts, recommendation(s), and an OPR if possible.
           15.21.1.7.2. The IPA Lead gives frequent opportunities during the three to four
           weeks of face-to-face meetings, for the SPO to provide additional facts and
           documentation and for the SPO to challenge the IPA findings and recommendations.
           15.21.1.7.3. The IPA briefing is not intended as a stand-alone briefing; it is
           developed as a companion briefing to accompany the Program milestone briefing.
           The Program Office is encouraged to prepare and present a response to the IPA for
           presentation at the milestone meeting.
15.22. Quick Reaction Capability. QRC acquisition programs are specifically designated by
the MDA for specialized procedures. These programs respond to urgent needs as documented by
validated urgent operational needs (UON), Joint UONs (JUON), and Chief of Staff top-down
direction. QRC programs do not operate outside of normal acquisition procedures, but rather
delegate certain authorities and fully leverage regulatory tailoring to satisfy near-term urgent
AFPAM63-128 10 JULY 2014                                                                      153


warfighting needs. Rapid acquisition is accomplished through the use of cross-functional teams,
tightly scoped requirements, higher risk thresholds, concurrent activities, delegated authorities,
and a standardized process. Program Managers should consider the following best practices for
the type of system they are acquiring. This is not mandatory guidance but lists considerations,
best practices and direction while managing a program. Key tenets of QRC programs include:
   15.22.1. Delegations of authority for ACAT II/III QRC programs: The MDA is
   automatically delegated to the relevant PEO.
   15.22.2. Limited formal MDA reviews: QRC teams need to focus the bulk of their energy
   towards solving the warfighter problem. Therefore, formal reviews should be kept to a
   minimum. The QRC process only prescribes two mandatory MDA reviews: an MDD to
   enter the process and the Capability Transition Review (CTR) to exit the process and
   determine further disposition of the solution (further development, sustainment, or
   demilitarization and disposal).
   15.22.3. Deferred documentation. QRC programs should defer most regulatory
   documentation and detailed planning until at least the CTR. Major deferrals should be
   documented in an ADM.
   15.22.4. Expedited staffing. When reviewing QRC program documentation, functional
   organizations and commands must treat QRC programs as higher priority than non-QRC
   programs. QRC programs by definition are essential to the fulfillment of an urgent need and
   should receive immediate attention. It is expected organizations should use streamlined
   staffing mechanisms to support the short timelines associated QRC programs.
   15.22.5. Rapid delivery – QRC/UON. Quick Reaction Capability (QRC) programs provide
   rapid solutions to validated warfighter urgent needs. By their very nature, urgent needs are
   operational gaps associated with loss of life or an inability to complete critical missions.
   Urgent needs are documented as validated urgent operational needs (UON), Joint UONs
   (JUON), and Chief of Staff top-down direction. Schedule is paramount when executing a
   QRC program, and both cost and performance should be traded off respectively. QRC
   program teams must be able to think creatively and develop simple, quick responses. Every
   day lost to non-value added bureaucratic tasks or over-design is a day lost to the warfighter.
   Ideally, QRC programs should field an initial capability within 180 days of urgent need
   validation. This aggressive schedule requirement is intended to encourage simple solutions.
   The discussion below highlights key QRC enablers.
   15.22.6. Oversight. MDA for ACAT II and III QRC programs is automatically delegated to
   the PEO to allow local, rapid decision-making. In addition, the process only specifies two
   formal MDA reviews: a MDD and a Capability Transition Review (CTR). The MDD is the
   entrance ramp to QRC acquisition and the CTR is the exit ramp to determine the QRC
   solution’s future state. Depending on the situation, these reviews may be virtual, though the
   MDA still needs to document the decisions made in an ADM. The QRC process is designed
   to push authority to the lowest levels and ensure the PM has appropriate freedom of action to
   execute an MDA-approved COA.
   15.22.7. Scope. To quote Gen George S. Patton, “A good plan, violently executed now, is
   better than a perfect plan next week.” QRC programs, though not limited in policy to a
   specific ACAT threshold, are not intended for large-scale or complex solutions. To field
154                                                              AFPAM63-128 10 JULY 2014


  rapidly, PMs must aggressively question requirements and deflect pressures to deliver more
  capability than is absolutely needed to mitigate the identified gap. Following initial fielding,
  there should be time to examine further increments of capability. Note: The implementing
  command or Chief of Staff should endorse a course of action that extends beyond 180 days
  from urgent need validation to initial fielding. Long (greater than 180 days) schedules
  should be the rare exception.
  15.22.8. Testing. QRC programs must craft very limited and well-integrated test strategies
  focused on defining the capabilities and limitations of the solution. The testing should
  provide knowledge to the lead command, warfighter, and PM, to determine whether the
  solution is a militarily useful capability. Under normal circumstances, the test organization
  would submit a final test report prior to a programmatic decision to field a capability. QRC
  programs are tightly constrained by schedule and should likely use interim raw data to assess
  a capability’s readiness for fielding. If the testing does not uncover critical issues that would
  preclude fielding, the lead command and PM should execute the fielding plan prior to receipt
  of final test reports.
  15.22.9. High Performance Teams (HPT). Successful rapid capability programs exhibit
  close-knit cross-functional teams and a high degree of trust. The QRC team should include
  key stakeholders and any functional organizations needed to speed along certifications and/or
  waivers. The lead command and PM are responsible to fully leverage the HPT and ensure
  seamless communication between the members.
  15.22.10. COAs. QRC COAs are streamlined acquisition strategies which include the
  HPT’s approach for development, fielding, and initial sustainment. The HPT must
  coordinate an integrated strategy that considers those threshold activities required to quickly
  provide a capability to the warfighter. Critical path activities must be worked in parallel to
  mitigate the risk of schedule delays. In support of a MDD, the COA should normally be
  presented to the MDA in briefing format and appended to the ADM as formal
  documentation. It is important to note the MDD must occur within 45 days of urgent need
  validation, and the COA may not be complete at that point. The MDA should document the
  intent for incomplete documentation and analyses, and defer non-value added activities until
  at least after initial fielding. The primary intent of the MDD is to provide reasonable
  assurance to the MDA that the team understands the requirements and has developed an
  appropriately aggressive COA to meet the warfighter need.
  15.22.11. Risk. The MDA must have a higher risk tolerance for QRC programs and be
  willing to leverage all regulatory/statutory authorities to field a rapid solution. The MDA
  needs to coordinate with the lead and warfighting commands, and build a strategy that
  manages risk within tight constraints.
  15.22.12. Additional information about QRCs is in AFI 10-601.
AFPAM63-128 10 JULY 2014                                                                      155


                                          Chapter 16

                                 PROGRAM STRUCTURE

16.1. Increments. The decision on how to structure a program’s increments must rely on robust
and frequent interchanges with the full range of stakeholders including the oversight authority.
The following section offers some thoughts for consideration as the team, including the users and
testers, discusses the desired length, content and scope of each increment.
   16.1.1. Increment Length. When building an evolutionary acquisition strategy, project
   leaders must decide how long to make each increment. Long increments have a few benefits,
   including less frequent milestones, and less frequent CDD updates. However, long
   increments are also harder to scope, cost & test. They have a higher risk of Nunn-McCurdy
   breach, make it harder to add new requirements, face an increased technology maturity risk
   and require more time to achieve IOC. In addition, long increments do not get products to
   the customer in a reasonable timeframe to meet outcome expectations.
       16.1.1.1. Short increments basically provide the inverse. They are easier to scope, cost
       and test, and are less likely to experience a Nunn-McCurdy breach. With a series of short
       increments, it is easier to integrate mature technologies and new capabilities into
       subsequent increments. Shorter increments also mean capabilities are fielded more
       frequently. The drawback of course is more frequent milestones and CDD updates.
       However, short increments will likely be ACAT II or III with less oversight, more
       streamlining, and more delegations.
       16.1.1.2. As a general rule, increments should be as short as possible, because the
       objective of evolutionary acquisition is to provide improved capabilities on a short
       timeline. However, each increment should be sufficiently long to deliver a meaningful
       capability boost over the previous increment. It is important to avoid the superficial
       appearance of speed and to ensure each increment provides “a militarily useful and
       supportable operational capability,” in the words of DoDI 5000.02.
   16.1.2. Increment Considerations. The Fiscal Year 2009 National Defense Authorization
   Act added a new section to Title 10 (section 2430a) that permits the Secretary of Defense
   (delegated to the Under Secretary of Defense for Acquisition, Technology and Logistics) to
   designate subprograms within a Major Defense Acquisition Program (MDAP). That is, when
   an MDAP requires the delivery of two or more categories of end items that differ
   significantly in form and function, subprograms may be established for baseline development
   and reporting purposes. The law stipulates that when one subprogram is designated within an
   MDAP, all remaining elements (increments or components) of the program should also be
   appropriately organized into one or more other subprograms. Project leaders should consider
   the following factors when structuring a program into increments:
       16.1.2.1. The ability to manage the effort, including manpower resources (e.g. cost
       estimators). Based upon many factors such as industrial capacity, complexity, manning
       of both Government and contractor personnel and needed skill sets, consider dividing the
       requirements into a manageable increment structure.
       16.1.2.2. Ability to deliver capabilities faster. Each increment has its own delivery date;
       consciously breaking up delivery into discernible time-phased segments. The first
156                                                             AFPAM63-128 10 JULY 2014


      obligation to Full Deployment Decision (FDD) is 5 years for MAIS, consider similar time
      objectives from MS A to IOC for non-MAIS programs with the goal of faster delivery.
      16.1.2.3. Fleet or system downtime. Consider how to more efficiently introduce a new
      capability while allowing maximum operational capability. Consider an incremental
      approach that minimizes s/w or h/w retrofit that will impact system or fleet availability.
      16.1.2.4. Definable unique capability. Each increment should provide a unique,
      definable, producible, sustainable, and testable capability. These capabilities should be
      documented in a new or updated CDD, CPD, or AF Form 1067 to include objectives,
      thresholds and a set of KPPs, as required. Since a program increment must provide a
      military useful capability and satisfy particular operational requirements, with no
      guarantee that future increments should be funded, products must be subjected to
      rigorous developmental and operational test requirements.
      16.1.2.5. Ability to integrate increments/Interrelationships among the increments. When
      considering integration risk, all increments will need to integrate and be operationally
      compatible with each other and have no negative impact to the operational use of other
      increments if any one increment is cancelled. Consider access to ICDs and potential
      proprietary data of the parent system and other incremental “siblings” under parallel
      development. Each increment may have its own KPPs or play a role in satisfying one or
      more KPPs. To form a new increment of militarily useful capability, there must be
      traceability to a CDD/CPD that relates requirements (especially KPPs) to increments.
      16.1.2.6. Sequential vs. Parallel Development - program dependencies on existing efforts
      or efforts under consideration. Consider the degree to which increments should be
      developed in parallel, based on how the capability developed in any one increment affects
      the development of the other increment(s), easing integration. Consider any efforts
      outside of a program that affect the development of the program’s increments and how
      the effort affects the timing and sequential or parallel nature of a program’s increments.
      Sequential and Parallel efforts should be portrayed on the Integrated Master Schedule
      showing relationships and dependencies impacting program execution.
      16.1.2.7. Milestone documentation and reviews (amount of documentation, number of
      reviews, and compliance). Breaking a program into a series of increments requires each
      increment to have its own APB, CDD, and other documents, or be parts/annexes of
      overarching program documents. The PM should evaluate whether or not an overarching
      CDD or an individual CDD covers the program. Each increment should have its own
      milestone, technical, and test reviews, and be subject to individual compliance standards.
      16.1.2.8. Cost. Each increment must take into account the total life cycle cost. Consider
      affordability, current budgetary constraints, and funding uncertainty. Fiscal discipline and
      transparency of estimates will contribute to increment success.
      16.1.2.9. Significant changes in configuration or capability. Consider increments if a
      new capability (updated technology) would result in significant configuration changes.
      Design increments to optimize configuration control and minimize impact to users. If an
      increment creates a new system configuration that increment should include funding to
      retrofit previous configurations, or the funding to retrofit must be explicitly identified.
AFPAM63-128 10 JULY 2014                                                                        157


     16.1.2.10. Budgetary constraints. Cost in any one increment impacts the development of
     all increments. FYDP resources at the program level require the practice of strong fiscal
     discipline to ensure increments remain viable as planned; as any increment whose
     resources suffer due to cost issues in other increments should require rationale for
     continuation.
     16.1.2.11. Exposure to cost, schedule, performance, and budget risk. The PM must
     ensure the user fully understands the cost/schedule/performance risks associated with the
     proposed increment structure. The sponsoring MAJCOM must consider the degree to
     which the AF Corporate Structure would be willing to risk AF total obligation authority
     when scoping the increments with respect to total funding required, duration of
     investment commitment before IOC, and probability of program success.
     16.1.2.12. ACAT Level/Oversight. Breaking into increments may drive separate ACAT
     levels, MDA, and oversight based on each increment’s size, interest, and place within the
     overarching program. The amount of visibility to Air Staff, OSD, & Congress may
     change based on the increment structure.
     16.1.2.13. Acquisition Strategy (competition, data access, OEM limitations, maximum
     allowable length of contracts). Increments have a significant interplay with acquisition
     strategy and contracts. Competition is required prior to MS B (e.g. Competitive
     prototyping). Additional competition is desired throughout the life-cycle, possibly for
     each increment depending on the program’s acquisition strategy. Data access and data
     rights are critical if each increment is, or could possibly be, developed by separate
     contractors. This will add cost to each increment. OEM limitations could drive the
     length of the contracts and competition. Maximum allowable length of contract could be
     driven by external conditions and could drive (perhaps limit) increment scope.
     16.1.2.14. Severable Increments. The capability of each increment must be complete in
     and of itself, and not be dependent on future or parallel increments.
     16.1.2.15. Systems Engineering. As a basis, there needs to be a flow down of
     requirements from a System of Systems (or Enterprise) architecture (and
     CONOPS/Enabling Concept of Operations) to System requirements and CONOPS, and
     on to lower-level requirements and CONOPS. In this flow down process and analysis of
     the trade-offs between various factors (such as requirements, CONOPS, technology
     maturity, design challenges, schedule, cost, etc.), consider how potential separate
     increments (either sequential or in parallel) contribute to better solutions to the capability.
     16.1.2.16. Increment Development & Production Timing. A large gap in production
     may result in the need for significant investment to re-establish or retool and may drive
     an evolutionary strategy.
     16.1.2.17. Amount of Non-Recurring Engineering (NRE). If the scope of the overall
     NRE effort (in terms of cost and schedule) is significant, a separate increment should be
     considered in order to provide a high confidence program with a manageable scope of
     control.
     16.1.2.18. User Delivery Needs. Consider user timing and needs when determining
     evolutionary strategy.
 158                                                            AFPAM63-128 10 JULY 2014


       16.1.2.19. Level of Risk within the Program. Examine individual program risks and
       consider how breaking out these risks into increments would improve the opportunity to
       perform risk burn-down activities as early as possible. Examine the combined risks in
       each increment and consider the overall risk to successful integration and execution of
       each increment. Include manufacturing readiness (to include DMS Issues) and
       producibility in these risk assessments. Explicitly consider technology maturity. When
       structuring program increments, the maturity level of the associated technology is a
       factor. If technology for a planned increment is not TRL 6 or higher, MS B success is
       highly unlikely. Technology maturation for future increments is a consideration in
       increment identification.
       16.1.2.20. Requirements Maturity. When a desired capability is identified but not all end
       state requirements are well refined, then those less-refined requirements should be
       candidates for future increments. Refinement occurs through demonstration and risk
       management with continuous user feedback ensuring each increment provides the user
       with the best possible high confidence capability.
       16.1.2.21. Maintenance & Sustainability. Consider sustainment impacts in increment
       identification, to include: Compliance with Core and 50/50 reqts; Partnering for depot
       maintenance; Impacts on product support; Impact to Operational Maintenance; Other
       ‘ilities’.
   16.1.3. Increment Scope. There are many ways to align related increments to programs. In
   an ideal world, it might be preferred for each increment to be its own short, small program.
   When the program/increment is completed, it would then end and further efforts that were
   not part of the original increment plan would be initiated as new programs.
       16.1.3.1. However, in actual practice multiple increments are often merged into a single
       program, as depicted in Figure 16.1. In this scenario, as new content is added to existing
       programs via additional increments, the risk of Nunn-McCurdy breach rises.

Figure 16.1. Merged Increments to Single Program.




       16.1.3.2. Figure 16.2 illustrates the clearest delineation of Increments and Programs.
       However, MDAP rules do not permit this structure, as an MDAP must include all
       planned increments in the program.
AFPAM63-128 10 JULY 2014                                                                159


Figure 16.2. Direct Increment to Program.




      16.1.3.3. Finally, Figure 16.3 represents an increment-to-program mapping which is
      deliberately determined for each development effort. As mentioned previously, the COA
      delineated in AFI 63-101/20-101 offers further specifics on how to craft and implement
      this strategy.


Figure 16.3. Increments to Multiple Programs.




16.2. Subprograms for MDAPS
   16.2.1. The decision whether to establish subprograms for an MDAP requires careful
   analysis and must be made on a case-by-case basis. Structuring an MDAP with subprograms
   should reflect the way the program is being managed, and represent the most efficient and
   informative way to convey information about a program to senior defense acquisition
   officials as well as to the Congress. When a MDAP is divided into increments, the
   increments could be designated as subprograms. Pitfalls of this approach include:
160                                                           AFPAM63-128 10 JULY 2014


      16.2.1.1. When one subprogram is established, all remaining elements should be
      organized into one or more other subprograms.
      16.2.1.2. In the event a subprogram has a [Nunn-McCurdy] breach, the program
      certification required must be done on the full program level.
  16.2.2. Using a subprogram designation is therefore high risk. If such an approach is
  directed by OSD, careful selection of increment content is paramount. Consider the
  following:
      16.2.2.1. Avoid structuring small subprograms such that a small increase could derail an
      entire overarching MDAP.
      16.2.2.2. Ensure each subprogram can be properly baselined in an APB.
      16.2.2.3. Account for technology development, risk reduction, testing, and sustainment.
AFPAM63-128 10 JULY 2014                                                                      161


                                          Chapter 17

                 POLICY COORDINATION, REVIEW, AND WAIVERS

17.1. Integrated Life Cycle Management Publication Coordination. Major Command
(MAJCOM) Commanders are requested to convene a high performance team (HPT)-based
process for the review and coordination of official ILCM AF departmental publications (e.g.
AFPDs, AFIs, AFMANs, and AFPAMS). These publications are the authoritative voice of the
Headquarters Air Force (HAF) and document how ILCM requirements established by law, the
President, the Secretary of Defense (SECDEF), and the SECAF are to be fulfilled.
   17.1.1. The HPT consists of the appropriate subject matter expertise relevant to the content
   of the publication under review. The purpose of the HPT is to facilitate AFI 33-360,
   Publications and Forms Management, technical/functional staffing in order to develop a
   timely, adjudicated, consolidated and integrated position on behalf of the MAJCOM
   Commander. Additionally, the HPT should review the publication with regards to higher
   authority (e.g. public law, statute, DoD issuances), HAF senior leadership direction, and the
   ability to implement a standardized process across the MAJCOM. The HPT should provide
   recommendations and supporting rationale for all comments to increase the quality of the
   ILCM publication.
   17.1.2. MAJCOM Commanders should assign a lead office responsible for staffing,
   identification of relevant subject matter experts and process owners to support the HPT, and
   act as the single point of contact between the MAJCOM and the HAF publication OPR.
   MAJCOM Commanders can designate a lower-level office to provide the response and sign
   off on the coordination form, but are responsible for ensuring the correct offices within their
   organization review the publication.
17.2. Waivers. Waivers from guidance must be based on a programmatic course of action
approved by the Service Acquisition Executive (SAE) or Milestone Decision Authority (MDA)
through the program’s governance chain of authority and documented in the appropriate program
documentation. Notification must be made to Headquarters Air Force (HAF) in accordance with
AFPD 63-1/20-1.
17.3. Changes. Refer recommended changes and questions about this publication to
SAF/AQXA using the AF Form 847, Recommendation for Change of Publication; route AF
Form 847s from the field through MAJCOM publications/forms managers.




                                            Dr. William A. LaPlante
                                            Assistant Secretary of the Air Force (Acquisition)
 162                                                           AFPAM63-128 10 JULY 2014


                                        Attachment 1
         GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION

References
Acquisition Intelligence Guidebook V2.0, 1 Aug 2012
AFFARS, Air Force Supplement to the Federal Acquisition Regulation
AFH 23-123, Materiel Management Handbook, Vols 1-3, (date varies by volume)
AFH 36-2235 Vol III, Information for Designers Of Instructional Systems, 1 Nov 2002
AFH 63-1402, Aircraft Information Program, 19 Mar 2001, [to be replaced by AFH 63-134,
Implementing Aircraft Information Programs]
AFI 10-501, Program Action Directives (PAD) and Programming Plans (PPLAN)Plans
(PPLAN), 5 Jan 1994
AFI 10-503, Base Unit Beddown Program Strategic Basing, 27 Sep 2010
AFI 10-601, Operational Capability Requirements Development, 6 Nov 2013
AFI 14-111, Intelligence Support to the Acquisition Life Cycle, 18 May 2012
AFI 16-402, Aerospace Vehicle Programming, Assignment, Distribution, Accounting, and
Termination, 30 May 2013
AFI 16-403, Updating the USAF Program Installations, Units, and Priorities and Movement of
Air Force Units, 25 Jan 2011
AFI 21-103, Equipment Inventory, Status and Utilization Reporting, 26 Jan 2012
AFI 21-113, Air Force Metrology and Calibration Management, 23 Mar 2011
AFI 23-101, Air Force Materiel Management, 8 Aug 2013
AFI 31-401, Information Security Program Management, 01 Nov 2005
AFI 32-9001, Acquisition of Real Property, 27 Jul 1994
AFI 32-9005, Real Property Accountability and Reporting, 14 Aug 2008
AFI 33-141, Air Force Information Technology Portfolio Management and IT Investment
Review, 23 Dec 2008
AFI 33-210, Air Force Certification and Accreditation (C&A) Program (AFCAP), 23 Dec 2008
AFI 33-332, Air Force Privacy and Civil Liberties Program, 5 Jun 2013
AFI 33-360, Publications and Forms Management, 25 Sep 2013
AFI 33-364, Records Disposition – Procedures and Responsibilities, 22 Dec 2006
AFI 36-2101, Classifying Military Personnel (Officer and Enlisted), 25 Jun 2013
AFI 36-2201, Training Development, Delivery, and Evaluation, (date varies by volume)
AFI 36-2251, Management of Air Force Training Systems, 5 Jun 2009
AFI 38-201, Management of Manpower Requirements and Authorizations, 30 Jan 2014
AFPAM63-128 10 JULY 2014                                                                   163


AFI 61-204, Disseminating Scientific and Technical Information, 30 August 2002
AFI 63-101/20-101, Integrated Life Cycle Management, 7 Mar 2013
AFI 63-103, Joint Air Force-National Nuclear Security Administration (AF-NNSA) Nuclear
Weapons Life Cycle Management, 24 Sep 2008
AFI 63-114, Quick Reaction Capability Process, 04 Jan 2011
AFI 63-131, Modification Management, 19 Mar 2013
AFI 63-133, Aircraft Information Program, 06 Feb 2001
AFI 63-138, Acquisition of Services, 21 May 2013
AFI 63-602, Defense Production Act Title I – Defense Priorities and Allocations System, 18 Dec
2003
AFI 63-603, Defense Production Act Title III Program, 17 Dec 2003
AFI 65-501, Economic Analysis, 29 Aug 2011
AFI 65-508, Cost Analysis Guidance and Procedures, 6 Jun 2012
AFI 65-509, Business Case Analysis, 19 Sep 2008
AFI 65-601, Vol. 1, Budget Guidance and Procedures, 16 Aug 2012
AFI 91-105, Critical Components, 29 Nov 2012
AFI 99-103, Capabilities Based Test and Evaluation, 16 Oct 2013
AFMAN 23-122, Materiel Management Procedures, 8 Aug 2013
AFMAN 23-110, USAF Supply Manual, 1 Apr 2009
AFMAN 33-363, Management of Records, 1 Mar 2008
AFMAN 33-407, Air Force Clinger-Cohen Act (CCA) Compliance Guide, 24 Oct 2012
AFMAN 63-119, Certification of System Readiness for Dedicated Operational Test and
Evaluation, 20 Jun 2008
AFMAN 65-506, Economic Analysis, 29 Aug 2011
AFMCI 23-121, AFMC Improved Item Replacement Program (IIRP) and Demand Reduction
Initiative (DRI) Guidance and Procedures, 28 Sep 2006
AFMCMAN 21-2, Engineering Data Storage, Distribution, and Control, 25 June 1997
AFPAM 63-113, Program Protection Planning for Life Cycle Management, 17 Oct 2013
AFPD 10-9, Lead Command Designations and Responsibilities for Weapon Systems, 8 Mar 2007
AFPD 31-4, Information Security, 1 Sep 1998
AFPD 61-2, Management of Scientific and Technical Information, 28 Mar 2014
AFPD 63-1/20-1, Integrated Life Cycle Management, 3 Jul 2012
AFPD 65-5, Cost and Economics, 5 Aug 2008
Air Force Award-Fee Guide, 12 Jul 2010
 164                                                         AFPAM63-128 10 JULY 2014


Air Force Drawing 9579776, Revision K, 19 Aug 2010
Chief Financial Officers Act of 1990
CJCSI 3170.01, Joint Capabilities Integration and Development System, 10 Jan 2012
CJCSI 6212.01F, NET Ready Key Performance Parameter (NR KPP)Interoperability and
Supportability of Information Technology and National Security Systems, 21 Mar 2012
CJCSM 3170.01, Operation of the Joint Capabilities Integration and Development System, 1
May 2007
Code of Federal Regulations
Defense Acquisition Guidebook (formerly DoD 5000.2-R)
DFARS PGI 217.7506, Spare Parts Breakout Program
DFARS Subpart 227.71, “Rights in Technical Data, 2 March 2011”
DoD 4100.39-M, Federal Logistics Information System (FLIS) Procedures Manual, (date varies
by volume)
DoD 4120.24-M, Defense Standardization Program (DSP) Policies and Procedures, 3 Mar 2000
DoD 4140.1-R, Supply Chain Material Management Regulation, 23 May 2003
DoD 5010.12-M, Procedures for the Acquisition and Management of Technical Data, 14 May
1993
DoD 5200.1-M, DoD Information Security Program, 24 Feb 2012
DoD 5400.11-R, Department of Defense Privacy Program, 14 May 2007
DoD 7000.14-R, DoD Financial Management Regulations (FMRS), (date varies by volume)
DoD 8910.1-M, Department of Defense Procedures for Management of Information
Requirements , Jun 1998
DoD and NASA Incentive Contracting Guides , 1969
DoD Contracts Incentives Guide
DoD Guide to Uniquely Identifying Items, 1 Oct 2008
DoD Manufacturing Readiness Assessment Deskbook, 2 Jul 2009
DoD Product Support Manager Guidebook, Apr 2011
DoDD 2140.2, Recoupment of Nonrecurring Costs (NC) on Sales of U.S. Items, 22 May 2013
DoDD 5000.01, The Defense Acquisition System, 12 May 2003
DoDD 5015.2, DoD Records Management Program, 6 Mar 2000
DOD 5200.2-R, DOD Personnel Security Program, January 1987
DODI 5230.24, Distribution Statements on Technical Documents, 23 Aug 2013
DoDD 8320.03, Unique Identification (UID) Standards for a Net-Centric Department of
Defense, 23 Mar 2007
AFPAM63-128 10 JULY 2014                                                                   165


DoDI 2010.06, Materiel Interoperability and Standardization with Allies and Coalition Partners,
29 July 2009
DoDI 4151.19, Serialized Item Management (SIM) for Materiel Maintenance, 9 Jan 2014
DoDI 4161.02, Accountability and Management of Government Contract Property, 27 Apr 2012
DoDI 4630.8, Procdures for Interoperability and Supportability of Information Technology (IT)
and National Security Systems (NSS), 30 Jun 2004
DoDI 4650.1, Policy for Management and Use of the Electromagnetic Spectrum, 9 Jan 2009
DoDI 5000.02 Interim, Operation of the Defense Acquisition System, 25 Nov 2013
DoDI 5000.64, Accountability and Management of DoD Equipent and Other Accountable
Property, 19 May 2011
DoDI 5000.67, Prevention and Mitigation of Corrosion on DoD Military Equipment and
Infrastructure, 1 Feb 2010
DoDI 5000.69, DoD Joint Services Weapon and Laser System Safety Review Processes, 9 Nov
2011
DoDI 5030.55, DoD Procedures for Joint DoD-DOE Nuclear Weapons Life-Cycle Activities, 25
Jan 2001
DODI 5200.1, DOD Information Security Program and Protection of Sensitive Compartmented
Information, 9 Oct 2008
DoDI 5200.39, Critical Program Information (CPI) Protection Within the Department of
Defense, 16 Jul 2008
DoDD 5250.01, IMD Cost Methodology Guidebook, 22 Jan 2013
DoDI 8320.04, Item Unique Identification (IUID) Standards for Tangible Personal Property, 16
Jun 2008
DoDI 8500.2, Information Assurance IA) Implementation, 6 Feb 2003
DoDI 8510.01, Risk Management Framework (RMF) for DoD Information Technology (IT), 12
Mar 2014
DoDI 8580.1, Information Assurance (IA) in the Defense Acquisition System, 9 Jul 2004
e-Government Act of 2002
Federal Acquisition Regulation
Federal Financial Management Improvement Act of 1996
Federal Information System Controls Audit Manual (FISCAM), 2 Feb 2009
Federal Records Act of 1950
Government Management Reform Act of 1994
Government Performance and Results Act of 1993
HAF MD 1-10, Assistant Sectretary of the Air Force (Acquisition), 27 Jun 2013
 166                                                            AFPAM63-128 10 JULY 2014


IEC/ISO 10303, Standard for the Exchange of Product model data (STEP), Jan 2001
IEEE 12207.0, Standard for Information Technology – Software Life Cycle Processes, May 1998
ISO/ICE 15418, Information technology -- Automatic identification and data capture techniques
-- GS1 Application Identifiers and ASC MH10 Data Identifiers and maintenance, 2009
ISO/ICE 15434, Information technology -- Automatic identification and data capture techniques
-- Syntax for high-capacity ADC media, 2006
ISO/IEC 16022, Information technology -- Automatic identification and data capture techniques
-- Data Matrix bar code symbology specification, 2006
MIL-HDBK 1908B , Definition of Human Factors Terms, 16 Aug 1999
MIL-HDBK-245D, Preparation of Statement of Work (SOW), 3 April 1996
MIL-HDBK-288, Review and Acceptance of Engineering Drawing Package, 14 Jan 1991
MIL-HDBK-338, Electronic Reliability Design, Oct 1998
MIL-HDBK-470, Designing and Developing Maintainable Products and Systems, 25 Sept 2007
MIL-HDBK-759, Human Engineering Design Guidelines, Jul 1992
MIL-STD-1472, Human Engineering, 11 Jan 2012
MIL-STD-1787, Aircraft Display Symbology, 5 Jan 2001
MIL-STD-31000, DOD Standard Practice Technical Data Packages, 5 Nov 2009
MIL-STD-411, Department of Defense Design Criteria Standard: Aircrew Station Alerting
System, 10 Mar 1997
MIL-STD-46855, DoD Standard Practice Human Engineering Requirements For Military
Systems, Equipment, and Facilities, 24 May 2011
MIL-STD-881, Work Breakdown Structures, 03 Oct 2011
MIL-STD 130, Identification Marking of U.S. Military Property, 16 Nov 2012
MIL-STD-882E, DoD Standard Practice for System Safety, 11 May 2012
National Defense Authorization Act
NORAD Instruction (NI) 10-3, Mission Integrity, Change Control Management, and Test
Control for the ITW/AA System, 9 Sep 2011
OMB Circular A-123 - Management's Responsibility for Internal Control, 21 Dec 2004
OMB Circular A-130, Management of Federal Information Resources, 27 Jul 2010
OUSD(C) Financial Improvement and Audit Readiness (FIAR) Guidance, Nov 2013
PoPS Operations Guide, Version 9.6, Jul 2007.
Risk Identification: Integration & Ilities (RI3) Guidebook, Dec 2008
Risk Management Guide for DoD Acquisition, Aug 2006
SD-2, DoD Acquisitions: Buying Commercial Items and Non-developmental Items, 1 Jan 2010
AFPAM63-128 10 JULY 2014                                                              167


T.O. 00-35D-54-WA-1, USAF Deficiency Reporting, Investigation, and Resolution, 1 Nov 2011
T.O. 00-5-3-WA-1, AF Technical Order Life Cycle Management, 15 Mar 2014
USSTRATCOM Instruction (SI) 534-2208-10, Mission Integrity, Change Control Management,
and Test Control for the ITW/AA System, 25 Mar 2011
Prescribed Forms
No forms are prescribed by this publication
Adopted Forms
DD Form 250, Material Inspection and Receiving Report
AF Form 525, Recommended Records Disposition
AF Form 847, Recommendation for Change of Publication
AF Form 1067, Modification Management
AF Form 1341, Electronic Record Inventory

Abbreviations and Acronyms
ACAT—Acquisition Category
ACE—Acquisition Center of Excellence
ADM—Acquisition Decision Memorandum
AECO—Advance Engineering Change Order
AESO—Advance Engineering Supplemental Order
AETC—Air Education and Training Command
AF—(United States) Air Force
AF/A4/7—Deputy Chief of Staff, Logistics, Installations, and Mission Support
AF/A6—Chief of Warfighting Integration and Chief Information Officer (CIO)
AF/A8—Deputy Chief of Staff for Plans and Programs
AFFARS—Air Force Federal Acquisition Regulation Supplement
AFI—Air Force Instruction
AFIT—Air Force Institute of Technology
AFMC—Air Force Materiel Command
AFNIC—Air Force Network Integration Center
AFPAM—Air Force Pamphlet
AFPD—Air Force Policy Directive
AFRC—Air Force Reserve Command
AFRL—Air Force Research Laboratory
 168                                                     AFPAM63-128 10 JULY 2014


AIP—Aircraft Information Program
AIS—Automated Information System
AIT—Automatic Identification Technology
AML—Acquisition Master List
AoA—Analysis of Alternatives
APAT—Acquisition Process Architecture Team
APB—Acquisition Program Baseline
APUC—Average Unit Procurement Cost
AS—Acquisition Strategy
ASACoE—Application Software Assurance Center of Excellence
ASTK—Acquisition Sustainment Tool Kit
ASP—Acquisition Strategy Panel
AT&L—Acquisition, Technology, and Logistics
ATO—Authority to Operate
AV—All Viewpoint
BES—Budget Estimate Submission
C4ISP—Command, Control, Communication, Computer and Intelligence Support Plan
CAD—Computer Aided Drafting/Computer Aided Design
CAGE—Commercial and Government Entity
CAP—Contractor Acquired Property
CARD—Cost Analysis Requirements Description
CBA—Capability Based Assessment
CC—Commander
CCA—Clinger Cohen Act
CDA—Current Design Activity
CDD—Capability Development Document
CDR—Critical Design Review
CDRL—Contract Data Requirements List
CFO—Chief Financial Officer
CLIN—Contract Line Item Number
CLS—Contractor Logistics Support
CM—Corrective Maintenance
AFPAM63-128 10 JULY 2014                                                             169


CMRS—Collaboration and Measurements Requirements Summary
CNEO—Change Notice Engineering Order
CONOPs—Concept of Operations
CPAF—Cost-Plus Award Fee
CPD—Capability Production Document
CPI—Critical Program Information
CPIF—Cost Plus Incentive Fee
CRA—Cost Risk Assessment
CTA—Capability Threat Assessments
CTE—Critical Technology Elements
CV—Capability Viewpoint
DAE—Defense Acquisition Executive
DAES—Defense Acquisition Executive Summary
DAF—Department of the Air Force
DAG—Defense Acquisition Guidebook
DAMIR—Defense Acquisition Management Information Retrieval
DAU—Defense Acquisition University
DBS—Defense Business System
DBSMC—Defense Business System Management Committee
DCAA—Defense Contract Audit Agency
DCMA—Defense Contract Management Agency
DCR—DOTmLPF-P Change Recommendation
DFARS—Defense Federal Acquisition Regulation Supplement
DIACAP—DoD Information Assurance Certification and Accreditation Process
DID—Data Item Descriptions
DIV—Data and Information Viewpoint
DMAWG—Depot Maintenance Activation Working Group
DMI—Depot Maintenance Interservice
DMSMS—Diminishing Manufacturing Sources and Material Shortages
DoDD—Department of Defense Directive
DoDI—Department of Defense Instruction
DOTMLPF—Doctrine, Organization, Training, Material, Leadership, Education, Personnel, and
Facilities
 170                                                          AFPAM63-128 10 JULY 2014


DOTMLPF—P—Doctrine, Organization, Training, Material, Leadership, and Education,
Personnel, Facilities and Policy
DP—Development Planning
DRU—Direct Reporting Unit
DSOR—Depot Source of Repair
DT&E—Developmental Test and Evaluation
EA—Economic Analysis
EDM—Engineering Data Manager/Engineering Data Management
EITDR—Enterprise Information Technology Data Repository
EMD—Engineering and Manufacturing Development
EO—Engineering Order
ESOH—Environmental, Safety and Occupational Health
EVM—Earned Value Management
FAR—Federal Acquisition Regulation
FFP—Firm Fixed Price
FFRDC—Federally Funded Research and Development Centers
FIAR—Financial Improvement and Audit Readiness
FIP—Federal Information Processing
FISCAM—Federal Information System Controls Audit Manual
FISMA—Federal Information Security Management Act
FMS—Foreign Military Sales
FOA—Field Operating Agency
FOC—Full Operational Capability
FPAF—Fixed-Price with Award Fee
FPIF—Fixed-Price Incentive
FPRA—Forward Price Rate Agreements
FRP—Full Rate Production
FYDP—Future Years’ Defense Program
GFE—Government Furnished Equipment
GFP—Government Furnished Property
GOSG—General Officer Steering Group
GPR—Government Purpose Rights
HAF—Headquarters Air Force (The Secretariats and Air Staff)
AFPAM63-128 10 JULY 2014                                                          171


HFE—Human Factors Engineering
HQ AFMC/A4—Directorate of Logistics and Sustainment, Air Force Material Command
HSI—Human Systems Integration
IA—Information Assurance
IBR—Integrated Baseline Review
IC—Intelligence Community
ICD—Initial Capabilities Document
ICE—Independent Cost Estimate
ICP—Inventory Control Point
ICS—Interim Contract Support
IDE—Integrated Data Environment
ILCM—Integrated Life Cycle Management
IMDS—Integrated Maintenance Data System
IMP—Integrated Master Plan
IMS—Integrated Master Schedule
IOC—Initial Operational Capability
IOT&E—Initial Operational Test and Evaluation
IPA—Independent Program Assessment
IPS—Intellectual Property Strategy
IPT—Integrated Product Team
IRAD—Independent Research and Development
IRB—Investment Review Board
ISO—International Organization for Standardization
ISP—Information Support Plan
IT—Information Technology
IUID—Item Unique Identification
JCIDS—Joint Capability Integration and Development System
JCTD—Joint Capability Technology Demonstration
JEDMICS—Joint Engineering Data Management Information and Control System
KPP—Key Performance Parameters
KSA—Key System Attributes
LCCE—Life Cycle Cost Estimate
 172                                                 AFPAM63-128 10 JULY 2014


LCMC—Life Cycle Management Center
LCRM—Life Cycle Risk Management
LCSP—Life Cycle Sustainment Plan
LDTO—Lead Developmental Test Organization
LHA—Logistics Health Assessments
LR—Limited Rights
LRIP—Low Rate Initial Production
LRU—Line Replaceable Unit
M&S—Modeling and Simulation
MAIS—Major Automated Information System
MAJCOM—Major Command
MAR—Monthly Acquisition Report
MER—Manpower Estimate Reports
MDA—Milestone Decision Authority
MDAP—Major Defense Acquisition Program
MDD—Materiel Development Decision or Maintenance Data Documentation
MFP—Materiel Fielding Plan
MOA—Memorandum of Agreement
MOE—Measure of Effectiveness
MOP—Measure of Performance
MOS—Measure of Suitability
(ms)MOU—Memorandum of Understanding
MRA—Manufacturing Readiness Assessments
MRR—Materiel Release Review
MS—Milestone
MSA—Materiel Solution Analysis
MUA—Military Utility Assessments
NEPA—National Environmental Policy Act
NGB—National Guard Bureau
NGS—Non-Government Standards
NRE—Non-Recurring Engineering
NSS—National Security System
AFPAM63-128 10 JULY 2014                                          173


OA—Operational Assessment
OEM—Original Equipment Manufacturer
OIPT—Overarching IPT
O&M—Operations and Maintenance
O&S—Operations and Support
OSD—Office Secretary of Defense
OSHA—Occupational Safety and Health Administration
OSS&E—Operational Safety, Suitability, and Effectiveness
OTA—Operational Test Agency
OT&E—Operational Test and Evaluation
OV—Operational Viewpoint
PAD—Program Action Directive
PAUC—Program Acquisition Unit Cost
PBA—Performance Based Agreement
PBL—Performance Based Logistics
PDAQ—Product Data Acquisition
PDR—Preliminary Design Review
PE—Program Element
PESHE—Programmatic Environment, Safety, and Occupational Health
PEM—Program Element Monitor
PEO—Program Executive Officer
PGM—Product Group Manager
PHS&T—Packaging, Handling, Storage, and Transportation
PIA—Privacy Impact Assessments
PM—Program Manager
PMO—Program Management Office
POC—Point of Contact
POE—Program Office Estimates
POM—Program Objective Memorandum
PoPS—Probability of Program Success
PPLANS—Programming Plans
PPP—Program Protection Plan
 174                                                         AFPAM63-128 10 JULY 2014


PRA—Performance Risk Assessment
PSM—Product Support Manager
PV—Project Viewpoint
QRC—Quick Reaction Capability
RAMS—Reliability, Availability Maintainability and Supportability
RCA—Regulatory Contracting Approval
RDS—Records Disposition Schedule
RFP—Request for Proposal
RI3—Risk Identification: Integration & -Ilities
RMP—Risk Management Plan
RPIE—Real Property Installed Equipment
SAE—Service Acquisition Executive
SAF/AQ—Assistant Secretary of the Air Force (Acquisition)
SAF/AQX—Deputy Assistant Secretary for Acquisition Integration
SAF/FMC—Deputy Assistant Secretary (DAS) of the Air Force for Cost and Economics
SAF/GCQ—Deputy General Counsel (Acquisition)
SAR—Selected Acquisition Report
SATAF—Site Activation Task Force
SCP—Service Cost Position
SE—Systems Engineering
SEAM—Systems Engineering Assessment Model
SE/ATS—Support Equipment/Automatic Test System
SEP—Systems Engineering Plan
SFR—System Functional Review
SIM—Serialized Item Management
SMART—System Metric and Reporting Tool
SMCA—Single Manager for Conventional Ammunition
SORAP—Source of Repair Assignment Process
SORN—System of Records Notice
SRA—Schedule Risk Assessment
SRR—System Requirements Review
SSOM—Standard Surveillance Operating Manual
AFPAM63-128 10 JULY 2014                                                             175


SSP—Source Selection Plan
STA—System Threat Analysis
STAR—System Threat Assessment Report
StdV—Standards Viewpoint
STP—System Training Plan
SV—System Viewpoint
SvcV—Services Viewpoint
T&E—Test and Evaluation
TBD—To Be Determined
TCTO—Time Compliance Technical Order
TD—Technology Development
TEI—Text Element Identifiers
TEMP—Test and Evaluation Master Plan
TES—Test and Evaluation Strategy
TO—Technical Order
TPM—Technical Performance Measures
TPT—Training Planning Team
TRA—Technical Readiness Assessment
TRL—Technology Readiness Level
TSP—Transition Support Plan
TSRA—Training System Requirements Analysis
TTP—Technology Transition Plan
UID—Unique Identification
UII—Unique Item Identifiers
UON—Urgent Operational Need
USC—United States Code
USD (AT&L)—Under Secretary of Defense, Acquisition, Technology, and Logistics
WAWF—Wide Area Workflow
WBS—Work Breakdown Schedule

Terms
Acquisition—The conceptualization, initiation, design, development, testing, contracting,
production, deployment, and disposal of a directed and funded effort that provides a new,
 176                                                           AFPAM63-128 10 JULY 2014


improved, or continued materiel, weapon, information system, logistics support, or service
capability in response to an approved need.
Acquisition Center of Excellence (ACE)—A structure to provide direct program acquisition
planning and execution (pre- and post-award) support to acquisition leadership and program
teams. ACEs exist at SAF and field centers levels. Center ACEs focus on the “nuts and bolts”
aspects of the program and documentation. The SAF-level ACE function is performed by
SAF/AQXC and builds on Center ACE work by adding its expertise and the perspective of the
SAE, HAF functional staffs, OSD staffs, and Congress. Center ACEs focus on all programs; the
SAF ACE focus is primarily on ACAT I, ACAT IA and non-delegated ACAT II programs.
Air Force Engineering Data Group (AFEDG)—A group of representatives from Air Force
activities chartered to review and recommend changes to engineering drawing policy. Functions
involved with Air Force engineering drafting issues select representatives for the group.
Air Force Engineering Drawing—An engineering drawing generated with an Air Force
drawing number and Commercial and Government Entity (CAGE) code in the title block as the
Original design activity. An engineering drawing acquired through the design activity transfer
process is also considered an Air Force engineering drawing.
Capability—The ability to achieve a desired effect under specified standards and conditions
through combinations of ways and means to perform a set of tasks. It is defined by an
operational user and expressed in broad operational terms in the format of a Joint or Initial
Capabilities Document (ICD) or a joint Doctrine, Organization, Training, Materiel, Leadership
and Education, Personnel, and Facilities (DOTMLPF) change recommendation. In the case of
materiel proposals, the definition will progressively evolve to DOTMLPF performance attributes
identified in the Capability Development Document (CDD) and the Capability Production
Document (CPD).
Capability Directorates—Directorates under SAF/AQ responsible for policy, direction,
resource allocation, and oversight of programs within their mission area as assigned by SAF/AQ.
They facilitate the interaction between the SAE and the PEO and function as program focal point
and conduits for interfaces with Congress, OSD, JCS, other services Air Staff, MAJCOMs, and
foreign governments or international organizations. CDs provide acquisition inputs to
Programming, Planning, and Budgeting Execution (PPBE).
Center Intelligence Office/Intelligence Division—The singular focal point at each AFMC
installation specifically dedicated to supporting research, development, test, evaluation and
sustainment activities with analytical services and intelligence products and information.
Component Acquisition Executive (CAE)—Term used by the DoD for Service Acquisition
Executive (SAE). The preferred AF term for this person is the Service Acquisition Executive.
See Service Acquisition Executive (SAE).
Concept—A prospective materiel solution to an identified operational capability need.
Concept Characterization and Technical Description (CCTD)—CCTD’s document the
results of early System Engineering (SE) and concept development activities and are the
principal artifacts of Early SE. They provide relevant information about prospective materiel
solutions to the Lead Command and SAF/AQ in support of MDD and AoAs (AFI 10-601).
AFPAM63-128 10 JULY 2014                                                                         177


CCTDs are also key input documents to the AoA Study Plan. The AF Early SE Guide and the
AF CCTD Guide provide additional information.
Concept of Operations (CONOPS)—States broad mission areas in which the system will be
expected to perform. It describes the using command’s approach to the deployment,
employment, and operation of a new or upgraded system or capability being advocated to meet
identified tasks or missions. It need not be exclusive to a single system, command, or service,
but it can rely on other systems and organizations as required.
Contract Support (CS)—A generic term for the support of a system, subsystem, training
system, equipment, or end item provided by a commercial vendor pending transition to, or in lieu
of, organic support.
Contract Sustainment Support (CSS)—A planned contractor support method used to provide
all or part of the logistics support elements for a system, subsystem, training system, equipment,
or end item for extended periods of time or for the life cycle.
Contractor Drawing—A drawing generated by a contractor with that contractor's drawing
number and CAGE code in the title block as the original design activity.
Contractor Logistics Support (CLS)—A method of contract support for a program, system,
subsystem, training system, equipment, or end item used to provide all or part of the sustainment
elements in direct support of the approved sustainment strategy. It may include work managed
and/or accomplished by the Government but for which the contracted communities are
responsible for performance output.
Controlled Unclassified Information (CUI)—Unclassified information, including technical
data, to which access or distribution limitations have been applied in accordance with United
States laws, policies, and regulations. Examples include Unclassified Scientific and Technical
Information (STINFO), Unclassified Export Controlled Information, Unclassified Proprietary
(Intellectual Property), Information exempted from public release by Freedom of Information
Act (FOIA) (For Official Use Only (FOUO)), Competition Sensitive, Source Selection
Information, and Controlled Unclassified Military Information (CUMI).
Core Capability—Skills and resources maintained within organic repair depots to meet
contingency requirements. Core comprises a minimum level of mission-essential capability
either under the control of the individual DoD component or a consolidated capability under the
control of a jointly determined DoD component where economic and/or strategic considerations
warrant.
Course of Action (COA)—A planning and decision process that culminates in a MAJCOM
decision.
Critical Component—A component which is or contains information and communications
technology including hardware, software, and firmware, whether custom, commercial, or
otherwise developed, and which delivers or protects mission critical functionality of a system or
which, because of the system’s design, may introduce vulnerability to the mission critical
functions of a system.
Critical Program Information (CPI)—Program information, technologies, or systems which, if
disclosed or compromised, would degrade combat effectiveness, shorten the expected combat
effective life of the system, significantly alter technological capabilities or program direction, or
 178                                                               AFPAM63-128 10 JULY 2014


require additional research, development, test, and evaluation (RDT&E) resources to counter the
impact of the compromise. CPI can be classified information or controlled unclassified
information (CUI) about technologies, processes, applications, or end items. CPI includes but is
not limited to: system capabilities and vulnerabilities, CPI inherited from another programs and
CPI identified in pre-acquisition activities or as a result if non-traditional acquisition techniques
(e.g. Joint Concept Technology Development, flexible technology insertion); components,
formulas, algorithms, ranges, frequencies, specialized hardware/software, programs, engineering,
design, or unique manufacturing processes; system capabilities or vulnerabilities; and other
information. CPI includes combinations of technologies, subsystems, and systems that
individually may not be considered CPI.
Critical Technology Elements (CTE)—A technology element is “critical” if the system being
acquired depends on this technology element to meet operational requirements (with acceptable
development, cost, and schedule and with acceptable production and operation costs) and if the
technology element or its application is either new or novel. Said another way, an element that is
new or novel or is being used in a new or novel way is critical if it is necessary to achieve the
successful development of a system, its acquisition, or its operational utility.
Current Design Activity—The design activity currently responsible for the design of an item.
This may be the original Design Activity or a design activity to which the design responsibility
has been transferred. (ASME Y14.100)
Data Rights—Different classes of licenses that the Government may purchase by contract.
Defense Acquisition Executive (DAE)—The USD(AT&L) who has responsibility for
supervising the Defense Acquisition System. The DAE takes precedence on all acquisition
matters after the Secretary and the Deputy Secretary.
Depot Maintenance—Material and/or software maintenance or repair requiring the overhaul,
upgrade or rebuild of parts, assemblies, subassemblies or software programs, regardless of
source of funds, location, or if accomplished organically or commercially. The term does not
include procurement of modifications for performance improvement. It does include testing,
installation of parts for modifications, and reclamation of materiel. Reference Title 10, USC,
Section 2460.
Depot Maintenance Capability—The aggregation of all resources required to perform depot
maintenance. These resources include facilities, skilled personnel, tools, test equipment,
drawings, technical publications, ongoing training, maintenance personnel, engineering support,
and spare parts.
Depot Maintenance Interservice (DMI)—The review/study process used for assignment of the
final Depot Source of Repair (DSOR) for depot level maintenance. This process is intended to
identify existing depot repair sources for new acquisitions programs and thereby preclude
inadvertently duplicating depot maintenance workload assignments. The process also identifies
opportunities for joint contracting for further cost savings and will identify alternate sources of
repair for existing depot programs planned for relocation.
Depot Source of Repair (DSOR)—Combination of a two-part process (source of repair
assignment process (SORAP) and the depot maintenance interservice (DMI) recommendation)
that results in a final assignment of a coordinated, joint service recommendation for assignment
of the depot repair source to a specific organic depot maintenance activity or to the commercial
AFPAM63-128 10 JULY 2014                                                                      179


sector. The first part is done within the AF to determine whether to use organic or contract
repair source. The second part is done within the Joint Service community to determine which
specific DoD organic repair source or commercial sector will be used. It is designed to ensure
compliance with all applicable factors, including public law, which merit consideration in
achieving best value depot maintenance source of repair (SOR).
Design Activity—An activity that has, or had, responsibility for the design of an item. (ASME
Y14.100)
Design Activity Transfer Drawing—A drawing with the drawing number and CAGE code of
the original design activity to which an Air Force CAGE code has been added to transfer the
design to a specific Air Force activity. It is subject to all conditions imposed on an Air Force
drawing.
Development Planning (DP)—DP encompasses the engineering analysis and technical planning
activities that provide the foundation for informed investment decisions on the fundamental path
a materiel development will follow to meet operational needs effectively and affordably. DP
facilitates integrated capability development. Early planning, analysis, and systems engineering
activities provide linkages among operational needs, system performance requirements,
technology needs and opportunities, and potential life cycle costs, and establishes a technical
foundation for materiel development. As a result, requirements will be fiscally and
technologically informed; concepts will be mature and, fiscally and technically feasible; and
areas for Science &Technology investment will be identified to reduce technology risks.
Direct Sale Agreement (DSA)—An arrangement, currently authorized primarily for depot
maintenance activities designated as Centers of Industrial and Technical Excellence (CITE), and
other working capital funded industrial facilities under specified circumstances, whereby military
and commercial entities enter into a contractual relationship for the sale of depot maintenance
articles and/or services to an outside (non-government) entity, usually a contractor.
Drawing Change—Any change to an original drawing by direct manual or electronic means, or
by a separate engineering change order.
Electromagnetic Compatibility (EMC)—The ability of systems, equipment, and devices which
utilize the electromagnetic spectrum to operate in their intended operational environments
without suffering unacceptable degradation or causing unintentional degradation because of
electromagnetic radiation. It involves the application of sound electromagnetic spectrum
management; system, equipment, and device design configuration that ensures interference-free
operation; and clear concepts and doctrines that maximize operational effectiveness.
End Item—Final combination of assemblies, components, parts, and materiel that performs a
complete operational function and needs no further augmentation to make it ready for its
intended use.
Energetics (Energetic Materials)—Chemical compounds, or mixtures of chemical compounds,
that are divided into three groups according to use: explosives, propellants, and pyrotechnics.
Energetic materials are sensitive to four external energy sources; these are impact, shock,
electrostatic, and thermal.
Engineering Order—A basic form document that allows you to supplement design information
for an existing drawing.
 180                                                             AFPAM63-128 10 JULY 2014


Enterprise Architecture—A strategic information asset base, which defines the mission, the
information necessary to perform the mission, the technologies necessary to perform the mission,
and the transitional processes for implementing new technologies in response to changes in
mission needs. An enterprise architecture includes a baseline [as-is] architecture, target [to-be]
architecture, and a sequencing plan.
Family of Systems (FoS)—A set or arrangement of independent systems that can be arranged or
interconnected in various ways to provide different capabilities. A family of systems is basically
a grouping of systems having some common characteristic(s). The mix of systems can be
tailored to provide desired capabilities, dependent on the situation.
Fielding—Occurs when supported and supporting commands collaboratively plan and execute
the delivery and bed-down of an operationally effective and suitable platform or system, or a
major system modification/upgrade, from a total system capability perspective, that is
sustainable over its planned life cycle.
Government Purpose Rights (GPR)— Permit the Government to use data or software for any
Government Purpose which does not include competition against the owner of the data or
software in the commercial marketplace.
Horizontal Protection—Common security countermeasures for protecting similar technologies
used by more than one program or technology project. It may extend across military
Components. Horizontal protection ensures cost-effective application of technology protection
efforts. (See DoDI 5200.39.)
Human Systems Integration (HSI)—The integrated and comprehensive analysis, design, and
assessment of requirements, concepts, and resources for system manpower, personnel,
environment, training, safety, occupational health, habitability, personnel survivability, and
human factors engineering.
Increment—Militarily useful and supportable operational capability that can be effectively
developed, produced, acquired, deployed, and sustained. Each increment of capability will have
its own set of threshold and objective values set by the user. See Threshold Objective.
Incremental Development—Evolutionary acquisition process where using evolutionary
acquisition the desired capability is identified, an end state requirement is known, and that
requirement is met over time by developing several increments, each dependent on available
mature technology.
Information Technology—Any equipment or interconnected system or subsystem of equipment
that is used in the automatic acquisition, storage, manipulation, management, movement, control,
display, switching, interchange, transmission, or reception of data or information by the
executive agency. IT includes computers, ancillary equipment, software, firmware and similar
procedures, services (including support services), and related resources, including National
Security Systems (NSS). It does not include any equipment that is acquired by a federal
contractor incidental to a federal contract.
Integrated Life Cycle Management (ILCM)—The seamless governance, transparency, and
integration of all aspects of infrastructure, resource management, and business systems necessary
for successful development, acquisition, fielding, and sustainment of systems, subsystems, end
items, and services to satisfy validated warfighter capability needs.
AFPAM63-128 10 JULY 2014                                                                        181


Integrated Testing—The collaborative planning and collaborative execution of test phases and
events to provide shared data in support of independent analysis, evaluation, and reporting by all
stakeholders particularly the developmental (both contractor and government) and operational
test and evaluation communities.
Interim Contract Support (ICS)—A temporary support method for an initial period of
operation for a system, subsystem, training system, equipment, or end item.
Joint Capability Technology Demonstration (JCTD)—Demonstration of the military utility of
a significant new technology and an assessment to clearly establish operational utility and system
integrity.
Key Performance Parameters (KPP)—Those minimum attributes or characteristics considered
most essential for an effective military capability.
Lead Developmental Test and Evaluation Organization (LDTO)—The lead government
developmental test organization on the Integrated Test Team (ITT) that is qualified to conduct
and/or be responsible for overseeing a confederation of Developmental Test and Evaluation
(DT&E) organizations, each with different but necessary skills, in support of an acquisition
program.
Lead Major Command—The command that serves as operators’ interface with the Program
Manager for a system as defined by AFPD 10-9, Lead Command Designation and
Responsibilities for Weapon Systems.
Lead System Integrator (LSI)—1) “Lead system integrator with system responsibility” means
a prime contractor for the development or production of a major system if the prime contractor is
not expected at the time of award to perform a substantial portion of the work on the system and
the major subsystems, 2) “Lead system integrator without system responsibility’ means a
contractor under a contract for the procurement of services whose primary purpose is to perform
acquisition functions closely associated with inherently governmental functions with regard to
the development or production of a major system.
Life Cycle—The span of time associated with a system, subsystem, or end item that begins with
the conception and initial development of the requirement, continues through development,
acquisition, fielding, and sustainment until the time it is either consumed in use or disposed of as
being excess to all known materiel requirements.
Limited Rights (LR)—Permit the Government to make internal use of data, but does not permit
disclosure outside the Government except in very limited circumstances.
Low Rate Initial Production (LRIP)—Production of the system in the minimum quantity
necessary to: provide production-configured or representative articles for operational tests;
establish an initial production base for the system; and permit an orderly increase in the
production rate for the system sufficient to lead to full-rate production upon the successful
completion of operational testing.
Maintainability—The ability of an item to be retained in, or restored to, a specified condition
when maintenance is performed by personnel having specified skill levels, using prescribed
procedures and resources, at each prescribed level of maintenance and repair.
Measure of Effectiveness (MOE)—(DOD) A criterion used to assess changes in system
behavior, capability, or operational environment that is tied to measuring the attainment of an
 182                                                               AFPAM63-128 10 JULY 2014


end state, achievement of an objective, or creation of an effect A measure of operational success
that must be closely related to the objective of the mission or operation being evaluated. For
example, kills per shot, probability of kill, effective range, etc. A meaningful MOE must be
quantifiable and a measure to what degree that real objective is achieved. See also combat
assessment; mission.
Measure of Performance (MOP)—(DOD) A criterion used to assess friendly actions that is
tied to measuring task accomplishment. Measures of lowest level of performance representing
subsets of measure of effectiveness (MOEs).Examples are speed, payload, range, time on station,
frequency, or other distinctly quantifiable performance features.
Measure of Suitability (MOS)—A MOS typically relates to readiness or operational
availability, and hence reliability, maintainability, and the item’s support structure. Several
MOSs and/or MOPs may be related to the achievement of a particular MOE.
Milestone (MS)—Major decision points that separate the phases of an acquisition process.
Milestone Decision Authority (MDA)—The individual designated in accordance with criteria
established by the USD (AT&L) to approve entry of an acquisition program into the next phase.
Military Utility Assessment (MUA)—A determination of how well a capability or system in
question responds to a stated military need, to include a determination of its potential
effectiveness and suitability in performing the mission. It is a "characterization" of the capability
or system as determined by measures of effectiveness, measures of suitability, measures of
performance, and other operational considerations as indicators of military utility, as appropriate,
and answers the questions, "What can it do?" and "Can it be operated and maintained by the
user?"
Mission Critical System—System whose operational effectiveness (OE) and operational
suitability (OS) are essential to successful mission completion or to aggregate residual combat
capability. If this system fails, the mission most likely will not be completed. Such a system can
be an auxiliary or supporting system, as well as a primary mission system.
Modification—For the purposes of this instruction, a modification is defined as a change to the
form, fit, function, or interface (F3I) of an in-service, configuration-managed AF asset.
Modifications are primarily defined by their purpose. A capability modification alters the F3I of
an asset in a manner that requires a change to the existing system, performance, or technical
specification of the asset. Such modifications are generally accomplished to add a new
capability or function to a system or component, or to enhance the existing technical
performance or operational effectiveness of the asset. A sustainment modification alters the F3I
of an asset in a manner that does not change the existing system, performance, or technical
specification of the asset. Such modifications are generally accomplished to correct product
quality deficiencies, or to bring the asset in compliance with, or to maintain the established
technical or performance specification(s) associated with the asset. Sustainment modifications
may also include efforts that are accomplished for the primary purpose of improving the
reliability, availability, maintainability, or supportability of an asset, or to reduce its ownership
costs.
Non-Developmental Item (NDI)—Any previously developed item of supply used exclusively
for governmental purposes by a Federal agency.
AFPAM63-128 10 JULY 2014                                                                          183


Non-Standard Rights—Also called “Special” or “Specifically Negotiated” Rights. Any rights
negotiated by the contractor and agency that are different than the foregoing classes of rights and
having a degree to which these are permitted depends on the particular regulations applicable to
the procurement.
Non—Technical Information Text—Supplemental textual information, such as rights status
clarification or other research information needed for clarification or expansion of original
drawing information to fulfill the requirements of the completed technical data package.
Operability—The ability to keep a system or subsystems in a functioning and operating
condition and also work together to accomplish a common task or mission - The argument could
be presented that the human plays a large and important role in this specialty, as well. A non-
optimized design of the human- machine interface will adversely affect this characteristic of the
system.
Operational Assessment (OA)—An analysis of progress toward operational capabilities made
by an operational test organization, with operator support as required, on other than production
systems. The focus of an operational assessment is on significant trends noted in development
efforts, programmatic voids, areas of risk, adequacy of requirements, and the ability of the
program to support adequate operational testing. Operational assessments may be made at any
time using technology demonstrators, prototypes, mockups, engineering development models, or
simulations, but will not substitute for the dedicated OT&E necessary to support full production
decisions.
Operational Capability Requirements (OCR)—A system capability or characteristic to
accomplish approved capability needs. Operational (including supportability) requirements are
typically performance parameters, but they may also be derived from cost and schedule. For
each parameter, an objective and threshold value must also be established.
Operational Effectiveness (OE)—Measure of the overall ability to accomplish a mission when
used by representative personnel in the environment planned or expected for operational
employment of the system considering organization, doctrine, tactics, supportability,
survivability, vulnerability, and threat.
Operational Safety—The condition of having acceptable risk to life, health, property, and
environment caused by a system or end-item when employing that system or end-item in an
operational environment. This requires the identification of hazards, assessment of risk,
implementation of mitigating measures, and acceptance of residual risk in accordance with the
process in MIL-STD-882.
Operational Suitability—The degree to which a system can be placed and sustained
satisfactorily in field use with consideration given to availability, compatibility, transportability,
interoperability, reliability, wartime usage rates, maintainability, safety, human factors,
habitability, manpower, logistics supportability, natural environmental effects and impacts,
documentation, and training requirements.
Operational Test and Evaluation (OT&E)—1) The field test, under realistic combat
conditions, of any item of (or key component of) weapons, equipment, or munitions for the
purpose of determining the effectiveness and suitability of the weapons, equipment, or munitions
for use in combat by typical military users; and the evaluation of the results of such test. 2)
Testing and evaluation conducted in as realistic an operational environment as possible to
 184                                                             AFPAM63-128 10 JULY 2014


estimate the prospective system's operational effectiveness, suitability, and operational
capabilities. In addition, OT&E provides information on organization, personnel requirements,
doctrine, and tactics. It may also provide data to support or verify material in operating
instructions, publications, and handbooks.
Organic—Logistics support provided by Government-owned material/ equipment/ facilities and
Government personnel.
Original Design Activity—The design activity originally responsible for the design and
identification of an item whose drawing number and activity identification is shown in the title
block of the drawings and associated documents. (ASME Y14.100)
Performance Based Contracting—Structuring all aspects of an acquisition around the purpose
of the work to be performed with the contract requirements set forth, in clear, specific, and
objective terms with measurable outcomes as opposed to either the manner by which the work is
to be performed or by broad and imprecise statements of work.
Performance Based Agreement (PBA)—An agreement between organic entities to delineate
measurable performance outcomes that correspond to support requirements and the resources to
achieve both. PBAs are to support established performance baselines and define required
metrics necessary to achieve the performance requirements. They may be used as a basis for
support arrangements or contracts and as a tool to ensure accountability in meeting requirements
by defining the expectations, range of support requirements, and roles and responsibilities.
Performance Based Logistics (PBL)—Product support strategy where PM develops and
implements strategies that optimize total system availability while minimizing cost and logistics
footprints. Trade-off decisions involving cost, useful service, and effectiveness should consider
corrosion prevention and mitigation. Sustainment strategies should include the best use of public
and private sector capabilities through government/industry partnering initiatives, in accordance
with statutory requirements.
Personnel Survivability—The area of survivability which consists of those system design
features that reduce the risk of fratricide, detection, and the probability of being attacked; and
that enable the crew to withstand man-made hostile environments without aborting the mission
or suffering acute chronic illness, disability, or death.
Pre-Operational Support (POS)—Support for test and evaluation efforts, system risk reduction
and demonstration, production readiness or other temporary periods during the acquisition or
modification of a system, equipment or end item.
Product Group Manager (PGM)—Designated individual for overall management of a
specified product group; includes responsibility for cost, schedule and performance aspects along
with the sustainment elements of the group’s products. PGMs should support overall system
objectives as required by the PM. The PGM is not a DoDD 5000.01 Program Manager (PM) of
an acquisition program unless assigned separately and in accordance with guidance on assigning
PMs.
Product Support Manager—The individual responsible for managing the package of support
functions required to field and maintain the readiness and operational capability of major weapon
systems, subsystems, and components, including all functions related to weapon system
readiness, in support of the program manager’s life cycle management responsibilities.
AFPAM63-128 10 JULY 2014                                                                       185


Product Support Strategy—The planning and directing for effective integrated logistics
support throughout the life cycle of a weapon system that will maximize system capabilities,
reduce the logistics footprint, minimize total system sustainment cost, and satisfy the
requirements of the warfighter.
Program—Systems, subsystems, end items, services, or activities on the AF Acquisition Master
List (AML), weapon or business system in sustainment, weapon systems designated in AFPD
10-9 (Lead Command Designation and Responsibilities for Weapon Systems), or identified as
Services Category activities.
Program Executive Officer (PEO)—The individual dedicated to executive management and
supervision of a portfolio of mission-related ACAT and selected programs. The PEO is
accountable to the SAE.
Program Manager (PM)—The DoDD 5000.01 designated individual with responsibility for
and authority to accomplish program objectives for development, production, and sustainment to
meet the user’s operational needs. The PM for acquisition programs should be accountable for
credible cost, schedule, performance, and materiel readiness to the MDA. ACAT I, ACAT IA,
and ACAT II PM should be appointed by the SAE and the PEO. Delegated ACAT II and III PM
should be appointed by the PEO. The PM for sustainment programs should be accountable for
credible cost, schedule, performance, and materiel readiness to the AFMC/CC or designee.
Program Protection Plan (PPP)—Program managers employ system security engineering
practices to prepare the principal document that identifies a system’s critical program elements
(critical program information (CPI) and critical components), threats, and vulnerabilities
throughout the system’s life cycle, the PPP. Program protection is a comprehensive effort that
encompasses the security, technology transfer, intelligence, and counterintelligence processes
through the integration of embedded system security processes, security manpower, equipment
and facilities. It is the integrating process for managing risks to AF warfighting capability from
foreign intelligence collection; from hardware, software, and cyber vulnerability or supply chain
exploitation; and from battlefield loss throughout the system life cycle. Program protection
procedures and program protection planning throughout the life cycle are discussed in detail in
AFPAM 63-113, Program Protection Planning for Life Cycle Management. Also see Interim
DoDI 5000.02.
Programmatic Environment, Safety, and Occupational Health (ESOH) Evaluation
(PESHE)—A required program office document that describes the PM’s strategy for integrating
across the ESOH disciplines and into systems engineering using a882 System Safety
methodology; provides a repository for ESOH risk data; provides a method for tracking progress;
and includes a compliance schedule for appropriate portions of Occupational Safety and Health
Administration (OSHA) (29CFR §1910 and §1926), National Environmental Protection Act
(NEPA) (42 USC §4321), Environmental Impact Assessment Program (EIAP) (32 CFR 989),
and Executive Order 12114 (Environmental Effects Abroad of Major Federal Actions). The
PESHE is developed for MS B, and updated for MS C, for the Full-Rate Production Decision
Review/Full Deployment Decision Review, and as required throughout the life of the program.
Prototype—A model suitable for evaluation of design, performance, and production potential.
Note: The AF uses prototypes during development of a technology or acquisition program for
verification or demonstration of technical feasibility. Prototypes may not be representative of the
final production item.
 186                                                              AFPAM63-128 10 JULY 2014


Public-Private Partnership (P-PP)—A cooperative arrangement between a depot-level
maintenance activity and one or more private-sector entities to perform DoD or defense-related
work, utilizing DoD personnel, facilities and equipment. Reference Title 10 U.S.C. 2474.
Real Property Installed Equipment (RPIE)—Equipment permanently installed in or attached
to buildings or structures that becomes part of the real property. It includes: Heating,Ventilation,
and Air Conditioning systems (HVAC), elevators, fume hoods, exhaust systems, etc.
Relative Environment—The specific subset of the operational environment that is required to
demonstrate critical "at risk" aspects of the final product performance in an operational
environment. It is an environment that focuses specifically on stressing the technology in
question. Not all systems, sub-systems, and/or components need to be operated in the
operational environment in order to satisfactorily address performance margin requirements.
Note: A relevant environment is required for Technology Readiness Levels 5 and 6.
Release Activity—The activity responsible for ensuring all required administrative actions
accomplished before a drawing is released.
Reliability—The ability of a system and its parts to perform its mission without failure,
degradation, or demand on the support system.
Research, Development, Test and Evaluation (RDT&E)—The type of funding appropriation
(3600) intended for research, development, test and evaluation efforts. (DoD 7000.14-R, Vol.
2A, and AFI 65-601, Vol. 1, Budget Guidance and Procedures) Note: The term “research and
development” (R&D) broadly covers the work performed by a government agency or the private
sector. “Research” is the systematic study directed toward gaining scientific knowledge or
understanding of a subject area. “Development” is the systematic use of the knowledge and
understanding gained from research for the production of useful materials, devices, systems, or
methods. RDT&E includes all supporting test and evaluation activities.
Restricted Rights (RR)—These apply only to computer software and generally restrict the
Government’s use to a single computer per copy of software, and prohibit all but backup or
archival copies.
Revision—Any change to an original drawing which requires the revision level to be advanced.
(ASME Y14.35M)
Safety—Freedom from conditions that can cause death, injury, occupational illness, damage to
or loss of equipment or property, or damage to the environment.
Seamless Verification—A concept for structuring test and evaluation (T&E) to more effectively
support the requirements and acquisition processes so new capabilities are brought to users more
quickly. Seamless verification promotes using integrated testing procedures coupled with tester
collaboration in early requirements definition and system development activities. It shifts T&E
away from the traditional "pass-fail" model to one of providing continuous feedback and
objective evaluations of system capabilities and limitations throughout system development.
Senior Procurement Executive (SPE)—The SPE is the individual responsible for management
and direction of the procurement system including implementation of the unique procurement
policies, regulations, and standards of the AF. The SPE under 41 USC §414 is the Assistant
Secretary of the Air Force (Acquisition) (SAF/AQ) as delegated. Delegation is contained in the
AFPAM63-128 10 JULY 2014                                                                      187


HAF Mission Directive 1-10, Assistant Secretary of the Air Force (Acquisition), 08 Apr 2009
and SECAF memo 21 Aug 06, Air Force Acquisition Authorities and Responsibilities.
Service Acquisition Executive (SAE)—The SAE is the individual responsible for the
development of programs to meet defined needs, and as such develops, coordinates, and
integrates plans, policy, and programs for systems and the acquisition of AF programs. The SAE
for AF programs is the Assistant Secretary of the Air Force (Acquisition) (SAF/AQ) as delegated
by the Secretary of the Air Force (SECAF); authority remains with the SECAF if not delegated.
Delegation is contained in the HAF Mission Directive 1-10, Assistant Secretary of the Air Force
(Acquisition), 08 Apr 2009 and HAF Mission Directive 1-2, Undersecretary of the Air Force, 08
Sep 2008.
Software Maintenance—Those activities necessary to correct errors in the software; add
incremental capability improvements (or delete unneeded features) through software changes;
and adapt software to retain compatibility with hardware or with other systems with which the
software interfaces. Software maintenance comprises software maintenance performed on
military materiel (e.g. weapon systems and their components, space control systems and their
components, automated test equipment and test package sets, and systems integration
laboratories).
Source of Repair (SOR)—An industrial complex (organic, commercial contract, or inter-service
facility) with required technical capabilities to accomplish repair, overhaul modification, or
restoration of specific types of military hardware or software.
Source of Repair Assignment (SORA)—A part of the total Depot Source of Repair (DSOR). It
is the primary process by which the AF postures its depot level workloads for both hardware and
software. It applies to both new acquisition and fielded programs.
Specification—A document intended primarily for use in procurement which clearly and
accurately describes the essential technical requirements for items, materials, or services,
including the procedures by which it will be determined that the requirements have been met.
Specifications may be prepared to cover a group of products, services, or materials, or a single
product, service, or material, and are general or detail specifications.
Stakeholders—Individual or organizational entities (users, developers, acquirers, technologists,
testers, budget analysts, sustainers, and industry) that are, or will be, associated with
implementing and supporting the associated system, subsystem, or end-item capability
requirements.
Standard Commercial License Rights— Rights provided by the contractor in a license that
accompanies commercial software. In most contexts, the Government is obligated to abide by
the commercial license accompanying commercial software.
Supply Chain Management—Strategy for integrated life cycle management (ILCM) enterprise
sustainment that integrates acquisition of assets, supply, maintenance, and distribution functions
with the physical, financial, information, and communications networks in a results-oriented
approach to satisfy materiel requirements.
Supply Chain Risk Management—A systematic management process identifying
susceptibilities, vulnerabilities and threats throughout DoD’s “supply chain” and development of
mitigation strategies to combat those threats whether presented by the supplier, the supplied
 188                                                             AFPAM63-128 10 JULY 2014


product and its subcomponents, or the supply chain (e.g., initial production, packaging, handling,
storage, transport, mission operation and disposal).
Support Equipment/Automatic Test Systems (SE/ATS)—That equipment required to make a
system, end item or facility operational in its intended environment. It includes:
aeronautical/ground equipment e.g., maintenance stands, electrical generators, servicing carts,
etc; test measurement diagnostic equipment (TMDE) e.g., automatic test equipment (ATE),
oscilloscopes, multimeters, etc.; tools e.g., torque wrenches, manufactured jigs, borescopes, etc.;
and automatic test systems (ATS) e.g., ATE, test program sets (TPSs), and interface test adapters
(ITAs).
Support Equipment Family—Support equipment that is interoperable and has the capability to
support a variety of weapon system requirements through flexible hardware or software
architectures that permit addition or expansion of capability with minimal impact to the support
equipment logistics support profile.
Supportability—The degree to which the planned logistics support allows the system to meet its
availability and wartime usage requirements. Planned logistics support includes the following:
test, measurement, and diagnostic equipment; spare and repair parts; technical data; support
facilities; transportation requirements; training; manpower; and software.
Survivability—The ability of a system, subsystem, component, or equipment to withstand the
effects of adverse environmental conditions such as battle damage, Chemical, Biological,
Radiological, and Nuclear warfare, weather, or Acts of God that could otherwise render the ship,
aircraft, or weapon system unusable or unable to carry out its designed function. Survivability
also enables rapid restoration of the system, subsystem, component, or equipment to increase the
sustainability of the war-fighting operations. A survivability analysis, accomplished early in the
acquisition phase, influences the design and identifies additional support resources required to
maintain system readiness.
Sustainability—The ability to maintain the necessary level and duration of operational activity
to achieve military objectives – Sustainability is a function of providing for and maintaining
those levels of ready forces, materiel, and consumables necessary to support military effort.
Sustainment—Continuing materiel support which consists of the planning, programming, and
execution of a logistics support strategy for a system, subsystem, or major end item to maintain
operational capabilities from system fielding through disposal.
System—Any organized assembly of resources and procedures united and regulated by
interaction or interdependence to perform a set of specific functions.
System of Systems (SoS)—A set or arrangement of interdependent systems that are related or
connected to provide a given capability. The loss of any part of the system could significantly
degrade the performance or capabilities of the whole. The development of an SoS solution will
involve trade space between the systems as well as within an individual system performance.
System Training Plan (STP)—An iterative planning document that defines the justification,
design, development, funding, resources, support, modification, operation, and management of a
Training System. The STP is designed to provide for planning and implementation of training
and to make sure all resources and supporting actions required for establishment and support are
considered. The STP may be a stand-alone document or part of a Life Cycle Management Plan
AFPAM63-128 10 JULY 2014                                                                       189


(AS) or other appropriate planning document when LCMP is not required. All references to the
STP in this document incorporate the possibility that the intended documentation may be part of
a AS.
Systems Engineering (SE)—An interdisciplinary approach encompassing the entire set of
scientific, technical, and management efforts needed to conceive, evolve, verify, deploy, and
support an integrated and life cycle balanced set of system solutions that satisfy customer needs.
Systems engineering, through technical and management processes, addresses architectures;
requirements development; design; technical management; test and evaluation; verification and
validation; operational safety, suitability, and effectiveness (OSS&E); environment, safety, and
occupational health (system safety); and human systems integration. These fundamental
elements must be accomplished on all development, acquisition, and sustainment activities to
develop a relevant technical knowledge base that is matured, maintained, and transferred in a
disciplined manner.
Tailoring—The manner in which certain core issues (program definition, program structure,
program design, program assessments, and periodic reporting) are addressed in a particular
program. The Milestone Decision Authority (MDA) seeks to minimize the time it takes to
satisfy an identified need consistent with common sense, sound business management practice,
applicable laws and regulations, and the time sensitive nature of the requirement itself. Tailoring
may be applied to various aspects of the acquisition process, including program documentation,
acquisition phases, the time and scope of decision reviews, Supportability Analysis, and
decisions levels consistent with all applicable statutory requirements.
Technical Data—Information, regardless of the form or method of the recording, of a scientific
or technical nature, including computer software documentation. It includes information
required for the design, development, production, manufacture, assembly, operation, training,
testing, repair, maintenance, or modification of defense articles. Relative to software it includes
information on system functional design, logic flow, algorithms, application programs, operating
systems, and support software for design, implementation, test operation, diagnosis, and repair.
It does not include computer software or data incidental to contract administration or general
scientific, mathematical, or engineering principles commonly taught in schools, or information in
the public domain.
Technical order (TO)—AF procedures developed or acquired for performance of organic
operation, maintenance, inspection, modification, or management (exclusive of administrative
procedures) of centrally-acquired and managed AF systems or commodities. TOs include paper
and digital media developed to Technical Manual Specifications and Standards (TMSS),
contractor-developed manuals adopted for AF use, and approved commercial-off-the-shelf
(COTS) manuals. The term “Technical Order (TO)” is equivalent to the DoD term “Technical
Manual (TM)”.
Technology Readiness Assessment (TRA)—A systematic, metrics-based process and
accompanying report that assesses the maturity of Critical Technology Elements (CTE) used in
systems. The resulting TRA report details how the CTEs are identified, why they are important
to the program, and a program-independent assessment of their maturity. The TRA also provides
supporting information for the Title 10 (§2366b) Milestone Decision Authority certification that
the technology in the program has been demonstrated in a relevant environment for major
defense acquisition programs (MDAP) prior to Milestone B approval.
 190                                                            AFPAM63-128 10 JULY 2014


Test and Evaluation (T&E)—The act of generating empirical data during the research,
development or sustainment of systems, and the creation of information through analysis that is
useful to technical personnel and decision makers for reducing design and acquisition risks. The
process by which systems are measured against requirements and specifications, and the results
analyzed so as to gauge progress and provide feedback.
Test and Evaluation Master Plan (TEMP)—A document detailing the overall structure and
objectives of the T&E program. It provides a framework within which to generate detailed T&E
plans, and it documents schedule and resource implications associated with the T&E program.
The TEMP identifies the necessary developmental, operational, and live-fire test activities. It
relates program schedule, test management strategy and structure, and required resources to
critical operational issues (COIs); critical technical parameters; objectives and thresholds
documented in the requirements document; and Milestone decision points. The TEMP may be
included in an AS as a T&E annex.
Test and Evaluation Strategy—The overarching integrated T&E outline for the entire
acquisition program that describes how operational capability requirements will be tested and
evaluated in support of the acquisition strategy. Developed prior to Milestone A, the T&E
strategy addresses modeling and simulation, risk and risk mitigation, development of support
equipment, and identifies how system concepts will be evaluated against mission requirements,
among other things. The T&E strategy is a precursor to the test and evaluation master plan.
Testable—The attribute of being measurable with available test instrumentation and resources.
Note: Testability is a broader concept indicating whether T&E infrastructure capabilities are
available and capable of measuring the parameter. The difference between testable and
measurable may indicate a test limitation. Some requirements may be measurable but not
testable due to T&E infrastructure shortfalls, insufficient funding, safety, or statutory or
regulatory prohibitions.
Total Contract Training (TCT)—A contractor support (CS) method to provide a contractor-
operated performance-based training system.
Total Ownership Cost (TOC)—Total ownership cost encompasses all cost associated with
development, production, operations, support, and disposal of a weapon system.
Training Devices—Aircrew training systems, maintenance training systems, ground based
training systems, training devices for mission command and control, training equipment,
range/scoring systems, maintenance trainers, physiological/aeromedical and treatment devices,
space and missile training devices/systems, etc., which provide individual training for personnel
assigned as pilots, navigators, radar operators, flight engineers, maintenance personnel, boom
operators, load masters, gunners, and/or crew training in aspects of the operational mission. The
term “training devices” does not include trainer aircraft.
Training Planning Team (TPT)—Responsible accomplishing the Training System
Requirements Analysis (TSRA) and then documenting training requirements for inclusion in the
Acquisition Strategy (AS) or the System Training Plan (STP). It is recommended that TPT
meetings will be held annually. This meeting will maintain and document training system
quality and concurrency with the operational system. The TPT should be established and
operational before the system acquisition strategy is developed, as early as Milestone A (Defense
Acquisition Board); the acquisition strategy will be coordinated by the TPT Chair.
AFPAM63-128 10 JULY 2014                                                                       191


Training System Requirements Analysis (TSRA)—The TSRA is a formal and systematic
front-end analysis of the weapon system to determine training system requirements and provides
alternative solutions for a training system acquisition or modification. The TSRA uses the
Instructional System Development (ISD) process and supportability analyses to address total
training requirements (training hardware, software, facilities, instructional media, etc.)
throughout the life cycle of the weapon system being defined.
Unlimited Rights (UR)— Permits the Government to use technical data and computer software
without and limits whatsoever, including offering the data to other companies for their
competition with the owner of the data or software in the commercial marketplace as well as in
the Government marketplace.
Validated Needs—Capability objectives identified and approved by the capability based
planning (CBP) process, or requirements development within the CBP process.
Verification, Validation, and Accreditation (VV&A)—A continuous process in the life cycle
of a model or simulation as it gets upgraded or is used for different applications.
— Verification: Process of determining that modeling and simulation (M&S) accurately
represent the developer’s conceptual description and specifications.
— Validation: Rigorous and structured process of determining the extent to which modeling and
simulation (M&S) accurately represent the intended real world phenomena from the perspective
of the intended M&S user.
— Accreditation: The official determination that a model or simulation is acceptable for use for
a specific purpose.
Vulnerability—The characteristics of a system that causes it to suffer a definite degradation
(loss or reduction of capability to perform its designated mission) as a result of having been
subjected to a certain (defined) level of effects in an unnatural (man-made) hostile environment.
Vulnerability is considered a subset of survivability. Vulnerability in an information system is a
weakness in system security procedures, internal controls, or implementation that could be
exploited.
Warfighter—An individual or organization who executes military force or is responsible for
making operational decisions that result in the use of military force. The term includes field
level personnel assigned to an Air and Space Expeditionary Force (AEF) whose duties support
AF core competencies and distinctive capabilities.
Weapon System—A combination of elements that function together to produce the capabilities
required for fulfilling a mission need, including hardware, equipment, software, and all
performance based logistics (PBL) sustainment elements, but excluding construction or other
improvements to real property.
XR—Term used to describe the Developmental Planning/Capability Planning/Requirement
Directorates located at the product centers.


Websites
Note: Some websites require Air Force Portal sign-on or membership to gain access
 192                                                            AFPAM63-128 10 JULY 2014


United States Code (main search page): http://www.gpoaccess.gov/uscode/index.html
DoD Issuances: http://www.dtic.mil/whs/directives/index.html
Defense Acquisition Guidebook (formerly DoD 5000.2-R):
https://acc.dau.mil/CommunityBrowser.aspx?id=46365&lang=en-US
Federal Acquisition Regulation :
http://farsite.hill.af.mil/reghtml/regs/far2afmcfars/fardfars/far/Far1toc.htm#TopOfPage
AFFARS : http://farsite.hill.af.mil/vfaffar1.htm
Acquisition Community Connection: https://acc.dau.mil
Air Force e-Publishing website: http://www.e-publishing.af.mil/.
Air Force Privacy Office: http://www.privacy.af.mil/main/welcome.asp
Air Force Records Disposition Schedule (RDS):
https://www.my.af.mil/afrims/afrims/afrims/rims.cfm
AFPEO/CM website: http://ww3.safaq.hq.af.mil/organizations/peo/
Defense Logistics Information Service Commercial and Government Entity (CAGE) Code:
http://www.dlis.dla.mil/cage_welcome.asp
DFARS: http://www.acq.osd.mil/dpap/dars/dfarspgi/current/index.html
DoD Anti-Tamper website: http://at.dod.mil/
DoD ATS Executive Directorate: http://www.acq.osd.mil/ats
DoD Contracts Incentives Guide:
https://learn.dau.mil/CourseWare/801321_2/module_1/docs/incentivesguide-0201.htm
DoD Corrosion and Prevention Guidebook (Home - CorrDefense): http://www.corrdefense.org/
DoD Diminishing Manufacturing Sources and Material Shortages (DMSMS) Guidebook:
http://www.dmsms.org/
DoD Financial Management Regulation (FMR): http://comptroller.defense.gov/FMR.aspx
DoD Information Technology Standards Registry (DISR) on-line website:
https://acc.dau.mil/CommunityBrowser.aspx?id=148577
DoD Item Unique Identification of Government Property Guidebook:
http://www.acq.osd.mil/dpap/pdi/uid/guides.html
DSOR Guide: https://acc.dau.mil/CommunityBrowser.aspx?id=46407
Earned Value Management: http://www.acq.osd.mil/evm/ /
IT Lean reengineering Guidebook: https://acc.dau.mil/CommunityBrowser.aspx?id=292300
JCTD webpage: http://www.acq.osd.mil/rfd/
Military Engineering Data Asset Locator System (MEDALS): https://www.dlis.dla.mil/medals/
Office of Management and Budget Circular A-11, Part 7:
http://www.whitehouse.gov/sites/default/files/omb/assets/a11_current_year/part7.pdf
AFPAM63-128 10 JULY 2014                                                                 193


OSD’s Military Equipment Website: http://www.acq.osd.mil/me/
OSD Item Unique Identification Website: http://www.acq.osd.mil/dpap/pdi/uid/index.html
Product Data Acquisition website:
https://www.my.af.mil/gcss-af/USAF/site/ACQUISITION/ACE/PLM
Risk Management Guide for DoD Acquisition : http://www.acq.osd.mil/se/docs/2006-RM-
Guide-4Aug06-final-version.pdf
SAF/FM New Start Homepage (on AF Portal): https://www.my.af.mil/gcss-
af/afp40/USAF/ep/browse.do?programId=407864&pageId=1073762823&channelPageId=-
351617&parentCategoryId=-351659
SAF/AQX-ACE website: https://www.my.af.mil/gcss-
af/USAF/ep/browse.do?programId=1442689&channelPageId=-2055590&parentCategoryId=-
2076886
Wide Area Workflow (WAWF) process: https://wawf.eb.mil/
 194                                                       AFPAM63-128 10 JULY 2014


                                      Attachment 2
       HSI-RELATED STANDARDS, HANDBOOKS, AND DIDS; SAMPLE HSI
               CHECKLISTS; AND KEY HSI-RELATED TERMS

A2.1. Human System Integration Tables and Checklists.

Table A2.1. HSI-Related Standards, Handbooks, and DIDs.
                                   Sample HSI and HSI DIDs
                                Title                                  Item
                                  HSI in Systems Engineering
Systems Engineering Management Plan (SEMP)                     DI-SESS-81785
                                  Human Systems Integration
Human Systems Integration Program Plan (HSIPP)                 DI-HFAC-81743
Human Systems Integration Report (HSIR)                        DI-HFAC-81883
                                    Manpower and Personnel
Logistics Product Data Summaries                               DI-SESS-81759
Technical Report – Study Services                              DI-MISC-80508
                                            Training
Training Situation Document                                    DI-SESS-81517
Instructional Performance Requirements Document                DI-SESS-81518
Instructional Media Requirements Document                      DI-SESS-81519
Instructional Media Design Package                             DI-SESS-81520
Training Program Structured Document                           DI-SESS-81521
Training Evaluation Document                                   DI-SESS-81524
Test Package                                                   DI-SESS-81525
Instructional Media Package                                    DI-SESS-81526
Training System Support Document                               DI-SESS-81527
                                  Human Factors Engineering
DoD Design Criteria Standard Human Engineering                 MIL-STD-1472
DoD Standard Practice for Human Engineering Requirements for   MIL-STD-46855
Military Systems, Equipment, and Facilities
Human Engineering Simulation Concept (HESC)                    DI-HFAC-80742
Human Engineering Test Plan (HETP)                             DI-HFAC-80743
Human Engineering Test Report (HETR)                           DI-HFAC-80744
Human Engineering System Analysis Report (HESAR)               DI-HFAC-80745
Human Engineering Design Approach Document – Operator          DI-HFAC-80746
(HEDAD-O)
Human Engineering Design Approach Document – Maintainer        DI-HFAC-80747
(HEDAD-M)
Critical Task Analysis Report (CTAR)                           DI-HFAC-81399
Human Engineering Program Plan (HEPP)                          DI-HFAC-81742
Anthropometry of U.S. Military Personnel (Metric)              DoD-HDBK-743
Electronic Reliability Design Handbook                         MIL-HDBK-338
Designing and Developing Maintainable Products and Systems     MIL-HDBK-470
AFPAM63-128 10 JULY 2014                                                                195


Human Engineering Design Guidelines                                 MIL-HDBK-759
Definitions of Human Factors Terms                                  MIL-HDBK-1908
                         Environment, Safety and Occupational Health
Environmental Engineering Considerations and Laboratory Tests       MIL-STD-810
DoD Standard Practice for Systems Safety                            MIL-STD-882
Safety Design Requirements for Military Lasers and Associated       MIL-STD-1425
Support Equipment
Electroexplosive Subsystem Safety Requirements and Test Methods MIL-STD-1576
for Space Systems
Ammunition Data Card                                                DI-MISC-80043
System Safety Hazard Analysis Report                                DI-SAFT-80101
Safety Assessment Report                                            DI-SAFT-80102
Engineering Change Proposal System Safety Report                    DI-SAFT-80103
Waiver or Deviation System Safety Report                            DI-SAFT-80104
System Safety Program Progress Report                               DI-SAFT-80105
Health Hazard Assessment Report (HHAR)                              DI-SAFT-80106
Radiation Hazard Control Procedures                                 DI-SAFT-80184
Safety Engineering Analysis Report                                  DI-MISC-80370
Technical Report – Study Services                                   DI-MISC-80508
Safety Studies Plan                                                 DI-SAFT-81066
Threat Hazard Assessment                                            DI-SAFT-81124
Hazard Assessment Test Report                                       DI-SAFT-81125
Vibration Test Data                                                 DI-SAFT-81128
Explosive Hazard Classification Data                                DI-SAFT-81299
Mishap Risk Assessment Report                                       DI-SAFT-81300
Hazardous Material Management Program (HMMP) Report                 DI-MISC-81397
Hazardous Materials Management Program (HMMP) Plan                  DI-MGMT-81398
Ozone Depleting Substance (ODS) Plan                                DI-SAFT-81479
Failure Mode, Effects, Criticality Analysis Report                  DI-ILSS-81495
System Safety Program Plan (SSPP)                                   DI-SAFT-81626
                                           Survivability
Human Systems Integration Program Plan (HSIPP)                      DI-HFAC-81743
                                           Habitability
Color Coordination Manual(s) for Habitability Spaces                DI-MISC-81123
Human Systems Integration Program Plan (HSIPP)                      DI-HFAC-81743

Table A2.2. Sample HSI Checklist (by Phase).
Capabilities Based Assessment
Draft an initial Target Audience Description for the populations expected to operate,
maintain, sustain, and train the capability
Document the analyses needed to identify the human related impacts on operational
performance for each potential materiel solution
Review the CONOPS and Operational Concepts for new and existing tactics and doctrine
that may influence capability design and human related requirements
 196                                                             AFPAM63-128 10 JULY 2014


Review the projected threat environment for potential new threats and the impact they
may have on crew survivability
Materiel Solutions Analysis
Review historical information from similar predecessor systems to identify potential to
encounter similar HSI issues
Ensure all HSI drivers of the concept definition are identified, captured and managed as an
integral human centered design effort
Define and relate human performance to capability needs, the updated CONOPS and
emerging system architecture views.
Assess and document derived human related requirements at the system performance level
Establish a process for identification, tracking and mitigation of human related risks into
the program’s risk management activities
Analyze human related implications of critical performance elements of the alternative
materiel solutions
Determine costs associated with HSI support through the system life cycle and ensure
these costs are included in cost estimates
Support identification and allocation of functions and tasks to humans and automation
Support identification of preliminary environment, safety and occupational health risks for
materiel solution alternatives
Plan and execute modeling and simulation activities, to include mock-ups, prototypes and
other simulations, to address human-related risks
Technology Maturation & Risk Reduction
Assess human readiness/suitability of potential critical technology items and identify the
human/system performance impacts if a technology item is unavailable
Identify human capability/limitation related requirements in any system or subsystem
performance specification, solicitation, contract and evaluation criteria
Establish HSI related measures for use in system test activities
Plan and execute modeling and simulation activities, to include mock-ups, prototypes and
other simulations, to address human-related risks
Participate in engineering trade studies to ensure human capabilities/limitations and their
impact on operational performance is included
Ensure that human related risks are included in the comprehensive risk assessment
Identify human related system performance attributes and develop language for inclusion
of these attributes in requirements documents
Analyze all identify subsystem human interfaces to establish performance and design
requirements
Develop a plan for ensuring continuity of user representation throughout the acquisition
life cycle
Engineering & Manufacturing Development
Provide support to Program IPTs to ensure human considerations are included in design
analyses and decision making
Support efforts to update hazard and risk analysis and identify the impacts of those to the
HSI domains
AFPAM63-128 10 JULY 2014                                                                       197


Ensure HSI-critical design specifications are included in requirements tracking system and
detailed design specifications
Plan for and support DT&E/OT&E activities to ensure the impact of human capabilities
and limitations on operational performance are captured
Refine human related requirements which have significant impact on Total Life Cycle
costs, including manpower, training and safety/hazard mitigation requirements
Ensure requirements for all personnel groups who interact with the system are considered
in system/subsystem specifications
Participate in DT&E, OT&E, LFT&E and operational assessments of the system’s ability
to meet HSI-related requirements and roll up assessments into the program risk
management process
Plan for and execute transition of HSI-related system elements (training requirements,
hazard mitigation measures, etc.) from acquisition community to user community
Production & Deployment
Support development and implementation of training programs and devices
Participate in operational, maintenance, and sustainment assessments of the system’s
ability to meet HSI-related requirements and roll up assessments into the program risk
management process
Analyze any operational deficiencies in the system’s ability to meet HSI-related
requirements to help determine and assess corrective actions. Include these deficiencies
in risk management activities
Capture relevant lessons learned from IOT&E and catalogue for use in future modification
or system development efforts
Operations & Support
Acoustical Energy
Participate in system safety/incident reviews to identify human related root causes
Evaluate test results for HSI implications and to determine the effectiveness of design
decisions and risk mitigation measures
Monitor engineering change proposals and modification plans to mitigate unintended
consequences between HSI domains

Table A2.3. Sample HSI Checklist (by Domain).
Manpower
1. Is there a legacy system to use as a manpower baseline?
2. Do the manpower levels need to be constrained to the same level as the predecessor system?
3. Will the manpower mix (military, civilian, contractors) change significantly?
4. Is there a mandate to optimize or reduce manpower authorizations?
5. Have manpower authorizations been justified and/or modified to meet mission needs?
6. Will an increase in end-strength be required?
7. What are the end-strength offsets?
8. Approximately how many authorizations will it take to operate, maintain, train and support
the full capability? (Full capability includes all operational and maintenance [local and remote]
components.)
 198                                                              AFPAM63-128 10 JULY 2014


9. What manpower estimate was used for the affordability assessment?
10. How does the manpower estimate compare to current requirements and authorizations?
11. How much could manpower grow before it would impact the affordability decision?
12. If the manpower estimate is greater than authorizations, what is the resource sponsor’s
position regarding funding?
Personnel
1. Are there any current or projected recruiting, retention, and/or career development issues for
the personnel who are most likely to be required to operate, maintain, and support the capability?
2. Are there any current or projected pay/bonus/incentives required for the personnel
communities who are most likely to be required to man the capability? Does this affect cost
estimates and affordability assessments?
3. Are there any career path implications based on manning concepts being considered?
4. Are there any implications for rotation, deployed time, turnover/detailing based on the
manning strategy discussed?
5. Will significantly new skill sets, knowledge bases, and abilities be required to support the
capability?
6. Is there a need for increased experience or pay grades?
7. Is there a desire and/or need for unique combinations of skill sets, knowledge bases, and
abilities?
8. Are the skill sets, knowledge base, and abilities required by the new capability projected to be
available in sufficient numbers in the timeframe required?
9. Are there any known or projected changes to gender mix, cognitive abilities, physical
characteristics, psychomotor skills, and/or experience level?
10. Does the materiel solution take into account the projected personnel pool?
11. Does the materiel solution require a change in the Air Force Specialty Code (AFSC)
structure? See AFI 36-2101, Classifying Military Personnel (Officer and Enlisted).
12. Are new AFSCs required? Can the AFSCs be combined?
13. Are current accession screening methods (i.e., ASVAB) sufficient to ensure the new
capability can be operated, maintained, and supported?
Training
1. Was any part of the capability gap related to human performance or training deficiencies?
2. Could temporary or interim training be implemented to partially satisfy the capability gap,
and/or improve mission performance with current systems until the proposed materiel solution
can be developed and deployed?
3. Will deployment/employment of the new capability change tactics and decision-making?
4. Will changes in either individual or team training be required to address the change to tactics
and/or decision–making?
5. Has the crew been tested for preliminary workload estimates in visual, auditory, motor, and
cognitive capacity? Do they meet requirements?
6. Is there any new training needed to address unique combinations of skill sets, knowledge
bases, and abilities, such as that required for new AFSC, changes to existing AFSCs?
7. Will there be sufficient time to adjust and implement required changes to training?
8. Have total system operational performance, support, or life cycle cost objectives and
thresholds been defined?
AFPAM63-128 10 JULY 2014                                                                        199


9. Will the materiel solution change who is to be trained (Active Duty, Air Force Reserve, Air
National Guard, Civilian, or Contractor)?
10. Will the materiel solution change who is to conduct the training (Government, Contractor)?
11. Will the materiel solution change where the training is conducted (Contractor Facilities, AF
Technical Centers)?
12. Will the materiel solution impact the timing of the training (Duration, Availability)? Does
this affect cost estimates and affordability assessments?
13. Will the materiel solution change the method of training used (Classroom, Computer-based,
On-the-job)?
Human Factors
1. Does the materiel solution being discussed present any significant challenges, implications or
constraints in the following areas:
Work/living space (especially number/size of berthing spaces)
System or display integration
Operability/Maintainability
Anthropometry/Ergonomics
Automation
Ambient environment
2. Does the materiel solution require a new system interface or modification to an existing
interface?
3. Does the materiel solution require new forms of collaboration between humans and/or across
systems?
4. Are there new lighting conditions? (night, all weather)
5. Is there special gear required that may impact task performance (Mission Oriented Protective
Posture (MOPP) Gear, Cold Weather Gear)?
6. Are there manpower or personnel issues that may impact the system interface
(Anthropometry)?
7. Will new technology impact the interface (Automation, Aiding)?
8. Does the materiel solution require the performance of additional tasks?
9. Are there specific performance thresholds and objectives that impact mission outcome?
10. Are there time limitations for task accomplishment?
11. Are there accuracy requirements for task accomplishment?
Environmental
1. What types of Hazardous Materials (HAZMAT) will be required for Operations and
Maintenance? Can these be substituted and/or eliminated?

2. What are the anticipated air emissions from the system? Can they be reduced?
3. What are the anticipated hazardous waste streams? Can they be recycled and/or eliminated?

4. What are the noise levels for the system? Can they be reduced?
5. If HAZMAT and Waste cannot be eliminated then there will be additional training
requirements for their use, handling, storage and disposal.
6. What are the system demilitarization and disposal requirements? Will this process generate
waste with special handling/disposal requirements?
 200                                                              AFPAM63-128 10 JULY 2014



Safety (see MIL_STD-882)
1. Has a safety risk assessment been completed?
2. Have safety risks concerning power sources been considered?
Electrical
Mechanical
Hydraulics/Pneumatics
Chemical/explosive/propellants
3. Look for safety risks associated with:
Exposed, moving equipment
Radio Frequency (RF)/Microwave (MW) antenna
Hazardous materials or by-products
Combustion processes
High temperature devices
Vehicular movement/flight
Gun systems
Missile systems
4. Ensure design requirement statements have been developed to address/prevent the impact of:
Catastrophic loss of materiel system or Airman due to failure/malfunction of component or
procedural error/omission
Operational loss of system or disabling injury due to failure/malfunction of component or
procedural error/omission
Loss of system effectiveness or injury due to failure/malfunction of component or procedural
error/omission
5. Are all trade-offs or impact issues looked at for their effects on all other HSI domain as well
as system cost and performance requirements (e.g., excessive training and personnel capability
requirements to compensate for materiel system design weaknesses?
6. Are all functional, cost and performance data, as well as assumptions and other criteria,
consistent with other analyses being performed on the system?
7. Is the system safe to operate, maintain, repair, and support?
Occupational Health
Acoustical Energy
1. Does this system meet the standards for steady state noise under the most severe operational
and maintenance scenarios?
2. Does this system meet the standards for impulse noise under the most severe operational and
maintenance scenarios?
3. Does this system meet the standards for blast overpressure under the most severe operational
and maintenance scenarios?
Biological Substances
4. Does the system configuration preclude exposure to microorganisms, their toxins and
enzymes?
Chemical/Other Substances
5. Does this system produce or release any toxic substance during maintenance and operation?
6. Are personnel exposed to unacceptable levels of gases/vapors/fumes generated by the
operation?
AFPAM63-128 10 JULY 2014                                                                     201


7. Are there any unacceptable levels of toxic gases in the crew compartment when the vehicle is
operating and/or during weapons firing?
8. Will any materials used decompose or react under extreme heat (pyro lytic) or in the presence
of another substance to produce toxic fumes, gases, or vapors?
9. Is the crew effectively/adequately protected against Nuclear, Biological, and Chemical (NBC)
agents?
10. Has each chemical or toxic material used in or with the system been identified in the health
hazard assessment report?
11. Does a hazard from exposure to_______exist?
12. Are personnel adequately protected from fire extinguishing agents?
13. Do hazards from excessive dust in crew compartments exist?
Oxygen Deficient Atmosphere
13. Is there any potential for an oxygen deficient atmosphere in occupied spaces or
compartments?
14. Will occupied spaces contain Halon 1301 automatic fire extinguishing systems that comply
with Office of the Surgeon General (OTSG) and National Fire Protection Association (NFPA)
requirements?
Radiation Energy
15. Are there hazards or potential hazardous exposures from ionizing radiation sources during
operation training, and maintenance?
16. Are there hazards or potential hazardous exposures from non-ionizing sources during
operation, training, and maintenance?
17. Does the system contain any lasers detrimental to health?
18. Has the system been evaluated for potential radiation health hazards?
Physical Forces
19. Will this system cause any long term disability issues?
20. Is adequate protection provided to preclude trauma to the eyes or body surface during system
operation or from personal protective equipment?
21. Does the system meet vibration and shock requirements under all operational conditions?
22. Are there potential hazards from high pressure gases or fluids?

Temperature Extremes
23. Is there any potential exposure to extreme heat or cold during operation or maintenance that
will adversely affect personnel?
24. Does the system provide adequate heating, cooling, and ventilation under routine, severe,
and emergency conditions?
25. Are there any hazards associated with cryogenics?
Miscellaneous
26. Have health problems identified with reference systems and components been addressed and
abated in this system?
27. Are health hazards identified during DT&E,
 IOT&E and OT&E being resolved?
Habitability
 202                                                              AFPAM63-128 10 JULY 2014


1. Does the system exhibit unacceptable conditions that might affect human performance
capabilities (i.e., vision, olfaction, taste, hearing, reaction time, motor skills, strength, and
cognitive skills)?
2. What is the overall acceptability of the physical environment (i.e., noise, lighting, odor,
temperature control, humidity, temperature, contaminants)?
3. Have personnel services (i.e., nutrition, water, sleep, exercise, medical care (preventive,
diagnostic, treatment)) been considered?
4. Were living conditions (i.e., personal hygiene, body waste management, crew quarters, mess,
exercise area, recreation, trash, stowage, etc.) considered?
Personnel Survivability
1. Will the proposed capability increase the number/type (especially civilians and/or contractors)
of individuals placed in harm’s way?
2. Does the materiel solution introduce a new threat?
3. Does the materiel solution change egress systems requirements (if applicable)?
4. Was any part of the capability gap related to a fratricide incident or failure of personnel to
survive a mishap?
5. Does the Concept of Operations (CONOPS) for the proposed capability increase the
likelihood of fratricide and/or need for improved personnel survivability features?
6. Does the materiel solution impact Identify Friend/Foe (IFF) equipment?
7. Is the related IFF or target identification system effective to ranges at least as long as the
weapons range?
8. Is the system’s signature (visible, electromagnetic, etc.) similar to potential threat vehicles?
9. Is the IFF system a non-cooperative target recognition system (i.e., if an enemy tries to target
you to find your position, does the system refuse to cooperate so as not to give any information
to the enemy)?
10. Does the self-location equipment provide sufficient resolution to reduce fratricide?
11. Is the system’s ability to distinguish between friendly and enemy targets compatible with
mission oriented protective posture level IV (MOPP IV) (NBC individual protective equipment)
conditions?
Reduce Detectability
12. Does the materiel solution change detectability?
13. Is the system likely to be detected by unfriendly forces because of: Visible static signature?
Thermal (infrared) signature? Radio-frequency signature?
14. Have any electro-optical or optical components on the system been hardened to reduce
optical cross-sectional measurements that are the cause of wide-angle and at-range detection?
15. Will unfriendly forces’ use of obscurants prevent the system from detecting approaching
systems?
Reduce Probability of Attack
16. Does the materiel solution change the probability of attack?
17. Is the system able to deflect attack by the use of: Active ballistic interdiction to deflect or
destroy incoming munitions? Electronic jamming or spoofing of munitions sensors?
18. Has the system microprocessor code been protected from the presence or insertion of
malicious code?
19. Does the system present a unique or highly recognizable signature (visual, thermal, etc.)?
AFPAM63-128 10 JULY 2014                                                                     203


Reduce Damage
20. Does the materiel solution reduce damage?
21. Does the materiel solution require a change in attack and attack prevention measures?
22. Does the system adequately protect the crew from direct- and indirect-fire munitions through
the specific damage mechanism of spall?
23. Does the system provide crew protection from secondary explosions of the on-board
munitions if the system is attacked, by means of separation of ammunition storage in a
compartment isolated from the crew?
24. Does the system provide adequate crew protection from directed-energy weapons such as
lasers?
25. Does the system provide adequate warning and protection for the crew in a nuclear,
chemically or biologically contaminated environment?
26. Will the system be able to operate in the presence of external electromagnetic environmental
effects without affecting crew members and other military personnel?
Minimize Injury
27. What are the potential sources for personnel injury in the system design and when the
Airman and equipment are functioning in the field?
28. What is the system’s ability to prevent further injury to the Airman after being attacked or
exposed to a hostile environment?
29. What is the ability of the system to support treatment and evacuation of the injured?
Minimize Physical and Mental Fatigue
30. What are the physical constraints and workload placed on the Airman by the system?
31. What are the cognitive constraints and workload placed on the Airman by the system?
32. What is the system’s ability to minimize the effect of environmental stressors on the
Airman?
33. What is the system’s ability to minimize the effect of mechanical (system-produced)
stressors on the Airman?
34. What is the system’s compatibility with crew life support and continuous operations?
Survive Extreme Environments
35. What are the extreme environments in which the Airman will use the system?
36. What is the system’s ability to minimize the effect of arctic temperatures?
37. What is the system’s ability to minimize the effect of high climatic temperatures?
38. What are the special considerations concerning extreme conditions to maintain an
individual’s life when operating in a sea or air environment until rescued or an improved
situation on land is reached?
A2.2. Key HSI-Related Terms . Following are explanations or clarifications of terms used in
an HSI context:
Accessible—Refers to a design that enables ingress, egress, reach, and/or pass-through a
confined space to perform a specified task set. See MIL-HDBK 1908B for a detailed
description.
Accessibility—A measure of the relative ease of admission to the various areas of an item for the
purpose of operation or maintenance
 204                                                              AFPAM63-128 10 JULY 2014


All-weather (or Adverse-weather) —Typically used when trying to describe the environmental
conditions under which a system must operate without performance degradation. The term is
usually intended to ensure that weather conditions (such as rain, fog, heat or cold) do not
adversely impact the system or the operator.
Cognitively Ergonomic Display/Control Configuration—Arrangement of components of
information and control/input devices in complex, multiple display/control systems to most
effectively support all critical task functional sequences and decision processes; complementary
with Situational Awareness goals and design considerations.
Comfortable—Refers to a desirable quality of a component system design such that it
accommodates the human body or parts, across a range of sizes and ensembles, so as not to cause
pain or interfere or stress the individual’s capacity to perform. Best practice is to describe the
impacts of the following specific comfort-related items on overall performance: pressure points,
hot spots, restrictions in movement, chafing, restricted space, restricted circulation, distraction,
blisters, forced extra movement, strains (physical, muscle), extra tension, long term injury,
weight distribution, or repetitive motion injuries.
Commonality—A quality that applies to materiel or systems; such as (a) possessing like and
interchangeable characteristics enabling each to be used, or operated and maintained, by
personnel trained on the others without additional specialized training; (b) having
interchangeable repair parts and/or components; and, (c) applying to consumable items
interchangeably, equivalent without adjustment.
Compatibility—The ability of systems, equipment, devices and materiel to operate in their
intended operational environments without suffering unacceptable degradation or without
causing unacceptable performance interactions or responses; it involves the application of sound
system, equipment, device and materiel design configurations that ensures interference-free
operation, and clear concepts that maximize operational effectiveness.
Dependability—The ability to fulfill the required performance under given conditions, taking
degradation of performance due to failure and maintenance into consideration.
Environment—Considers measures to directly protect the human element of the total system
from the operational environment (e.g., shock, vibration, extreme temperatures, etc.) and to
indirectly protect the human by protecting the environment (e.g., water, air, land, space,
cyberspace, markets, organizations, and the relationships that exist among them and with all
living things) from adverse effects associated with system development, manufacturing,
operations, sustainment, and disposal activities.
Fatigue—Refers to the consequence (both physical and mental) of sustained performance over
time. Best practice is to address specific metrics that can be impacted by fatigue such as: error
rate, error rate variability (more errors observed with prolonged performance), missed cues,
reduced vigilance (incorrectly categorized signals), reduced maximum strength (“lift”
requirement) or reduced sustained strength (“carry” requirement). Describe or specify the
expected task performance duration(s) so that test and evaluation activities can test performance
over time.
Force Protection (FP)—The FP Key Performance Parameter (KPP) may include many of the
same attributes as those that contribute to survivability; however, the FP KPP places emphasis on
protecting the system operator or other personnel rather than protecting the system itself (cf.
AFPAM63-128 10 JULY 2014                                                                          205


Survivability KPP). Many HSI-related requirements and attributes can be traced to and/or
derived from the FP KPP.
Habitability—Involves characteristics of system living and working conditions such as lighting;
ventilation; adequate space; vibration, noise, and temperature control; availability of medical
care, food, and/or drink services; suitable sleeping accommodations, sanitation and personal
hygiene facilities, and fitness/recreation facilities. Attention to such characteristics is necessary
to sustain high levels of personnel morale, motivation, quality of life, safety, health, and comfort,
contributing directly to personnel effectiveness and overall system performance, especially
during sustained/extended operations/performance. These habitability characteristics also
directly impact personnel recruitment and retention. Some operational/organizational, technical,
or mission issues may preclude completely addressing all habitability concerns: hence, other HSI
domains may need to engage to mitigate the resulting effects on system personnel and
performance.
Human Factors Engineering (HFE)—Involves an understanding of human capabilities (i.e.,
cognitive, physical, sensory, and team dynamic) and comprehensive integration of those
capabilities into system design beginning with conceptualization and continuing through system
disposal. A key objective for HFE is to clearly characterize the actual work to be performed, and
use this information in creating effective, efficient, and safe human hardware/software interfaces
to achieve optimal total system performance (i.e., use, operation, maintenance, support, and
sustainment). HFE seeks to maximize usability for the targeted range of users/customers and to
minimize design characteristics that induce frequent or critical errors.
Interchangeable—The ability of systems, units, or forces to replace like systems, units, or
forces that possess common capabilities and like characteristics to fulfill relevant requirements
without causing unacceptable performance degradations when exchanged.
Interoperability—The ability to operate in synergy in the execution of assigned tasks.
Alternatively, the ability of systems, system of systems, units or forces to provide data,
information, materiel, and services to, and accept the same from, other systems, units or forces
and use the data, information, materiel and services so exchanged, to enable them to operate
effectively together. Best practice is to describe the conditions and/or criteria that are necessary
to achieve interoperability.
Intuitive (control) —Intuitive here refers to a control that is typical of controls widely used
and/or is consistent in directions, locations or types of force applications in a way that most
intended users quickly understand and use as they support the intended task set.
Intuitive (display)—Intuitive refers to a display that, beyond readability and regardless of
modality, presents relationships among critical components of information in a way that most
intended users quickly understand as they support the intended task set.
Manpower—Addresses the number and type of personnel in the various occupational specialties
required and potentially available to train, operate, maintain, and support the deployed system -
The manpower community promotes pursuit of engineering designs that optimize the efficient
and economic use of manpower, keeping human resource costs at affordable levels.
Determination of required manpower positions must recognize the evolving demands on humans
(cognitive, physical, and physiological) and consider the impacts that technology can make on
humans integrated into a system.
 206                                                              AFPAM63-128 10 JULY 2014


Occupational Health—Promotes system design features and procedures that serve to minimize
the risk of injury, acute or chronic illness, and disability; and enhance job performance of
personnel who operate, maintain, or support the system. The occupational health community
prompts design features to prevent health hazards where possible and recommends personal
protective equipment, protective enclosures, or mitigation measures where health hazards cannot
be avoided. Prevalent issues include noise and hearing protection; chemical exposures and skin
protection; atmospheric hazards (e.g., confined space entry and oxygen deficiency); vibration,
shock, acceleration, and motion protection; ionizing/non‐ionizing radiation and personnel
protection; human factors considerations that can result in chronic disease or discomfort (e.g.,
repetitive motion injuries or other ergonomic‐related problems).
Personnel—Considers the type of human knowledge, skills, abilities, experience levels, and
human aptitudes (i.e., cognitive, physical, and sensory capabilities) required to operate, maintain,
and support a system; and the means to provide (i.e., recruit and retain) such people. Personnel
recruitment, testing, qualification and selection are driven by system requirements. The
personnel community helps define the human performance characteristics of the user population
and then determine target populations to select for occupational specialties, manage recruitment,
and track retention trends. The personnel community must manage occupational specialties to
include career progression and assignments. Adequate numbers of workers in these specialties
must be recruited, trained, and assigned to meet the entire career field need. Personnel
population characteristics can impact manpower and training as well as drive design
requirements.
Physically Ergonomic Display/Control Configuration—Combines anthropometric and
biomechanical considerations to generate a physical configuration of displays and controls that
optimize task set performance potential; most relevant in complex, multiple display/control
systems; Complementary to Cognitively Ergonomic Display/Control Configuration.
Readability (or Legibility)—Information presented on the display should exceed thresholds for
sensation and perception taking into account all environmental and ensemble conditions
anticipated in operational use. A display that is “readable” does not imply that the information
presented is also understandable to the user. The physical quality of readability must be
distinguished from the cognitive workload associated with “understandable.” See MIL-STD-
411, MIL-STD-1787, and/or MIL-STD-1472 for readability design standards.
Safety—Promotes system design characteristics and procedures to minimize the risk of accidents
or mishaps that cause death or injury to operators, maintainers, and support personnel; threaten
the operation of the system; or cause cascading failures in other systems. Using safety analyses
and lessons learned from predecessor systems, the safety community prompts design features to
prevent safety hazards to the greatest extent possible and to manage safety hazards that cannot be
avoided. The focus is on designs that have back-up systems, and where an interface with
humans exists, to alert them when problems arise and also help to avoid and recover from errors.
Prevalent issues include factors that threaten the safety of personnel and their operation of the
system; walking and working surfaces, emergency egress pathways; personnel protection
devices; pressure and temperature extremes; prevention/control of hazardous energy releases
such as mechanical, electrical, fluids under pressure, ionizing or non-ionizing radiation, fire,
and/or explosions. See MIL-STD-882.
AFPAM63-128 10 JULY 2014                                                                         207


Situation Awareness—Knowledge and understanding of the current situation which promotes
timely, relevant and accurate assessment of friendly, competitive and other operations within the
battle space in order to facilitate decision making. An informational perspective and skill that
fosters an ability to determine quickly the context and relevance of events that are unfolding.
Survivability—Addresses human-related characteristics of a system (e.g., life support, body
armor, helmets, plating, egress/ejection equipment, air bags, seat belts, electronic shielding, etc.)
that reduce susceptibility of the total system to mission degradation or termination; injury or loss
of life; and partial or complete loss of the system or any of its components. These issues must be
considered in the context of the full spectrum of anticipated operations and operational
environments and for all people who will interact with the system (e.g., users/customers,
operators, maintainers, and/or other support personnel). Adequate protection and escape systems
must provide for personnel and system survivability when they are threatened with harm.
Training—Encompasses the instruction and resources required to provide personnel with
requisite knowledge, skills, and abilities to properly operate, maintain, and support systems. The
training community develops and delivers individual and collective qualification training
programs, placing emphasis on options that enhance user capabilities to include operator,
maintainer, and support personnel; maintain skill proficiencies through continuation training and
retraining; expedite skill and knowledge attainment; and optimize the use of training resources.
Training systems, such as simulators and trainers, should be developed in conjunction with the
emerging system technology. The overall training system architecture is established from the
manpower and personnel analyses, and the training delivery system may be required prior to
fielding the system so that personnel can be adequately trained to operate, maintain, and support
the system when it is fielded. Therefore, it also is important to develop the training system
concurrent with the operational system. If engineering changes are made to the operational
system, associated training architecture and delivery system changes must be re‐evaluated,
re‐planned, and appropriate modifications funded.
User friendly (or Usability)—A term used to denote the ease with which people can use a
system, component, tool, or other human-made object with little or no reference to operations
manuals in order to achieve a particular goal. Best practice is to describe in terms of common,
standard conventions (e.g., color, shape, movement) and/or single-step-action (e.g.,
minimize/eliminate repetition of task steps). See other terms Accessible, Commonality, and
Intuitive.
User configurable (or Tailorable)—Able to make or adapt for a special need or purpose. Best
practice is to provide detailed descriptions of selectable features to accommodate the ergonomics
of the intended user(s) and/or configurable options for multiple applications and/or multiple
users.
Workload—Refers to perceived and actual performance-based level of physical and/or mental
effort necessary to perform a task set in relation to a finite capacity or set of capacities. Best
practice is to describe objective measures of workload such as error rate, time-to-complete, false
negatives, interference with other tasks, measures of multitasking, etc. Sample tools to measure
workload are the Improved Performance Research Integration Tool (IMPRINT) scale, NASA
Task Load Index (NASA TLX), electrocardiography (EKG), Infrared Facial Response, Galvanic
Skin Response (GSR), eye tracking, etc.
 208                                                           AFPAM63-128 10 JULY 2014


                                        Attachment 3
       ITEM UNIQUE IDENTIFICATION IMPLEMENTATION PLAN TEMPLATE

                          (Note: Remove italics prior to submission)

A3.1. System Description: Briefly describe the military system, its mission, and its position in
    Program System Name:                                      Program Element:
    Program Manager                                           PEO
    Date of Plan                                              ACAT
     Point of Contact
the acquisition life cycle. Explain the system and its point in the Program Development Life
Cycle and current acquisition timeline. Explain the integrated components required to obtain
the objective. Identify the overarching program if this system roles up to support a broader
mission capability. This description should provide background on what the focus of IUID
implementation will be for when the reader reviews other sections such as priorities, schedules
and resource requirements.
A3.2. References:
   A3.2.1. Include applicable DoD Instructions, Directive and publications on Unique
   Identification (UID), Item Unique Identification (IUID), and Serialized Item Management
   (SIM).
   A3.2.2. Include OSD IUID policy and guidance (see OSD IUID website at
   www.acq.osd.mil/dpap/pdi/uid/index.html).
   A3.2.3. Include applicable AF policy and publications on IUID, SIM and Serial Number
   Tracking (SNT).
   A3.2.4. Identify other IUID implementation plans for weapon systems, maintenance depots,
   contractor logistic support, Defence Logistics Agency, or other organizations that support
   this plan.
   A3.2.5. Identify IUID unique training requirements. All personnel involved in IUID
   marking and tacking must complete DAU courses CLC 33, Contract Format and Structure
   for DoD eBusiness; CLE 40, IUID Marking; CLM 200, Item Unique Identification (IUID) 3.
   Exemptions, Exceptions and Approval Source: Exemptions are intended to preserve DoD
   resources by allowing for flexibility in implementing IUID requirements for legacy assets.
   Plan to complete IUID markings and registration of all existing Supply Class II
   (Expendables) and Supply class IX (Reparable Items) ,as well as all embedded assets that
   meet the criteria for IUID by 31 December 2015. Additionally, programs that will be phased
   out of the inventory and will no longer be required to support FMS customer acquisitions or
   logistics support by 31 December 2015, may be exempt from IUID Requirements.
   Exemptions remove the UII creation, registration and physical marking requirements from
   all instances of an item. Exceptions are intended to alleviate the requirement on contractors
   to uniquely identify critical items needed to support contingency operations and should not
AFPAM63-128 10 JULY 2014                                                                   209


  be requested for resource or workload limitations. This does not remove the UII creation,
  registration, and physical marking requirement from being performed by the AF.
  A3.3.1. For exemptions, identify in Appendix A by NSN and part number, or manufacturer
  Contractor and Government Entity (CAGE) code and part number for non-stocklisted items,
  all items that are being submitted for exemptions as part of the plan. To qualify, the item
  must no longer be procured, stocked in inventory, and/or used in weapon systems owned by
  the Air Force, Army, Navy, or Marine Corps by 31 December 2015. Identify the attrition
  strategy being used as defined within AFMC Instruction 23-121, AFMC Improved Item
  Replacement Program (IIRP) and Demand Reduction Initiative (DRI) Guidance and
  Procedures, when complete attrition of the item population from inventory and use is
  expected, and state current asset balances for the NSN to include items in inventory and
  operational use.
  A3.3.2. For exceptions, see DFARS 211.274-2 for the process to request exceptions. In
  Appendix B, identify by National Stock Number (NSN) and part number, or manufacturer
  CAGE and part number for non-stocklisted items, all items where DFARS 211.274-2 has
  been invoked to support contingency operations. Identify the approval authority for each
  exception processed. Briefly state the strategy to ensure these items are uniquely identified
  and physically marked with a UII.
  A3.3.3. If not anticipating an exemption or exception, so state
  A3.3.4. Describe the overall IUID Implementation Strategy:
  A3.4.1. Parts-Marking—Describe the overall strategy to physically uniquely identify items
  that will be marked (including embedded and deployable items); describe how items will be
  marked (label, direct mark, etc.) and who will mark them.
     A3.4.1.1. Current and future contracts – Identify all contracts used by the program
     resulting in the delivery of items to the AF through new procurement and/or service
     contracts such as, but not limited to, contract repair, contract logistics support,
     performance based logistics, etc. (can be a separate appendix if necessary). State if
     DFARS Clause 252.211-7003 has been incorporated into the contract(s) and is being
     enforced. If the DFARS clause has not been incorporated and enforced, state an
     anticipated date when the DFARS clause will be incorporated and the plan of action. If
     applicable, provide rationale for each contract where modification to include the DFARS
     Clause 252.211-7003 is considered infeasible.
     A3.4.1.2. Legacy and depot-manufactured items—Identify all items by NSN and part
     number, or manufacturer CAGE and part number for non-stocklisted items, used by the
     program that meet the IUID criteria (can be a separate appendix if necessary). Identify
     the Source(s) of Supply according to the Federal Logistics Information System (FLIS),
     the Source(s) of Repair, and indicate if engineering analysis to determine how/where to
     physically mark the item has been completed. Also indicate if technical orders for the
     item have been updated.
     A3.4.1.3. Serialization and UII registration—Describe implementation plans for
     serialization and registration of UIIs for the following methods of executing parts-
     marking:
 210                                                              AFPAM63-128 10 JULY 2014


           A3.4.1.3.1. New procurements and            contract   repair—accomplished      by   the
           manufacturer or repair contractor.
           A3.4.1.3.2. Organic repair and depot-manufactured items –current standardization
           requires 18S serialization and registration of items through Triad software being
           implemented on organic parts-marking capabilities.
       A3.4.1.4. Technical document strategy—To minimize the non-recurring costs for parts
       marking, describe processes and efforts to mitigate or minimize non-recurring
       engineering costs applicable to engineering drawings and technical order updates. Also
       describe efforts to standardize engineering and technical order document updates.
       A3.4.1.5. Government—furnished equipment means an item of special tooling, special
       test equipment, or equipment, in the possession of, or directly acquired by, the
       Government and subsequently furnished to the Contractor (including subcontractors and
       alternate locations) for the performance of a contract.       State when DFARS clause
       252.211-7007, Reporting of Government-Furnished Equipment in the DoD Item Unique
       Identification (IUID) Registry. will be incorporated into contracts where GFE exists.
       Identify the strategy to uniquely identify and register GFE in the IUID Registry and Air
       Force Equipment Management System (AFEMS) as applicable. If virtual UIIs will be
       used, state the conditions under which they will be used, and the planned trigger events
       for applying physical UIIs to the GFE. Reference the DoD Guidelines for the Virtual UII,
       http://www.acq.osd.mil/dpap/UID/attachments/Virtual_UII_Guide_ver1_2a_28-
       20061128.pdf, to assist with identifying applicable conditions and with planning trigger
       events.
       A3.4.1.5.1. Maintenance Strategy—Identify the maintenance strategy or strategies used
       to support a weapon system. Identify how IUID requirements are being incorporated
       into:
Performance Based Agreements/Performance Based Logistics;
Organic Repair;
Contract Repair/Contract Depot Maintenance;
Contracted Logistics Support;
Contractor Owned and Maintained Base Supply (COMBS).
A3.5. IUID related actions to improve or enhance business processes.—Describe any plans to
utilize the data from IUID to improve current or planned business processes such as warranty
tracking, failure tracking, etc. Include activities to make use of automatic identification and data
capture in property management and maintenance.
A3.5.1. List Metrics
   A3.6.2. Measures of success—Identify metrics that the program will use to track the status
   of IUID implementation. Examples include:
Number of total items requiring IUID
Percent of total items assigned a virtual UII
Engineering analysis complete
AFPAM63-128 10 JULY 2014                                                                   211


Technical Orders updated
Percent of items assigned a UII reported to the IUID Registry
Percent of items assigned a virtual UII that had been physically marked with an IUID-compliant
Data Matrix
Percent of total items marked with an IUID-compliant Data Matrix
Percent of new contracts including the IUID clause
Percent of existing contracts modified for IUID requirements
   A3.5.3. Identify the exit criteria for completion of IUID implementation for the program:
   Examples include:
100% legacy items marked and in registry
100% marking equipment for new items in place and tested
Infrastructure in place at depot; personnel 100% trained
A3.6. Provide schedule by completing Elements and Dates below: When a date cannot be
provided, an explanation of when a date can be provided.
      212                                                             AFPAM63-128 10 JULY 2014


                                 Elements and Dates                           Qtr/FY Start    Qtr/FY
                                                                                  Date       Complete
                                                                                               Date
1.0         Preparation Activities
1.1         Priorities for application of IUID and associated data transfer
            established (specify in paragraph 7)
1.2         Listing of items requiring IUID completed for each application
            priority (1.1 application categories)
1.3         IUID marking strategy studies and ROM estimates completed
1.4         Cross Program/Service AIS integration strategy studies and
            ROM estimates completed
1.5         Budget for IUID implementation submitted
2.0         Implementation/Execution Activities
2.1         Legacy contracts (issued prior to 1 Jan 2004) modified
2.2(.1-.    Key Program trigger events (physical marking of items)
N)          identified
2.3         Progress Reviews
3.0         Capability Achieved (physical marking & ability to transfer
            Pedigree data to Registry)
                                                                                             Full
                                                                                             Capability
                                                                                             Achievem
                                                                                             ent Date
3.1         All New solicitations include IUID Requirement                                   N/A
3.2         Legacy Contracts Modified
3.3         Assignment of Virtual UIIs
3.4         Property in possession of contractors (in-plant GFP – virtual)
3.5         Items in operational use
3.6         Items in inventory (ICPs, Supply Depots etc)
3.7         Depot maintenance & manufactured items achieved
3.8          IUID capable AIS
   A3.7. Established priorities for application of IUID and associated data transfer: insert priority
   number 1 (high), 2 (med), 3 (low).
   _____New solicitations:
   _____Ongoing Contracts:
   _____GFE in possession of contractors:
   _____Operational Fleet:
   _____Assets in inventory:
   _____Assets in Depot Maintenance:
   A3.8. Key trigger events for physical marking of UIIs on legacy and GFE items: Examples
   include during production, following testing, prior to shipping, by government upon delivery,
   etc.
   _______________________________________________________________
   _______________________________________________________________
AFPAM63-128 10 JULY 2014                                                                         213


A3.9. Program IUID Related Budget Information by Fiscal Year (Table 2). Provide plan on
how unfunded requirements will be addressed (POM, reduce performance requirements, etc.). If
new acquisition and will be included in contract, so state. If funding requests have been
submitted via POM/APOM process, state the outcome of these budgetary submissions. If budget
submissions were rejected or not funded, provide the rationale.
Program          FY   FY                         FY              FY            FY
Resources
Required for
Infrastructure
Required for
Manpower
Required for
Training
           Total
Funded
Delta
DATA ITEM DESCRIPTION

Title: IUID Marking Plan

Number: DI-MGMT-TBD                                           Approval Date: TBD
AMSC Number: TBD                                              Limitation: N/A
DTIC Applicable: No                                           GIDEP Applicable: No
Office of Primary Responsibility: TBD                         Applicable Forms: N/A

Use/relationship:

The IUID Marking Plan details the contractor’s strategy to execute marking requirements identified
in the Government Statement of Work/Objectives and/or DFARS. The Plan fully documents the
scope of meeting MIL-STD-130, current edition, with the Contractor’s marking requirements,
marking methodology/strategy, data management, quality assurance, facilities and marking
equipment, technical data package requirements, and the master schedule to help the Government
manage marking activities in a cost effective and timely manner. This Data Item Description (DID)
contains format and content preparation instructions for the data product generated by the specific
and discrete task requirement as delineated in the contract. This DID may be applied in any contract
which contains a requirement for marking parts and equipment with IUID data matrices.

Requirements:

Reference Documents: The applicable issue of the documents cited herein, including their approval
dates and dates of any applicable amendments, notices, and revisions, should be as cited in the
current issue of the DoDISS at the time of the solicitation.
Contractor’s format is acceptable.
The document should cover the following elements:
Scope of Marking Requirements
List/Detail items/assets to be marked within the scope of the plan
Marking Methodology/Strategy
 214                                                                  AFPAM63-128 10 JULY 2014


Describe which type of marking methodology should be used (i.e., Direct or Indirect Part Marking,
Data Plate Modification, etc.)
Describe the Imprint Method (i.e., Chemical Etch, Dot Peen, Laser, Thermal Transfer, Ink Jet, Photo
Etch, etc.)
Marking Specifications
Identify applicable marking drawings
Machine Readable Mark Generation Instructions
Define the construct method ((i.e., (Construct 1 - 18S, 25S) (Construct 2 - 1P, 1T))
Determine the Enterprise Identifier (EID) (i.e., CAGE, DUNS, DoDACC/MAPAC, or GS1)
Determine the level of serialization (i.e., Part, Lot, Batch, Enterprise, etc)
If using Construct 1 - 18S, identify the sequence number generation process
Determine other data elements required to include in the data matrix symbol (30P and 30T)
Determine the Human Readable Mark Generation elements to be included on the label (i.e.,
Nomenclature, Stock Number, Part Number, Serial Number, CAGE, etc.)
Describe which type of material should be used for the creation of the Mark (i.e., Aluminum,
Polyacrylic, Metal Foil, Polyester, Polyvinyl, Aluminum Foil, Stainless Steel, etc)
Describe the overall layout of the Mark including (Reference Tech Data as applicable):
Size (Length, Width, Thickness, etc)
Shape (Circle, Square, Rectangle, Rounded Corners, etc)
Layout/Order (Location of Human and Machine Readable elements)
Marking Location on Asset
Type of Lettering (Font, Font Size, Color, etc)

 Attachment Method (Adhesive, Screws, Rivets, Bag and Tag, etc.
Data Management
Describe the systems required to incorporate Serial Number Tracking (SNT) and communicate IUID
data to the Program Manager
Describe the systems required to assign Unique Item Identifiers (UIIs) and register Unique
Identification (UID) information to the Department of Defense UID Registry as well as the Air Force
Registry
Quality Assurance
Describe the verification processes and any sampling techniques which ensure the Machine Readable
Information (MRI) complies with applicable standards as prescribed in MIL-STD-130, Current
Edition.
Identify a format for reporting verification results to include pass/fail and any acceptance criteria
from MIL-STD-130, Current Edition, paragraph 5.2.7.2.
Describe the process for identifying and reporting deficiencies in the mark properties, as well as
repair and replacement procedures
Include UID CDRLs as part of the surveillance method or Quality Assurance processes
Facilities and Marking Equipment.
Describe all facilities and equipment required to meet marking requirements on a production basis,
including floor space, electrical, gas, water, air, vacuum, environmental, safety, network/internet, etc.
Technical data package requirements
Master Schedule
AFPAM63-128 10 JULY 2014                                                                         215


DATA ITEM DESCRIPTION

Title: IUID Marking Activity and Verification Report

Number: DI- MGMT-TBD Approval Date: TBD
AMSC Number: TBD Limitation: N/A
DTIC Applicable: No GIDEP Applicable: No
Office of Primary Responsibility: TBD Applicable Forms: N/A

Use/relationship:

The IUID Marking Activity and Verification Report is a tabular list which provides the data resulting
from IUID marking activities such as: physical asset marking, registration, verification, inventory
audits, quality audits, and other asset life cycle activities. A key attribute for the report is the
Verification column which indicates (Pass/Fail) for each item verified. A “Pass” value should be
assigned to records whose barcodes meet or exceed MIL-STD-130 quality requirements. This Data
Item Description (DID) contains format and content preparation instructions for the data product
generated by the specific and discrete task requirement as delineated in the contract. This DID may
be applied in any contract which contains a requirement for marking parts and equipment with IUID
bar code labels.

Requirements:

1. Reference documents: MIL-STD-130
2. The document should be developed in the Contractor’s format.
3. The tabular report should include the following alphanumeric fields:
• UII
• Construct Type
• Enterprise Identifier
• EID Type
• OEM Part Number
• Serial Number
• Equipment Nomenclature
• National Stock Number (NSN)
• Activity/Event
• Activity/Event Date
• Verification (Pass/Fail)
 216                                                            AFPAM63-128 10 JULY 2014


                                           Attachment 4
       GUIDELINES FOR ACQUISITION SUBJECT MATTER EXPERTS DURING
                        DEVELOPMENT PLANNING

A4.1. Introduction. The identification of sound requirements is the result of understanding the
proper structure and having the right expertise to address development planning. This
attachment provides some best practices to successfully conduct development planning.
A4.2. Organization.
   A4.2.1. Development and acquisition organizations, typically a XZ or XR office, should
   inform the requirements generation process and is responsible for translating high-level
   operational requirements into more detailed system requirements. They, with the help of all
   stakeholders, generate and analyze alternative system concepts. They provide balanced
   estimates of effectiveness, performance, cost, schedule, and risk to the stakeholders for
   selection of preferred concepts. Risk estimates include assessing the impact of new
   technologies on the system, which is done in conjunction with the technologists. Once a
   system concept is selected, the Integrated Lifecycle Management (ILCM) governance
   structure manages the development, procurement, delivery, and continued evolution of the
   system over its life cycle. The program office provides supporting technical and
   programmatic rationale throughout the life cycle of the system.
   A4.2.2. A technology organization, typically AFRL or the early SE staff conducting concept
   development planning, is responsible for ensuring the latest relevant technologies are
   considered, and they conform to the desired time frame. It is their job to suggest new
   concepts are made possible by emerging technologies, as well as technologies that should
   improve or enhance a system’s effectiveness or performance, and/or reduce its cost. They
   are also responsible for estimating the uncertainties associated with new technology and, in
   conjunction with the system analysts, help assess the impact of any new technology.
   Conversely, they should gain insight as to the warfighters’ needs and should be able to better
   focus their technology roadmaps.
   A4.2.3. Materiel support should be provided during CBP as a requirement of a supporting
   MAJCOM. In addition, as system and program office teams are spun off from the mission-
   based and enabling development planning teams, the team construct provides an independent
   mechanism to verify the resulting systems and SoS support for the warfighter-identified
   capability need.
A4.3. Capability Requirements Generation.
   A4.3.1. Capability Requirements are analogous to MAJCOM requirements as defined in the
   CDD. This level of requirements is capability-based or mission-based and is often expressed
   as KPPs or KSAs. Operational requirements should:
        A4.3.1.1. Be capabilities-based.
        A4.3.1.2. Be broad and few in number.
        A4.3.1.3. Drive the derivation       of   lower-level   system   requirements   on   the
        concept/materiel option/program.
AFPAM63-128 10 JULY 2014                                                                       217


      A4.3.1.4. Remain largely unchanged through the first development cycle and the
      achievement of IOC.
   A4.3.2. KPPs should be:
      A4.3.2.1. Broad and few in number
      A4.3.2.2. Comprehensive, clearly, and simply defined covering required capabilities,
      availability, and reliability
      A4.3.2.3. Sufficiently complete to be the source of lower-level system requirements
      A4.3.2.4. Sufficiently mature that little change is needed after MS B, and prior to IOC
   A4.3.3. CONOPS needs to be developed sufficiently to ensure that the concept can:
      A4.3.3.1. Perform in the environment in which it will operate
      A4.3.3.2. Handle the expected throughput
      A4.3.3.3. Meet response-time operational requirements
A4.4. Materiel Solutions Analysis Development.
   A4.4.1. System concept development should:
      A4.4.1.1. Minimize complexity both within the system and with regard to the system’s
      external interfaces.
      A4.4.1.2. Avoid the use of high-risk, immature technologies
      A4.4.1.3. Favor concepts that can achieve initial operational capability (IOC) in fewer
      than about five years
      A4.4.1.4. Conduct tradespace analysis before selecting a final concept.
   A4.4.2. System architecture should:
      A4.4.2.1. Partition to achieve segments that can be procured and tested separately
      A4.4.2.2. Minimize the complexity of the interfaces among the segments
      A4.4.2.3. Establish data, structure, and architecture standards where appropriate
      A4.4.2.4. Maintain the independence of the functional system requirements
   A4.4.3. Performance assessment should be sufficient to:
      A4.4.3.1. Predict performance against mission needs
      A4.4.3.2. Assess the impact of segment-level performance on end-to-end performance
      A4.4.3.3. Include the development of system performance models to support
      performance assessment in the acquisition phase
   A4.4.4. Risk identification should:
      A4.4.4.1. Identify the top-level risk factors that are inherent in the concept, architecture,
      and CONOPS
   A4.4.5. Cost estimates should:
 218                                                             AFPAM63-128 10 JULY 2014


       A4.4.5.1. Support capability/cost tradeoff analysis to ensure people understand the cost
       impact of achieving each capability and help weigh the cost and benefits of each concept
       A4.4.5.2. Be compared to available funding to ensure preferred concepts are affordable
       A4.4.5.3. If necessary, facilitate the development of a cost model that is extended later to
       support “should costs” for contractor-proposed options
A4.5. Additional Resources.
   A4.5.1. The AF/A5R Web page has additional information for High Performance Team
   (HPT) participation and JCIDS document creation and review.
AFPAM63-128 10 JULY 2014                            219


                                   Attachment 5
                 EXAMPLE PROGRAM MANAGER CHARTERS

Figure A5.1. Example PM Charter.
 220                                                                                  AFPAM63-128 10 JULY 2014


Figure A5.2. Example PEO Charter.




                  PROGRAM EXECUTIVE OFFICER
                    In accordance with statute, Department of Defense,
                        Air Force life cycle directives and instructions
                                                        and
                    My Appointment, by the Secretary of the Air Force,
                                                       as the
                                  Air Force Acquisition Executive
                                               I hereby designate

                                    Rank, First, Mi, Last
                                  As the Program Executive Officer for

                                           Portfolio Title
       As Program Executive Officer, your primary responsibility shall be dedicated to executive management
          and successful execution of a portfolio of assigned programs. You will ensure insert PEO mission
                                                       statement.

    You will, as the responsible management official, provide overall direction and guidance for development,
        acquisition, testing, product improvement, and fielding while ensuring total ownership cost reduction.
         You will ensure interoperability through joint standards, open architectures and systems-of-systems
       concepts. Your decisions will consider and quantify impacts across the life cycle of your programs. You
       will establish processes that facilitate communication, cooperation, information exchange, and collective
                                  decision-making between and among organizations.
   You will maintain the U.S. Air Force capability-based and life cycle objectives in managing your programs
    and will report directly to me on your assigned programs. In no case will there be more than two levels of
      review between the Program Manager and the Milestone Decision Authority (MDA). You will keep the
     leadership fully informed of status and report any matters that could affect the U.S. Air Force’s ultimate
                                           commitment to the program.
       You will place primary management emphasis and oversight on balancing cost, schedule, performance,
           and supportability while capitalizing on acquisition transformation initiatives. You will ensure
                  compliance with applicable national policies and life cycle policies and directives.
   You will lead and directly control assigned program managers. You will ensure that acquisition workforce
   career development and competency standards are actively pursued. You will also serve as an advocate to
      ensure the necessary force structure is in place to support acquisition career development programs.
            You are hereby appointed authority as the Program Executive Officer for the management of
         assigned programs. Unless rescinded, this designation will remain in effect until your reassignment.



                                                                         Air Force Acquisition Executive
AFPAM63-128 10 JULY 2014                                                                        221


                                          Attachment 6
                       SUSTAINMENT METRICS CALCULATIONS

A6.1. Introduction. This attachment provides standard calculations for selected sustainment
metrics for a variety of system types including aircraft, missiles, munitions, trainers, subsystems,
software, space systems, automated information systems, and ground communications-
electronics. The metrics are not meant to be all inclusive and can be tailored or revised based on
the needs of the program. Whenever possible, however, the calculation provided here should be
used to ensure standardization.
A6.2. Mandatory Sustainment Metrics. The Joint Capabilities Integration and Development
System, identifies Availability as the Sustainment Key Performance Parameters (KPP) (and two
mandatory supporting Key System Attributes (KSA) (Reliability and Operational and Support
Cost) that will be developed for all ACAT I programs. ACAT II and below programs, with
materiel solutions, should include the Sustainment KPP or Sponsor defined sustainment metrics.
   A6.2.1. Materiel Availability. Materiel Availability is the measure of the percentage of the
   total inventory of a system operationally capable (ready for tasking) of performing an
   assigned mission at a given time, based on materiel condition. This can be expressed
   mathematically as (number of operational end items/total population). The total population
   of operational end items include those in training, attrition reserve, and temporarily in non-
   operational materiel condition, such as depot level repair.
   A6.2.2. Materiel Reliability. Materiel Reliability is a measure of the probability that the
   system will perform without failure over a specific interval. Reliability must be sufficient to
   support the war fighting capability needed. Materiel Reliability is generally expressed in
   terms of a mean time between failures (MTBF), and once operational can be measured by
   dividing actual operating hours by the number of failures experienced during a specific
   interval. Reliability may initially be expressed as a desired failure-free interval that can be
   converted to MTBF for use as a KSA (e.g., 95 percent probability of completing a 12-hour
   mission free from mission-degrading failure; 90 percent probability of completing 5 sorties
   without failure). Specific criteria for defining operating hours and failure criteria must be
   provided together with the KSA. Single-shot systems and systems for which other units of
   measure are appropriate must provide supporting analysis and rationale.
Material Reliability = Mean Time Between Failure
Material Reliability = Total Operating Hours
                       Total Number of Failures
   A6.2.3. Ownership Cost. Ownership Cost provides balance to the sustainment solution by
   ensuring that the operations and support (O&S) costs associated with materiel readiness are
   considered in making decisions. For consistency and to capitalize on existing efforts in this
   area, the Cost Assessment and Program Evaluation (CAPE) O&S Cost Estimating Structure
   should be used in support of this KSA. Only the following cost elements are required: 2.0
   Unit Operations (2.1.1 (only) Energy (fuel, petroleum, oil, lubricants, electricity)); 3.0
   Maintenance (All); 4.0 Sustaining Support (All except 4.1, System Specific Training); 5.0
   Continuing System Improvements (All). Fuel costs should be based on the fully burdened
   cost of fuel. Costs are to be included regardless of funding source. The KSA value should
 222                                                             AFPAM63-128 10 JULY 2014


   cover the planned life cycle timeframe, consistent with the timeframe used in the Materiel
   Availability KPP. Sources of reference data, cost models, parametric cost estimating
   relationships, and other estimating techniques or tools must be identified in supporting
   analysis. Programs must plan for maintaining the traceability of costs incurred to estimates
   and must plan for testing and evaluation. The planned approach to monitoring, collecting,
   and validating operating and support cost data to supporting the KSA must be provided.
   Ownership Cost is the sum of the O&S costs using the CAPE O&S Cost Estimating Structure
   Selected cost element associated with Material Readiness as follows:
       A6.2.3.1. 2.0 Unit Operations (2.1.1 (only) Energy (Fuel, POL, Electricity)) plus
       A6.2.3.2. 3.0 Maintenance (ALL) plus
       A6.2.3.3. 4.0 Sustainment Support (All except 4.1, System Specific Training) plus
       A6.2.3.4. 5.0 Continuing System Improvement (ALL).
       A6.2.3.5. Although not required for the KSA (since it relates to material readiness), the
       PM may also want to consider other CAPE elements, such as personnel, as cost
       considerations when making decisions based on TOC.
A6.3. Aircraft Systems
   A6.3.1. Overview.     The following section defines selected mission capability and
   supportability measures for aircraft systems.
   A6.3.2. Availability and Sustainability Measures
       A6.3.2.1. Mean Time Between Critical Failure (MTBCF). Use MTBCF to measure the
       average time between failures of mission-essential system functions. Critical Failures
       occur when mission essential systems become inoperable or operate outside their
       specified range of performance. MTBCF includes critical failures of all hardware and
       software that occur during mission and non-mission time. Express MTBCF as:
                           MTBCF =  Number of operating hours
                               Number of critical failures
       A6.3.2.2. Mission Capable (MC) Rate. Use the MC rate to measure how long, in percent
       of possessed time, a system can perform at least one of its assigned missions. Base the
       MC rate on the sum of the fully mission capable (FMC) and partially mission capable
       (PMC) rates, expressed as: The overall MC requirement addresses different design
       missions, the expected percentages of equipment use, and the desired MC rate for each
       mission. FMC status indicates that an aircraft can perform all of its assigned missions.
       PMC status indicates that an aircraft can perform at least one, but not all of its assigned
       missions. A multi-mission aircraft may be PMC even if it is unable to accomplish its
       primary mission. Consider system operating time when determining MC rate
       requirements in that the more a system operates in a given period of time, the more
       downtime for corrective and preventative maintenance is required. The MC rate is
       affected by, but does not accurately account for preventative maintenance efforts.
                         MC Rate = FMC hours + PMC Hours x100
                                    Possessed Hours
AFPAM63-128 10 JULY 2014                                                                     223


     A6.3.2.3. Utilization Rate (UR). Express UR as flight hours or sorties per aircraft per
     relevant period of time, such as a day or month, as follows:
               Daily wartime sortie UR = Average number of sorties per day
                                       Average number of aircraft authorized
     A6.3.2.4. Essential System Repair Time per Flight Hour (ESRT/FH). Use ESRT/FH to
     compare clock time needed to repair mission-essential equipment and operating time
     measured in flying hours. ESRT/FH addresses both corrective maintenance (CM) and
     preventive maintenance (PM) performed on mission-essential equipment.              This
     measurement pertains only to full system list (FSL) equipment. Express this calculation
     as:
                         ESRT/FH = Elapsed PM + Elapsed CM
                                   Flight Hours
  A6.3.3. Mission Reliability Measures:
     A6.3.3.1. Weapon System Reliability (WSR). Use WSR to measure the probability that a
     system will perform satisfactorily for a given mission time when used under specified
     operational conditions. Compute WSR by dividing the number of missions completed
     successfully by the number of missions attempted. Define “mission” in terms of start-
     finish criteria, factor in the effect of crew changes, and relate the success of the mission
     to the satisfactory performance of mission-essential items during the mission. Base WSR
     on a design reference mission profile to allow for translation of WSR into contractual
     requirements. Determine functional profiles for storage, build-up, preflight, takeoff,
     ingress, over-target, weapons delivery, egress, landing, and shutdown. Determine
     environmental profiles such as temperature, air density, humidity, vibration, shock, and
     corrosive agents. Determine mission critical systems for these profiles and establish a
     single peacetime and wartime WSR value for each given mission. Exception: If the
     peacetime mission length differs significantly from the wartime mission length, establish
     two values for WSR. When more than one type of mission is specified, state the
     percentage of time and the desired WSR for each mission. Express this calculation for
     WSR as:
                                WSR = Successful Missions
                                    Total Missions
     A6.3.3.2. Break Rate (BR). Use break rate to measure the percentage of sorties from
     which an aircraft returns with an inoperable mission-essential system that was previously
     operable. Break rate includes “Code 3” conditions, such as ground and air aborts.
     Calculate BR as:
    Break rate (%) = Number of aircraft Code 3 breaks during measurement period x 100
                         Number of sorties flown during period
     A6.3.3.3. Combat Rate (CR). Use the combat rate to measure the average number of
     consecutively scheduled missions flown before an aircraft experiences critical failures.
     Combat Rate reflects the philosophy that scheduling and completing a mission are more
     important than changing it mid-flight because of equipment failures. Express CR as:
             Combat Rate =                  Number of successful sorties flown
224                                                           AFPAM63-128 10 JULY 2014


                (Number of scheduled missions – Number of ground aborts – Number of air
                                         aborts)
      A6.3.3.4. Mean Time Between Critical Failure (MTBCF). Use MTBCF to measure the
      average time between failures of mission-essential system functions. Critical failures
      occur when mission essential systems become inoperable or operate outside their
      specified range of performance. MTBCF includes critical failures of all hardware and
      software that occur during mission and non-mission time. Express MTBCF as:
                           MTBCF = Number of operating hours
                                Number of critical failures
  A6.3.4. Logistics Reliability Measures:
      A6.3.4.1. Mean Time Between Maintenance (MTBM). Use MTBM to measure the
      average flying hours between scheduled and unscheduled maintenance actions. Select an
      appropriate MTBM parameter based on MAJCOM requirements. Current and planned
      information systems permit tracking of standard MTBM parameters, such as inherent
      malfunctions, induced malfunctions, no-defect events, total corrective events, preventive
      maintenance, mean time between removal, and mean time between demand. Specify
      peacetime and wartime values for MTBM if equipment used during these periods differ.
      Express MTBM for a selected type of maintenance event as:
         MTBM      =                 Flight Hours
                       Number of Maintenance Actions (of selected type)
  A6.3.5. Maintainability Measures:
      A6.3.5.1. Mean Downtime (MDT). Use MDT to measure the average elapsed time
      between losing MC status and restoring the system to at least PMC status. Downtime
      includes on-equipment (and in some instances off-equipment) repair labor time; non-
      labor time, such as cure time for composites; maintenance and supply response time;
      administrative delays; and time for other activities that result in NMC status, such as
      training and preventive maintenance. MDT requirements must take into account field
      conditions, such as technical order availability and adequacy; support equipment
      capability and availability, supply levels, and manning (including experience level and
      structure of duty shifts). MDT mainly addresses unscheduled maintenance, but it can also
      include scheduled maintenance, such as scheduled inspections. Develop a single
      peacetime and wartime value for MDT. Exception. When you expect maintenance or
      support conditions in wartime to differ significantly from those in peacetime, describe
      those differences and describe separate values for MDT. Express MDT as:
                                MDT =           NMC Time
                                            Number of Downing Events
      A6.3.5.2. Fix Rate (FR). Use FR to calculate the percentage of aircraft that return as
      Code 3 and must be returned to MC status within a specified amount of time (for
      example, 70 percent in 4 hours or 85 percent in 8 hours). The FR time requirement
      includes direct maintenance time and downtime associated with administrative and
      logistics delays. Express FR as:
                   Fix Rate = Number of aircraft fixed within “X” hours
AFPAM63-128 10 JULY 2014                                                                    225


                                Total number of broken aircraft
       A6.3.5.3. Mean Repair Time (MRT). Use MRT to measure the average on-equipment
       and/or off-equipment corrective maintenance time in an operational environment. State
       MRT requirements for on-equipment at the system level and for off-equipment at the line
       replaceable unit (LRU) level. MRT starts when the technician arrives at the aircraft site
       for on-equipment maintenance or receives the LRU at the off-equipment repair location.
       MRT includes all necessary corrective maintenance actions such as preparation; LRU
       access; troubleshooting; removing and replacing parts; repairing, adjusting; checking
       functions; and curing. Do not include maintenance or supply delays in MRT calculations.
       Express MRT as:
                     MRT (overall) = Total corrective maintenance time
                                   Total number of maintenance events

           MRT (on-equipment) = Total on-equipment corrective maintenance time
                              Total number of on-equipment maintenance events

          MRT (off-equipment) = Total off-equipment corrective maintenance time
                              Total number of off-equipment maintenance events
Note: MRT uses crew size in the calculation of manhours and MTTR does not use crew size in
the calculation of hours.
   A6.3.6. Manpower Measures:
       A6.3.6.1. Maintenance Man-Hours per Life Unit (MMH/LU). MAJCOMs base their
       maintenance man-hours per flying hour (MMH/FH) on their specific needs. Specify
       MMH/FH peacetime and wartime values, since equipment usage, maintenance needs, and
       support concepts may differ during these periods. Current and planned maintenance
       information systems permit tracking of the following:
          A6.3.6.1.1. MMH/FH, support general work unit code (WUC 01-09)
          A6.3.6.1.2. MMH/FH, corrective (WUC 11-99) for inherent malfunctions, induced
          malfunctions, no-defect actions, or total events
          A6.3.6.1.3. MMH/FH, product improvement (time compliance technical order)
          A6.3.6.1.4. MMH/FH, preventive maintenance (time change items)
          A6.3.6.1.5. MMH/FH, all categories totaled
       A6.3.6.2. Maintenance Personnel per Operational Unit (MP/U). Use MP/U to measure
       the total number of direct maintenance personnel needed for each specified operational
       unit to perform direct on-equipment and off-equipment maintenance. Develop manpower
       projections to support specified operating and maintenance concepts, taking into
       consideration basing, deployment, and operational scenarios. MP/U calculations include
       direct on-equipment and off-equipment maintenance personnel and specialties related to
       direct on-equipment and off-equipment support, such as structural repair (including sheet
       metal and composites) and nondestructive inspection. When analyzing manpower
       requirements, MAJCOMs should consider and use projected MC, PMC, MRT, and
       MTBM rates, coupled with aircraft battle damage repair analyses to determine overall
 226                                                              AFPAM63-128 10 JULY 2014


       manpower needs. MP/U calculations exclude maintenance staff agencies, logistics
       command section operations and support personnel, powered support equipment
       personnel, and munitions supply and missile maintenance personnel.
   A6.3.7. Deployability Considerations. MAJCOMs must consider building in deployability
   when describing top-level mission capability and supportability requirements for aircraft
   systems. Address capability of the system to be deployed to the theater of operations within
   the constraints of the user-defined requirements.
       A6.3.7.1. Deployability Footprint. Deployability footprint is defined by the manpower,
       materiel, equipment, and infrastructure required to support the design reference mission
       profile under peacetime, wartime, or other contingency operations. As a basis of measure
       use, for example, equivalent pallet positions.
       A6.3.7.2. Logistics Follow-on Support. Logistics follow-on support specifies the
       manpower, materiel, and equipment required to sustain the design reference mission
       profile under peacetime, wartime, or other contingency operations. Logistics support
       requirements must account for manpower, materiel, and equipment directly or indirectly
       associated with the weapon system under consideration. Logistics requirements are
       included in Initial Capability Documents (ICD), Capabilities Development Document
       (CDD), Concept of Operations (CONOPS), and other Acquisition Documents.
A6.4. Strategic or Tactical Ground-Launched Missiles
   A6.4.1. Overview. Use the following mission capability and supportability measures for
   strategic or tactical ground-launched missiles.
   A6.4.2. Availability and Sustainability Measures:
       A6.4.2.1. Mission Capable (MC) Rate. Use MC rate to calculate the percentage of
       possessed time that a weapon system can perform its assigned mission. MC rate is
       defined as the combination of the fully mission capable (FMC) and partially mission
       capable (PMC) rates. It can be obtained using the status reporting system defined in AFI
       21-103, Equipment Inventory, Status and Utilization Reporting. MC rate is equal to the
       number of alert hours divided by the number of possessed hours (PH). Express MC as:
                      MC rate = Alert hours/PH = FMC rate + PMC rate

Note: Since these systems offer little or no repeat mission capability, calculate a single MC
requirement for both peacetime and wartime.
   A6.4.3. Mission Reliability Measures:
       A6.4.3.1. Weapon System Reliability (WSR). Use WSR to measure the probability that a
       given system in MC status will successfully complete its designated mission or function.
       Operational commands base WSR on their specific requirements. For intercontinental
       ballistic missile (ICBM) systems, WSR gives the probability that an ICBM, launched in
       reaction to a valid execution order, will deliver a warhead that will detonate as planned in
       the target area. Express WSR as:
                           WSR = SAR x LR x COMR x IFR x RSR
AFPAM63-128 10 JULY 2014                                                                        227


         A6.4.3.1.1. Strategic alert reliability (SAR) represents the probability that a deployed
         missile can react to a valid launch order. It is based on the ratio of FMC missile hours
         to total missile hours available.
         A6.4.3.1.2. Communications reliability (COMR) represents the probability that a
         combat crew in the deployed force will receive a transmitted launch order. It does not
         consider enemy action.
         A6.4.3.1.3. Launch reliability (LR) represents the probability that an MC missile will
         launch as planned and that the ancillary equipment functions properly. It does not
         take into account enemy action.
         A6.4.3.1.4. Inflight reliability (IFR) represents the probability that a launched missile
         will properly signal a re-entry vehicle and place it in the correct ballistic trajectory so
         that it impacts in the target area.
         A6.4.3.1.5. Re-entry subsystem reliability (RSR) represents the probability that a
         properly positioned re-entry subsystem will successfully deploy a re-entry vehicle so
         that it detonates a warhead in the target area.
     A6.4.3.2. Mean Time Between Maintenance (MTBM). Use MTBM to measure the
     average life units between maintenance events, as the using command defines them. Use
     PH as the time base for missiles. PHs may include time in which the system is not
     operating or is in a storage or dormant condition. Current and planned maintenance
     information systems permit tracking of several MTBM parameters including inherent
     malfunctions, induced malfunctions, no-defect events, total corrective events, preventive
     maintenance, and mean time between removal (MTBR). Specify the same peacetime and
     wartime value for MTBM and MTBR, if possible, using a standard term. Use an
     appropriate MTBM or MTBR parameter based on specific MAJCOM needs.
  A6.4.4. Maintainability Measures:
     A6.4.4.1. Mean Downtime (MDT). Use MDT to measure the average elapsed time
     between losing MC status and restoring the system to at least PMC status. Downtime
     continues until maintenance personnel return the system to at least PMC status.
     Downtime includes maintenance and supply response, administrative delays, actual on-
     equipment repair, and activities that result in not mission capable (NMC) status, such as
     training and preventive maintenance. When computing MDT, also consider TO
     availability and adequacy, support equipment capability and availability, supply levels,
     manning, experience levels, and shift structure. Specify a single peacetime and wartime
     MDT value. Note: Do not confuse MDT, which describes an operational environment,
     with mean time to repair (MTTR) which is used as a contractual term.
     A6.4.4.2. Mean Repair Time (MRT). Use MRT to measure the average on-equipment
     and/or off-equipment corrective maintenance time in an operational environment. State
     MRT needs for on-equipment at the system level and off-equipment at the line
     replaceable unit (LRU) level. MRT starts when the technician arrives at the missile site
     for on-equipment maintenance or receives the LRU at the off-equipment repair location.
     The time includes all maintenance done to correct the malfunction, including preparing
     for tests, troubleshooting, removing and replacing parts, repairing, adjusting, and
     conducting functional checks. Exception: Do not include maintenance or supply delays
 228                                                          AFPAM63-128 10 JULY 2014


       in MRT calculations. Note: Do not confuse MRT, an operational term, with MTTR,
       which is used as a contractual term. Express MRT as:
                    MRT (overall) = Total corrective maintenance time
                                  Total number of maintenance events

          MRT (on-equipment) = Total on-equipment corrective maintenance time
                         Total number of on-equipment maintenance events

          MRT (off-equipment) = Total off-equipment corrective maintenance time
                              Total number of off-equipment maintenance events
   A6.4.5. Manpower Measures:
       A6.4.5.1. Maintenance Man-Hours per Life Unit (MMH/LU). Use MMH/LU to measure
       the average man-hours per life unit needed to maintain a system. Base missile time on
       PHs, in most cases. Current and planned maintenance information systems permit
       tracking of the following:
          A6.4.5.1.1. MMH/PH, support, general (WUC 01-09)
          A6.4.5.1.2. MMH/PH, corrective (WUC 11-99) for inherent malfunctions, induced
          malfunctions, no-defect actions, or total events
          A6.4.5.1.3. MMH/PH, product improvement (TCTO)
          A6.4.5.1.4. MMH/PH, preventive maintenance (time change items)
          A6.4.5.1.5. MMH/PH, total of the above categories establish a single required
          peacetime and wartime value. Use an appropriate MMH/LU based on specific
          MAJCOM needs. PH is commonly used, but other life units may be more appropriate
          for different systems.
       A6.4.5.2. Maintenance Personnel per Operational Unit (MP/U). Use MP/U to calculate
       the number of maintenance personnel needed to support an operational unit under
       specified operating and maintenance concepts. Develop manpower projections to support
       operating and maintenance concepts.
          A6.4.5.2.1. Exception: Do not include depot-level personnel and other manpower
          excluded by AFI 38-201, Management of Manpower                  Requirements and
          Authorizations , when calculating MP/U. Specify peacetime and wartime levels of
          manning for Air Reserve Component (ARC) maintenance organizations. Peacetime
          MP/U reflects the number of full-time personnel needed to support daily peacetime
          flying operations. Wartime MP/U includes full-time and traditional reservists and is
          normally identical to the MB/U established by the gaining MAJCOM for a similar
          unit.
A6.5. Air Launched Missiles and Munitions
   A6.5.1. Air Launched Missiles and Munitions. Use the following mission capability and
   supportability measures for air-launched missiles and munitions.
   A6.5.2. Availability and Sustainability Measures:
AFPAM63-128 10 JULY 2014                                                                      229


       A6.5.2.1. Missile/Munitions Availability.       Missile/munitions availability is the
       percentage of total owned inventory capable of performing its intended mission.
       Calculate availability as the quotient of serviceable inventory divided by total owned
       inventory quantity. Unserviceable missiles/munitions include both those in the inventory
       in an unserviceable condition code and those that may be in depot for any type of
       maintenance action. Specify a single (both peacetime and wartime) value of availability,
       as requirement.
Note: MC rate can be used as an alternate measure of missile/munitions availability. MC rate
may be more appropriate for systems that are inspected periodically and have maintenance data
tracked in a maintenance data reporting system.
       A6.5.2.2. Mission Capable (MC) Rate. Use MC rate to measure the percentage of
       possessed time that a system can perform any of its assigned missions. Establish required
       MC values for specific missions at the wartime utilization or sortie rate. Calculate the
       MC rate as the sum of FMC and PMC rates as follows:
               MC rate = ((FMC hours + PMC hours) / Possessed hours) x 100
       A6.5.2.3. Storage Reliability. Use storage reliability to calculate (at a wing level) the
       percentage of possessed or authorized missiles/munitions that can perform their intended
       functions. Storage reliability is defined as the probability assets pulled from storage are
       operationally ready by passing any technical order required pre-use visual and/or
       electronic inspections.
   A6.5.3. Mission and Logistics Reliability Measures:
       A6.5.3.1. Weapon System Reliability (WSR). Use WSR to measure the probability that
       an available or MC weapon system will successfully complete its designed mission or
       function. When defining “mission,” take into account storage, alert, captive-carry, launch,
       and flight of the item. Calculate the value of WSR by dividing the number of successfully
       completed missions by the number of attempted missions. Success of the mission should
       relate performance to design capability. For most munitions, there may only be one
       mission, and thus a need for only one WSR value. Peacetime missions for missiles may
       significantly differ from wartime missions. In such cases, develop a WSR value for each
       mission. If platform environments differ dramatically, either provide a WSR value for
       the harshest environment or develop WSR values for each environment or pylon.
       A6.5.3.2. Mean Time Between Maintenance (MTBM). Use MTBM to calculate the
       average life units between maintenance events, as defined by the operational command.
       Apply MTBM to those items that operate or are active during times other than actual free
       flight. If reported, use captive-carry and ground operating hours as the time base for
       applicable items; otherwise, use PHs. PHs include time in which the system is not
       operating or is in a storage or dormant condition. Current and planned maintenance
       information systems permit tracking of several standard MTBM parameters, including
       inherent malfunctions, induced malfunctions, no-defect events, total corrective events,
       preventive maintenance, and mean time between removal (MTBR).
   A6.5.4. Maintainability Measures:
 230                                                            AFPAM63-128 10 JULY 2014


       A6.5.4.1. Mean Downtime (MDT). Use MDT to measure the average elapsed time
       between losing MC status and restoring the system to at least PMC status. Downtime
       includes maintenance and supply response, administrative delays, actual on-equipment
       repair activities that result in not mission capable (NMC) status, such as training and
       preventive maintenance. When calculating MDT, also consider TO availability and
       adequacy, support equipment capability and availability, supply levels, manning,
       experience levels, and shift structure. Note: MDT describes an operational environment;
       it is not the same as the contractual term, mean time to repair (MTTR).
       A6.5.4.2. Mean Repair Time (MRT). Use MRT to measure the average on-equipment
       and/or off-equipment corrective maintenance time in an operational environment. State
       MRT requirements for on-equipment at the system level and off-equipment at the LRU
       level. MRT starts when the technician arrives at the system or equipment for on-
       equipment maintenance or receives the LRU at the off-equipment repair location. The
       time includes all actions taken to correct the malfunction, such as preparing tests,
       troubleshooting, removing and replacing parts, repairing, adjusting, and conducting
       functional checks. Express MRT as:
                     MRT (overall) = Total corrective maintenance time
                                    Total number of maintenance events

           MRT (on-equipment) = Total on-equipment corrective maintenance time
                              Total number of on-equipment maintenance events

          MRT (off-equipment) = Total off-equipment corrective maintenance time
                              Total number of off-equipment maintenance events
Exception: Do not include maintenance or supply delays when calculating MRT.
Note: Do not confuse the operational term MRT with the contractual term MTTR.
   A6.5.5. Manpower Measures:
       A6.5.5.1. Maintenance Man-Hours per Life Unit (MMH/LU). Use MMH/LU to calculate
       the average man-hours per life unit needed to maintain a system. Use the MTBM life
       units as the time base for maintenance man-hours. Operational commands define
       MMH/LU according to their specific needs. Current and planned maintenance data
       collection and processing systems use PHs as the time base and permit tracking of several
       standard MMH/PH terms. Establish a single required peacetime and wartime MMH/LU
       value. Use an appropriate MMH/LU measure based on specific MAJCOM needs. PH is
       commonly used, but other life units may be more appropriate in some cases.
   A6.5.6. Deployability Considerations. MAJCOMs must consider building in deployability
   when describing top-level requirements for air-launched missiles and munitions. Address
   capability of the system to be deployed to the theater of operations within the constraints of
   the user-defined requirements.
       A6.5.6.1. Deployment Footprint. See A6.3.7.1
       A6.5.6.2. Logistics Follow-on Support. See A6.3.7.2
A6.6. Trainers and Support Equipment
AFPAM63-128 10 JULY 2014                                                                   231


  A6.6.1. Trainers and Support Equipment. This category includes the equipment needed to
  operate and maintain a weapon system, such as trainers and training equipment, all mobile
  and fixed equipment, and ground segment equipment for ground-launched missile systems.
  A6.6.2. Availability and Sustainability Measures:
     A6.6.2.1. Mission Capable (MC) Rate. Use MC rates to calculate the percentage of
     possessed time that equipment can perform any of its assigned missions. Calculate the
     value of MC by using the sum of fully mission capable (FMC) and partially mission
     capable (PMC) rates. Express MC as:
                         MC = FMC Hours + PMC Hours X 100
                                  Possessed Hours
     A6.6.2.2. Materiel Availability. Use MA to calculate the percentage of time that
     operational equipment can satisfy critical mission needs relative to the designated
     operational capability (DOC). Express all times in clock hours. MA is similar to MC rate
     except that system status depends on current use of the system as well as the DOC. For
     example, a system with several DOC missions can be MC if at least one of those
     missions can be accomplished. However, if an immediate need exists for a mission
     capability that is “down” while other mission capabilities are “up”, the overall system is
     considered to be “down.” Express MA as:
                   MA = Total operating hours – Total downtime hours
                                Total operating hours
     A6.6.2.3. Utilization Rate (UR). Use UR to calculate the average life units used or
     missions attempted per system during a specified interval of calendar time. Establish
     required peacetime and wartime UR values. Express this term as a ratio of planned or
     actual operating hours to PHs for a given calendar period. For example:
                                  UR = Operating hours
                                            PH
  A6.6.3. Reliability Measures:
     A6.6.3.1. Mean Time Between Critical Failure (MTBCF). Use MTBCF to measure the
     average time between failures of mission essential system functions. For ground
     electronic systems, MTBCF equals the total equipment operating time in hours, divided
     by the number of mission essential system failures. MTBCF includes all critical hardware
     and software failures that occur during mission and non-mission time. Express MTBCF
     as:
                          MTBCF = Number of operating hours
                               Number of critical failures
     A6.6.3.2. Mean Time Between Maintenance (MTBM). Use MTBM to calculate the
     average life units between maintenance events. Use the operating hours, if reported, as
     the time base for applicable items; otherwise, use PHs. Apply MTBM to items in active
     operation for long periods of time. Current and planned maintenance information
     systems permit tracking of several standard MTBM measures, including inherent
     malfunctions, induced malfunctions, no-defect events, total corrective events, preventive
 232                                                             AFPAM63-128 10 JULY 2014


       maintenance, and mean time between removal (MTBR). Use the appropriate MTBM or
       MTBR measure based on specific MAJCOM needs.
   A6.6.4. Maintainability Measures:
       A6.6.4.1. Mean Downtime (MDT). Use MDT to measure the average elapsed time
       between losing MC status and restoring the system to at least PMC status. Downtime
       includes maintenance and supply response, administrative delays, actual on-equipment
       repair, and activities that results in not mission capable (NMC) status, such as training or
       preventive maintenance. When computing MDT, also consider TO availability and
       adequacy, support equipment capability and availability, supply levels, manning,
       experience levels, and shift structure.
       A6.6.4.2. Mean Repair Time (MRT). Use MRT to measure the average on-equipment
       and/or off-equipment corrective maintenance time in an operational environment. State
       MRT requirements for on-equipment at the system level and off-equipment at the
       assembly, subassembly, module, or circuit card assembly level. MRT starts when the
       technician arrives at the system or equipment for on-equipment maintenance or receives
       the assembly, subassembly, module, or circuit card assembly at the off-equipment repair
       location. The time includes all maintenance done to correct the malfunction, including
       test preparation, troubleshooting, removing and replacing parts, repairing, adjusting, and
       conducting functional checks. Express MRT as:
                     MRT (overall) = Total corrective maintenance time
                                      Total number of maintenance events

Exception: MRT does not include maintenance or supply delays.
Note: MRT uses crew size in the calculation of man-hours and MTTR does not use crew size in
the calculation of hours.
   A6.6.5. Manpower Measures:
       A6.6.5.1. Maintenance Man-Hours per Life Unit (MMH/LU). Use MMH/LU to measure
       the average man-hours per life unit needed to maintain a system. Use an appropriate
       MMH/LU term based on specific MAJCOM needs. Use PHs as the time base for ground
       electronic systems. Current and planned maintenance information systems permit
       tracking of several standard MMH/PH terms (see A3.5.1)
       A6.6.5.2. Maintenance Personnel per Operational Unit (MP/U). Develop manpower
       projections to support operating and maintenance concepts.
Exception: When calculating MP/U, do not include depot level and other personnel that are
excluded from maintenance planning factors by AFI 38-201.
   A6.6.6. Deployability Considerations. MAJCOMs must consider building in deployability
   describing top-level requirements for trainers and support equipment systems. Address
   capability of the system to be deployed to the theater of operations within the constraints of
   the user-defined requirements.
       A6.6.6.1. Deployment Footprint. See A6.3.7.1.
       A6.6.6.2. Logistics Follow-on Support. See A6.3.7.2.
AFPAM63-128 10 JULY 2014                                                                      233


A6.7. Subsystems, Line Replaceable Units, And Modules
   A6.7.1. Overview. Use the following mission capability and supportability measures for
   subsystems, line replaceable units, and modules.
   A6.7.2. Availability and Sustainability Measures:
       A6.7.2.1. Operational Availability (Ao). Use Ao to measure the percentage of time that a
       subsystem, line replaceable unit (LRU), or line replaceable module (LRM) can
       satisfactorily perform in an operational environment. Ao for subsystems, LRUs, and
       LRMS is similar to the MC rate for aircraft, communications, electronics, and some
       missile systems. Express Ao as:
                                     Ao =   MTBDE
                                        MTBDE + MDT
Mean time between downing events (MTBDE) is the average time between events that bring the
system down, including critical or non-critical failures, scheduled maintenance, and training.
Mean downtime (MDT) is the average elapsed time to restore the subsystem, LRU, or LRM to
full operational status, following a downing event. Note: A0 does not express whether an item
can operate over a specific period of time. This characteristic is covered in WSR.
       A6.7.2.2. Other Parameters. For subsystems, LRUs, and LRMs, apply the definitions and
       discussion of the appropriate reliability and maintainability measures as described for the
       parent system in this instruction.
   A6.7.3. Deployability. MAJCOMs must consider building in deployability when describing
   top- level requirements for aircraft subsystems, line replaceable units, and modules. Address
   capability of the system to be deployed to the theater of operations within the constraints of
   the user-defined requirements.
       A6.7.3.1. Deployability Footprint. See A6.3.7.1.
       A6.7.3.2. Logistics Follow-on Support. See A6.3.7.2.
A6.8. Software Design
   A6.8.1. Overview. MAJCOMs must consider software design and supportability measures
   when describing top-level logistics requirements for weapon system and support system
   software.
   A6.8.2. Software Maturity. Use software maturity to measure the progress of software
   development toward satisfying operational requirements. This progress is based on the
   number and category/priority of problems that require software changes. Software maturity
   measures the rate at which software problems are discovered and resolved. Software
   problems are those which require software changes to correct errors in system design and
   improve or modify a system’s function. Use Table A6.8.1. to assign a priority/severity level
   and associated weighting factor to each software problem. As you make software changes to
   correct the problems, sum the weighted problems that are originated and closed. Keep
   statistics and plot the results over time to provide indicators of overall software maturity.
   Indicators include trends of the accumulated weighted software unique failures versus time,
   the difference between the weighted software failures discovered versus the weighted
   software failures resolved, the average severity of the software failures versus time and the
 234                                                             AFPAM63-128 10 JULY 2014


   time necessary to implement software changes. Document software severity levels and
   weights in the AF Deficiency Reporting System IAW T.O. 00-35D-54-WA-1 until the new
   software Deficiency Reporting process is developed.
Table A6.8.1. Software Severity Levels and Weights.
Priority/Severity     Impact                Description                       Severity
      Level                                                                   Weight
                                                                              (Points)
       1           System Abort       A software or firmware problem             30
                                      that results in a system abort or
                                      loss.
       2           System             A software or firmware problem              15
                   Degraded – No      that severely degrades the
                   Work Around        system and no alternative work
                                      around exists.
       3           System             A software or firmware problem              5
                   Degraded –         that severely degrades the
                   Work Around        system and an alternative work
                                      around exists (e.g., system
                                      rerouting through operator
                                      actions).
       4           Software           An indicated software or                    2
                   Problem            firmware problem that doesn’t
                                      severely degrade the system or
                                      any essential system function.
       5           Minor Fault        All other minor deficiencies or             1
                                      nonfunctional faults.


       A6.8.2.1. Software Severity Levels and Weights. Although the total number of weighted
       software problems discovered and resolved may be very large, the resulting difference
       between problems discovered and resolved must be kept to a minimum. This is
       especially true for mission-critical, safety-critical, and high-reliability systems. None of
       the indicators in and of themselves are direct measures of software maturity, but must be
       considered together. Begin measuring software maturity after the software is placed
       under formal configuration control. Continuous measurement helps to prevent software
       from entering the field with known problems that could abort or degrade the mission (see
       IEEE 12207). Assign weighted points to program restarts or reboots—whether or not
       they are successful—based on the impact an unsuccessful restart or reboot had, or would
       have had, on the mission.
       A6.8.2.2. Growth Capacity. Use growth capacity to calculate a computer system’s
       capacity to handle added functions and system users. Growth capacity ensures that
       sufficient processing power and memory exists to make room for required changes after a
       system is delivered to the field. For example, growth capacity may be stated as a
       requirement for the delivered computer system to have a minimum of “X” percent of
       reserve computer memory in contiguous memory locations, a minimum of “Y” percent
AFPAM63-128 10 JULY 2014                                                                      235


     reserve timing for each computational cycle, an overall average of “Z” percent for all
     cycles, and the capability to expand by “A” percent.
     A6.8.2.3. Block Release Cycle. Use block release cycle to calculate the anticipated
     frequency and number of software changes needed periodically. After a system is fielded,
     appropriate personnel normally develop and release new versions of software based on a
     block release cycle. Define this cycle using the interval of time during which personnel
     make software block changes and the number of changes in the block. For example,
     express block release cycle requirements as “block releases every ‘X’ months with an
     average of ‘Y’ changes per release.”
     A6.8.2.4. Reliability. Use reliability to calculate the probability that software will remain
     failure-free for a specified time under specified conditions. In a system context, software
     reliability is the probability that software will not cause failure of the system for a
     specified time under specified conditions. Sources of failure include system inputs and
     uses as well as existing software faults. Count software defects that cause the system to
     fail in the system-reliability allocation. In cases where this is not practical, specify
     software reliability separately. State the reliability requirement as:
                    MTBCF = Cumulative central processing unit time
                                  Cumulative failures
     A6.8.2.5. Machine Independence. Use machine independence to calculate software
     dependence on the machine’s architecture. Machine-dependent software is tied to the
     inherent architecture of the computer processor. Machine-dependent software is generally
     more expensive to support over the software’s life cycle than software that can run on
     several machines. A change in the processor forces a change in the machine-dependent
     code. Assess costs and risks associated with modifying machine-dependent code. The
     percentage of machine-dependent code varies with different systems under development.
     Communication systems, such as network control systems or operating systems, may
     contain significant amounts of machine-dependent code because their functions are
     closely tied to the hardware. State requirements for machine-dependent software as:
            Amount of machine independent code = “X” percent of total code
        A6.8.2.5.1. Calculate machine independence for each module. If a module contains
        machine-dependent code, then the entire module qualifies as machine dependent. This
        encourages developers to use machine-dependent code in only a few small modules
        and helps to ensure that developers create software that personnel can easily and
        inexpensively modify. Exception: Do not assess machine dependence for assembly
        languages or special-purpose processors that use their own languages. Both of these
        cases require 100-percent machine- dependent software.
     A6.8.2.6. Software Maintainability. Software maintainability is the ease in which
     changes to software source code and its associated documentation can be made. Software
     maintainability can be indirectly measured by evaluating the characteristics which impact
     future modifications.     These characteristics include documentation (organization,
     description, and traceability); source code (modularity, description, consistency,
     simplicity, expandability testability, and traceability); and implementation (modularity,
 236                                                            AFPAM63-128 10 JULY 2014


       convention, simplicity, testability, and design). Use automated software evaluation tools
       to support the measurement of software maintainability.
       A6.8.2.7. Software Support. MAJCOMs and SMs determine organizational and depot
       level support.
A6.9. Space, Space Surveillance, And Missile Warning Systems
   A6.9.1. Overview. Use the following definitions, mission capability and supportability
   measures for space, space surveillance, and missile warning systems.
   A6.9.2. Availability and Sustainability Measures. The majority of space systems are forward
   deployed and perform at the same level of operational intensity in peacetime as in time of
   conflict. These systems are normally employed in networks (systems of systems) and can
   usually be described as being composed of space, launch, control, and user segments.
   Operational availability, operational dependability, and mission reliability parameters should
   be specified for each segment as well as the overall system. The methodologies used to
   combine the segment-level parameters into system-level parameters should be stated. The
   segments are defined as:
       A6.9.2.1. Space segment - the satellites, payloads, and platforms that are placed into
       orbit to provide operational forces with intelligence, communications, navigation
       mapping/geodesy, meteorological, or surveillance information.
       A6.9.2.2. Launch segment - the two basic types of launch vehicles (expendable and
       reusable) and their associated launch processing facilities and range support.
       A6.9.2.3. Control segment - the resources which perform the functions required to
       monitor and control the orbiting space vehicles of the space segment.
       A6.9.2.4. User segment - the transmit and/or receive equipment to communicate with the
       payload or control segment, processing equipment, and communications equipment
       linking the processed payload information to the end user.
   A6.9.3. Top Level. MAJCOMs must consider the following measures in describing top-
   level mission capability and supportability requirements for space, space surveillance, and
   missile warning systems.
       A6.9.3.1. Operational Availability (Ao). Ao is the probability that a system can be used
       for any specified purpose when desired. Ao includes both the inherent RAM and
       deployability parameters and logistics support effectiveness of the system that relates to
       the total time the system might be desired for use. Ao is defined as follows:
                                     Ao = Uptime X 100
                                     Uptime + Downtime
       A6.9.3.2. Operational Dependability (Do). Operational dependability of the system
       measures the probability that the system is operating satisfactorily at any point in time
       when measured under specified conditions where downtime for scheduled maintenance
       and training is excluded. It is expressed as follows:
                             Do = Active Hours – NMCU Hours
                                      Active Hours
AFPAM63-128 10 JULY 2014                                                                       237


       A6.9.3.3. Mission Reliability. Mission reliability (denoted Rm) is the probability that the
       system is operable and capable of performing its required function for a stated mission
       duration or at a specified time into the mission. Rm is based on the effects of system
       reliability during mission time only. Rm does not take into account system
       maintainability. There are many missions and systems that do not allow restoration of
       specific functions during the mission. For systems whose times to failure exhibit an
       exponential probability density function (i.e., systems which exhibit constant failure
       rates), Rm is defined as:
                                         Rm = e-(t/MTBCF)
where “t” is the average mission time. If the system is used under significantly different mission
lengths, the specific mission time should be used to determine the Rm for each mission.
A.6.9.3.3.1. Mean Time Between Critical Failure (MTBCF). MTBCF is a measure of the
average operating time between failures of mission-essential functions. MTBCF is defined as
follows:
               MTBCF         = Operating hours or        Active hours –NMC hours
                          Number of critical failures    Number NMCMU events
A.6.9.3.3.2. Mean Time To Restore Function (MTTRF). MTTRF is the average elapsed time,
as a result of a critical failure, required to restore a system to full operating status. MTTRF
includes administrative and logistics delay times associated with restoring function following a
critical failure. MTTRF is defined as follows:
                                   MTTRF = NMCMU Hours
                                          NMCMU Events
       A6.9.3.4. Logistics Reliability. Logistics reliability is a measure of the system’s
       frequency of maintenance under defined operational and support concepts, using specific
       logistics resources. A measure of logistics reliability is mean time between maintenance
       (MTBM). It is the average time between all maintenance events, that is, both scheduled
       and unscheduled events. MTBM is most often defined as follows:
                             MTBM = Number of operating hours
                                    Number of maintenance events
This is equivalent to:
                                MTBM =       (MTBUM)(MTBSM)
                                              MTBUM + MTBSM
where MTBUM is the mean time between unscheduled maintenance and MTBSM is the mean
time between scheduled maintenance and are most often defined as:
                          MTBUM =    Number of operating hours
                                Number of unscheduled maintenance events
and
                         MTBSM =         Number of operating hours
                                    Number of scheduled maintenance events
 238                                                             AFPAM63-128 10 JULY 2014


       A6.9.3.5. Mean Repair Time (MRT). MRT is the average on-equipment and/or off-
       equipment corrective maintenance times. It includes all maintenance actions needed to
       correct a malfunction, including preparing for test, troubleshooting, removing and
       replacing parts, repairing, adjusting, reassembly, alignment and adjustment, and
       checkout. MRT does not include administrative and logistics delays. MRT is most often
       defined as:
                  MRT = Repair Manhours (ON) + Repair Manhours (OFF)
                         Repair Actions (ON) + Repair Actions (OFF)
Note: MRT differs from the contractual term mean time to repair (MTTR) in that it measures
maintenance activities that occur in the operational environment.
       A6.9.3.6. Launch Segment Specific Parameters:
          A6.9.3.6.1. Maintenance Man Years Per Launch (MMY/L). MMY/L is the total
          manpower-maintenance resource requirements associated per launch. MMY/L
          includes non-mission time (for example, launch pad preparation and build-up) and
          active mission time (for example, prelaunch, launch, and postlaunch operations).
          A6.9.3.6.2. Pad Turnaround Time. This is the total time associated with the
          preparation and configuration of the pad after the launch of a similarly configured
          launch vehicle.
       A6.9.3.7. Contact Success Rate (CSR). Contact Success Rate is the ratio of successful
       contacts with respect to total attempts. The Contact Success Rate metric is calculated
       only at the Network level since a complete end-to-end configuration is required for a
       successful satellite contact. The Network Utilization metric is also calculated only at the
       Network level as a measure of overall AFSCN antenna utilization.
                           CSR = (Number of successful contacts)
                                  (Total number of contacts)
       A6.9.3.8. Space MICAP. A space MICAP is an item, that when it fails, causes a System
       Reporting Designator (SRD) down. This is not restricted to Single Point of Failure items,
       but could be the loss of a final triple redundant part in a SRD.
       A6.9.3.9. Single Point of Failure (SPF). A space SPF item is a single item type within a
       SRD, that when it fails, brings a SRD down.
       A6.9.3.10. Training Systems/Devices. Space systems trainers are required to be
       supported/managed by the SM on an equal priority to the space system they serve. This
       includes configuration management and sustainment.
       A6.9.3.11. Modification and Change Management. Hardware and software modifications
       and changes must be accomplished IAW AFI 63-131, NORAD Instruction (NI) 10-3,
       Mission Integrity, Change Control Management, and Test Control for the ITW/AA System
       and STRATCOM Instruction (SI) 534-22, Mission Integrity, Change Control
       Management, and Test Control for the Integrated Tactical Warning and Attack
       Assessment (ITW/AA) System. For Launch and Test Range System, modification
       management is described in the Headquarters Air Force Space Command (HQ AFSPC)
       Launch and Test Range System (LTRS) Requirements/Modifications Validation Board
       (RVB) Management Guide.
AFPAM63-128 10 JULY 2014                                                                       239


A6.10. Automated Information Systems (AIS)
   A6.10.1. Overview. Use the following mission capability and sustainability measures for
   automated information systems (AIS).
   A6.10.2. Availability and Sustainability Measures:
      A6.10.2.1. Operational Dependability (Do). Use operational dependability to determine
      the percentage of the time the AIS is able to satisfy the need for critical management
      information. Mean time between critical failure (MTBCF) is based on user-provided
      guidance on information criticality and timing for Do to be meaningful. Mean time to
      restore function (MTTRF) is the average time required after a critical failure has
      occurred.
                              Do =      (MTBCF)      x 100
                                     (MTBCF + MTTRF)
      A6.10.2.2. Operational Availability (Ao). Use operational availability to determine the
      percentage of time the system can be used to perform any assigned task, critical and non-
      critical. Ao is calculated using mean time between downing events (MTBDE) and mean
      downtime (MDT).
                                Ao =      (MTBDE)    x 100
                                       (MTBDE + MDT)
   A6.10.3. Reliability Measures:
      A6.10.3.1. Mean Time Between Critical Failure (MTBCF). Use MTBCF to measure the
      average time between failures of mission-essential system functions. For AIS, MTBCF
      equals the total equipment operating time in hours, divided by the number of mission-
      essential system failures. MTBCF includes all critical hardware and software failures
      that deny the user critical management information based on user-determined critical and
      timing requirements. Express MTBCF as:
                       Number of operating hours = Active hours – NMCMU hours
         MTBCF =
                        Number of critical failures     Number of NMCMU events
      A6.10.3.2. Mean Time Between Downing Events (MTBDE). Use MTBDE to calculate
      the average life units between downing events, scheduled and unscheduled. Use
      operating hours, if reported, as the time base for applicable items; otherwise, use PHs.
   A6.10.4. Maintainability Measures:
      A6.10.4.1. Mean Downtime (MDT). Use MDT to measure the average elapsed time
      between losing full operating status and restoring the system to at least partial operating
      status. The downtime clock continues to run until maintenance personnel return the
      system to a user-acceptable level of system operability. When computing MDT also
      consider TO availability and adequacy, support equipment capability and availability,
      supply levels, manning, experience levels, and shift structure.
      A6.10.4.2. Mean Time to Restore Functions (MTTRF). This pertains to the average total
      elapsed time, as the result of a critical failure, required to repair and restore a system to
      full operating status with respect to providing critical information to the user. Users
 240                                                            AFPAM63-128 10 JULY 2014


       quantify and qualify the degree of MTTRF acceptable to perform assigned tasks
       effectively. Quantifiable objective evaluation criteria (average in hours) represent user
       satisfaction with the MTTRF of the AIS to support the performance of assigned tasks
       effectively. Express MTTRF as:
                            MTTRF =       Total critical restore time
                                         Number of critical failures
   A6.10.5. Manpower Measures:
       A6.10.5.1. Maintenance Man-Hours per Life Unit (MMH/LU). Use MMH/LU to
       measure the average man-hours per life unit needed to maintain a system.
   A6.10.6. Deployability Considerations. MAJCOMs must consider building in deployability
   when describing top-level requirements for automated information systems. Address
   capability of the system to be deployed to the theater of operations within the constraints of
   the user-defined requirements.
       A6.10.6.1. Deployment Footprint. See A6.3.7.1
       A6.10.6.2. Logistics Follow-on Support. See A6.3.7.2
A6.11. Ground Communications-Electronics (C-E)
   A6.11.1. Overview. Use the following mission availability, capability, and supportability
   measures for ground communications-electronics (C-E), to include ground space C-E. For
   space-based systems, ITWAA Systems and Cheyenne Mountain, NORAD Instruction (NI)
   10-3 and STRATCOM Instruction (SI) 508-10 must be used in conjunction with this
   attachment. See AFI 21-103, Equipment Inventory, Status and Utilization Reporting, for
   glossary of references and supporting information (terms).
   A6.11.2. Availability and Sustainability Measures. MAJCOMs must consider availability
   and sustainability measures when describing top-level logistics requirements for ground
   communications-electronics systems. Use the equations in this attachment to develop these
   measures.
   A6.11.3. Availability. Availability is the probability of a system being fully mission capable
   (FMC) or partially mission capable (PMC), at a random moment in time, or equivalently, the
   percent of the desired operating time a system is FMC or PMC. It is expressed using one of
   the following formulas.
       A6.11.3.1. Operational Availability (Ao). Operational availability measures the
       probability that, at any point in time, the system is either operating or can operate
       satisfactorily when operated under specified conditions. It is the preferred method of
       defining availability in capability requirements documents. It can be expressed as
       follows:
               Ao =    Active hours – Downtime = Active hours – NMC hours
                         Active hours              Active hours
Downtime and NMC hours account for situations when the system is not mission capable for any
reason.
AFPAM63-128 10 JULY 2014                                                                        241


       A6.11.3.2. Operational Readiness (OR). The operational readiness of the system
       measures the probability that the system is operating satisfactorily at any point in time
       when measured under specified conditions where downtime for scheduled maintenance
       and training is excluded. It is expressed as follows:
                              OR =     Active hours – NMCU hours
                                          Active Hours


Not mission capable unscheduled (NMCU) refers to those times when the system is not mission
capable because of unscheduled maintenance and associated delays.
       A6.11.3.3. Utilization Rate (UR). Utilization rate is the average use of a system during a
       specified period of calendar time. Mathematically, it is the ratio of active hours to
       possessed hours in a given calendar period.
                              UR =      Active hours
                                        Possessed Hours
   A6.11.4. Reliability. Reliability is the probability that a system and its parts will perform its
   mission without failure, degradation, or demand on the support system. Reliability is used to
   calculate the probability of mission success and to determine logistics needs.
       A6.11.4.1. Mean Time Between Critical Failure (MTBCF). MTBCF is a measure of the
       average operating time between failures of mission-essential system functions. MTBCF
       equals the total system operating time divided by the number of mission downing events,
       including all disabling hardware and software failure events. MTBCF excludes scheduled
       maintenance, and it can be expressed as follows:
             MTBCF =           Operating hours or         Active hours –NMC hours
                          Number of critical failures     Number NMCMU events
       A6.11.4.2. Mean Time Between Failures (MTBF).MTBF is a measure of the average
       operating time between any failure of the system, excluding scheduled maintenance. It
       can be expressed as follows:
                 MTBF = Operating hours or Active hours – NMC hours)
                       Number of failures Number of PMCMU + NMCMU events
       A6.11.4.3. Mean Time Between Maintenance (MTBM). MTBM measures the average
       operating time between maintenance events, scheduled and unscheduled. It can be
       expressed as follows:
             MTBCF     = Operating hours or Active hours – NMC hours)
                Number of maintenance events Number of PMCM + NMCM events
   A6.11.5. Maintainability. Maintainability is the ability of equipment to be maintained, and is
   typically expressed as the average time to complete a maintenance action.
       A6.11.5.1. Mean Downtime (MDT). MDT is a measure of the average time between
       losing MC or PMC status and restoring the system to MC or PMC status. It includes, but
       is not limited to, active maintenance, maintenance and supply delays, administrative
242                                                           AFPAM63-128 10 JULY 2014


      delays, scheduled maintenance, and all activities that result in NMC status, such as
      training and preventive maintenance. MDT can be expressed as follows:
                    MDT = Downtime (in hours) =    NMC hours
                       Number of downing events Number NMC events
      A6.11.5.2. Mean Repair Time (MRT). MRT is a measure of the average maintenance
      repair hours per maintenance repair actions from Job Data Documentation (JDD). MRT
      includes all maintenance done to correct the malfunction, including preparation, LRU
      access, troubleshooting, removing and replacing parts, repair, adjusting, and conducting
      functional checks. MRT is expressed as follows:
                 MRT       =         On-Equip + Off-Equip Repair Hours
                                    On-Equip + Off Equip Repair Actions
  A6.11.6. Manpower. Manpower is an estimate or requirement for human resources to
  support operation and maintenance. Lead commands must consider manpower measures
  when describing top-level logistics requirements.
      A6.11.6.1. Maintenance Labor-Hours per Active Hour (MLH/AH). The general formula
      for MLH/AH is obtained by dividing the total maintenance labor-hours by the active
      system hours accrued as shown by the following formula:
                   MLH/AH = On-Equip + Off-Equip Maintenance Time
                                    Active Hours
      A6.11.6.2. Maintenance Personnel per Operational Unit. This is the estimated manpower
      to support maintenance and operation. It does not include depot-level personnel and
      others that are excluded from maintenance planning by AFI 38-201.
  A6.11.7. System Deployability. Lead commands must consider deployability in describing
  top-level logistics requirements for C-E systems. Deployability considers whether or not the
  system can be deployed to a theater of operations within the constraints of the user-defined
  requirements and logistics planning factors such as:
      A6.11.7.1. Manpower (operations and maintenance)
      A6.11.7.2. Maintenance concept
      A6.11.7.3. Interoperability
      A6.11.7.4. Electromagnetic compatibility
      A6.11.7.5. The deployed environment (climate and terrain)
      A6.11.7.6. Safety
      A6.11.7.7. Support equipment (test equipment, mobile electric power generators, tools,
      environmental control units)
      A6.11.7.8. Transportation and basing factors, such as the system’s weight and cube, and
      the number and types of vehicles required to transport the system to the deployed
      destination
      A6.11.7.9. System/equipment set-up and tear-down times
      A6.11.7.10. Supply support
AFPAM63-128 10 JULY 2014                                                               243


     A6.11.7.11. Software support
     A6.11.7.12. Network Support
     A6.11.7.13. Depot-level support
  A6.11.8. Deployment Footprint. The manpower, materiel and equipment required to support
  a deployment is often referred to as the deployment footprint. One common way to express
  the deployment footprint is the number of equivalent airlift pallet positions required to
  deploy a system. The number of personnel required to operate and maintain the deployed
  system must also be factored into the deployment footprint.
 244                                                              AFPAM63-128 10 JULY 2014


                                          Attachment 7
                               INDUSTRIAL PREPAREDNESS

A7.1. Defense Production Act, Title I: Defense Priorities and Allocation System (DPAS).
The purpose of DPAS is to ensure the timely availability of industrial resources to meet national
defense and emergency preparedness requirements. Through DPAS, defense programs are
assigned an industrial priority rating of either DX or DO. The Secretary of Defense authorizes
the use of the DX rating. These ratings allow defense orders to take priority over commercial
orders and other lower rated defense orders. The industrial priority rating applies to all contracts
and cascades from the prime through all levels of vendors. Another feature of DPA is the
Special Priorities Assistance (SPA) process. The SPA process is used on an individual action
basis to expedite product delivery to meet a specific date or to accelerate a delivery due to a
change in military urgency. SPA can also be used to resolve delivery conflicts, place additional
orders, verify information supplied by customers and vendors, request rating for items not
automatically rated, and ensure compliance with DPAS. The Department of Commerce
administers DPAS. See AFI 63-602, Defense Production Act Title I – Defense Priorities and
Allocations System, for information on DPAS and the SPA process.
A7.2. Defense Production Act, Title III, Expansion of Productive Capacity and
Supply. The Defense Production Act Title III authorizes the President to use various forms of
financial incentives to develop and promote measures for the expansion of production capacity
and of production and supply of materials and facilities necessary for national defense. The
program is administered by OUSD(AT&L). The AF is the Executive Agent for the program.
See AFI 63-603, Defense Production Act Title III Program, for information on the AF Title III
program.
A7.3. Defense Production Act Title VII, Authority to Review Certain Mergers,
Acquisitions and Takeovers. The Defense Production Act Title VII establishes the Committee
on Foreign Investment in the United States (CFIUS) as the mechanism to support Presidential
review and, if the President finds it necessary, to prohibit or limit foreign direct investment that
threatens national security. The Secretary of the Treasury chairs the CFIUS Department.
SAF/AQR has the responsibility for providing AF input to CFIUS through OUSD(AT&L).
A7.4. 10 USC §2521 Manufacturing Technology (ManTech) Program. The purpose of
ManTech is to pursue revolutionary manufacturing technology solutions. The AF ManTech
program pursues manufacturing technologies to enable affordable manufacturing development,
production, and sustainment capabilities for emerging science and technology for applications;
mature and validate emerging manufacturing technologies to support implementation in industry
and Air Logistics Centers; and promote efficiency and value-added processes throughout the
industrial enterprise value chain (i.e., from prime contractors to their sub-tier suppliers). The
ManTech program is led and executed by AFRL/RX.
A7.5. Industrial Base Assessments. AFRL/RX performs Industrial Base Assessments to
identify shortfalls in industrial capability and/or capacity needed to support current and future
military operations during times of peace, war, crisis, or emergency. These assessments support
the AF input to the DoD Annual Industrial Capabilities Assessment and identify industrial base
risks requiring program manager, PEO, or corporate AF attention.
AFPAM63-128 10 JULY 2014                                                                                                          245


                                                         Attachment 8
                                 FORMAT FOR NEW START VALIDATION

In accordance with AFI 63-101/20-101, I have reviewed AFI 65-601 and DOD FMR Vol III
Chap 6 and confirmed the following prior to approving this action (one of the following must be
answered yes and acknowledged (signed-off) by the Program Manager and Program’s Chief
Financial Officer (CFO) or Program Control Chief): If no items can be answered YES, then the
Program Office should contact its respective PEM/CD at the HAF as delineated in AFI 63-
101/20-101 in order to coordinate New Start Notification package
 Program is budgeted and appropriated. Effort was budgeted in the President’s Budget                           YES      NO
 Submission and is consistent with program direction provided by Defense Appropriations
 Conference language and/or marks. Fiscal year of President’s Budget Submission must
 match fiscal year of funds being used. (If conditions delineated above are satisfied, then
 this effort is not a new start and as such requires no additional Congressional
 notification/approval. Mark Yes in the column to the right and sign off at bottom of sheet
 as required).

 Program is a Congressional Add. Effort was not requested in the President’s Budget                            YES      NO
 Submission, but funds were appropriated by the Defense Appropriations Conference and
 effort is consistent with program direction provided by Defense Appropriations Conference
 language and/or marks. Fiscal year of marks must match fiscal year of funds being used.
 (If conditions delineated above are satisfied, then this effort is not a new start and requires
 no additional Congressional notification/approval. Mark Yes in the column to the right
 and attach SAF/AQX Program Authorization (PA) and sign-off at bottom of sheet as
 required).

 Program is an out-of-cycle New Start. Effort is an out-of-cycle new start for which                           YES      NO
 Congressional notification/approval has been accomplished as reflected on the Secretary of
 the Air Force funds release document. (If conditions delineated above have been verified,
 mark Yes in the column to the right and attach SAF/AQX or AF/ILS Program Authorization
 (PA) supporting this action).

 SAF/HAF has advised this Program Office that a new start notification is not required                         YES      NO
 (Mark Yes in the column to the right and attach supporting documentation from SAF/AQX
 or AF/FMB).

 ________________________________                             _____________________________
 Program Manager (Name/Grade)                                       Signature and Date

_________________________________       _____________________________
 CFO/Program Control Chief (Name/Grade)       Signature and Date
Department of Defense Appropriations Act 2000, Public Law 106-79 Sec. 8096. None of the funds in this Act may be used to compensate a
DOD employee who initiates a New Start program without notification to OSD and the Congressional Defense Committees, as required by DOD
financial management regulations.
 246                                                           AFPAM63-128 10 JULY 2014


                                        Attachment 9
  AIR FORCE DRAWING APPROVAL, RELEASE AND NUMBERING PRACTICES

A9.1. Drawing Approval and Release. The following provides recommended program
verification procedures for approval and release of Air Force CAGE/generated engineering
drawings and associated documentation. Manually applied signatures or electronically applied
signature equivalents can signify completion of each action.
   A9.1.1. Technician (mandatory): Verifies the preparing technician developed and examined
   the completed work as directed and the required information is accurately delineated.
   A9.1.2. Checker (mandatory): Verifies conformance to standards and guidance cited
   therein, ensuring the content is correct and complete.
   A9.1.3. Project Engineer (mandatory): Verifies compliance with the applicable engineering
   design criteria. Certifies all coordinating signatures are included as required.
   A9.1.4. Engineering Approval (optional): Verifies cognizance and approval of the project
   by the design engineer's supervisor.
   A9.1.5. Coordinating Signatures (optional). Verifies additional professional approval as
   determined by the project engineer. (Most signing engineers represent a specialized
   engineering discipline such as corrosion, environmental, safety, reliability, nuclear, etc.)
   A9.1.6. Air Force Authentication (mandatory): Verifies all requirements are satisfied. The
   engineering drawing is now under formal configuration management control and is
   technically ready for final release. The authentication authority is:
       A9.1.6.1. The chief/lead Air Force engineer or other designated agent for Air Force
       engineering data generated organically, or
       A9.1.6.2. A contractor preparing Air Force drawings delegated this authority tasked to
       deliver drawings that include their release control authority signatures.
A9.2. Air Force Release. The locally designated activity signs the Air Force release verifying
that:
   A9.2.1. Drawing activities have accomplished administrative control functions as follows:
   The EO contains the minimum required signatures. The Engineering Approval signature is
   verified against the current authorized Chief/lead Engineering Authority list supplied by
   SPO. The EO is verified to be the correct type for the current CAGE code (information EO
   and AESO for contractor CDA; Information EO Change notice (CNEO) and advanced EO
   (AECO) for Air Force CDA. The Distribution Statement is included and matches the
   drawing, the revision level matches the current indexing in JEDMICS.
   A9.2.2. The complete engineering drawing (defined by the design or design change) has
   been released.
   A9.2.3. The "X" has been removed from the engineering drawing number.
A9.3. Air Force Engineering Drawing Numbers
   A9.3.1. Each Air Force drawing preparation activity annually acquires blocks of official Air
   Force drawing numbers from HQ AFMC/A4UE.
AFPAM63-128 10 JULY 2014                                                                      247


   A9.3.2. HQ AFMC/A4UE distributes blocks of Air Force drawing numbers to authorized
   activities upon their request. Engineering activities shall submit annual distribution requests
   no later than December 1st. Additional drawing numbers may be requested at any time.
      A9.3.2.1. Drawing number requests shall include:
          A9.3.2.1.1. Quantity of drawing numbers required
          A9.3.2.1.2. Full office postal address
          A9.3.2.1.3. Commercial and Government Entity (CAGE) Code
          A9.3.2.1.4. Point of contact name, phone number (voice and FAX), E-mail address.
      A9.3.2.2. Submit request via letter, FAX, or E-mail to office listed in table A9.1:

Table A9.1. HQ AFMC/A4UE Contact Information.

AFMC/A4UE 4375 Chidlaw Rd., Rm S008
Wright-Patterson AFB OH 45433-5006
E-mail: AFEDG.JEDMICS@WPAFB.AF.MIL
FAX: DSN 787-5881            Comm (937) 656-0534

   A9.3.3. Use the prefix “X” in the drawing number blocks until Air Force Release action. Do
   not prefix references to Air Force drawings with an "X". The “X” is removed only upon Air
   Force release action.
   A9.3.4. Air Force drawing numbers are applied as assigned without modification except for
   associated list identification as provided in ASME Y14.34, Associated Lists.
   A9.3.5. Air Force drawing numbers are assigned only during the calendar year for which
   they are issued. Unassigned drawing numbers shall not be “carried over” to the following
   calendar year. (i.e., CY 2013 number assignment ceases 31 December 2013, CY 2014
   number assignment commences 1 January 2014)
   A9.3.6. Designated focal points assigned blocks of Air Force drawing numbers administer
   those drawing numbers. Each focal point maintains permanent records of assigned Air Force
   drawing numbers. When Air Force drawing numbers are transferred or reassigned to a
   different CAGE Code identified activity, the focal point maintains a permanent record of the
   transfer action and reports the following transfer actions to AFLCMC/HIAM:
      A9.3.6.1. The drawing number(s) transferred. If a block of numbers is transferred, the
      first and last number of the block is sufficient. If the transfer involves numbers not in
      sequence, list each number transferred.
      A9.3.6.2. The date of transfer.
      A9.3.6.3. The full identification of the receiving activity including full postal address,
      focal point name, phone number (voice and FAX), and E-mail address.
   A9.3.7. Air Force Dash Numbering System.
 248                                                          AFPAM63-128 10 JULY 2014


        A9.3.7.1. The standard Air Force dash numbering system for items and assemblies is
        based on long standing practices. While many drawing functions, both Government and
        non-Government, are often more complex, most of them have evolved from this base.
           A9.3.7.1.1. Detailed Item Dash Numbers. Use odd dash numbers on all defined
           detail items. Use even numbers for the opposite or mirror-image items. Do not use
           dash numbers ending in "9" or "0" (Table A9.2).
           A9.3.7.1.2. Assembly Dash Numbers. Use dash numbers beginning with an odd
           number and ending with "0" for all defined assemblies. Use dash numbers beginning
           with an even number and ending with "0" for the opposite or mirror-image assemblies
           (Table A9.2).
           A9.3.7.1.3. Tabulated alignment. You may use corresponding dash numbers for
           tabulated Air Force drawings that relate to other tabulated drawings or standards
           when necessary for clear cross-referencing and identification.
           A9.3.7.1.4. Variations.   Submit requests for variations of the Air Force dash
           numbering system to ESC/HGGI. Fully describe the variations you're requesting,
           explain why you need the variations, and assess the impact on your project if the
           variation is not approved. Your request will be evaluated by ESC/HGGI and
           submitted with disposition recommendation through HQ AFMC/A4UE for final
           resolution.

Table A9.2. Dash Numbers.
  Parts                                             Assemblies
  Shown              Opposite                       Shown              Opposite
  -01                -02                            -10                -20
  -03                -04                            -30                -40
  -05                -06                            -50                -60
  -07                -08                            -70                -80
  (do not use numbers ending in 9 or 0)             -90                -100
  -11                -12                            -110               -120
  -13                -14                            -130               -140
  -15                -16                            Etc
  Etc
AFPAM63-128 10 JULY 2014                                                                    249


                                      Attachment 10
  STANDARDS AND MANUALS FOR ENGINEERING DRAWINGS AND RELATED
                       DOCUMENTATION

A10.1. Overview. Table A10.1 provides references identified are Non-Government Standards
(NGS) for Engineering Drawing Preparation. When Government standards or specifications are
revised, superseded, or cancelled the following codes and standards shall apply.

Table A10.1. Non-Government Standards (NGS) for Engineering Drawing Preparation.
ASME          American Society of Mechanical Engineers
Three Park Avenue, New York, NY 10016-5990
Order Dept: 22 Law Drive, P.O. Box 2900, Fairfield NJ 07007-2900
http://www.asme.org/catalog/
ASME B46.1 Surface Texture (Surface, Roughness, Waviness, and Lay)
ASME Y14.1 Decimal Inch Drawing Sheet Size and Format
ASME Y14.1M          Metric Drawing Sheet Size and Format
ASME Y14.2 Line Conventions and Lettering
ASME Y14.3 Multi- and Sectional-View Drawings
ASME Y14.4M          Pictorial Drawing
ASME Y14.5 Dimensioning and Tolerancing
ANSI Y14.6 Screw Thread Representation
ANSI Y14.7.1 Gear Drawing Standards – Part 1: For Spur, Helical, Double Helical, and Rack
ANSI Y14.7.2 Gear and Spline Drawing Standards – Part 2: Bevel and Hypoid Gears
ASME Y14.8 Casting and Forgings
ASME Y14.13M         Mechanical Spring Representation
ASME Y14.24          Types and Applications of Engineering Drawings
ASME Y14.31          Undimensioned Drawings
ASME Y14.34          Associated Lists
ASME Y14.35M         Revision of Engineering Drawings and Associated Documents
ASME Y14.36M         Graphic Symbols for Heat-Power Apparatus
ASME Y14.38          Abbreviations and Acronyms
ASME Y14.41          Product Definition Data Set Practices – Digital
ASME Y14.42          Digital Approval Systems
ASME Y14.43          Dimensioning and Tolerancing for Gages and Fixtures
ASME Y14.100         Engineering Drawing Practices



AIIM          Association for Information and Image Management
1100 Wayne Avenue, Silver Spring, MD 20910
http://www.aiim.org/ ANSI/AIIM MS4         Flowchart Symbols and Their Use in Micrographics
 250                                                           AFPAM63-128 10 JULY 2014


AWS American Welding Society
550 NW Le Jeune Road, Miami, FL 33135
http://www.aws.org/
ANSI/AWS A2.4        Standard Symbols for Welding, Brazing, and Nondestructive Examination
ANSI/AWS A3.0        Welding Terms and Definitions, Including Terms for Brazing, Soldering,
Thermal Spraying, and Thermal Cutting

ASTM          American Society for Testing and Materials
100 Barr Harbor Drive, West Conshohocken, PA 19428
http://www.astm.org/
IEE/ASTM SI 10       American National Standard for Use of the International System of Units
(SI): The Modern Metric System

EIA           Electronic Industries Alliance
2500 Wilson Blvd., Arlington, VA 22201
http://www.eia.org/
EIA 632       Processes for Engineering A System


IEEE          Institute of Electrical and Electronics Engineers
445 Hose Lane, Piscataway, NJ 08855
http://www.ieee.org/
ANSI/IEEE 91          Graphic Symbols for Logic Functions
IEEE 91a      Supplement to Graphic Symbols for Logic Functions
ANSI/IEEE 260.1       Letter Symbols for Units of Measurement (SI Units, Customary Inch-
Pound Units, and Certain Other Units)
ANSI/IEEE 260.3       Mathematical Signs and Symbols for Use in Physical Sciences and
Technology
ANSI/IEEE 280         Letter Symbols for Quantities Used in Electrical Science and Electrical
Engineering (Same as ANSI Y10.5)
IEEE 315      Graphic Symbols for Electrical and Electronics Diagrams
ANSI/IEEE 315a        Supplement to Graphic Symbols for Electrical and Electronics Diagrams
IPC Institute for Interconnecting and Packaging Electronic Circuits

2215 Sanders Road, Northbrook, IL 60062
http://www.ipc.org/
IPC D-325     Documentation Requirements for Printed Boards, Assemblies, and Support
Drawings
ANSI/IPC D-350       Printed Board Description in Digital Form
ANSI/IPC T-50F       Terms and Definitions for Interconnecting and Packaging Electronic
Circuits
IPC 2221      Generic Standard on Printing Wiring Board Design
AFPAM63-128 10 JULY 2014                                                                     251




ISO            International Organization for Standardization
http://www.iso.ch/iso/en/prods-services/ISOstore/store.html
American National Standards Institute (ANSI)
West 43rd Street, 4th floor, New York, NY 10036
http://webstore.ansi.org/ansidocstore/default.asp
ISO 10303      Standard for Exchange of Product model data (STEP)
Note: This reference is to a family of application protocols (AP).
ISO 10303-239           Application Protocol: Product Life Cycle Support


SAE           Society of Automotive Engineers
400 Commonwealth Drive, Warrendale, PA 15096
http://www.sae.org/
SAE AS 1290 Graphic Symbols for Aircraft Hydraulic and Pneumatic Systems

A10.2. Requirements Manuals. Table A10.2 is a list of commercially available drawing
requirements manuals is current as of the publication of this Instruction and does not constitute
endorsement nor imply required use. These manuals may be locally purchased and used directly
or to supplement the standards listed above and tailored as required. Note: When there is a
conflict with the DRM the respective ASME document takes preference.

Table A10.2. Commercially Available Drawing Requirements.
Drawing Requirements Manual
Global Engineering Documents
15 Inverness Way
Englewood CO 80112-5704
(800) 854-7179
FAX (303) 397-7935
http://global.ihs.com/

Modern Drafting Practices and Standards Manual
Genium Publishing Corp.
1174 Riverfront Center
Amsterdam NY 12010
(800) 243-6486
FAX (518) 842-1843
http://www.dz.genium.com
 252                                                           AFPAM63-128 10 JULY 2014


                                       Attachment 11
                       CONFIDENCE LEVEL CONSIDERATIONS

A11.1. Documenting the Confidence Level. The confidence level used in establishing the cost
estimate is required to be included in many documents including:
   A11.1.1. The ADM approving the APB
   A11.1.2. Any cost estimates for MDAPs or MAIS programs prepared in association with the
   Independent Cost Estimates (ICEs)that are required in advance of: 1) MS A, MS B, low rate
   initial production (LRIP), and full rate production (FRP); 2) Any certification pursuant to
   sections 2366a, 2366b, or 2433a of Title 10, USC; 3) Any report pursuant to section 2445c of
   Title 10 USC; and 3) At any time specified by the MDA or the D,CAPE.
   A11.1.3. For MDAPs, the confidence level statement should also be included in the next
   selected acquisition report (SAR) prepared in compliance with section 2432 of Title 10 USC,
   and for MAIS, in the next quarterly report prepared in compliance with section 2445c of Title
   10 USC
A11.2. Considerations. When recommending or selecting a Confidence Level (CL) for an
estimate (MDAP, MAIS, or ACAT II), which will directly affect the program budget, the
program team and Milestone Decision Authority (MDA) considers a program's requirements,
cost and schedule, interfaces or criticality to other programs, and technical and programmatic
maturity. Where these considerations are found to be exceptional with respect to acquisition
programs in general, they may be used to justify a higher CL estimate for developing a program
budget. These considerations are used:
   A11.2.1. (1) by the program team in formulating a CL recommendation,
   A11.2.2. (2) at the MDA level in determining the appropriate CL for the program and in
   documenting the rationale for the choice of the CL in the Acquisition Decision Memorandum
   (ADM), and
   A11.2.3. (3) by the Air Force Corporate Structure (AFCS) in evaluation of program
   funding.
A11.3. Examples. The following list gives examples of possible considerations. This list is
then augmented with other exceptional aspects of the individual acquisition program that affect
the choice of the program's CL.
   A11.3.1. Requirements
   A11.3.2. Low level of detail with respect to granularity of requirements (e.g. completeness
   of the Capability Based Assessment (CBA), system requirements are traceable to operational
   requirements, degree to which requirements are finite and testable)
   A11.3.3. Incremental strategy in providing capability
   A11.3.4. Warfighter requirements vs. business system requirements
   A11.3.5. Air Force specific requirements (importance of joint requirements would need to be
   accompanied by a funding mandate)
   A11.3.6. Increment delivering multiple Key Performance Parameters (KPPs)
AFPAM63-128 10 JULY 2014                                                                     253


  A11.3.7. Major risk areas from Capabilities Review and Risk Assessment (CRRA)
  A11.3.8. Tier I Weapons Systems
  A11.3.9. Sufficient number of test articles and Test & Evaluation infrastructure for
  completing the development program
  A11.3.10. Time critical delivery (schedule urgency)
  A11.3.11. Low confidence in quality/completeness of cost estimate
  A11.3.12. Degree to which schedule and cost uncertainties are integrated, and time-phasing
  of budget
  A11.3.13. Development Test/Production schedule phase concurrency Interfaces/Criticality
  to Other Programs and/or Other Program Increments
  A11.3.14. Several other programs dependent on the program in question (type of program
  dependencies in a system-of-systems)
  A11.3.15. Foundational increment (e.g. platform) Programmatic/Technical
  A11.3.16. Degree to which significant functional groups (e.g. contracting, systems
  engineering, logistics, T &E, risk management) believe the level of acquisition strategy detail
  is appropriate
  A11.3.17. Cost type strategy
  A11.3.18. Developmental Planning (Pre Milestone A)
  A11.3.19. Between Milestone A & Milestone B (Tec1mology Development Phase)
  A11.3.20. Post Milestone B
  A11.3.21. Technology Readiness and Manufacturing Readiness Levels are appropriate for
  Milestone events
  A11.3.22. History of like/similar program execution problems due to risk realization
  A11.3.23. Prevention of Class A type incidents (Safety Issues)
  A11.3.24. Confidence Level Considerations Schedule/Cost
 254                                                                       AFPAM63-128 10 JULY 2014


                                               Attachment 12
                            PROGRAM TERMINATION TEMPLATES

A12.1. Termination ADM
MEMORANDUM FOR AF PEO ____________

FROM: SAF/AQ
1060 Air Force Pentagon
Washington, DC 22030-1060

SUBJECT: Termination Acquisition Decision Memorandum (T-ADM) for __________________________________

Purpose: This T-ADM serves as direction to terminate the_____________________________________ program.
The PM is ____________________ (XXX/XXX) and the PEO is __________________________ (XXX/XXXX).

Decisions:
In accordance with ________________________, I render a Program Termination Decision for ________.
Or: As a result of the FY13 President’s Budget eliminating funding for the ___________________ program, I
direct the program to stop work and end contract activity in a prudent manner and take appropriate steps to
efficiently close out all ongoing efforts in accordance with current laws and regulations.
NOTE: For termination of modification programs or incrementally developed programs with fielded
systems/capability, the following specific guidance is provided: ______________________________.

Tasking/Action Items:
The PM will identify a Termination Contracting Officer (TCO) to work closely with the PM and the Contracting
Officer to facilitate the termination process.
The Contracting Officer will ensure completion of all actions required IAW law and regulation, to include all
necessary notifications.
The Program PCO/ACO, as appropriate, will obtain legal review/input.
The program should present to me within XX days the detailed termination plan and the status of actions that have
been taken.
The plan will be coordinated like an acquisition plan, to include pertinent notifications.
The termination plan should address program termination activities required by FAR Part 49 and AFI 63-101/20-101
and include the following:
Organizational responsibilities for termination
Program documentation and records
Assumptions
Contract and legal status
Funding
Personnel
Agreements, Performance Based Arrangements, Public-Private Partnerships, and Commitments
GFP and equipment deliverables




Points of Contact: _____________________, SAF/xxxx, (                       @mail.mil) DSN: xxx-xxxx.
AFPAM63-128 10 JULY 2014                                                                      255


                                                John Q. Smith
                                    Air Force Service Acquisition Executive

                           (or DoD MDA, as appropriate; if so, prepare on OSD letterhead)
                           (or PEO as MDA, if applicable; if so, prepare on PEO letterhead)


1 Attachment:
TBD

DISTRIBUTION:
OSD Offices

HAF Offices

MAJCOM CC & Offices

Product Center Offices

Related Customers/Stakeholders
A12.2. Termination Plan
Purpose. This plan delineates responsibilities to efficiently terminate the __________Program.

Program/System Description. Include function and technical description of the program/system
to be terminated. Include the identification of salvageable technologies and other deliverables
and any other pertinent issues that require approval. State any dependencies that exist between
this program and any other program(s).

 Program Information.
Program Name:                                       ACAT:                      Phase:
Joint Program: (Yes or no)                          Foreign Military Sales: (If yes, whom?)
Implementing Command:
Sponsoring Command:
Participating Command(s):
OT&E Agency:
MDA:
PEO:
PM:
PCO:
TCO:
PEM:
Organizational Responsibilities. Identify those management responsibilities and tasks that the
gaining organization (if any) will need to continue after termination. When appropriate, address
any provisions required to facilitate the termination of the program/system. Areas to be
addressed in this paragraph include:
 256                                                              AFPAM63-128 10 JULY 2014


SAF/AQ will: Identify based on program.
The Program Manager will:
1) Determine organization responsible for termination activities;
2) Schedule of all termination and transition activities (e.g, like an Acquisition Plan schedule);
3) Determine planned date of SPO disestablishment and facility transition or closure;
4) Determine turnover of facilities, permanent documents, and documents of historical value;
5) Determine disposition of related efforts (including those that should be considered for ACAT
status);
6) Determine impact to other programs and a plan to mitigate such impact;
7) Determine known industrial base impacts;
8) Determine disposition of technology, GFP and documentation;
9) Identify organizations’ responsibilities; and,
10) Identify enterprise/architectural impacts.
The PCO and/or TCO, as appropriate, will:
Determine status of contracting activities and the contract;
Identify location of the Termination Contracting Officer (TCO) who will handle the settlement;
Create a plan to conclude open contracts in the most advantageous way to the government;
Identify potential for claims against the government;
Coordinate with the FM community for cognizance on financial closeout activities; and,
Arrange for disposition of accumulated equipment and property.
The Program Lead Financial Manager will:
                1) Identify funds necessary for termination of the program;
                2) Determine un-liquidated obligations;
                3) Identify all outstanding contingent liabilities; and,
                4) Ensure financial closeout and disposition of unobligated funds.
e. Personnel Activities – With the assistance of the Personnel office, the PM will:
Create a Human Resource plan to complete termination activities;
Determine release or reassignment of Government personnel;
Determine disposition of support contractors; and,
Determine disposition of manpower spaces.
The MAJCOM/A5 will: Identify based on program.
DCMA-xxxx will: Identify based on program.
The NGB (if applicable) will: Identify based on program.

Item Documentation and Records.

Technology, property and document disposition
Configuration Management
Engineering Responsibility, Engineering Data and Technical Data Package
Capabilities/Requirements Realignment
Impact to other programs and mitigation approach
Logistics Support (including Facilities Disposition/Hardware Realignment)
AFPAM63-128 10 JULY 2014                                                                          257


Software Fielding, Replication, Distribution, and Maintenance
Transportation and Packaging
Product Assurance Responsibility
Safety
Human Systems Integration (manpower positions, personnel assignments, etc)
Security Classification Guidance
Environmental Documentation
Other responsibilities as specified in the Termination Acquisition Decision Memo

6. Assumptions.

7. Contract Status and Legal Issues. Contracts Status: Open contracts/contractor(s)/time to
completion/contract amount(s)/type dollars. Also include description of contracting activities,
status of contracts, and contract- related responsibilities (such as notifications and required
deliveries) pertinent to the termination process. Address termination and/or modification of
existing contracts to include termination costs and un-liquidated obligations. Legal Issues:
Contract-related/personnel or labor/local government, etc.
Current open contract information is as follows (Remove lines that are not applicable):
Number:
Contractor:
Contract Type:
End Date:
Current Value (All CLINs and fees paid):
Award Fees Available:
Total Invoiced to Date:
Current Funded Amount:
Other Pertinent Data:
Outstanding Contract Change Proposals:
CCP #, short name, action pending (e.g., definitize UCA, settle proposal preparation cost)
          c. Legal Issues:
              i. Pending legal issues (Claims, Request for Equitable Adjustment, etc.)

8. Funding Summary. RDT&E/Procurement/O&M/FutureYears Defense Plan. Include
portrayal of the overall budget and funding to include funds necessary for termination of the
program and any anticipated future funding needs. Identify unobligated funds. Establish a
timetable for withdrawal of program funds and address the status of all funding actions that have
an actual or contingent liability.
    Termination results in adjustments to current and out year funding as noted below

                                                   RDT&E
                                                                      Then Year & in Thousands
                     PE             BPAC              FY11    FY12     FY13 FY14 FY15 FY16       FY17
 258                                                                         AFPAM63-128 10 JULY 2014


Available            XXX      XXX                      XXX      XXX      XXX      XXX      XXX      XXX       XXX
                     X                                 X        X        X        X        X        X         X
Required                                               XXX      XXX      XXX      XXX      XXX      XXX       XXX
                                                       X        X        X        X        X        X         X
Excess                                                 XXX      XXX      XXX      XXX      XXX      XXX       XXX
                                                       X        X        X        X        X        X         X

                                               PROCUREMENT
            Appro             BPA      Mo                 Then Year & in Thousands
              p       PE        C      d#     FY10 FY11 FY12 FY13 FY14 FY15                         FY16      FY17
Availabl    3010     XXX      XXX      N/A    XXX XXX XXX XXX XXX XXX                               XXX       XXX
e                    X        X               X    X    X      X        X      X                    X         X
Require                                       XXX XXX XXX XXX XXX XXX                               XXX       XXX
d                                             X    X    X      X        X      X                    X         X
Excess                                        XXX XXX XXX XXX XXX XXX                               XXX       XXX
                                              X    X    X      X        X      X                    X         X

Availabl    3020                              XXX      XXX      XXX      XXX      XXX      XXX      XXX       XXX
e                                             X        X        X        X        X        X        X         X
Require                                       XXX      XXX      XXX      XXX      XXX      XXX      XXX       XXX
d                                             X        X        X        X        X        X        X         X
Excess                                        XXX      XXX      XXX      XXX      XXX      XXX      XXX       XXX
                                              X        X        X        X        X        X        X         X

Availabl    3080                              XXX      XXX      XXX      XXX      XXX      XXX      XXX       XXX
e                                             X        X        X        X        X        X        X         X
Require                                       XXX      XXX      XXX      XXX      XXX      XXX      XXX       XXX
d                                             X        X        X        X        X        X        X         X
Excess                                        XXX      XXX      XXX      XXX      XXX      XXX      XXX       XXX
                                              X        X        X        X        X        X        X         X

Total Excess Procurement                      XXX      XXX      XXX      XXX      XXX      XXX      XXX       XXX
                                              X        X        X        X        X        X        X         X



Instructions:
1. Enter the funding amount released in the fiscal years with active funding (e.g. FY10 – 12) in the “Available” row
    for each funding line.
2. Enter FY13 PB amounts (or most current PB) for the FYDP (F13 – 17) in the “Available” row for each funding
    line.
3. Enter the amount required to terminate the program in the “Required” row for each appropriation.
4. Excess is the difference between the “Available” and “Required” funding line.
9. Termination Actions and Milestones. Document the termination process, including lessons
learned from the program being terminated. Provide a schedule that identifies tasks and
milestones for activities involved in termination, to include the planned date of program office
disestablishment and facility transition. Include other pertinent milestones, such as the
following:

a. Service Acquisition Executive briefed on Termination Plan                                   DD MMM YY
b.USD (AT&L) briefed on Termination Plan                                                       DD MMM YY
c. Congressional Notification made by SAF/LL                                                   DD MMM YY
d. Issue Termination Notice to prime contractor                                           DD MMM YY
AFPAM63-128 10 JULY 2014                                                                            259

e. Contractor notified, w/request for termination settlement proposal (TSP)                 DD MMM YY
f. TCO conducts opening conference on termination process                                   DD MMM YY
g. Contractor provides estimate of termination cost                                         DD MMM YY
h. Receive Contractor’s TSP                                                                 DD MMM YY
i. TCO negotiates final settlement and issues contractual modifications                     DD MMM YY


Authorizations/Personnel Summary. Include proposed disposition of all manpower
authorizations and personnel involved in the termination including those required for completion
of close-out activities and those available for reassignment. When appropriate, include the
schedule of proposed draw down of manpower authorizations by fiscal quarter or if possible by
month. Provide the personnel summary in formats similar to the following:
MILITARY: (Current Auth) (On Board) (Required After**)

CIVILIAN: (Current Auth) (On Board Auth) (Required After**)

CONTRACTOR: (Current Auth) (On Board Auth) (Required After**) ** Identify the functions
(as stated in item 4) of all personnel resources required after termination.


           Position                                                              EFF DATE
                        PAS      OSC      AFSC OCC GRADE                  PEC             REMARKS
           Number                                                                MM/DD/YY

 From: XXXX XXX XXX XXX XXX                        XXX     XXX            XXX
   To:
 From:                                                                    XXX
   To:
Agreements and Commitments. Identify any Memoranda of Agreement/Understanding that
supports the program system being terminated. Address withdrawal from any
agreement/understanding including international programs.

COORDINATION:

PM, PCO, TCO, DCMA, MAJCOM RQMTS DIR, PEO, CENTER/CC, CAPABILITIES
DIR, MDA



_________________________                                    ____________________________

     (Losing Organization)                                              (Gaining Organization)

Automated Information Systems: ________________________________________

                                                          (Functional Proponent)
 260                                                         AFPAM63-128 10 JULY 2014


Headquarters, United States Air Force Review: ______________________________

                                                           (SAF/AQX)

APPROVAL: ___________________________________

             (Air Force Service Acquisition Executive)



(Note: This Template is applicable to ACAT I programs where the SAE is the MDA, but it can
be tailored for use on ACAT ID/IAM programs where the DAE is the MDA, or for use on
ACAT II & III programs where the PEO is the MDA.)
AFPAM63-128 10 JULY 2014                                261


                                   Attachment 13
                ACQUISITION PROGRAM BASELINE EXAMPLES

Figure A13.1. Signature Page Example.
 262                     AFPAM63-128 10 JULY 2014


Figure A13.2. Example.
AFPAM63-128 10 JULY 2014           263



Section B Example.




Figure A13.3. Section C Example.
 264                            AFPAM63-128 10 JULY 2014


Figure A13.3. Example (cont).
AFPAM63-128 10 JULY 2014                                                                          265


                                          Attachment 14
       ACQUISITION PROGRAM TECHNICAL CERTIFICATIONS SUMMARY

A14.1. Overview. Table A14.1 identifies a non-exhaustive list of program and system-level
technical certifications. This table is referenced in Defense Acquisition Guidebook - Chapter 4.
Some of the certifications listed are applicable across DoD, whereas others are Service specific.
Programs are advised to use the list as a starting point for identifying applicable certification
requirements. Program Managers and Systems Engineers should consult ODASD (SE), joint,
and Service-specific domain experts to determine other certifications that may be required.

Table A14.1. DoD Acquisition Program Technical Certifications Summary.
                           Source Requirement/
Certification              Guidance            Title
Air Force Training
System and Device          AFI 36-2251         Management of Air Force Training Systems
Simulator Certification
Air Transportability       MIL-STD-1366E       Interface Standard for Transportability Criteria
Certification              AFMAN 24-204        Preparing Hazardous Materials For Military Air
                           MIL-HDBK-516        Shipments
                                               Airworthiness  Certification Criteria
                           Joint Service
                           Specification       JSSG-2000 Air System
                           Guide(JSSG)-2000
                           JSSG-2001           JSSG-2001 Air Vehicle
Airworthiness
                           JSSG-2005           JSSG-2005 Avionic Subsystem, Main Body
Certification
                           JSSG-2006           JSSG-2006 Aircraft Structures
                           JSSG-2007           JSSG-2007 Engines, Aircraft, Turbine
                           JSSG-2008           JSSG-2008 Vehicle Control and Management
                           AFPD 62-6           SystemAirworthiness
                                               USAF   (VCMS)
Assessment of
Operational Test           DoDI 5000.02        Operation of the Defense Acquisition System
Readiness (AOTR)
                                               Interoperability and Supportability of
                           DoDD 4630.5         Information Technology(IT) and National
Authorization to Operate                       Security Systems (NSS)
(ATO)                                          Procedures for Interoperability and
                           DoDI 4630.8         Supportability of Information Technology (IT)
                                               and National
                                               Policy        Security Systems
                                                      and Procedures           (NSS) and Use
                                                                      for Management
                           DoDI 4650.01
Command,                                     of the Electromagnetic Spectrum
Control,                   DoDI 5000.02      Operation of the Defense Acquisition System
Communications,                              Net Ready Key Performance Parameter (NR
Computers, &               CJCSI 6212.01F
                                             KPP)
Intelligence (C4I)                           Department of the Navy Implementation and
Supportability                               Operation of the Defense Acquisition System
Certification              SECNAVINST 5000.2
                                             and the Joint Capabilities Integration and
                                             Development System
266                                                                  AFPAM63-128 10 JULY 2014


                          Source Requirement/
Certification             Guidance            Title
                          DoDI 5000.02        Operation of the Defense Acquisition System
Clinger-Cohen Act
(CCA) Compliance          Section 8808 of Public
Certification                                    Department of Defense Appropriation Act, 2003
                          Law 107- 248

                          DoDD 8500.01           Information Assurance (IA)
                          DoDI 5000.02           Operation of the Defense Acquisition System
                          Section 3502 of Public E-Government Act of 2002
                          Law 107-347            National Security Directive - National Policy for
                          NSD-42                 the Security of National Security
                          CNSSI-1253/CNSSI- Telecommunications         andand
                                                 Security Categorization   Information Systems
                                                                              Control Selection
                          1243a                  for National Security Systems
Risk Management           CNSSI-4009             National Information Assurance Glossary
Framework for DoD IT                             National Institute of Standards and Technology
                          NIST-SP-800-37         (NIST) - Guide for Applying the Risk
                                                 Management Framework to Federal Information
                                                 Systems
                                                 National Institute of Standards and Technology
                          NIST-SP-800-39
                                                 (NIST) - Managing Information Security Risk
                                                 National Institute of Standards and
                          NIST-SP-800-53         Technology (NIST) - Recommended
                                                 Security Controls for Federal
Full Materiel Release                            Information  Systems
                                                 Instructions for       andRelease, Fielding, and
                                                                  Materiel
                          DA PAM 700-142         Organizations
Certification                                    Transfer
                                                 DOD Ammunition and Explosives Safety
Hazards of                DoDM 6055.09-M
                                                 Standards
Electromagnetic
Radiation to Ordnance     NAVSEAINST             Naval Sea Command instructions
(HERO) Certification      8020.7B                (NAVSEAINST) 8020.7b

                                            Interface Control Standard for Mode 4/5
                          DoD AIMS 04-900
Identification Friend                       Cryptographic Computer
or Foe (IFF)                                Technical Standard for the ATCRBS/IFF/Mark
Equipment                                   XIIA Electronic Identification and Military
Certification             DoD AIMS 03-1000B
                                            Implementation of Mode S and Classified
                                            Addenda
Independent Logistics
                          SECNAVINST             Independent Logistics Assessment and
Assessment (ILA) and
                          4105.1B                Certification Requirements
Logistics Certification
                                                 Interoperability and Supportability of
                          DoDD 4630.5            Information Technology (IT) and National
Interim Approval
to Operate                                       Security Systems
                                                 Procedures        (NSS)
                                                            for Interoperability and
                          DoDI 4630.8            Supportability of Information Technology (IT)
(IATO)
Certification                                    and
                                                 Net National
                                                     Ready Key Security Systems
                                                                  Performance    (NSS) (NR
                                                                               Parameter
                          CJCSI 6212.01F
                                                 KPP)
AFPAM63-128 10 JULY 2014                                                                                267


                              Source Requirement/
Certification                 Guidance            Title
Insensitive Munitions         DoDD 5000.01        The Defense Acquisition System
Certification                 CJCSI 3170          Joint Capabilities Integration and Development
                                                  System
                                                  Net Ready Key Performance Parameter (NR
                              CJCSI 6212.01F
                                                  KPP)
Joint Interoperability Test
Certification                                        Compatibility, Interoperability, and Integration
                              AFI 33-108             of Command, Control, Communications, and
                                                     Computer (C4) Systems
Joint Military Intelligence                          Joint Military Intelligence Requirements
                            CJCSI 3312.01B
Certification                                        Certification
                                                     DoD Modeling and Simulation (M&S)
                         DoDI 5000.61                Verification, Validation, and Accreditation
Modeling and
Simulation Verification,                             (VV&A)
                                                     Department of Defense Standard Practice:
Validation, and                                      Document of Verification, Validation and
Accreditation            Mil-STD 3022
                                                     Accreditation (VV&A) for Models and
                                                     Simulations
Nonnuclear Munitions
Safety Board
                              AFI 91-205             Nonnuclear Munitions Safety Board
(NNMSB)
Certification
                              DoDI 5000.02             Operation of the Defense Acquisition System
                              DOT&E TEMP               Director, Operational Test & Evaluation TEMP
                              Guidebook                Guidebook
                              Section 139 of title 10,
Operational Test &                                     Director of Operational Test and Evaluation
                              United States Code
Evaluation (OT&E)
                              Section 2399 of title Operational test and evaluation of defense
Readiness Certification
                              10, United States Code acquisition programs
                              AFI 99-103               Capabilities-Based Test And Evaluation
                                                       Certification Of System Readiness For
                              AFMAN 63-119
                                                       Dedicated Operational Testing
Radio Frequency
Radiation (RFR)
                                                     Protecting Personnel from Electromagnetic
Hazards                       DoDI 6055.11
                                                     Fields
(RADHAZ)
Certification
                                                       Policy and Procedures for Management and Use
                              DoDD 4650.01
                                                       of the Electromagnetic Spectrum
Spectrum Certification        Section 305 of title 47, United States Code - Government owned
Compliance                    United States Code       stations
                              Section 102 – 538 of
                                                     Public Law 102- 538, 104
                              Public Law
268                                                            AFPAM63-128 10 JULY 2014


                  Source Requirement/
Certification     Guidance            Title
                                          Definitions, findings, policy (901);
                  Section 901 - 904 of Establishment; assigned functions
                  title 47, United States (902); Spectrum management
                  Code                    activities (903); General
                                          administrative provisions (904)

                                          Office of Management & Budget Circular A-
                  OMB CIRCULAR
                                          11:Preparation & Submission of Budget
                  NO. A- 11, Part 2
                                          Estimates
Training Device
                  AFI 36-2251             Management of Air Force Training Systems
Certification
Ultrahigh
Frequency (UHF)
Satellite
Communication
                                          Narrowband Satellite Communications
(SATCOM)          CJCSI 6251.01D
                                          Requirements
Demand Assigned
Multiple Access
(DAMA)
Certification
