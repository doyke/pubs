BY ORDER OF THE                              AIR NATIONAL GUARD PAMPHLET 21-106
CHIEF NATIONAL GUARD BUREAU
                                                                      10 NOVEMBER 2011
                                                           Certified Current 20 August 2013
                                                                               Maintenance

                                                        MAINTENANCE DATA SYSTEMS
                                                                        ANALYSIS


              COMPLIANCE WITH THIS PUBLICATION IS MANDATORY

ACCESSIBILITY: Publications and forms are available on the e-Publishing website at
               www.e-publishing.af.mil for downloading or ordering.

RELEASABILITY: There are no releasability restrictions on this publication.
OPR: ANG/A4MM                                      Certified by: NGB/A4 (Col Michael Ogle)
                                                                                Pages: 131
Supersedes:   ANGPAM21-101,
              29 January 1999
This publication interfaces with AFI 21-101 ANG Supp I, Aircraft and Equipment Maintenance
Management, and provides the ANG Analyst a reference tool with which to train themselves and
others in the performance of their assigned duties and responsibilities. It is not all-inclusive but
covers common areas in the analysis arena. It does not duplicate technical instructions or written
regulations. It does provide the user more in-depth knowledge of the job and details the impact
that the user can have on the organization. The guidance provided is for the user's information
and is non-directive in nature.



Chapter 1—THE MAINTENANCE DATA SYSTEMS ANALYSIS CAREER FIELD                                                                                    7
       1.1.    Vision: ....................................................................................................................     7
       1.2.    Specialty Summary. ...............................................................................................               7
       1.3.    The Career Field Education and Training Plan (CFETP). .....................................                                      7
       1.4.    Unit Level Assignments: ........................................................................................                 7
       1.5.    Command Level Assignments: ..............................................................................                        7
       1.6.    Specialty Qualifications. ........................................................................................               7
       1.7.    The ―Maintenance Data Analyst‖. .........................................................................                        7
       1.8.    KEYS TO EFFECTIVENESS. ..............................................................................                            8
       1.9.    Responsibility. .......................................................................................................          9
       1.10.   Marketing Your Services. ......................................................................................                 10

Chapter 2—DATA INTEGRITY                                                                                                                       11
       2.1.    What .......................................................................................................................    11
       2.2.    Why is Data Integrity Important? ..........................................................................                     11
Figure 2.1.    Reliability, Accuracy, and Efficiency. ...................................................................                      11
       2.3.    Who Uses the Data Collected? ..............................................................................                     11
Figure 2.2.    Command Badges. .................................................................................................               11
       2.4.    Data. .......................................................................................................................   11
       2.5.    Collection of data. ..................................................................................................          11
       2.6.    At base level. ..........................................................................................................       12
       2.7.    Data Integrity Team Goals. ....................................................................................                 12
       2.8.    Data Integrity. ........................................................................................................        12
Figure 2.3.    Finger Pointing. .....................................................................................................          12
Figure 2.4.    Reference Box. .......................................................................................................          13
ANGPAM21-106 10 NOVEMBER 2011                                                                                                                 3


Figure 2.5.    The On-Equipment Process. ..................................................................................                   14
       2.9.    THE NUTS AND BOLTS OF DATA INTEGRITY. ............................................                                             14
Figure 2.6.    Administrative Note. ..............................................................................................            15
       2.10.   Team‘s Primary Goal. ............................................................................................              15
       2.11.   Team composition and responsibilities: .................................................................                       15
Figure 2.7.    Remember Box. .....................................................................................................            16
       2.12.   Meat and Potatoes. .................................................................................................           16
Figure 2.8.    Records Checks Box. .............................................................................................              16
Figure 2.9.    Debrief Check. .......................................................................................................         17
Figure 2.10.   MDD/JDD Check. .................................................................................................               17
Figure 2.11.   AF Portal. ...............................................................................................................     17
       2.13.   IMDS-CDB/G081 Users Group. ............................................................................                        18
       2.14.   What you need to accomplish: ...............................................................................                   18
       2.15.   Having your first and subsequent meeting(s). .......................................................                           18
       2.16.   Now ........................................................................................................................   19
       2.17.   Some closing advice. .............................................................................................             19

Chapter 3—TRAINING                                                                                                                            20
       3.1.    A sound, viable training program will make you money in the long run. .............                                            20
       3.2.    To have an effective training program requires planning. .....................................                                 20
       3.3.    Now, put it all together in one master program file and put it into practice. .........                                        20
       3.4.    Let the people that are fully qualified become task trainers and task certifiers for
               your office. 21
       3.5.    If you find yourself in a one deep analysis section, training becomes difficult but
               not impossible. ................................................................................................... 21
       3.6.    The Analysis career field also has a Single Point of Contact (SPOC) group that
               helps with training and aids in the development of the career field. ......................                                    21

Chapter 4—WHAT SHOULD YOU ANALYZE?                                                                                                            22
       4.1.    Deciding what to analyze and how can be the most challenging part of the
               analysis career field. 22
       4.2.    Example of a deficiency analysis effort gone wrong, made right. .........................                                      23

Chapter 5—MAINTENANCE CAPABILITY FORECASTING                                                                                                  24
       5.1.    Capability Forecasting. ..........................................................................................             24
       5.2.    Airframe Capability Forecast, Wartime Capability. ..............................................                               25
 4                                                                            ANGPAM21-106 10 NOVEMBER 2011


       5.3.   Facility Capability Formulas. .................................................................................              26

Chapter 6—AIRCRAFT RELIABILITY, MAINTAINABILITY AND AVAILABILITY                                                                           28
       6.1.   Although reliability, maintainability and availability are each important elements
              of life cycle cost and performance, you need to address each separately and with
              varying degrees of importance when dealing with mature weapon systems. ........                                              28
       6.2.   By far the best gains in weapon system improvement after production are in the
              area of component and system reliability. .............................................................                      28
       6.3.   Maintainability is a factor mostly influenced during the product design phase and
              can not be easily changed after full scale development and production. ...............                                       28
       6.4.   Availability is typically a result of the combined component reliability,
              maintainability, number of spares procured, and the length of the logistics
              support pipeline (i. .................................................................................................       28

Chapter 7—ANALYTICAL METHODS                                                                                                               30
       7.1.   A number of analytical methods are available and although each one has validity
              in a given situation. ................................................................................................       30
       7.2.   ANALYSIS PROCESS: ........................................................................................                   30
Figure 7.1.   The Analysis Process. ............................................................................................           31
       7.3.   Major Studies or Special Studies: ..........................................................................                 35
       7.4.   These are the basic functions that can make your analysis system a useful adjunct
              to decision making. ................................................................................................         35
       7.5.   How to Perform an Analysis Study. ......................................................................                     35
Table 7.1.    Six Basic Questions. ..............................................................................................          35
       7.6.   Maintenance Analysis Referrals: ...........................................................................                  36
       7.7.   Report Formats: .....................................................................................................        37
       7.8.   Objective: ...............................................................................................................   37
       7.9.   Examples. ...............................................................................................................    37

Chapter 8—PRESENTATION TECHNIQUES                                                                                                          39
       8.1.   Choose the Chart to Fit the Data: ...........................................................................                39
       8.2.   Format for Legibility: ............................................................................................          39
       8.3.   Use Tables to Organize Detailed Data: ..................................................................                     39
       8.4.   Use Text Lists for Simple Information: .................................................................                     39
       8.5.   Use Appropriate Line Weights: .............................................................................                  39
       8.6.   Use Color to Convey Information: ........................................................................                    39
       8.7.   Fit the Background to the Occasion: ......................................................................                   39
ANGPAM21-106 10 NOVEMBER 2011                                                                                                               5


      8.8.    Combine Colors Carefully: ....................................................................................                39
      8.9.    Format Text for Readability: .................................................................................                40
      8.10.   Compensate for Screen Resolution: .......................................................................                     40
      8.11.   Use Movement to Advantage: ...............................................................................                    40
      8.12.   The Real Secret: .....................................................................................................        40

Chapter 9—TREND ANALYSIS                                                                                                                    41
      9.1.    What is trend analysis? The determination of the direction a set of portrayed data
              is taking. 41
      9.2.    Time Series Analysis. ............................................................................................            41
      9.3.    Testing Significance of Trend. ...............................................................................                41

Chapter 10—CONTROL CHARTS                                                                                                                   42
      10.1.   This chapter covers four types of control charts. ...................................................                         42
      10.2.   The purpose of a control chart is to monitor variation and to detect the presence
              of assignable causes for variation. .........................................................................                 42
      10.3.   Control Limits. .......................................................................................................       42
      10.4.   Interpretation. .........................................................................................................     42
      10.5.   Types of Control Charts. ........................................................................................             43
      10.6.   Standard Deviation. ...............................................................................................           43
Table 10.1.   Standard Deviation. ...............................................................................................           44
      10.7.   Charts for Averages. ..............................................................................................           44
Table 10.2.   Charts for Averages. ..............................................................................................           44
      10.8.   P Charts. .................................................................................................................   44
Table 10.3.   P Charts. .................................................................................................................   45
      10.9.   The C chart is different from a P chart in that it plots the number of defects rather
              than percent defective. ....................................................................................... 45
Table 10.4.   The C Chart. ...........................................................................................................      46

Chapter 11—DETERMINING SAMPLE SIZE                                                                                                          47
      11.1.   We don't normally use sampling techniques in our day-to-day analyses. ..............                                          47
      11.2.   Proportion Estimates. .............................................................................................           47
Table 11.1.   Proportion Estimates. .............................................................................................           47

Chapter 12—IMDS DATABASE MANAGEMENT                                                                                                         49
      12.1.   Data Base Management (DBM): ...........................................................................                       49
      12.2.   Duties and Responsibilities: ...................................................................................              49
 6                                                                        ANGPAM21-106 10 NOVEMBER 2011


      12.3.   IMDS Subsystem Managers: .................................................................................             49
Table 12.1.   Checklist for Unit Performing DBM Functions. ....................................................                      49

Chapter 13—THE DEPLOYED ANALYST                                                                                                      54
      13.1.   MOBILITY COMMMITMENT: ...........................................................................                     54
      13.2.   READINESS: ........................................................................................................   54
      13.3.   ROLE OF THE ANALYST: .................................................................................                 54

Attachment 1—ABBREVIATIONS AND ACRONYMS                                                                                              56

Attachment 2—SYSTEM RELIABILITY AND CAPABILITY.                                                                                     100

Attachment 3—HELPFUL HINTS FOR DATA INVESTIGATION                                                                                   108

Attachment 4—MAINTENANCE ANALYSIS DATA REQUEST                                                                                      113

Attachment 5—SAMPLE ATTRITION SPREADSHEET                                                                                           115

Attachment 6—EMERGENCY AIRCRAFT INCIDENT CHECKLISTS                                                                                 117

Attachment 7—7401 PROCEDURAL GUIDANCE                                                                                               122
ANGPAM21-106 10 NOVEMBER 2011                                                                   7


                                           Chapter 1

         THE MAINTENANCE DATA SYSTEMS ANALYSIS CAREER FIELD

1.1. Vision: Provide expert proactive analytical support to all our Customers. This vision
statement was developed by the ANG Maintenance Management Office for the Maintenance
Data Systems Analysis career field. This vision statement will serve as a model for all Air
National Guard Maintenance Data Systems Analysts. This vision statement provides focus for
all decisions relating to the Maintenance Data Systems Analysis career field.
1.2. Specialty Summary. Air Force Enlisted Classification Directory (AFECD), Page 133,
"Maintenance Management Systems Career Field (2R)," provides a specialty summary, an
overview of duties and responsibilities, and specialty qualification requirements.
1.3. The Career Field Education and Training Plan (CFETP). The Career Field Education
and Training Plan (CFETP), Section B, provides career progression and specific information
relating to the duties and responsibilities of the Maintenance Data Systems Analyst (MDSA)
Apprentice, Journeyman, Craftsman and Superintendent. For a complete listing of these duties
refer to your most current CFETP 2R0X1.
1.4. Unit Level Assignments: Vacancies are advertised locally. Assignments are unit directed.
The selectee is chosen from a list of the most eligible and qualified candidates.
1.5. Command Level Assignments: At the Air National Guard Readiness Center, assignments
are advertised and the most qualified applicant is selected.
1.6. Specialty Qualifications. Knowledge of operations and maintenance organizations is
mandatory. Also important is knowledge of principles of management procedures applying to
aircraft, missiles, communication-electronics, space systems or related equipment; applied
statistical, analytical, and presentation techniques and concepts; data systems design procedures;
MIS and small computer operation and use; and concepts and application of directives. Career
Field Training is provided via AETC formal schools, Career Development Courses (CDC), on-
the-job training (OJT), computer based training systems (CBT‘s), exportable courses, mobile
training teams, REMIS Schools, MAJCOM Schools, and IMDS/G081 computer based training
courses listed in the Air Force Enlisted Classification Directory (AFECD)
1.7. The ―Maintenance Data Analyst‖. There are extreme differences in definitions applied to
a maintenance data analyst and more precisely maintenance data analysis. Are you a statistical
clerk, a data analyst, a data base manager, a historian with a specific objective in mind or a
manager? What is maintenance data analysis? The maintenance data analyst provides
information and analyses to all levels of management using statistical and analytical skills to
convert raw data into meaningful information using the assessments to answer the why, how,
when and where questions. The goal is to track, analyze, and present information to help senior
leadership assess the health of the units‘ weapon systems and equipment. The analyst also
provides feedback to the decision-makers to maintain a reliable weapon system through the
effective and efficient use of available resources. Most importantly they can identify impending
problems so the maintenance manager can take action to fix them before they occur.
Maintenance analysts are experts on the Maintenance Information System (MIS) capabilities and
limitations for collecting raw data from across the maintenance complex (including debriefing,
maintenance operations center, and the production work centers.
 8                                                       ANGPAM21-106 10 NOVEMBER 2011


     1.7.1. Analysis by any definition is not a fine art. No one has a patent on it and guidance on
     an effective approach to analysis has been limited at best. Those of us who consider
     ourselves professional analysts use the educational tools provided to us, factor in experience,
     expand our education and experience, capitalize on professional reading, learn from past
     successes or failures and keep the management objectives and quality goals in mind to
     provide the best analysis program possible. Analysis is of no benefit if management does not
     respond to the input. If they don‘t respond, your program is out of tune with management
     needs and should be revised. With this in mind let‘s state that, maintenance data analysis is
     the art of analyzing maintenance information and presenting the results to management for
     appropriate action.
     1.7.2. Management needs vary from organization to organization; therefore, each analysis
     program will vary. The organization determines what kind of analysis program your element
     will support and ultimately what kind of individual analysts the people assigned will become.
     For example, workings at Air Force wing level, analysts are exposed to nearly all facets of
     the analysis field.
     1.7.3. Aircraft maintenance metrics are critical to the data analysis and maintenance
     managers to gauge organizations‘ effectiveness and efficiency. Metrics tell you where you
     have been, where you are going, and how you will get there. Management must have
     accurate and reliable information to make decisions. Primary concerns of maintenance
     managers are how well the unit is meeting mission requirements, how to improve equipment
     performance, identifying emerging support problems, and projecting future trends. Metrics
     are a crucial form of information used by maintenance managers to improve the performance
     of maintenance organizations, equipment, and people when compared with established goals
     and standards. Metrics must be accurate, consistent, clearly understood and communicated.
     They must be based on a measurable, well-defined process. Commanders and maintenance
     managers must properly evaluate maintenance metrics and rely upon the maintenance
     analysis section for unbiased information.
1.8. KEYS TO EFFECTIVENESS. Credibility and Communication.
     1.8.1. Credibility. Without credibility your duties and responsibilities will be of little use to
     your organization. The old saying, ―A man‘s word is his bond,‖ can be directly applied to
     analysts. Because management decisions are made based on our analytical reports, it is
     imperative that these management decisions be accurate and timely. Credibility means
     reports and data should be checked and double checked. Another pair of eyes works great
     since the originator can easily get into a mindset concerning their intention to state, write, or
     display. With the widespread use of computers, it is easy for an analyst to use standardized
     spreadsheets that could have been developed inaccurately. Formulas or methods of
     computations change from time to time so you need to be on guard to ensure that your
     worksheets change according to instructions/pamphlets.
        1.8.1.1. Be realistic. When asked for a special study, estimate when you can provide the
        study and then work to meet that suspense. Annotate the request, select a point of
        contact, compile the vital information, establish the purpose, determine the format to
        present the information (slides, charts, or letter format), and determine procedures
        required to meet the suspense. Attachment 7 is a sample Logistics Analysis Data
        Request.
ANGPAM21-106 10 NOVEMBER 2011                                                                 9


   1.8.2. Communication. Without question, this area is a very important part of an effective
   analysis function. Establishing sound communication rapport with all maintenance areas will
   go a long way toward making your job easier and more productive. Remember that close
   and frequent face-to-face contact between managers and analysts is the primary ingredient in
   successful analysis. Make sure you understand the question and that what you thought they
   said is really what you heard and what they need to fulfill their requirement. You must also
   have good communication both horizontally and vertically within the organization to ensure
   the accuracy of your data. The key is to be an effective listener as well as a speaker.
       1.8.2.1. Methods of Communication: oral/verbal (face to face); written (notes, requests,
       e-mail); visual (spreadsheets, examples, etc.)
       1.8.2.2. Keys to communication:
           1.8.2.2.1. Formulate your questions so they cannot be answered with a simple ―Yes‖
           or ―No.‖
           1.8.2.2.2. Listen to what the individual is saying - don‘t interrupt.
           1.8.2.2.3. Process and evaluate the information.
           1.8.2.2.4. Restate to the individual what you thought you heard.
           1.8.2.2.5. Allow the individual to clarify any misinterpretation.
           1.8.2.2.6. Present your facts and ask any remaining questions.
1.9. Responsibility. Your primary responsibility as an analyst is to provide information and
analyses to all levels of management using your statistical and analytical skills to convert raw
data into meaningful information. Your assessments must be objective and answer the "why,
how, when, and where" questions. You will interface with the Operations Group (OG) and
Maintenance Group (MXG) and other maintenance work centers during your analyses and
investigations. Although you serve the entire maintenance community, your loyalty must be to
the MXG. You must be responsive to mission needs and to the needs of maintenance managers
while upholding the integrity and timeliness of reporting. Your goal is to assess the
organizational health of the unit and provide feedback to the decision-makers. You must
develop ways to highlight problem areas that need management attention. Finally, you must
educate maintenance managers about your abilities and capabilities. Don't be content to sit
around and wait for direction. You are the troubleshooter. You must seize the initiative, using
your imagination and technical skills to develop effective methods of implementing your
expertise to improve the maintenance complex, i.e., be pro-active. The following is an example
of how an analyst can be effective.
   1.9.1. You must analyze maintenance performance. A beneficial starting point is a review of
   the previous months‘ flying and maintenance performances.
       1.9.1.1. What did not meet expectations? Identify which indicators were deficient, and
       then begin investigating. Never give the quick answer, dig deep into the data to find the
       answers.
       1.9.1.2. Do not hesitate to say ―No trend noted‖ but be able to support your conclusion
       with good analysis. Always remember, any subject can be explored to the point of
       becoming a special study, but doesn‘t always require one.
 10                                                     ANGPAM21-106 10 NOVEMBER 2011


   1.9.2. Another way to be effective and gain credibility is to complete a thorough analysis
   that results in recommending a change to problem areas. Visit your maintenance sections,
   investigate problem areas, and search for solutions.
1.10. Marketing Your Services. As an analyst, you are a vital part of your unit. You can‘t
help your unit if your unit doesn‘t know who you are or what you do. You must market your
services; analytical studies, trend analyses, database management skills, statistical extraction, etc.
These are the subjects that analysts are trained to do and are uniquely qualified to do within a
unit. No one (including: schedulers, training managers, flight chiefs, maintenance supervisors,
or MXG personnel) is able to analyze as well as you. Treating your customers with respect,
listening to their feedback, discovering exactly their purpose, will minimize rework. Advise
your customers, educate them, but don‘t force a one-product-fits-all mentality on them.
   1.10.1. Examples of opportunities to market your services are: staff meetings, IMDS/G081
   user‘s meetings and training classes, scheduling meetings, newcomers' orientations, and local
   maintenance meetings. Utilize every opportunity to spread the word, becoming part of the
   solution, not part of the problem. You must get out from behind your desk and get to know
   and be known by all maintenance personnel. You‘ll have a better understanding of your
   unit‘s needs and your customers will have better access to you.
   1.10.2. Concluding, you must market your services, which are valuable contributions to your
   unit and will result in making your unit more efficient, less costly, safer, and better.
ANGPAM21-106 10 NOVEMBER 2011                                                                  11


                                           Chapter 2

                                     DATA INTEGRITY

2.1. What IS Data Integrity?
   2.1.1. Data – An individual or group of FACT(s), statistics or information.
   2.1.2. Integrity – Completeness, perfection, soundness.
2.2. Why is Data Integrity Important?
   2.2.1. Collecting and maintaining accurate aircraft maintenance data is the foundation for the
   effective and efficient management of the fleet and a fundamental requirement for aircraft
   safety.
   2.2.2. Management of our aging fleet depends on the availability, accuracy and the ability to
   effectively analyze maintenance data.

Figure 2.1. Reliability, Accuracy, and Efficiency.




                                                             .
2.3. Who Uses the Data Collected?
   2.3.1. Air Logistics Centers (ALCs) and System Program Offices.
   2.3.2. Air National Guard Bureau.
   2.3.3. Managers and Supervisors.
   2.3.4. Maintenance Analysis.

Figure 2.2. Command Badges.


                                                                       .
2.4. Data. Data is used to collect information on unit operations, measure effectiveness,
reliability and maintainability; and provides critical information feedback to managers. The data,
once manipulated, provides a measure of the reliability and maintainability of our weapon
systems which are key factors that influence new weapon systems design, effectiveness, and
logistical support and life cycle costs
2.5. Collection of data. Collection of data is intended to provide critical information feedback
to managers throughout the Air Force, Air National Guard, Air Force Reserve and Defense
Contractors.
 12                                                      ANGPAM21-106 10 NOVEMBER 2011


2.6. At base level. At base level, it is designed to assure effective management of the
maintenance operations such as resources, tools, equipment, skills and personnel production.
Agencies above base level use the data as a source of assessing performance and support
requirements of weapon systems and equipment.
2.7. Data Integrity Team Goals.
      2.7.1. Aim towards reducing errors thus enabling the collection of error free data.
   2.7.2. It should be essential that our database paints an accurate picture of the work we do
   and the problems we encounter.
2.8. Data Integrity. Data Integrity A.K.A. DI.
      2.8.1. A Data Integrity Team SHOULD:
         2.8.1.1. Monitor and recommend actions for problem data reporting.
         2.8.1.2. Educate managers and technicians on proper documentation methods and
         practices.
      2.8.2. A Data Integrity Team SHOULD NOT be used to ―point fingers‖ or as a scorecard
      against a particular work center or individual.

Figure 2.3. Finger Pointing.




                                                            .
      2.8.3. All personnel in the unit are involved to some extent in the documentation,
      processing, review, retrieval or application of maintenance data. The data entry made by a
      technician becomes an element in the database used for management decision making. If
      entries are incorrect, incomplete or entered in error, the database is impaired; consequently,
      decisions made based on erroneous data are less sound. Unit managers and production
      personnel are responsible for ensuring accuracy and completeness.
   2.8.4. Each MDC transaction shall be checked to ensure coding is according to 00-20
   series technical orders and applicable ACC, AMC and ANG publications and reflect the
   proper entries; this includes off-equipment MDC. Narratives should be descriptive and
   portray actual actions accomplished. Errors found will be corrected in the MIS. Once
   errors have been corrected assistance should be provided to preclude recurrence.
ANGPAM21-106 10 NOVEMBER 2011                                                            13


Figure 2.4. Reference Box.




                                              .
   2.8.5. AFI 21-101_ ANGSup mandates Element/Work Center Supervisors will review work
   center MIS data entries for previous day and all preceding non-duty days for accuracy and
   completeness.
 14                                                   ANGPAM21-106 10 NOVEMBER 2011


Figure 2.5. The On-Equipment Process.




                                                                                             .
2.9. THE NUTS AND BOLTS OF DATA INTEGRITY.
   2.9.1. Ok, it‘s time to get down to the basics o what you‘re actually supposed to do.
         2.9.1.1. Baby Steps! Let‘s start with defining how you‘re going to accomplish this task.
         Best to start with an Operating Instruction (collective groan) so not only do you have a
         plan but everyone else knows what the plan is as well.
         2.9.1.2. Do you need an OI? Absolutely! Here‘s a guideline of what your OI should
         contain:
      2.9.2. References.
ANGPAM21-106 10 NOVEMBER 2011                                                                    15


       2.9.2.1. As a minimum, you should site the following basic publications and any other
       local requirements such as your IMDS/G081 OI. AFI 21-101_ANGSUP, Aerospace
       Equipment Maintenance Management; AFI 21-103, Equipment Inventory, Status and
       Utilization Reporting; T.O. 00-20-1, Aerospace Equipment Maintenance Inspection,
       Documentation, Policy and Procedures; T.O.            00-20-2, Maintenance Data
       Documentation.

Figure 2.6. Administrative Note.




                                                                        .
   2.9.3. What‘s your purpose?
       2.9.3.1. State the purpose of what you intend your Data Integrity Team (DIT) to
       accomplish. If your DIT also functions as your User Group, here‘s where you need to
       identify that. Remember, you are responsible for the overall management of the DIT but
       not responsible for identifying or correcting errors.
2.10. Team’s Primary Goal. Basically your team‘s primary goal should be to identify, conquer
and kill documentation problems and prevent them from happening again; keep in mind this is
not carte blanc to point fingers at a work center or individual but rather to help your unit process
good data on a continued basis.
2.11. Team composition and responsibilities:
   2.11.1. Identify who will look at what and when, who will do corrections, how they will
   accomplish reviews/corrections, what their time line is, and how will you track it. This is
   where a Letter of Delegation (LOD) to supplement your OI would be helpful, as you would
   identify your members and their specific areas of responsibilities (IMDS – subsystem
   managers, G081 – areas of expertise). In the event of inspection, it is easier for your
   inspector to validate that you not only have a formalized team but your members meet the
   requirements of AFI 21-101_ANGSUP (required participation by at least one representative
   from each squadron that repairs aircraft, and PS&D, the MOC, CSSM, EM, Debrief, and QA
   as needed; members must be at least 5-levels and familiar with the unit‘s assigned weapon
   system(s)
    16                                                 ANGPAM21-106 10 NOVEMBER 2011


Figure 2.7. Remember Box.




                                                                 .
2.12. Meat and Potatoes. Guidelines on what everybody is supposed to be doing so you know
when to crack the whip.
      2.12.1. AFTO Form 781A reviews.
         2.12.1.1. Audit for mismatch of discrepancies in aircraft forms versus IMDS/G081.
         Look for jobs that are signed off in the forms but not closed in the MIS or jobs closed in
         MIS but still open in the aircraft forms; missing jobs in aircraft forms or corrective
         actions in MIS that do not match forms, etc. When the two differ, the responsible work
         center will be charged with an error and have it included in the error rate.

Figure 2.8. Records Checks Box.




.
      2.12.2. Aircraft Status: Audit for complete work unit codes/ REFDES codes, ensure status
      changes correspond with sequence of events, job status is in accordance with the
      MESL/MEL; ensure the correct JCN is used for status. The office of primary responsibility
      should be your MOC. With that said, it would be a good idea to have a representative from
      your MOC as a member of your DIT. This is a huge task so frequency of this review should
      be weekly at minimum, daily would be preferable.
      2.12.3. Debrief: Audit for correct landing status break/fix open debriefs and missing
      debriefs. If your unit does not have a debrief section your MOC should be accomplishing
      this task. Again frequency as a minimum should be weekly however daily is preferable.
ANGPAM21-106 10 NOVEMBER 2011                                                                  17


Figure 2.9. Debrief Check.




                                                                                           .
    2.12.4. MDD/JDD Reviews.
       2.12.4.1. Run maintenance action review background reports for all work accomplished
       by squadron and work center.

Figure 2.10. MDD/JDD Check.




                                                                                           .
       2.12.4.2. Audit JCN/WCE data for the following data elements:
          2.12.4.2.1. Work Unit Codes.
          2.12.4.2.2. Action Taken.
          2.12.4.2.3. How Malfunction.
          2.12.4.2.4. When Discovered.
          2.12.4.2.5. Start and Stop Times.
          2.12.4.2.6. Crew Size.
          2.12.4.2.7. Category Labor.
          2.12.4.2.8. Corrected by and Inspected by.
          2.12.4.2.9. Narratives should be descriptive and portray actions accomplished.

Figure 2.11. AF Portal.




.
 18                                                   ANGPAM21-106 10 NOVEMBER 2011


       2.12.4.3. Identify suspected errors on the report by circling or marking the report and
       give report to appropriate work center for correction. Identify and count the
       documentation errors. This process can be aautomated and/or electronic.
       2.12.4.4. Develop a system to track the number of errors by work center and squadron.
       2.12.4.5. Establish a 5 day suspense to correct errors and report back to the DIT.
   2.12.5. Other areas for your DIT to look at:
       2.12.5.1. Consider tracking areas for possible data integrity problems, i.e. actual canns
       against the MOC‘s cann log, departure reliability, error rates for PS&D, MOC, and
       debrief for ops events cancelled, but not input into IMDS-CDB/G081, that cause the
       flying hour reports to be incorrect. Look at ways to track status errors from MOC,
       debriefing errors such as no WUC/LCN loaded for a Code-3 PRD, or deviations not
       loaded correctly.
2.13. IMDS-CDB/G081 Users Group. Whether or not you choose to form a separate team
from your DI Team, you are still required to establish a IMDS-CDB/G081 Users Group to:
   2.13.1. Identify user problems.
   2.13.2. Provide on the spot training to correct user documentation problems.
   2.13.3. Discuss other issues relating to operation of the system.
   2.13.4. As stated earlier, identifying these members on a letter of delegation (LOD) would
   be a good idea. If they are responsible for a certain sub-system or have a specific area of
   expertise, also identity that information in your LOD. Good news! A senior maintenance
   leader chairs this working group, not the MDSA.
2.14. What you need to accomplish:
   2.14.1. Meetings – are required to be held at least quarterly and/or prior to loading a IMDS-
   CDB release/G081 major program change.
   2.14.2. Agendas – will be published and sent to all work centers prior to all meetings.
   Consider utilizing the read receipt function to ensure members receive the information.
   2.14.3. Minutes – will be published and sent to all work centers.
2.15. Having your first and subsequent meeting(s). Congratulations! You‘ve come a long
way in establishing your DI program, now let‘s talk about having a meeting and what are you
supposed to be doing.
   2.15.1. Communicate! Identify deficiencies and determine root causes for data
   discrepancies.
   2.15.2. As a group, providing suggestions to reduce/eliminate errors and the means to correct
   data errors in a timely manner.
   2.15.3. Identify the need and form a plan to ensure that every member in the Maintenance
   Complex is adequately trained on correct ATFO form 781 and MIS entries.
   2.15.4. Strive to continually educate members throughout Maintenance Complex on
   maintenance documentation processes, changes as they arise and training tools available to
   them. The better their data the more accurate your metrics are.
ANGPAM21-106 10 NOVEMBER 2011                                                                   19


2.16. Now …what are you suppposed to do with the data collected?
   2.16.1. Maintain cumulative uncorrected and corrected error rates, report this information on
   your NGB 7401. Create a graph of error rates by shop and by errors for a visual presentation
   of problem areas to help focus on the root causes.
   2.16.2. Analyze the error rate data, identify where errors are occurring; in addition to the
   hard data you collect, use the information gained during your data integrity meeting to
   determine if an analysis referral is needed. Brief this information regarding errors, where
   they are occuring and what the team is going to do about it to your MXG/CC monthly.
2.17. Some closing advice.
   2.17.1. Data integrity is not rocket science, but you may feel like a dentist pulling teeth to
   get your program up and running. Education and culture can be another determining factor
   on whether or not your program fails or suceedes. Your first order of business in establishing
   a solid program should be to sit down with your MXG/CC and get a feel for his/her
   philosophy, what are their expectation, what do they want to accomplish, etc. Having an
   established DI program is also one of your MXG/CC‘s checklist items, so it would behoove
   the both of you to form a united front.
   2.17.2. It starts from the top and rolls downhill; but with his/her backing, your partnered DI
   values should be more accepted as a matter of unit policy rather than a silly and cumbersome
   requirement you are trying to enforce. This is a huge task, it will take time to establish and a
   lot of work and continued tweaking of your program to get it right. What works for one unit
   doesn‘t necessarily work for another, but don‘t struggle trying to reinvent the wheel; call
   your SPOC, call a fellow analyst whom you know to have a good program, surf the CoP; you
   may be in a ‗one-deep‘ shop, but you are not alone. There is a plethora of resources available
   to you, take whats out there that will work for your unit and make it your own.
 20                                                    ANGPAM21-106 10 NOVEMBER 2011


                                           Chapter 3

                                          TRAINING

3.1. A sound, viable training program will make you money in the long run. A well-
developed training plan ensures expertise when absences occur. Upgrade training is vital to skill
level progression, promotion eligibility, and selection. 2R0X1s must know both analysis
methodology and DBM. That reality can occur only by having a viable training program.
3.2. To have an effective training program requires planning. You need several basic
elements in your program:
   3.2.1. A Master Task Listing (MTL) - The 2R0 Career Field Education and Training Plan
   (CFETP) or electronic equivalent fulfills this requirement.
   3.2.2. A Master Job Qualification Standard - The AF Form 797, completed with duties that
   are not contained in the STS or require further breakdown, satisfies this need. The AF Form
   797 has 3 blocks, which state: ―MAJCOM Directed Use Only.‖ Complete these blocks with
   3-, 5-, and 7-level tasks. List all office tasking on this form and write/type an X in the block
   if required at that level. See AFI 36-2201, Air Force Training Program, for further guidance.
   3.2.3. Coding List – You may use color-coding, symbols, or other means to identify
   individual duty position requirements, core tasks, qualification tasks, additional duties, and
   so forth. It is advisable to also develop a legend to aid understanding when outside agencies,
   newly assigned personnel, or other supervisors in the work center are using or are initially
   exposed to the MTL. Be very careful when developing a coding system so you do not
   develop a rigid or inflexible training plan. Training plans must be flexible in order to meet
   manning, equipment, or TDY fluctuations. Care should be taken to ensure all supervisors,
   trainers, and task certifiers can define and use the MTL. Refer to your Unit Training
   Manager and applicable AFI‘s to ensure your coding meets the standards set forth.
3.3. Now, put it all together in one master program file and put it into practice. Let‘s say
you have two in-bound people. One is a 5-level who just got promoted and requires 7-level
upgrade training. This person will also need qualification training for the new office. The other
is a 7-level, fully qualified, but has never worked in your command. What do you do? Look at
each separately.
   3.3.1. Meet with the 5-level, interview this person with their training records, and determine
   their present state of training. Transcribe the records to a new STS or change what is in the
   existing records accordingly. Don‘t hesitate to certify something if they know the subject
   already (and meet your standard), and don‘t hesitate to disqualify them in a task if you think
   that they need additional training in this subject, explaining why. Insert your office 797 in
   the records. Next, identify which code which you used to mark your STS and 797 masters to
   reflect tasks required for 7-level upgrade and qualification training. Then, circle the STS and
   797 item tasks required for that person. You can conduct both upgrade and qualification
   training together since this individual requires only 7-level tasks. You would not want to
   train the person on 5-level qualification items. Appoint a trainer to the individual, conduct an
   initial interview with both, addressing your expectations and desired outcome, and hold
   periodic training meetings to discuss progress. A word about trainers: In today‘s 2R0 world,
ANGPAM21-106 10 NOVEMBER 2011                                                                   21


   there are plenty of cross-trainees. Do not hesitate to use your qualified personnel as trainers,
   regardless of rank. The trainee is here to learn.
   3.3.2. Let‘s examine the inbound 7-level. This person is already upgraded. Follow the same
   procedure you did for the other person but select the 7-level qualification tasks only. The
   main difference between the 5-and 7-level tasks might simply be that one individual can
   demonstrate, while the other individual can apply logic, interpret, and analyze.
3.4. Let the people that are fully qualified become task trainers and task certifiers for your
office. There is no golden rule on who should do what. The bottom line is get your people
trained. Build yourself a program and put it into use. Don‘t depend on schools to train your
personnel for you. You must find a way to do it!
3.5. If you find yourself in a one deep analysis section, training becomes difficult but not
impossible. There is an ANG Maintenance Management Community of Practice (COP) on the
AF Portal for your use. On the COP are many documents for your analysis program but most
importantly, a listing of Guardwide Analysis POC‘s. Use this information to make contact with
counterparts and ask them for assistance in your duties.
3.6. The Analysis career field also has a Single Point of Contact (SPOC) group that helps
with training and aids in the development of the career field. The SPOCs are listed in the
Analyst POC listing. Contact the 2R Career Field Functional Office (NGB/A4MM) at DSN 278-
8478.
 22                                                   ANGPAM21-106 10 NOVEMBER 2011


                                          Chapter 4

                            WHAT SHOULD YOU ANALYZE?

4.1. Deciding what to analyze and how can be the most challenging part of the analysis
career field. Consider the sort of areas that show unit production. For each area considered, a
measure of how well the unit produced, there is also a down side for how the unit may have been
deficient - an area where you might focus. These areas are not all encompassing, challenge
yourself by searching out any and all problem areas. Sometimes it‘s beneficial to analyze
something positive. This can keep the maintainers from thinking all you do is point out the
negatives.
   4.1.1. Flying Performance Effectiveness - How well did we meet our flying program? When
   it is not achieved, you might ask either: ―Was the program realistic?‖ or ―What caused us not
   to make the goal?‖
   4.1.2. Flying Scheduling Effectiveness - A measure of the scheduling efforts. You should
   concern yourself with the deviations that prevent you from executing an effective flying
   schedule. Do you have too many aborts? Why? There are many questions you should ask to
   explain emerging trends. You should be concerned with identifying and troubleshooting
   negative performance.
   4.1.3. Aborts - Ground and air aborts hinder your effectiveness. What causes them? Stack
   your aborts by system, by tail number, etc. Keep records, track performance of each system
   and aircraft for comparison: mission capable, TNMCM, and TNMCS - the effectiveness and
   non-effectiveness of the aircraft we possess. What drives them? Analyze high 10 reports
   from IMDS/G081, track specific components. Material deficiency reporting comes to mind.
   These indicators help measure how well our maintenance practices and supply efforts are
   working toward meeting the mission. Dig into causes.
   4.1.4. Cannibalizations - Are the procedures correct? Which components are having high
   numbers? Why aren‘t the parts available? What about seasonal variances?
   4.1.5. Aircraft Breaks / Break Rates -. What causes aircraft to break? Analyze pilot/crew
   reported discrepancies for trends and other problems.
   4.1.6. Fix times - ANG may measure 4-, 8-, 12- or 24-hour fix intervals on each broken
   aircraft in the unit depending on the type of aircraft assigned to your unit. When these time
   intervals are not met, you may want to determine why. Concentrate on the aircraft systems
   and components involved.
   4.1.7. Issue Effectiveness - Although a supply indicator, it has a meaningful impact on the
   maintenance effort. A good analysis would show how the supply system at the unit impacts
   the unit cannibalization and mission capability statistics.
   4.1.8. Repair Cycle - Off-equipment maintenance concerns would be how well the repair
   cycle process is working; examples follow. Which components require excessive pre-time to
   get into the cycle? Which require an extended repair time? Are they moving back to the line
   in a timely manner after repair or is this driving the unit to cannibalize items? This could
   also carry your analysis efforts into the overall 2LM program the unit may be involved in.
ANGPAM21-106 10 NOVEMBER 2011                                                                23


4.2. Example of a deficiency analysis effort gone wrong, made right.
   4.2.1. An analysis office was showing slides to the Wing Commander on a regular basis.
   The Commander questioned the high steady level of TNMCS time during recent months.
   Nothing on the chart pointed to a negative trend. In fact, there was somewhat of a positive
   trend because the unit had recently resolved problems experienced due to shortages in
   specific engine components. This was coupled with inadequate phase dock facilities that had
   also been driving TNMCS several months earlier. Analysts noticed no negative trend in
   recent months.
   4.2.2. The analyst's obvious concern was that future prevention of the proverbial ―getting
   caught with your pants down.‖ The analyst, with deficiency analysis, told what had
   happened, then assigned each individual some deficiency analysis type duties to prevent a
   similar recurrence. One was charged to monitor system and component trends in status and
   one monitored pilot reported discrepancies for emerging trends. They learned that positive
   trends as indicators may not result in a positive outcome. Best of all, they began to talk to
   each other. They never had that sort of problem again.
   4.2.3. Oh, you want to know how the story ends? Okay. It seemed that the only time the
   nozzles were breaking was while refueling F-4 aircraft flown by the German Air Force on
   refueling training missions. Their pilots simply were not fluent in English and barely
   understood the signals of the boom operator. Since the refueling nozzle on the F-4 is behind
   the pilot, out of his vision, he could not tell when he was disconnected. Consequently, the
   pilots would pull away when full and before the operator disconnected, so the still embedded
   nozzle would break. Had someone in analysis been tracking breaks on a daily basis, they
   would have realized the existence of a possible emerging problem and avoided
   embarrassment. Fortunately, there was no major loss of life or property.
   4.2.4. We analysts have the tools available. Many products in IMDS/G081 are dedicated to
   failure analysis, defect reporting, system reliability and capability, etc. Our job is to use
   them. What is the purpose of gathering so many bits of data if we do not analyze it?
   Analysis determines causes for negative performance, which promotes positive performance.
   Good Luck!
 24                                                     ANGPAM21-106 10 NOVEMBER 2011


                                            Chapter 5

                     MAINTENANCE CAPABILITY FORECASTING

5.1. Capability Forecasting. Capability forecasting is composed of three parts; airframe
capability, personnel capability and facility capability. All three are considered when
determining the unit‘s capability to support the operational flying requirement for a specified
month, quarter or fiscal year. The computation of airframe capability will determine if the unit‘s
available aircraft can support operational requirements. Personnel capability computations will
determine if the unit's workforce can support requirements, or if it can‘t, the shortfalls can be
identified and actions taken to enable personnel to meet requirements or identify problems which
are outside the unit's control, such as manning imbalances, skill level imbalances, etc. Facility
capability computations will determine if you have the facilities such as phase docks, fuel barns,
etc., to support the requirements. A sample spreadsheet is located at the ANG Maintenance
Management (COP) on the Air Force Portal. It is located under Document Management,
Crosstell Information, 138MXG Maintenance Capability Summary FY10.xlsx. When you
perform these forecasts, you will substitute your unit‘s information in the appropriate field.
   5.1.1. Airframe Capability Forecast, Peacetime.
       5.1.1.1. Basic assumption: the unit‘s maintenance policy of a 40-hour work week.
       Limited second shift for small servicing crews, and minimum essential maintenance
       personnel.
       5.1.1.2. Other maintenance factor: Providing aircraft as necessary for a Maintenance
       Trainer, Munitions Load Trainer, Aircraft Wash, and Scheduled Maintenance Actions
       that can be accomplished in less than two hours.
       5.1.1.3. Example, forecasting aircraft capability:
           5.1.1.3.1. Number of O&M days per year (excluding weekends and holidays): 208.
           5.1.1.3.2. Average number of possessed aircraft: 15 (example)
           5.1.1.3.3. Historical MC Rate: 76% (12 - 30 months data)
           5.1.1.3.4. Turn factor: .88 (historically) (sorties flown – code 3 sorties)
           5.1.1.3.5. Average sortie duration (ASD): 1.6 (forecast)
           5.1.1.3.6. Flying envelope: 8 hours (typical averaged)
           5.1.1.3.7. Crew preflight time: 0.5 (measured)
           5.1.1.3.8. Maintenance pre-flight time: 2.7 (measured)
           5.1.1.3.9. Other maintenance factor: (commitments): 15 aircraft (2-Shaw), (2-Alert),
           (1-Phase Insp.)
           5.1.1.3.10. Attrition rate: 8% (based on historic calculations, seasonally adjusted)
   5.1.2. Computations: Using the calculated Break Rate/Reliability Rate to determine the turn
   capability and sortie reliability (turn) rate.
ANGPAM21-106 10 NOVEMBER 2011                                                                      25


      5.1.2.1. Initial sortie capability = possessed aircraft minus other maintenance factor,
      times MC Rate, times O&M days per year. Initial sortie capability = 15 minus 3 times
      .76 times 208 = 1897.
      5.1.2.2. 1st turn aircraft available = initial sortie capability, times the turn factor (1) 1897
      times .88 = 1669.
   5.1.3. Sortie capability: initial sortie capability + 1st turn sortie capability (1) sortie
   capability: 1897 + 1669 = 3566 sorties.
   5.1.4. Projected sortie losses: sortie capability x Ground Abort Rate.
   5.1.5. Projected sortie losses: 3566 x 0.05 = 178 sortie.
   5.1.6. Maximum sortie capability:      sortie capability minus projected sortie losses
   Maximum sortie capability: 3566 - 178 = 3388 sorties.
   5.1.7. Maximum flying hour capability: maximum sortie capability times ASD maximum
   flying hour capability: 3388 X 1.6 = 5421 F.H.
   5.1.8. Maximum sortie UTE rate: maximum sortie capability divided by PAI divided by 12.
      5.1.8.1. Maximum sortie UTE rate: 3388 / 15 / 12 = 18 (rounded down)
      5.1.8.2. Maximum sortie UTE rate: 18 monthly sorties per possessed aircraft.
5.2. Airframe Capability Forecast, Wartime Capability. The following is an example of a
method of computing airframe capability for wartime:
   5.2.1. Basic Assumption: We would use all possessed aircraft to compute the maximum
   sortie capability.
      5.2.1.1. Best Case Forecast: assumes all aircraft are at home station or if deployed will
      fly at the same OPS TEMPO. Note: This forecast does not provide an aircraft for a
      Maintenance Trainer, Munitions Load Trainer, Aircraft Wash, or Scheduled Maintenance
      Actions which can be accomplished in less than two hours.
   5.2.2. Factors considered for generation of a sortie: (using example assumptions)
      5.2.2.1. Number of O&M days per year: 365 (full year)
      5.2.2.2. Possessed aircraft: 15.
      5.2.2.3. Historical MC Rate: 89.4 (wartime manning scenario)
      5.2.2.4. Turn factor: .88 (100 minus the Break Rate/Reliability Rate)
      5.2.2.5. Average sortie duration (ASD): 1.6 (proposed)
      5.2.2.6. Flying envelope: 24 hours (wartime)
      5.2.2.7. Crew pre-flight time: 0.5 (measured)
      5.2.2.8. Maintenance pre-flight 2.7 (measured)
      5.2.2.9. Attrition rate: 8% (historically)
   5.2.3. Computations: All results will be rounded to nearest whole number.
 26                                                      ANGPAM21-106 10 NOVEMBER 2011


       5.2.3.1. Initial sortie capability: possessed aircraft times MC Rate times O&M days per
       year. Initial sortie capability: 15 times .894 times 365 = 4895.
       5.2.3.2. 1st turn aircraft available: 15 x .88 = 13.2.
       5.2.3.3. 1st turn sortie capability: 13.2 x .894 times 365 = 4307.
       5.2.3.4. 2nd turn sortie capability: 13.2 x .88 = 11.6.
       5.2.3.5. 2nd turn sortie capability: 11.6 .894 times 365 = 3785.
       5.2.3.6. Sortie capability: initial sortie capability + 1st turn sortie capability + 2nd turn
       sortie capability.
          5.2.3.6.1. Sortie capability: 4895 + 4307 + 3785 = 12987.
       5.2.3.7. Projected sortie losses: sortie capability times attrition rate.
          5.2.3.7.1. Projected sortie losses: 12987 x 0.08 = 1039.
          5.2.3.7.2. Expected sortie production: sortie capability minus projected sortie losses.
          5.2.3.7.3. Expected sortie production: 12987 – 1039 = 11948.
       5.2.3.8. Maximum flying hour capability: maximum sortie capability times ASD.
          5.2.3.8.1. Flying hour computation: 11948 X 1.6 = 19116.8.
       5.2.3.9. Sortie UTE Rate: (per assigned aircraft) sortie capability divided by possessed
       aircraft divided by 12.
          5.2.3.9.1. Sortie UTE Rate: 11948 / 15 / 12.
          5.2.3.9.2. Sortie UTE Rate: 66.4 = 66 (rounded down)
5.3. Facility Capability Formulas.
   5.3.1. Dock Flying Hour/Sortie Capability:
       5.3.1.1. Number of inspections per dock =
                                      Work Days Per Month
                                           Avg Dock Days
       5.3.1.2. Number of inspections per month =number of inspections per dock x number of
       docks available.
       5.3.1.3. Dock flying hour capability = number of inspections per month x inspection
       cycle.
       5.3.1.4. Dock sortie capability =
                                    dock flying hour capability
                                       Average sortie length
   5.3.2. Facility Requirements/Dock Requirements:
       5.3.2.1. Number of inspections required =
                                      flying hours scheduled
ANGPAM21-106 10 NOVEMBER 2011                                                     27


                                    Inspection cycle
     5.3.2.2. Dock days required = number of inspections required x avg dock days per
     inspection.
     5.3.2.3. Number of docks required =
                                  dock days required
                                 Work days per month
 28                                                       ANGPAM21-106 10 NOVEMBER 2011


                                              Chapter 6

        AIRCRAFT RELIABILITY, MAINTAINABILITY AND AVAILABILITY

6.1. Although reliability, maintainability and availability are each important elements of
life cycle cost and performance, you need to address each separately and with varying
degrees of importance when dealing with mature weapon systems. Reliability is measured in
terms of how long an item functions/operates before it fails. It is this function that should be
monitored closely and reported faithfully. AFI 21-118 discusses Reliability and Maintainability.
6.2. By far the best gains in weapon system improvement after production are in the area
of component and system reliability. Reliability is also the best indicator to use in defining
requirements for fielded systems and if the systems are currently meeting mission needs.
   6.2.1. System reliability is a measure of how reliable each system is. To properly factor this
   rate, you must know how often the system is used and how often it fails. For example, if you
   are establishing the reliability of the autopilot system, you only count the sorties that the
   system was used. If the autopilot is engaged two times out of ten sorties and it fails both
   times, your reliability is zero. You would not give the system credit for being reliable when
   it wasn‘t used.
                                            # of times the system failed
                  System reliability =                                     X 100
                                            # of times the system used


6.3. Maintainability is a factor mostly influenced during the product design phase and can
not be easily changed after full scale development and production. Therefore, the degree that
maintainability can be influenced after production is limited and should be secondary to other
areas of possible improvement.
6.4. Availability is typically a result of the combined component reliability,
maintainability, number of spares procured, and the length of the logistics support pipeline
(i. e. repair concept, transportation, and handling). Changing the number of spares, repair
levels, or transportation processes can easily influence system availability. However, the cost to
gain improvements may be prohibitive by only changing this portion of RM&A equation.
   6.4.1. There are two components to availability; aircraft and personnel.
       6.4.1.1. Aircraft availability (AA). The percentage of a fleet not in a Depot possessed
       status or NMC aircraft (that are unit possessed). NOTE: The metric may be created at
       the Mission Design (MD)/ MDS level or may be grouped by fleet (e.g., Aggregate,
       Bombers, Fighters) to determine ―Aircraft Availability‖.
                                                     MC Hours*
                  Aircraft Availability =                                  X 100
                                               Total Possessed Hours**


 * MC Hours consists of Possession Purpose Codes (PPC): CA, CB, CC, CF, EH, EI, IF,
 PJ, PL, PR, TF, TJ, ZA, and ZB.
ANGPAM21-106 10 NOVEMBER 2011                                                               29


** Total Possessed Hours (TAI) consist of the following Possession Purpose Codes
(PPC): BJ, BK, BL, BN, BO, BQ, BR, BT, BU, BW, BX, CA, CB, CC, CF, EH, EI, DJ,
DK, DL, DM, DO, DR, IF, PJ, PL, PR, TF, TJ, XW, XZ, ZA, and ZB.


  6.4.2. Personnel Availability (PA). Personnel availability simply provides a measure of
  manning status. It compares the number of personnel authorized to the number of personnel
  available. A maintenance manager may find it useful to review data based on skill level. In
  which case, compare the personnel authorized to the number of personnel holding a specific
  skill level. The number authorized is based on the Unit Manning Document. The number
  available includes only those available for duty, which excludes those who are reassigned, on
  leave, Temporary Duty (TDY), etc.
     6.4.2.1. Because a workcenters availiabilty changes from day to day, in order to properly
     monitor availibility, it must be reported daily and compiled into a monthly figure.
                                           # of Personnel Available
           Personnel Availability =                                       X 100
                                       Total # of Personnel Authorized
 30                                                    ANGPAM21-106 10 NOVEMBER 2011


                                           Chapter 7

                                 ANALYTICAL METHODS

7.1. A number of analytical methods are available and although each one has validity in a
given situation. It is essential that the maintenance analyst have a complete understanding of
the methods listed below. They must also have a complete understanding of statistics and their
application as they apply to aircraft maintenance and related areas.
   7.1.1. Visual Observation: This method is valid under certain conditions; however, its value
   depends upon experience and knowledge of the observer. It is also limited by the fact that
   the information retained is not generally sufficient for a meaningful description of a lengthy
   or complex situation. When used, this method should be complemented with other methods.
   7.1.2. Comparative Analysis: This method may be performed statistically or visually and
   involves comparing two or more operations or items to identify variations or differences.
   Comparative analysis is an excellent first step in analyzing performance. While it can
   completely stand alone as a technique, it is generally employed with other methods. Care
   must be taken to ensure that like situations are compared.
   7.1.3. Statistical Analysis or Statistical Investigation: This method is the methodical study
   of data. It is used to reveal facts, relationships, and differences about data and data elements
   and is a useful adjunct to comparative and visual analysis.
   7.1.4. Analysis Process: This method is the methodical conversion (not collection) of raw
   data into a form useful for decision making and managerial control. It combines all three of
   the previous methods and expands the techniques involved to include research, investigate,
   identify, recommend, and follow-up. The combination and process expansion make it the
   strongest and most logistical of these methods. This method provides a full circle approach
   to problem solving.
   7.1.5. Record keeping: Keep records of all the data and projects in a dedicated folder as a
   backup to your work.
7.2. ANALYSIS PROCESS: The mission dictates maintaining a reliable weapon system
through the effective and efficient use of resources. As an analyst, you possess the key to assist
managers in placing things in their proper perspective. A note worthy of mention is that rates are
not everything. For example, 100% flying scheduling effectiveness could be achieved but at
what cost? A deviation may be the most efficient way at times. As an analyst, you should strive
to keep management focused. From spot checking for data integrity, to key quality indicator
measurements, from files maintenance to statistical computations, our process is technical and
time consuming.
   7.2.1. The analysis process is a 12-step approach and never ending cycle, so we symbolize it
   by using a circle (Figure 7.1). This is the process used by analysis to explore indications of
   performance.
ANGPAM21-106 10 NOVEMBER 2011                                                                31


Figure 7.1. The Analysis Process.




                                                                                       .
   7.2.2. Effective analysts know where they are going, have a pretty good idea how to get
   there, and are careful not to get sidetracked along the way. They constantly remind
   themselves that their primary responsibility is to provide meaningful information to help
   managers and supervisors make good decisions. Remember, the maintenance mission is to
   maintain a reliable weapon system through the effective and efficient use of available
   resources. As an analyst you possess the key to assist managers in placing things in the
   proper perspective. That is your goal!
   7.2.3. Are you accomplishing your goal? Don't answer too quickly! Gather the facts, all the
   facts. How do the facts compare with what you expected? Can you mold the facts to tell you
   a story? How do you think the story will end? What kinds of things impact your
   interpretation of the facts? Look at each of these things. Does something or someone else
   cause them? How, and why? If you do the above you will have just completed a thumbnail
   sketch of the analysis process. Success of this process requires you to be objective totally
   and without bias. This will allow you to fairly and accurately assess the condition of the
   organization and weapon system.
      7.2.3.1. Step 1 - Collect Data. The first step in the analysis process is to collect data.
      Data can be gathered from many different sources. The active analyst is constantly
      collecting data (information). In addition to those routine computer products, letters,
32                                                  ANGPAM21-106 10 NOVEMBER 2011


     schedules, and memos that pack filing cabinets, and computers everything you see and
     hear is a potential input to the analysis process. Data collection itself is a three-phase
     process. Briefly, collection is merely a process of extracting, sorting, and grouping
     information. The statistics you need for local reports takes you through all three phases.
     Which reports to use? Which codes do I need from the reports and what are they? In
     reality, these phases don't occur in any particular order, nor can a line be drawn where
     one phase stops and the next starts. The important thing is that all three phases do occur.
     Usually, worksheets are required during the data collection process. The thought process
     that‘s required for building an adequate worksheet allows you to think through the three
     phases of data collection prior to beginning. Periodically, ask the age old questions:
     who, what, when, why, where, and how, these questions will lead most processes in
     various directions for testing outcomes.
     7.2.3.2. Step 2 - Verify. The verification step cannot be overemphasized! This is the
     first step that makes or breaks credibility. An error is the result of a missed verification
     step, whether it was verifying the raw data or verifying (proofreading) the output product.
     The verification step occurs many times throughout the analysis process, and it requires
     you to understand the data systems from which you draw your information. Cross-
     checking from various sources (when possible) should be done to minimize variances in
     data.
     7.2.3.3. Step 3 - Organize/Compute. When you have an accurate and complete set of
     data with which to work, you must put it into a usable form. This is the
     organize/compute step. Tables, charts, and graphs are normally accepted forms of
     presentation. In addition, you may have to compute averages, rates, percents, etc.
     Further, it's a good idea to scrutinize the formulas used to ensure that the information was
     calculated properly and is presented in a form most meaningful to those who use it. As
     you accomplish this step, always remember that organizing your data to tell the story at a
     glance may save you time later.
     7.2.3.4. Step 4 - Compare. With your data in usable form, you have your first
     opportunity to compare it with standards, past performance, or expected results. This
     comparison is a relatively simple one. Is the trend stable, increasing or decreasing? Are
     there more deviations than expected? Were goals achieved? In making those
     comparisons, you will get an idea of the kind of questions that need to be answered.
     7.2.3.5. Step 5 - Review. While asking yourself simple questions about the data, you
     will probably come across some data that need extra explanation. Basically you want to
     provide additional information in any areas that raise obvious questions. By reviewing
     the data in this manner, and providing those answers, you're saving yourself the time
     spent having to re-accomplish it when the inevitable questions are asked. Your answers -
     or extra input - could come from the same source that you used in the first step of this
     process, they could come from other sources, or they could come from your personal
     knowledge as an analyst. Many analysis products will be completed at this stage of the
     analysis process. Prior to releasing these products (charts, reports, slides, etc.) another
     review of the material is required to ensure accuracy. This review may be accomplished
     by asking a coworker to review the output products.
ANGPAM21-106 10 NOVEMBER 2011                                                                 33


     7.2.3.6. Step 6 - Analyze/Interpret. It is at Step 6 of the analysis process that you
     determine which statistical tests best fit your data without distorting its true meaning.
     Now that you have gathered, verified, organized, compared, and reviewed your data, you
     must now determine if you need to proceed further in the analysis process. Analyzing is
     nothing more than a logical interpretation. The success of this thinking process is
     dependent upon your knowledge of the organization, your understanding of the statistics,
     and finally, your experience. Let's examine the last three items since the first one has
     already been discussed.
        7.2.3.6.1. Knowledge of the organization - be familiar with the makeup of your
        organization. This includes: the structure of the unit (who is responsible to whom?),
        the mission of the unit (overall objective), the responsibilities of each agency within
        the unit (how the system should work), the goal of the unit, and knowing the key
        people.
        7.2.3.6.2. Understanding the statistics - do you know what the information/study tells
        you? What are the elements that make up a particular statistic? Are they
        meaningful? What effect does a change in one element have on the statistic? What is
        the probable error in variance? What effect does a change in one statistic have on the
        values of other statistics? How accurate are your sources of data? If you understand
        your statistic, you can properly apply it. How does this result compare with other
        units with like equipment? If you have that understanding, you will reduce the risk of
        making statements or forming conclusions that are not valid. You are the statistical
        expert in the organization. The thorough and questioning analyst will deliberately
        examine and cross-examine the utility, meaning, and application of each statistic
        used.
        7.2.3.6.3. Experience - the final element required for successful interpretation
        (analysis) of your data deals with personal experience. Personal experience is
        "learning by doing." In this process it applies to logical thinking (cause and effect).
        Logical thinking is a learned skill, and by developing it (based on past mistakes), your
        analyses (interpretation) will be meaningful.
     7.2.3.7. Step 7 - Problem Solving.      The completed analysis generally provides for
     making one or two statements.
        7.2.3.7.1. ―My analysis indicates no problem." (The only way to get managers to
        accept this is by developing credibility.)
        7.2.3.7.2. ―My analysis indicates a problem does exist." If you make the second
        statement, you will have entered the problem solving step that is an integral part of
        the analysis process. Problem solving techniques will lead you through a step-by-step
        approach to defining the problem and laying the groundwork for further analysis.
        Select a method of problem solving that works best for you. One method might be:
        State the problem, gather data, list possible solutions, test the possible solutions, and
        select the best solution, act, and follow-up. Several types of problem solving methods
        may be found in your CDC material and statistical methods books.
     7.2.3.8. Step 8 - Research. Once you have determined that a problem does exist you are
     no longer doing a simple analysis. You are starting an in-depth study that will require
34                                                  ANGPAM21-106 10 NOVEMBER 2011


     additional research. This research generally involves gathering additional information
     which you will need to understand the problem fully. That may mean studying technical
     data, instructions, operating instructions, or basically trying to find out what others say
     about the problem or factors that surround it. It may mean reviewing job control logs,
     shop logs, memos, and other sources of information. Research means gathering all
     needed information, and every office on the base maintains information that could
     possibly aid in the problem solution. Ask yourself "What information do they have?" and
     "Can they help me solve the problem?" The point is that there is more to analysis than
     data printouts and quality assurance reports. After you have gathered the additional
     information, you must verify, compare, and analyze, etc., to determine the value of the
     additional data. At this point you should have a better idea as to the root cause of the
     problem.
     7.2.3.9. Step 9 - Investigate. This step will likely make you get out from behind the desk
     and visit the people in your organization. Ask questions and solicit ideas, follow up on
     statements (verify), and if you need technical help from quality assurance or other
     agencies, call on them. Evaluate all the information that you have obtained and if you've
     been thorough, you can be reasonably sure that you know why the problem exists. By
     this time the problem may have begun to disappear; however, a thorough investigation
     will determine if the problem has been truly solved, or if temporary (stopgap) measures
     have been applied. Even if the problem has been solved, complete the analysis process,
     document your findings, and inform management.
     7.2.3.10. Step 10 - Identify. Too often the analyst gets so involved in chasing all the
     other problems discovered during the investigation process that the main objective -
     identifying the cause of the problem - is forgotten. Throughout the course of your study,
     keep your objective in sight. And when you reach the point of forming your conclusion,
     identify those deficiencies that cause the problem to exist and document your findings.
     7.2.3.11. Step 11 - Recommend. Since the first step in the analysis process, you have
     been formulating possible solutions. Now you must pick the best one and recommend
     appropriate action. After all, you've been working the problem from the beginning and
     you may understand the problem, the causes, and the effects better than anyone else.. If
     you presented your study meaningfully, management should adopt your recommendation.
     But if they do not, don't worry. Your objective was to identify the cause and provide a
     logical explanation.
     7.2.3.12. Step 12 - Follow-Up. No analysis is complete without adequate follow-up
     action. Regardless of the product (analysis, study, or referral), follow-up action is
     necessary to complete the analysis process. Follow-up itself is an analysis and could start
     the entire analysis process again because it:
        7.2.3.12.1. Tells management whether or not they have chosen the proper action in
        response to the problem.
        7.2.3.12.2. Can better define the actual cost of those actions.
        7.2.3.12.3. Measures the success or failure of both the original product and resulting
        actions.
        7.2.3.12.4. Measures the effectiveness of the analysis process.
ANGPAM21-106 10 NOVEMBER 2011                                                                   35


            7.2.3.12.5. Provides results that generate effective management practices. Methods
            used in follow-up are no different than those used to produce the original product.
            This may seem extreme and taxing, but it doesn't require the degree of effort that
            went into the initial output. Continue to track all significant data/information used
            until you determine that the problem has undergone a long-term fix. Document your
            follow-up actions and inform management of the results -- both good and bad.
7.3. Major Studies or Special Studies: Studies are the ultimate analysis effort. Through study
or in-depth analysis of any subject, an analyst can wade through problematic symptoms and
eventually get to the root cause. Once the true cause has been identified, the long-term fix can be
initiated. An added benefit of a study results from overall depth of the effort. Normally, more
than one problem will be pinpointed through this effort. To gain the most in this area, turn your
analysts onto any problem or subject of importance and give them the time that they need to
investigate. You should be provided a recap of all findings, associated problems (or symptoms),
a definition of the most apparent root cause of the problem and a logical recommendation for
improvement.
7.4. These are the basic functions that can make your analysis system a useful adjunct to
decision making. Raw data is extremely limited in its usefulness to the organization. But
meaningful analysis can interpret that data and mold it into a viable management tool. "It is
better to understand a little than to misunderstand a lot." Anatole France.
7.5. How to Perform an Analysis Study. All studies involve answering six basic questions.

Table 7.1. Six Basic Questions.
      (1)    Who is having a problem?
      (2)    What has happened, what can be done about it?
      (3)    When did this happen?
      (4)    Why is this happening?
      (5)    Where does this happen?
      (6)    How to correct it?
   7.5.1. How well you answer these questions will decide how good a study it will be. If you
   don‘t answer all six questions, you have not really accomplished a study. You may have
   collected masses of data, but only when it is systematically used to answer these six
   questions have you actually analyzed the situation and completed the study. Basically, here
   is what each of the six questions should involve:
       7.5.1.1. Who are you doing the study for? This section is part of the Purpose: Examples
       might be: To investigate the perceived increase in failures of the F-15 Inertial Measuring
       Unit at the unit level and determine if a problem exists.
       7.5.1.2. What has happened? This section is titled, Assumptions and Limitations. This
       is when you state what you think are the assumptions, such as, the number of
       cannibalizations for system 46XXX (fuels) has a significant trend for #1 fuel boost pump
       over the last 24 months or the number of cannibalization for system 46XXX (fuels) has
       no significant trends. This is when you identify what you believe is the problem area to
       investigate. You must decide how many months of data to use, what type of data, should
       it be looked at across the whole wing or a single flying squadron, should it be limited to
 36                                                    ANGPAM21-106 10 NOVEMBER 2011


       the #1 boost pump or look at the whole fuel system? These are all limitations and are
       necessary to keep your study focused and manageable and to eliminate unnecessary
       information.
       7.5.1.3. When, why, and where is this happening? This is the Investigation and Findings
       area. Investigation: this is where we start to peel back the apple and get to the core of
       the problem. This is where you solicit information from supervisors and technicians in
       the appropriate areas to provide any facts that might shed light on the study, such as,
       technical orders which are not accurate, special tools needed, skill level imbalances, etc.
       Findings: once you have assembled all your facts from the data and investigations, you
       need to state the hard facts about the information, tell what you did and what you found.
       Keep it concise and if you have reached a conclusion don‘t state it here. Build your case
       by a logical progression of supporting facts. After you have built your case on the facts
       then you can state your conclusions in the next section of the study.
       7.5.1.4. What were the causes of the problem? This is the Conclusion. This section is
       exactly that; your conclusions based on your findings. There may be more than one
       conclusion from the study. In some cases the conclusion may be that there is no problem.
       In every case, however, you must be able to arrive at some conclusion from your efforts.
       7.5.1.5. How to correct it or what should be done about it? These are your
       Recommendations. They should be in line with your conclusions. If you conclude that
       there is no problem, it does not make sense to recommend a corrective action. On the
       other hand, if you have effectively determined why something is happening, then you
       should be able to recommend some action to correct it.
   7.5.2. The bottom line for an effective study is that simply crunching numbers will never
   answer all six questions. Data, statistics and investigation techniques will help to support
   your findings. An effective study must use both the technical expertise of Maintenance Data
   Systems Analysts and Deficiency Analysts to ensure a combined effort of data analysis and
   maintenance investigation. Working as a team and answering all six questions will make an
   effective study easy, productive and rewarding.
7.6. Maintenance Analysis Referrals: These are highly effective tools for getting many
agencies aware of a common problem. Referrals are simply tools to aid in process improvement
and should never be used to attach blame when a process is not working right. A referral is a
procedure used to identify, investigate, and propose corrective action for management problems.
Referral reports are used to start the referral procedure and document the corrective actions for
implementation and future reference. Due to the amount of investigation and research needed to
properly process referrals, take care to ensure they are not used for problems that can be resolved
more efficiently through verbal or less formal communications.
   7.6.1. Referrals are accomplished by completing an AF Form 2422, Maintenance Analysis
   Referral. Referrals are not determined by a quota system. They are used only when
   necessary to affect a permanent solution to a problem that cannot be solved by other means.
   Referral reports must be concise, accurate, and timely to provide maintenance managers with
   information for making decisions. Anyone can initiate a referral but MDSA is the OPR and
   maintains a log of all referrals, assigning a referral number before processing begins. The
   log should reflect the referral number, initiating agency, date, subject, and action taken.
   Route through the affected agencies for comments, with the final addressee as the
ANGPAM21-106 10 NOVEMBER 2011                                                                  37


   maintenance data systems analysis section. Retain copies and indicate whether additional
   monitoring or follow-up action is necessary. Provide a completed study to each group QA.
       7.6.1.1. Referrals should not be limited to JDD/MDC accuracy. Remember, if the real
       cause of the problem is not identified, a lasting fix will not occur.
7.7. Report Formats: Output products are the results of investigations, analyses, audits,
recurring reports, and studies. Most people are familiar with the recurring reports published by
analysis. But what about the other output products? What about the narrative assessment about
that study on anti-skid problems or the IMDS audit? What type of format should you expect?
The following is an example of one format type:
   7.7.1. Objective.
   7.7.2. Background.
   7.7.3. Scope.
   7.7.4. Discussion/Assumptions.
   7.7.5. Findings.
   7.7.6. Conclusion.
   7.7.7. Recommendations.
7.8. Objective: What you're trying to accomplish.
   7.8.1. To determine if the suspected problem is isolated to certain aircraft or fleet-wide, or
   normal seasonal variation.
   7.8.2. To measure the relationship between conditions (temperature) and specific problems.
   7.8.3. To uncover and interpret any trends in system reliability, maintenance practices,
   training and technical proficiency, or supply support.
7.9. Examples.
   7.9.1. Background - (Who initiated the study and when) Example: The Operations Group
   Commander directed this study on 12 December 2008. There is concern over the increase in
   anti-skid discrepancies by both aircrews and maintenance managers. Maintenance analysis
   and quality assurance were tasked to study and determine the extent of the problem.
   7.9.2. Scope - (How far did the study go) Example: This study was done in two phases.
   The initial phase was statistical analysis of discrepancies over the past five years. The
   second phase was a complete tail number analysis.
   7.9.3. Discussion/Assumptions - How are things normally done? Examples:
       7.9.3.1. Maintenance is required to check this item during preflight and thru-flight.
       7.9.3.2. Debriefers are specifically asking about this system (checklist item)
       7.9.3.3. Statistical analysis of data from FY03 – FY07 used for this study included the
       following:
38                                                  ANGPAM21-106 10 NOVEMBER 2011


         7.9.3.3.1. Regression analysis of system reliability with deseasonalized trend data to
         determine trend direction and rate of change. Kendall K test to determine probability
         of significance of the trend.
         7.9.3.3.2. Correlation analysis of temperature versus number of discrepancies
         observed (FY94 thru FY07)
  7.9.4. Findings - List each finding separately and explain. Examples:
     7.9.4.1. Only modest correlation between temperature and anti-skid problems.
     7.9.4.2. Most of the increase due to normal seasonal variation. Trend was not significant
     but was slightly higher than normal.
     7.9.4.3. Discrepancies are fleet-wide. No significant number for any single aircraft.
     7.9.4.4. QA reports show increasing numbers of discrepancies in this area.
  7.9.5. Conclusion - Summarize facts to answer objective of study. Example: The increase
  in anti-skid problems is mostly seasonal. However, the trend is increasing slightly indicating
  other forces are contributing. There is an increasing number of discrepancies in this area for
  QA inspections indicating a possible training or supervisory deficiency. A more thorough
  assessment of QA inspection history is required.
  7.9.6. Recommendations - What would you do if you were the LG? Examples:
     7.9.6.1. LG emphasizes potential problem to pilots and maintenance managers.
     7.9.6.2. QA increases emphasis on inspections in this area.
     7.9.6.3. Analysis/QA assesses inspection history for possible trends.
  7.9.7. Follow-up – see if improvements occur. A follow-up on any changes should occur
  following corrective measures. The changes should be measured against the pre-corrected
  information.
ANGPAM21-106 10 NOVEMBER 2011                                                                    39


                                            Chapter 8

                               PRESENTATION TECHNIQUES

8.1. Choose the Chart to Fit the Data: Use line charts to represent information flow that
changes over time, such as, mission capable rate. Use bar charts to compare individual data
points, like fighter squadron‘s comparisons, and pie charts to show proportions, such as, aborts
by system. Use organizational charts and flowcharts to diagram relationships and processes,
respectively.
8.2. Format for Legibility: Distinguish bars by using solid contrasting colors or simple hatch
patterns ( black and white printouts. Don‘t mix diagonal shadings--they create dizzying optical
effects. Limit pie charts to no more than 10 parts. Use minimum grid lines and reference points
to make the graph readable. They should float unobtrusively behind solid areas. If charts are
crowded, simplify them by suppressing alternate reference points. Use heavier solid lines to
delineate the most important series.
8.3. Use Tables to Organize Detailed Data: Charts are visual displays of relatively small
amounts of data, but tables are a more effective way to show lots of detailed information in a
small space. The use of both together is especially useful when audience members may need to
access and compare many individual data points.
8.4. Use Text Lists for Simple Information: Keep to the rule of six: Follow each bullet or
number with no more than six words and use no more than six lines per slide. Don‘t
underestimate the usefulness of builds, which can help you reveal progressive levels of detail or
sequential points.
8.5. Use Appropriate Line Weights: Dark lines on light backgrounds tend to look like they are
receding, so make them thicker; conversely, light lines tend to come forward on dark
backgrounds, so use thinner line weights in such circumstances. This rule applies to typefaces as
well: Use thicker, bolder text on light backgrounds and vice versa. Don‘t let chart lines get too
thick, though, or your reference points may be misinterpreted.
8.6. Use Color to Convey Information: Emphasize important points by setting them in
contrasting color, and key the audience into relationships by showing similar objects in similar
colors. To avoid confusion, don‘t use more than six or seven colors in all, and don‘t use more
than four colors for text slides. Use brighter colors for small items and illustrative graphics. Set
standard design rules for multiple graphic presentations. Be sure to reserve the most vivid colors
for emphasis and use them sparingly.
8.7. Fit the Background to the Occasion: For slides shown in a dark room, use light objects
on a dark background. Dark cool colors (like the blue family) are best, but you can also use very
hard warm colors (oxblood or dark green, for example). For overheads shown in a lighted room,
use dark objects on a light background.
8.8. Combine Colors Carefully: Foreground and background color combinations need high
contrast for visibility, so don‘t combine red and green, which are indistinguishable to those
suffering from the most common form of color blindness. Most audiences seem to prefer blue,
green, purple, orange, and yellow, in that order. Avoid vibrant colors, such as magenta, which is
 40                                                    ANGPAM21-106 10 NOVEMBER 2011


too hard on the eyes. Also, avoid exposing your audience to large areas of bright color, which
will tire the eyes; bright red is the worst offender, light blue the most restful.
8.9. Format Text for Readability: On a dark computer screen or slide background, bright
yellow-green text stands out the best; white and bright yellow are also quite visible and easy on
the eyes. Use neutral, dark shadows to increase text readability on color gradient backgrounds.
Use one or two (at the most) different typefaces for heads and other text elements. Don‘t use
similar serif or fonts in the same presentation.
8.10. Compensate for Screen Resolution: Text displayed on a computer screen is less legible
than printed type, so you should compensate by using simple letter shapes without delicate serifs
for on-screen presentations. Examples of highly legible fonts include Helvetica, Futura, and
some of the simpler bolder fonts designed specifically for laser printers and computer screens. In
addition, chart text should be no smaller than 1/20 the height of the chart, versus 1/50 for slides.
8.11. Use Movement to Advantage: You can use the dynamics of a screen presentation to
convey the direction of information flow. For example, you‘ll show more detail by zooming in
on an item; and you can effect a smooth concept transition with a dissolve or fade. To move to a
new concept, try fading to black between slides. Use animation to highlight key points, to
suggest change and movement, to show changes in physical properties, or to illustrate how a
device works. Last, you can use video clips to show objects in real time, if you‘re presenting
data about a new facility, for example, or illustrating a new maintenance process.
8.12. The Real Secret: Keep it simple and use special effects sparingly. Get to the point
quickly. The only way to get comfortable with presentations of this type is practice. You have
to spend time trying different colors, with different fonts, on both your computer screen and your
projection equipment. Time invested in advance will pay huge dividends on presentation day.
ANGPAM21-106 10 NOVEMBER 2011                                                                  41


                                           Chapter 9

                                     TREND ANALYSIS

9.1. What is trend analysis? The determination of the direction a set of portrayed data is
taking. It's what we‘re trained for. Our technical schools and CDCs give us guidance on how to
do this. For the Logistics Group managers who rarely have time to look past the next day's
flying schedule, this can be an eye opener. They need to see and understand current trend data
on any problem areas. That's why we have a Logistics Group dedicated analyst for their specific
needs. Sometimes all that is needed are WAG statistics/analysis. It's simply a matter of
retrieving and providing information. A Wild Approximate Guess (WAG) is an opinion and not
a fact.
9.2. Time Series Analysis. Trend analysis is more accurately called time series analysis. Our
CDC defines a time series as a systematic arrangement of observed occurrences of data by some
constant of time. For example, the number of landing gear failures for each month. The CDC
goes on to define trend as the variation that's almost always displayed by the values in a time
series. There are generally four types of trend variation. These are secular, seasonal, cyclical,
and irregular variation. The only ones I'll cover in this book are secular and seasonal variation.
Secular variation is caused by the general forces of nature and is the most common type of
variation. Seasonal variation is variation that occurs over periods of time shorter than a year.
All variation occurring over short periods of time may not be seasonal but most will be. To do
time series analysis you must have at least 24 values in the series. You need 5-7 years to do a
good seasonal analysis. When we do time series analysis there are two questions we must
answer. What is the direction of the trend? Is it significant enough for concern? Seasonal trends
can be shown against other seasons to show significance.
9.3. Testing Significance of Trend. We use Kendall's K test to test the significance of trends.
It will tell us the trend direction (increasing or decreasing) but not how much (slope of the
regression line). You don't have to compute and plot a regression line unless you want to display
the data and variation graphically. You can analyze the trend with just Kendall's K test. Two
conditions must be met before you can use this test. First, you should use at least 24 values.
Second, the data must be from a scale that can be ranked.
 42                                                    ANGPAM21-106 10 NOVEMBER 2011


                                           Chapter 10

                                     CONTROL CHARTS

10.1. This chapter covers four types of control charts. There are others, but these are the
only ones we've found useful in analysis. Control charts are very often misused by analysts
simply because the analyst doesn't understand which type to use. What most analysts know
about control charts is that you compute the standard deviation and mean average for your data.
Then you draw a chart with the mean as the centerline and set control limits at three standard
deviations above and below the mean. Well, this is the correct way to construct a chart for
individuals but not for the other types. We've seen analysts use this type of control chart to plot
monthly abort and mission capable rates. This is not an appropriate use of control charts. All
types of control charts are constructed the same way. They have a centerline, an upper control
limit, and a lower control limit. The primary difference is the measure of variability used and
computation of the control limits. This chapter will explain the uses and give some practical
applications for each type of control chart discussed.
10.2. The purpose of a control chart is to monitor variation and to detect the presence of
assignable causes for variation. Assignable causes are those you can identify and which
management can minimize or eliminate. For example, they could be errors in reporting, an aging
component, or a bad batch of parts. The control chart won't always detect assignable cause.
There may be too much normal variation and the control limits ineffective. But control charts
are good analysis tools and are our best method of statistical quality control.
10.3. Control Limits. Most analysts remember from school or CDCs that you set limits at three
standard deviations from the mean. Do you know why? When we select a control limit we're
basing it on an assumed risk or probability that a certain amount of the data being measured will
fall within those limits. For example, we know that in a normal distribution, 99.74 percent of the
data will fall within three standard deviations above and below the mean. If some data fall
outside these limits we know that there's only a 0.26 percent chance that the reason is due to
chance causes and a 99.74 percent chance that it's due to assignable causes. There is a theorem
in statistics called the Empirical Rule or Normal Rule which states that approximately 68 percent
of the data falls within one standard deviation of the mean, approximately 95 percent within two,
and approximately 99.7 percent within three standard deviations from the mean. But this is only
true for symmetrical or normal (bell-shaped) distributions. You can use the normal curve area
table to work backwards too and set your control limits based on a probability set by you. For
example, let's say you want to set your control limits so that if the data fall outside the control
limits you‘re 97 percent sure it is due to assignable causes. Look at the normal curve area table
until you find 0.4850 (half of .97 or 97 percent). Remember that the normal curve area table
shows data on only one side of the mean. See that 0.4850 corresponds to 2.17 in the z columns.
So set your limits at 2.17 standard deviations from the mean. If you can't find the exact number
you‘re looking for pick the closest one.
10.4. Interpretation. How to interpret control charts is not an exact science. There are
probably as many ways as there are applications. You and your experience at your particular
application will be the best judge of that. Just remember you're trying to identify anomalies for
investigation. Some ways to tell are:
ANGPAM21-106 10 NOVEMBER 2011                                                                       43


   10.4.1. One or more points exceed control limits.
   10.4.2. Too many points on one side of the centerline. (Exp. seven consecutive points, 10
   of 11 points, 16 of 20 points, etc.)
   10.4.3. Appearance of adverse trend. The method that you choose will probably be
   determined based on your experience and the amount of time available for investigations.
10.5. Types of Control Charts. We'll cover four types of control charts. These fall into two
categories, charts for variables and charts for attributes. Charts for variables are used when
quality is expressed as a variable. For example, man-hours to change a component, mean time
between failures, mean sorties between tire failures, etc. Charts for variables require that you
first quantify the data. The two charts for variables we'll cover are charts for individuals and
charts for averages. Charts for attributes are used when items can be placed into categories
based on some observed characteristic. For example, pass or fail, the component either worked
or failed, accept or reject, etc. The two charts for attributes we'll cover are P charts (also called p
bar charts) used for percent defective and C charts (also called c bar charts) for number of
defects. The Charts for individuals is used for plotting individual values. It uses the mean
average as its centerline and standard deviation as its measure of variability. It‘s useful for but
not limited to measuring repetitive type maintenance jobs or inspections where measurements are
made in terms of man-hours or clock hours to complete a task. You should use it if you want to
monitor every single action or if you don't have enough data to use a chart for averages. A
disadvantage to this chart is that the values need to be fairly symmetrical (from a normal
distribution) because the control limits are based on an assumption that the sampling distribution
values came from a normal distribution. So, if the distribution you're trying to measure is
skewed, your interpretation of the chart will be distorted. Application of this type of chart is
simple. If you want to monitor how long a work center takes to perform a certain task (i.e.
bench check an item in the shop), gather your past data (sample distribution) and compute the
mean and standard deviation. Make a chart and then plot each job in the order they occurred. If
jobs fall either above or below the control limits, I'd want to know exactly who did those jobs. If
there's a trend it could indicate that someone is very good at this task or very bad. It could also
mean someone might be taking short cuts. Or it could be a newly assigned person in training.
But if the new individual is the problem and he's been here for a while, then your original sample
is skewed and your control chart may show nothing unusual. So back up a few months and take
another sample. Then, measure the jobs from that point forward. Think, be creative. Whatever
the reason, identifying the trend and its cause is what you get paid for. Let's not go into detail
about constructing this chart because the CDCs provide excellent guidance in the construction.
10.6. Standard Deviation. Since you'll probably never deal with whole populations in analysis
we'll cover sample standard deviation. Standard deviation, as you should know, is one of the
measures of dispersion. Others include range, average deviation, and variance. The standard
deviation is also the square root of the variance. But you knew that too. Most calculators and
electronic spreadsheets on computers have function routines built in to calculate standard
deviation. If these are not available it's still a relatively simple task. There are two methods of
computing sample standard deviation. Since we‘re not sure which one you'll see in school or
CDCs, we'll cover both hoping you'll recognize one or the other. Both require setting up a table
and making a few calculations. Then substituting values into the formulas and solve. Below are
the formulas and tables with sample data. The steps are self-explanatory. Just take a few
minutes to look them over. Method I is also called the shortcut method.
 44                                                     ANGPAM21-106 10 NOVEMBER 2011


Table 10.1. Standard Deviation.
METHOD I:                        METHOD II:
       X          X²                X          X-Y       (X-Y)²
       4          16                 4              0          0
       2           4                 2             -2          4
       5          25                 5             1           1
       4          16                 4             0           0
       5          25                 5             1           1
       2            4                2             -2          4
       6          36                 6             2           4
      28         126                28             0          14


                                              X                    28
                               Y=                         =             =4
                                         # of samples              7
                                          s=1.53         s=1.53

10.7. Charts for Averages. These are used for measuring the same types of data as the charts
for individuals. But, instead of plotting every value, you plot sample means. Don't confuse this
chart with the bar x or mean chart. Charts for averages are useful when you're monitoring
random samples of large population distributions or monthly rates. It uses the same centerline
(mean average) but uses the standard error of the mean as its measure of variability instead of
standard deviation. So at least you won't have to compute standard deviation, right? Well, guess
again. Standard error of the mean uses standard deviation in its formula. In fact, it's just one
extra set. Compute the sample standard deviation for your past data. Since we‘re dealing with
samples, let's assume that your past data includes 5 samples with 10 values in each sample.
Compute your mean average and standard deviation from the entire 50 values, then use the
following formula to compute standard error of the mean (S)

Table 10.2. Charts for Averages.
      SS                           SS
       n                             n 1



(Sample > 30)                 (Sample < 30)

10.8. P Charts. You can associate P charts with percent defective. That is, the percent (or
proportion) of your sample that is defective. A defect is an error or discrepancy in an item,
which detracts from quality. Defects are not measured but counted. They either do exist or do
not exist. Let's say we‘re doing IMDS/GO-81audits or reviewing QAP reports. These are both
types of inspections and we‘re looking at pass and failure rates (percent defective). If you're
ANGPAM21-106 10 NOVEMBER 2011                                                                     45


monitoring QA inspections, you'll want to convert pass rates to failure rates. If the pass rate for
FS personal evaluations is 85.5 percent, then the failure rate is 14.5 percent. The centerline of
the P chart is not affected by changes in sample size but the control limits are. In fact the control
limits are different for every plotted value on a P chart. This doesn't mean that you have to
recalculate control limits every time you plot a value. You should only do this when a plotted
value comes close to one of the control limits. Generally, the P chart works just like the other
control charts we discussed. You calculate and plot a centerline and upper and lower control
limits based on historical data. As long as the plotted values are closer to the centerline (or
norm) than the control limits (extremes) then no worries. When a value falls outside the control
limits or comes close to the control limits, then you need to recalculate the limits to see, if
indeed, the value is out of control. Some values may fall outside of the straight-line control limit
but when you recalculate it might actually be within limits. The opposite might also be true of a
value that falls very close to the control limit but appears to be still in limits. That's why you
must check any that are close. The formulas for the P chart are:

Table 10.3. P Charts.
               CLPTotalNumberof Defectives(allsamples) X 100
                                  Total NumberInspected

                                                   P(100  P)
                                     UCLP3
                                                    n

                                                  P(100  P)
                                      LCLP3
                                                   n

Where : PAverage percent defective or centerline value
 nSample Size
An increase in sample size causes the control limits to move closer to the centerline. A decrease
in sample size causes the limits to move farther away from the centerline. Keeping this
relationship in mind can save you the trouble of recalculating control limits every time a value
falls close to the control limits. For example, let's say the upper control limit is 20.5 and the
value you're plotting is 20.3 percent. You know that the normal sample size is 65-80 but this
time it's 120. Now remember that an increase in sample size causes the control limits to move
closer to the centerline. You don't even need to recalculate. The value is out of control. If the
sample size had been 45 then the control limit would move in the other direction and the value
would not be out of control.
10.9. The C chart is different from a P chart in that it plots the number of defects rather
than percent defective. It is used to measure the discrepancies found during an inspection. For
example, the number of write-ups on a QA inspection. We keep referring to QA examples
because these are, after all, methods of statistical quality control. The sample size for your C
chart must always be the same and the opportunity for defects must not change. The centerline is
the average number of defects per unit inspected. The C chart formulas are:
 46                                                   ANGPAM21-106 10 NOVEMBER 2011


Table 10.4. The C Chart.
                 Total number of defects
         CL =
             Total number of inspections
       UCL = CL + 3 CL
       LCL = CL - 3 CL
Control limits on the C chart serve the same purpose as on the P chart. A point outside the
control limits indicates the presence of assignable cause for variation.
 ANGPAM21-106 10 NOVEMBER 2011                                                                 47



                                             Chapter 11

                                  DETERMINING SAMPLE SIZE

 11.1. We don't normally use sampling techniques in our day-to-day analyses. If we want to
 know the average time it takes to complete a task we don't sample 150 of 4,000 jobs. We
 average all 4,000 jobs. There is, however, one application of estimating sample sizes that all
 analysts should use. That's estimating sample sizes for inspections.
 11.2. Proportion Estimates. Sound like more QA stuff? It is. Although this has application in
 the QA division, we do quality control inspections too. We audit IMDS/GO81 to ensure that
 man-hours are being documented correctly, to ensure that work unit codes are being used
 correctly, etc. When possible we use all of the data. But there will be times when you'll have to
 settle for sampling. Let's assume that we‘re going to compute sample sizes for QA to determine
 how many inspections they should conduct each month. The formula for computing sample size
 is:

 Table 11.1. Proportion Estimates.

                  Selected margin of error                5
  p=                                               =             =2.5
           z score from the normal curve area
                                                          2
         table for the selected confidence level
          P(1-P)          .955(1-.955)        .042975
 n =          2
                    =              2     =                = 68.76 rounded to 69
           (p)               (.025)           .000625
 Where: P = The historical pass rate
    z score = 1.96 or (2) based on 95% confidence level

 The way we compute this sample size is in 5 steps.
       Step 1: Determine the historical pass rate. (P)
       Step 2: Select a margin of error. (Ex. + or - 5)
       Step 3: Select a confidence level. (Ex 95% or 99%)
       Step 4: Find Z score in normal curve area table for selected confidence level.
       Step 5: Compute sample size.


Let's say the historical pass rate for personal evaluations is 95.5 percent. We want a margin of
error of no more than 5 percent and a 95 percent confidence level. That takes care of steps 1-3.
In step 4, we don't have to look this one up because we already know that the area associated
with 95 percent under the normal curve is approximately two (1.96) standard errors of
proportion. So we compute the sample size to be 69 inspections. Now let's say QA completes
69 inspections and the pass rate is 92 percent. This means that we can be 95 percent (confidence
level) sure that if QA inspected all of the jobs, instead of just 69, the pass rate would be 92
     48                                                  ANGPAM21-106 10 NOVEMBER 2011


percent, + or - 5 percent. So we have a good representative sample of the overall health of the
unit. What if QA only did 20 inspections? Then the confidence level would have been around
70 percent. That means that even though we achieved a 92 percent pass rate for 20 inspections,
the confidence level is too low to believe it's a good representative sample of how well the unit is
performing maintenance. Now keep in mind that if you computed this for five different work
centers, you'd end up with 69 inspections for each. Of course that's unrealistic. The 69
inspections are for the entire wing. QA will have to decide how to distribute them across all
three squadrons. But remember, these procedures apply to random samples only.

 .
ANGPAM21-106 10 NOVEMBER 2011                                                             49



                                           Chapter 12

                             IMDS DATABASE MANAGEMENT

12.1. Data Base Management (DBM): The Maintenance Data Systems Analysis DBM
provides management control of the Integrated Maintenance Data System (IMDS). G0-81 does
not utilize a DBM. If the Ground Comm Section at a G0-81 Unit utilizes IMDS, request a letter
from the host DBM stating they will maintain the database for the section.
12.2. Duties and Responsibilities: Management of IMDS is performed through (NCC)
coordination with users, Defense Enterprise Computing Center (DECC), and the Network
Control Center. They ensure the DECC supports all requirements concerning the operations and
maintenance of the IMDS. They coordinate periodic saves of IMDS files to prevent loss of data
caused by computer failure. They notify users of any IMDS problems. They also ensure that
system security is maintained. They provide technical support to tenant users. More detailed
duties and responsibilities are contained in AFI 21-101_ ANG Sup, AFCSM 21-556 Vol 2 and
AFCSM 21-571 Vol 2.
12.3. IMDS Subsystem Managers: Each IMDS subsystem is controlled by a specific
subsystem manager who ensures that their personnel are qualified to use the IMDS and are
current with AFCSM 21-556 / 579. A list of subsystems and who is responsible for them are
contained in AFI 21-101_ANG Sup. NOTE: The following checklist is an example of how one
unit performs their DBM functions. It may be adapted, adopted or used for bench marking
purposes.

Table 12.1. Checklist for Unit Performing DBM Functions.
-       Background Product Requests (Other than DBM runstreams)
Host and DBM
       -- Build New Runstreams and Maintain Recurring Runstreams - The DBM section is
           the POC for this step. However, Analysts in the Analysis section can accomplish
           this step as required
        --   Schedule and Process (Unscheduled and Scheduled requests)
        --   Print / Download and Distribute Products
        --   Quarterly Runstream Review - This runstream review is a quality initiative to
             ensure we’re meeting our customers’ needs. Each customer reviews their
             requirements and updates them accordingly. This step is used in addition to
             customer initiated changes.
-       On-Line Requirements
Host    --      Daily Duties
             --- Check UVRXP in TIP - We verify the applicable dates.
             --- Ensure Beginning Of Day (BOD) date is current
             --- Check ICI switch (should be set to up)
    50                                                ANGPAM21-106 10 NOVEMBER 2011


Host        --- RRP (RRO) and subsequent ADDRESS files - This step is important to maintain
                an ongoing list of CEMS sequence numbers. The CEMS sequence numbers are
                for units that don’t use direct line reporting.
            --- Units using IMDS for reporting of engine status to CDB should:
               ---- Ensure NFS27E listings are sent to Engine Management Branch
               ---- Correct any NFS27R errors ASAP
               ---- Ensure Address Run Summaries (PCN SS024-506) are sent to the Engine
                    Management Branch
Host        --- RDT - RDT is a carry-over from pre-DDN file transfer. We monitor it and make
                sure the dates stay current because it’s still on the system.
Host        --- Monitor IMDS/SBSS - Every morning and regularly throughout the day, we test
                this interface and make sure it’s working. For this step, you must have a good
                working relationship with the SBSS computer room personnel.
Host        --- Process RLPA6ALL - After reviewing the RLPA5 output and printing or
                reviewing the needed print files, we “clean” up this output. Files you should
                review include: 4FP, 4PP, 3IP, 4QP, 3NP, and files other users have processed.
                We don’t print the listings. Instead, we use IPF to call up the listing file and
                check them for errors
Host        --- Review 4PP Listings - These files are especially important to make sure PQDRs
                are going out when they’re processed.
Host        --- Check IMDS E-Mail (IPM (X.400))** It is highly encouraged that you call the
                FAB and have your X.400 E-Mail auto forwarded.
Host           ---- Review 4FP listings - These listings show the MDC code tables downloaded
                    from REMIS to IMDS
Host           ---- Notify affected units of changes - We notify our tenant units when their
                    WUC tables are updated so they can run new CCL listings.
Host        --- GCSAS - This only applies to units who are using GCSAS for configuration
                management. We’ve dedicated one DBM as the GCSAS “guru.”
Host           ---- Process in approved tables and actual tables as needed
Host     -- Monthly
            --- RDT - Same as the daily requirements, but this is processed manually
            --- NDA500 - This is one of a three-step program we use to verify database structure
                integrity. It is processed at the beginning of each month.
            --- WAH - Monthly Man-hour Summary - No explanation needed!
-        Off-Line Requirements
ANGPAM21-106 10 NOVEMBER 2011                                                                      51


Host          --- IMDS Down Day Runs - We integrated most of our “major” down time causing
                  runs so they run one day each month. We schedule the runs on a Saturday each
                  month. The actual dates are projected a year in advance (FY). The dates are
                  coordinated so they don’t affect flying days and tenant unit UTAs.
Host as Req     -- Quarterly DMU CALC Verify - This is the third program in our three-step
                   process to verify database structure integrity. We process it at the beginning
                   of each quarter.
-        DBM Section Support
For Analysis Office Only
       -- Order RLP and Side-by paper
         -- LP Store items (Printer ribbons)
         -- Maintain Supply Log - We maintain a log so we can review our purchases. This
            allows us to provide a more accurate forecast of requirements.
         -- Provide Host/Tenant Support IAW Agreement with ANG or AFRES units
-         IMDS Down Time
          -- Scheduled Down Time Notification – Yearly and as required, we project our
             scheduled down time and notify our customers. As Required
-         Quarterly IMDS User’s Group Meeting
As Req    -- Schedule - Along with the survey, we provide an open forum for comments.
Req       -- Compile Meeting Minutes
Req       -- Distribute – Annually (at a minimum), distribute copies of our survey to every
             customer. This is a quality initiative to track customer satisfaction levels. Also, it’s
             a forum (aside from the phone) to provide anonymous feedback.
-         System Advisory Notices (SANS)
Host      -- Obtain and Print - Currently, we use two avenues to get SANS. The first is the FAB
             Diagnostics system at Gunter.
Host      -- Distribute and Comply with - Each SAN is reviewed for whom it affects and any
             steps needed to comply with it. SANS are distributed by E-Mail or BITS. We don’t
             distribute to our ANG/AFRES units. They have the capability to retrieve SANS for
             themselves.
Host and DBM -- Maintain SAN File - SANs are maintained in either ACTIVE or INACTIVE
                status. We use the FAB Diagnostics list to determine the status rather than
                use the time frame printed on the SAN.
Host      -- Maintain HUM and PAN files and distribute as required. (Access through the
             IMDS Home page.)
-         Difficulty Reports (DIREPS)
Host      -- Research and Compose
    52                                                ANGPAM21-106 10 NOVEMBER 2011


Host     -- Submit, Distribute, and File
Host     -- Maintain DIREP File (Review and Discard) - DIREPs are maintained as either
            VALID or INVALID. After completed and INVALID DIREPs reach their time limit
            (6 months), we discard them.
-        Software Releases
Host     -- Obtain and Review - Copies of the release are sent to every tenant unit.
Host     -- Coordinate - After coordinating with the RPC, we notify our customers of the date
            and time for the release load. Print doc files and comply with any pre-steps
            necessary.
Host     -- Process and Review - When the release is loaded, we spot-check the program
            version dates and accomplish any steps needed before the system is brought back
            up.
Host     -- File
-        ADPE Equipment Custodian (only if Work Group Manager duty is assigned)
DBM      -- Inventory
DBM      -- Equipment Movements
DBM      -- Compose, Submit, and File AF Form 3215
DBM      -- Update NAPZ00 and Local Data Base
-        Hardware Problems
Host Duty In Depth -- Troubleshoot - We’re the front line for troubleshooting IMDS
                      hardware problems. Most bases have WGMs handle hardware and
                      then turnover to Comm if necessary
Host     -- Equipment Repair and Replace
-        Communication Line Problems
Host     -- Coordinate line troubleshooting through the BNCC to the DECC.
Host     -- We maintain a log of all communication line problems we call into the BNCC. This
            allows us to track jobs through the process and make sure they get fixed.
-        Terminal Area Security
Host     -- Physical Terminal Security - This includes the Terminal Area Security Officer
            (TASO) program.
Host     -- USER-ID/Password ** Maintain a copy of all DD Form 2875
Host        ---    Issue - Not only do we issue UIDs from our office, we attend “Right Start”
                   briefings for the Maintenance Squadron and distribute them there.
Host        ---    Call the BNCC to have UIDs reset as required.
            --     TRIC Security
ANGPAM21-106 10 NOVEMBER 2011                                                             53


Host and DBM       --- Loads - After coordination with our subsystem managers, we developed
                       a base line TRIC security list and applied it to IMDS users. We don’t
                       restrict any inquiry screens or options.
-   Special Report Requests
Host and DBM -- Document - All special requests for data is documented on an in-house form
                and filed.
         -- QLPs - QLPs are written, compiled, and processed by all trained personnel within
            the office. It isn’t just a DBM function.
         -- IQUs - Same as QLPs.


ADDITIONAL HOST DUTIES
Major Release Conversions:
Accomplish Pre-conversion steps for schema changes
UDS Monitor:
Utilize the UDS Monitor to check for problems
 54                                                   ANGPAM21-106 10 NOVEMBER 2011



                                          Chapter 13

                                THE DEPLOYED ANALYST

13.1. MOBILITY COMMMITMENT: Any Analyst desiring a change of pace should try
deployments. You will experience everything from bare base operations to adjusting to a foreign
country's customs and culture. Most units have a mobility commitment so it needs to be
incorporated into our analysis readiness.
13.2. READINESS: Three areas are involved: personnel, equipment and deployed location
awareness. Analysts must be trained, qualified, and ready to go. ADP equipment and supporting
ADP supplies and software must be dedicated to accompanying the deployed analyst. It is
important to gain as much knowledge about the deployed location as possible.
   13.2.1. You will have to be prepared to handle a wide range of issues. For example what
   kind of electrical power is used at the deployed location, 110, 120, or 220? What style of
   electrical outlets and receptacles are there? Is this an Army or Navy owned location, if so
   GDSS2 will not be available and the firewall may be setup to block 3270 traffic (G0-
   81/IMDS)? Locally brought printer/copier paper may be a non-standard size paper and will
   not work in your printers and copiers.
   13.2.2. Determine which units you are going to be involved with. For example; if it is a
   PACAF unit they do not use G081 but IMDS. It would behoove you to try and determine
   what and who you will be dealing with; (Are there multiple types of aircraft (C130, C17,
   F16), and to whom will you be providing reports to; (there is a real possibility it will be
   several different reporting agencies and commands)?
   13.2.3. A checklist is essential to ensuring deployment readiness, otherwise essential items
   such as regulations and tech orders are apt to get left behind.
13.3. ROLE OF THE ANALYST: Besides your primary analysis duties, you can expect to
assist in a variety of jobs while deployed. You may be detailed, temporarily, to provide support
services. The point is that you will be in a new environment and will have to adjust and adapt to
some changes. The flip side of that is that you, the analyst, still have duties to perform or else
you should not have been part of the deployment.
   13.3.1. You will probably encounter drastic opposition to documentation and trying to do
   business as usual. However, lessons learned at deployed locations directly affect future real
   world scenarios. Problems that cannot be worked on location must be forwarded to parent
   wings and higher headquarters as soon as possible.
   13.3.2. One tendency that has been noted during deployments is that personnel do the
   minimum "just to get by and get it over with." This attitude may not affect one time
   deployments, but when it applies to rotational commitments its effect is felt long range. The
   proper attitude would be to perform to your highest ability so the system will work better for
   analysts replacing you. It is demoralizing to return to a deployed site months later and find
   the work environment the same or worse than you left it. Improvements/quality should be an
   ongoing process.
ANGPAM21-106 10 NOVEMBER 2011                                                              55


  13.3.3. Your deployment isn't complete until your trip report is accomplished. Keeping a
  daily logbook will help you reconstruct the events and identify the problems encountered,
  workarounds applied, and solutions achieved and needed improvements. Don't neglect to
  accentuate the positive as well as the negative. The difference analysts make in the support
  they provide will no doubt directly impact our career field.
  13.3.4. Another important aspect of the deployment is, when you return home. It is very
  important you inform your commanders of the issues. It helps if you categories them into
  two groups.
  13.3.5. The first one should be ones your commander and you can resolve.            Have a
  suggested resolution and a long term plan for keeping these issues updated.
  13.3.6. The second is the issues that will require resolution from outside your commander's
  realm of influence, control and responsibility. These should be brought to the attention of
  your SPOC and from there moved up the chain. This will allow the other Analysis the
  opportunity to review and provide input. Your issues may be unique but a large percentage
  of them will have been events or series of events other deployed Analysis went though. And
  there will be several different approaches and methods each Analysis used to resolve them.




                                          HARRY M. WYATT III, Lieutenant General,
                                          USAF
                                          Director, Air National Guard.
 56                                                  ANGPAM21-106 10 NOVEMBER 2011


                                        Attachment 1
          GLOSSARY OF REFERENCE AND SUPPORTING INFORMATION
Professional Library and File System-A professional library is essential to the success of any
office. Don't rely on someone to tell you everything you need to know. Knowledge is power.
You must be as self-sufficient as possible in our business. You'll need access to many reference
sources but not all have to be located in analysis. Electronic copies are recommended, this will
reduce time and cost of printing. Reference materials are a must for thorough analyses. Tailor
them to meet your needs and keep them current. References will range from wing instructions
all the way to Air Force and DOD materials. Many are available on the ANG Maintenance
Management COP. https://www.my.af.mil/afknprod/community/views/home.aspx?Filter=AN-
LG-02-37.      Some suggested items for your professional library are: Career Development
Courses (CDC) 2R051 and 2R071. Statistical Methods Books (local purchase) AF Metrics
Handbook.
AMC Metrics Handbook (HQ AMC Analysis Website)
ACCI 21-118, Improving Aerospace Equipment Reliability and Maintainability.
ACCI 21-165, Aircraft Flying and Maintenance Scheduling Effectiveness.
AFCSM 21-556 /579, IMDS Computer Operation Manual.
AFI 10-701, Operations Security.
AFI 21-101, Aircraft and Equipment Maintenance Management.
AFI 21-101ACCSUP, Aircraft and Equipment Maintenance Management.
AFI 21-101AMCSUP, Aircraft and Equipment Maintenance Management.
AFI 21-101ANGSUP, Aircraft and Equipment Maintenance Management.
AFI 21-103, Equipment Inventory, Status, and Utilization Reporting.
AFI 21-129, Two Level Maintenance and Regional Repair of AF Weapon Systems and
Equipment.
AFI 33-101, Communications and Information: Commanders Guidance and Responsibilities.
AFH 33-337, Tongue and Quill.
AFMAN 33-363, Management of Records.
AFI 36-2201, Air Force Training Program.
AFMCI 21-165, Aircraft Flying and Maintenance Scheduling Procedures.
AFPD 21-1, Air and Space Maintenance.
T.O. 00-20-1, Aerospace Equipment Maintenance Inspection, Documentation, Policy and
Procedures.
T.O. 00-20-2, Maintenance Data Documentation.

Abbreviations and Acronyms
2LM—Two Level Maintenance.
AA, A/A—Air abort, Aircraft Availability.
ACC—Air Combat Command.
ACMS—Advanced Configuration Management System.
ACM—Aircraft Configuration Management.
ACTS—Automated Component Tracking System.
ANGPAM21-106 10 NOVEMBER 2011                     57


ACT UTE—Actual Utilization.
AD—Addition (deviation)
ADPE—Automated Data Processing Equipment.
ADS—Automated Data Systems.
AETC—Air Education and Training Command.
AFCAT—Air Force Catalogue.
AFCSM—Air Force Computer Systems Manual.
AFMC—Air Force Materiel Command.
AFOSH—Air Force Occupational Safety and Health.
AFRES—Air Force Reserves.
AFSSI—Air Force Systems Security Instruction.
AGE—Aerospace Ground Equipment.
AGM—Air Surface Attack Guided Missile.
AGS—Aircraft Generation Squadron.
AID—Accident, Incident and Deficiencies.
AIM—Air Intercept Missiles.
ALCS—Airborne Launch Control System.
AMC—Air Mobility Command.
AMSA—Automated Maintenance Systems Analysis.
ANG—Air National Guard.
ANGRC—Air National Guard Readiness Center.
APG—Airplane General.
APH—Aircraft Possessed Hours.
ASD—Average Sortie Duration.
ASIP—Aircraft Structural Integrity program.
ASM—Automated Scheduling Module.
AT—Action Taken.
AT—Air Traffic Control (deviation)
ATE—Automatic Test Equipment.
ATS—Automated Training Subsystem.
AUR—Accomplishment Utilization Report.
AVDO—Aerospace Vehicle Distribution Office.
 58                                               ANGPAM21-106 10 NOVEMBER 2011


AVG—Average.
AVS—Aerospace Vehicle Status Report.
AVTR—Airborne Videotape Recorder.
AWM—Awaiting Maintenance.
AWP—Awaiting Parts.
BAI—Backup Aircraft Inventory.
BITE—Built-in Test Equipment.
BITS—Build-in-Tests.
BNCC—Base Network Control Center.
BOD—Beginning of Day.
BPO—Basic Post flight.
BSSU—Bench Stock Support Unit.
CAF—Combat Air Forces.
CAMS—Core Automated Maintenance System.
CANN—Cannibalization.
CANN RATE—Cannibalization Rate.
CANX—Cancel.
CAPCODE—Capability/reliability Code.
CATLAB—Category of Labor.
CAVR—Combat Airborne Video Recorder.
CDB—Central Data Base.
CDTS—Computer Directed Training System.
C-E—Communications-Electronic Equipment.
CEMS—Comprehensive Engine Management System.
CFETP—Career Field Education and Training Plan.
CETS—Contractor Engineering and Technical Services.
CFM—Computer Facility Manager.
CFRS—Computerized Fault Reporting System.
CHD—Comm-Electronics Delete History.
CHG—Chargeable (deviation)
CITS—Central Integrated Test System.
CMIS—Close-loop Maintenance Information System.
ANGPAM21-106 10 NOVEMBER 2011                             59


CND—Cannot Duplicate.
COB—Co-located Operating Base.
CORE—Combat Orientated Repair Evaluation.
CORI—Combat Orientated Repair Initiative.
COT—Current Operating Time.
CR—Component Repair.
CRF—Component Repair Flight.
CR&R—Calibration, Repair, and Return.
CRS—Component Repair Squadron.
CSAS—Configuration Status Accounting System.
CSRD—Computer Systems Requirements Document.
CUM—Cumulative.
CUM ACT—Cumulative Actual.
CUM PROG—Cumulative Programmed.
CX—Cancellation (deviation)
DAR—Data Automation Requirement/Record.
DBM—Data Base Manager.
DD—Delayed or Deferred Discrepancy.
DDS—Deferred Discrepancy Summary.
DEV—Deviation.
DIFM—Due-in From Maintenance.
DIG—Data Integrity Group.
DIREP—Difficulty Report.
DIT—Data Integrity Team.
DLH—Delete History.
DLR—Depot Level Repairable.
DMC—Defense Mega Centers.
DMU—Data Management Utility.
DOD—Department of Defense.
DOGM—Deputy Operations Group Commander for Maintenance.
DOI—Date of Installation.
DOM—Date of Manufacture.
 60                                              ANGPAM21-106 10 NOVEMBER 2011


DOR—Due-out Release.
DPU—Demand Processing Unit.
DR—Deficiency Report.
DUO—Due-out.
DUM—Detailed Utilization Maintenance.
EBO—Expected Back Orders.
ECMP—Engine Conditioning Monitoring Program.
ECMS—Engine Configuration Management System.
EDD—Estimated Delivery Date.
EHR—Event History Recorder.
EIMSURS—Equipment Inventory, Multiple Status, Utilization Reporting System.
EL—Early Landing (deviation)
EMF—Equipment Maintenance Flight.
EMS—Equipment Maintenance Squadron.
ENMCB—Engine Not Mission Capable-Both.
ENMCM—Engine Not Mission Capable-Maintenance.
ENMCS—Engine Not Mission Capable-Supply.
EOR—End of Runway.
EPT—Empty Pylon Test.
ESI—Equipment Status Inquiry.
ET—Early Take Off (deviation)
ETIC—Estimated Time in Commission.
EVL—Event List.
FAB—Field Assistance Branch.
FADM—Functional Area Documentation Manager.
FAM—Functional Account Manager.
FCF—Functional Check Flight.
FH—UTE - Flying Hour Utilization.
FMC—Fully Mission Capable.
FOB—Forward Operating Base.
FOD—Foreign Object Damage.
FOL—Forward Operation Location.
ANGPAM21-106 10 NOVEMBER 2011                                  61


FOM—Facilitate Other Maintenance.
FSA—File Server Administrator.
FSC—Federal Stock Classification.
FSE—Flying Scheduling Effectiveness.
FSS—Forward Supply Station.
FTP—File Transfer Protocol.
GA, G/A—Ground Abort.
GCSAS—Generic Configuration Status and Accounting Subsystem.
G0-81—CAMS for Mobility.
HHQ—Higher Headquarters.
HM—How Malfunction.
HOF—Health of Forces/Fleet.
HPO—Hourly Post flight.
HQ—Headquarters.
HSC—Home Station Check.
HTML—Hypertext Markup Language.
HUM—Heads-Up Message.
ICI—Interactive Communication Interface.
IFC—In Flight Check.
IFE—In Flight Emergency.
ILM—Intermediate Level Maintenance.
IM—Item Manager.
IMIS—Integrated Maintenance Information System.
INS—Inertial Navigation System.
INW—In-Work.
IPF—Interactive Processing Facility.
IPL—Immediately prior to Launch.
IPM—Information Processing Management.
IPI—In-Process Inspection.
IQU—Interactive Query Utility.
ISS EFF—Issue Effectiveness.
ISO—Isochronal Inspection.
 62                                               ANGPAM21-106 10 NOVEMBER 2011


JCN—Job Control Number.
JDD—Job Data Documentation.
JML (JSML)—Job Standard Master Listing.
JQS—Job Qualification Standard.
QPM-L—Logistics - Quality Performance Measures.
LANTIRN—Low Altitude Night Terrain Infrared Navigation.
LCAP—Logistics Compliance Assessment Program.
LCL—Lower Control Limits.
LG—Logistics Group.
LGND—Logistics Non-Delivery (deviation)
LIMFACS—Limiting Factors.
LL—Late Landing (deviation)
LRU—Line Replaceable Unit.
LSF—Logistics Support Flight.
LSS—Logistics Support Squadron.
LT—Late Takeoff (deviation)
MAIRS—Military Air Integrated Reporting System.
MAIS—Maintenance Analysis and Information System.
MAJCOM—Major Command.
MC—Mission Capable.
MCC—Mission Capability Code.
MCSP—Mission Completion Success Probability.
MD—Mission Design IE F-16.
MDC—Maintenance Data Collection.
MDD—Maintenance Data Documentation.
MDR—Materiel Deficiency Report.
MDS—Mission Design Series IE F-16D.
MESL—Mission Essential Subsystems List.
MICAP—Mission Capability.
MLIR—Monthly Logistics Indicator Report.
MIS—Maintenance Information Systems.
MMH/FH—Maintenance Man-hours per Flying Hour.
ANGPAM21-106 10 NOVEMBER 2011                            63


MND—Maintenance Non Delivery (deviation)
MOCC—Maintenance Operations Center Control.
MPE—Management Process Evaluation.
MPL—Maintenance Personnel Listing.
MPR—Maintenance Personnel Roster (listing)
MSB—Main Support Base.
MSE—Maintenance Scheduling Effectiveness.
MT—Maintenance (cx / deviation)
MTBCF—Mean Time Between Critical Failures.
MTBF—Mean Time Between Failure.
MTBM—Mean Time Between Maintenance.
MTBMA—Mean Time Between Maintenance Actions.
MTE—Multiple Tracked Equipment.
MTTR—Mean Time To Repair.
NAF—Numbered Air Force.
NDI—Non-Destructive Inspection.
NDT—Non-Destructive Testing.
NHA—Next Higher Assembly.
NIE—Normally Installed Equipment.
NI&RT—Numerical Index and Requirement Table.
NMC—Not Mission Capable.
NMCA—Not Mission Capable Airworthy.
NMCB—Not Mission Capable Both.
NMCBA—Not Mission Capable Both Airworthy.
NMCBSA—Not Mission Capable Both Scheduled Airworthy.
NMCBUA—Not Mission Capable Both Unscheduled Airworthy.
NMCB-S—Not Mission Capable Both Scheduled.
NMCB-U—Not Mission Capable Both Unscheduled.
NMCM-S—Not Mission Capable Maintenance Scheduled.
NMCM-U—Not Mission Capable Maintenance Unscheduled.
NMCMA—Not Mission Capable Maintenance Airworthy.
NMCS—Non-Mission Capable Supply.
 64                                             ANGPAM21-106 10 NOVEMBER 2011


NMCSA—Not Mission Capable Supply Airworthy.
NMCMSA—Not Mission Capable Maintenance Scheduled Airworthy.
NMCMUA—Not Mission Capable Maintenance Unscheduled Airworthy.
NRTS—Not repairable this station.
NSN—National Stock Number.
O&M—Operations and Maintenance.
OBTS—On-Board Test System.
OCF—Operational Check Flight.
OG—Operations Group.
OI—Operating Instruction.
OP—Operations (cx / deviation)
OND—Operations Non-delivery.
OPLAN—Operations Plan.
OPS TEMPO—Rate of Military Actions or Missions.
OS—Operations Squadron.
OSS—Operations Support Squadron.
OT—Other (cx/deviation)
OT&E—Operational Test and Evaluation.
OTU—Operating Time Update.
OWC—Owning Workcenter.
PAA—Primary Authorized Aircraft.
PAI—Primary Aircraft Inventory.
PAN—Partial Advisory Notice.
PDAI—Primary Development/Test Aircraft Inventory.
PDM—Programmed Depot Maintenance.
PE—Periodic Inspection.
PIM—Product Improvement Manager.
PIP—Product Improvement Program.
PIWG—Product Improvement Working Group.
PMAI—Primary Mission Aircraft Inventory.
PMC—Partial Mission Capable.
PMCB—Partially Mission Capable Both.
ANGPAM21-106 10 NOVEMBER 2011                                       65


PMCM—Partially Mission Capable Maintenance.
PMCS—Partially Mission Capable Supply.
PMEL—Precision Measurement Equipment Laboratory.
PMI—Preventative Maintenance Inspection.
PMO—Program Management Office.
POAI—Primary Other Aircraft Inventory.
POL—Petroleum, Oil, and Lubricants.
POS—Peacetime Operating Stock.
PPS—Product Performance Subsystem.
PQDR—Product Quality Deficiency Reporting.
PRAM—Productivity, Reliability, Availability and Maintainability.
PRD—Pilot reported discrepancy.
PROG UTE—Programmed Utilization.
PTAI—Primary Training Aircraft Inventory.
PWC—Performing Work-Center.
QA—Quality Assurance.
QAE—Quality Assurance Evaluator.
QAP—Quality Assurance Program.
QAR—Quality Assurance Representative.
QAT—Quality Assessment Tracking.
QEC—Quick Engine Change.
QLP—Query Language Processor.
QPA—Quantity Per Application.
QS—Quality Services.
QT—Qualification Training.
QVI—Quality Verification Inspection.
RACC—Reparable Asset Control Center.
RCM—Repair Cycle Monitor.
RCP—Repair Cycle Processing.
RCS—Reports Control Symbol.
RCSU—Repair Cycle Support Unit.
RCT—Repair Cycle Time.
 66                                               ANGPAM21-106 10 NOVEMBER 2011


RDD—Required Delivery date.
REJ—Reject List.
REMIS—Reliability and Maintainability Information System.
RIW—Reliability Improvement Warranty.
RLP—Remote Line Printer.
R&M—Reliability and Maintainability.
R&R—Remove and Replace.
RM&A—Reliability, Maintainability, and Availability.
RPC—Regional Processing Center.
RTOK—Retest OK.
SAAM—Special Assignment Airlift Mission.
SANS—System Advisory Notices.
SBLC—Standard Base Level Computer.
SBSS—Standard Base Supply System.
SCR—Special Certification Roster.
SE—Support Equipment.
SGA—Selective Generation Aircraft.
SHM—Significant Historical Maintenance.
SIOP—Single Integrated Operational Plan.
S/N—Serial Number.
SND—Supply Non-Delivery (deviation)
SORT UTE—Sortie Utilization.
SP—Spare (deviation)
SPD—System Program Director.
SPO—System Program Office.
SPOC—Single Point Of Contact.
SRAN—Stock Record Account Number.
SRD—Standard Reporting Designator.
SRU—Shop Replaceable Unit.
SSM—System Support Manager.
SU—Supply (cx/deviation)
SY—Sympathy (cx/deviation)
ANGPAM21-106 10 NOVEMBER 2011                        67


TAI—Total Aircraft Inventory.
TCI—Time Change Item.
TCS—TCTO Status Report.
TCTO—Time Compliance Technical Order.
TDI—Time Distribution Inspection.
TEC—Type Event Code.
TF—Total Flyable.
TIP—Transaction Interface Processor.
TIN—Turn In.
TISL—Target Identification Set Laser.
TMDE—Test Measurement and Diagnostic Equipment.
TMSM—Type Model Series Modification.
TNMCA—Total Not Mission Capable Airworthy.
TNMCM—Total Not Mission Capable Maintenance.
TNMCS—Total Not Mission Capable Supply.
TOAI—Total Overall Aircraft Inventory.
TOC—Technical Order Compliance.
TOT—Total Operating Time.
TPMCM—Total Partially Mission Capable Maintenance.
TPMCS—Total Partially Mission Capable Supply.
TRAP—Tanks, Racks, Adapters, and Pylons.
TRIC—Transaction Identification Code.
TRN—Turnaround Transaction.
TS—Tail number swap or interchange (deviation)
TSS—TCTO Status Summary.
UCL—Upper Control Limit.
UCR—Unsatisfactory Condition Reports.
UDS—Universal Data System.
UEM—Unit Engine Manager.
UERS—Unscheduled Engine Removals.
UJC—Urgency Justification Code.
UMD—Unit Manpower Document.
 68                                                    ANGPAM21-106 10 NOVEMBER 2011


UND—Urgency of Need Designator.
UPMR—Unit Personnel Management Roster.
UTE—Utilization.
VIRP—Variable Information Retrieval Program (G081 version of QLP)
WAG—Wild Approximate Guess.
WCE—Workcenter Event.
WCS—Weapons Control System.
WD—When Discovered.
WDM—Workdays per Month.
WES—Work Event Separator.
WOG—Work Order Generator.
WRM—War Reserve Materiel.
WRE—War Reserve/Readiness Engines.
WRM—War Reserve Materiel.
WSMIS—Weapon System Management Information System.
WUC—Work Unit Code.
WX—Weather (cx/deviation)
XB—Expendable, Base.
XD—Expendable, Depot.
XF—Expendable, Field.

Terms
Abscissa— The horizontal or X axis of the coordinate system. On a frequency distribution, the
abscissa typically measures the variable in question (performance measure), whereas the Y axis
(ordinate) represents the frequency of occurrence.
Alpha Error (or type 1 error)— The probability of being wrong whenever the null hypothesis
is rejected, or the probability of rejecting the null hypothesis when it should have been accepted.
By definition, then, the alpha error can only occur when H0 has been rejected.
Alternate Hypothesis— The opposite of the null hypothesis. The alternate hypothesis states
that chance has been ruled out—that there are population differences (when testing the
hypothesis of difference), or that a correlation does exist in the population (when testing the
hypothesis of association)
Analysis of Variance— Statistical test of significance developed by Sir Ronald Fisher. It is also
called the F ratio, or ANOVA, for ANalysis Of VAriance. The test is designed to establish
whether or not a significant (non-chance) difference exists among several sample means.
Statistically, it is the ratio of the variance occurring between the sample means to the variance
ANGPAM21-106 10 NOVEMBER 2011                                                                   69


occurring within the sample groups. A large F ratio, that is when the variance between is larger
than the variance within, usually indicates a non-chance or significant difference.
Beta Coefficient (b) or Slope— In a scatter plot, the slope of the regression line indicates how
much change in the Y variable accompanies a one-unit change in the X variable. When the slope
is positive (lower left to upper right), Y will show an increase as X increases, whereas a negative
slope (upper left to lower right)
     rSy
b=
      Sx
indicates a decrease in Y is accompanying an increase in X. In the regression equation, Y = bX +
a, the slope is symbolized by the b term.
Beta Error (or type 2 error) - The probability of being wrong whenever the null hypothesis is
accepted, or the probability of accepting the null hypothesis when it should have been rejected.
Bias - Sampling error, which is not random. Occurs when the difference between X and µ is
consistently in one direction. Bias results when samples are not representative of the population.
Central Limits Theorem - The theoretical statement that when sample means are selected
randomly from a single population, the means will distribute normally, even if the population
distribution deviates from normality. The theorem assumes that sample sizes are relatively large
(at least 30) and that they are all selected from one population.
Central Tendency (measures of) - A statistical term used for describing the typical, middle, or
central scores in a distribution of scores. Measures of central tendency are used when the
researcher wants to describe a group as a whole with a view toward characterizing that group on
the basis of its most common measurement. The researcher wishes to know what score best
represents a group of differing scores. The three measures of central tendency are the mean (or
arithmetic average), the median (or the midpoint of the distribution), and the mode (the most
frequently occurring score in the distribution)
Chi Square (x²) - A statistical test of significance used to determine whether or not frequency
differences have occurred on the basis of chance. Chi square requires that the data be in nominal
form, or the actual number of cases (frequency of occurrence) that fall into two or more discrete
categories. It is considered to be a non-parametric test (no population assumptions are required
for its use). The basic equation is as follows:
        ( f 0  f e )2
2          fe
                       .
when ƒ0 denotes the frequencies actually observed and ƒe the frequencies expected on the basis
of chance.
Coefficient of Contingency - A test of correlation on nominal data sorted into any number of
independent cells.
         2
  C
|      N  2 .
Coefficient of Determination (r2) - A method for determining what proportion of the
information about Y is contained in X; found by squaring the Pearson r.
Confidence Interval - The range of predicted values within which one can expect with a certain
degree of certainty that the true parameter value will fall. Usually, confidence intervals are
determined on the basis of a probability value of .95 (95% certainty) or .99 (99% certainty)
 70                                                    ANGPAM21-106 10 NOVEMBER 2011


Control Group - In experimental research, the control group is the comparison group, or the
group that receives zero magnitude of the independent variable. The use of a control group is
critical in evaluating the pure effects of the independent variable on the measured responses of
the subjects.
Correlated F Ratio - Statistical test of the hypothesis of difference among several sample means,
where sample selection is correlated. The correlated F requires interval data.
Correlated Samples - In experimental research, two or more samples that are not selected
independently. The selection of one sample determines how the other sample(s) will be selected,
as in a matched-group design.
Correlated t Ratio - Statistical test of the hypothesis of difference between two sample means,
where the sample selection is correlated. The correlated t requires interval data.
                 X        X
t 
                     1         2
                                           .
          2      2
      s       s     2 r1 , 2 s X s X
          x1     x2               1    2


Correlation Coefficient - A quantitative formulation of the relationship existing among two or
more variables. A correlation is said to be positive when high scores on one variable associate
with high scores on another variable, and low scores on the first variable associate with low
scores on the second. A correlation is said to be negative when high scores on the first variable
associate with low scores on the second, and vice versa. Correlation coefficients range in value
from +1.00 to -1.00. Correlation coefficients falling near the zero point indicate no consistent
relationship among the measured variables. In social research, the correlation coefficient is
usually based on taking several response measures of one group of subjects.
Cramer's V - A test of correlation on nominal data when the number of independent cells is
greater than four.
             x2
V                        .
          N ( k  1)
Cross-Sectional Research - Type of non-experimental research, sometimes used to obtain data
on possible growth trends in a population. The researcher selects a sample (cross section) at one
age level, say 20-year-olds, and compares these measurements with those taken on a sample of
older subjects, say 65-year-olds. Comparisons of this type are often misleading (today's 20-year-
olds may have very different environmental backgrounds, educational experience, for example,
than the 65-year-old subjects have)
Deciles - Divisions of a distribution representing tenths, the first decile representing the 10th
percentile, and so on. The 5th decile, therefore, equals the 50th percentile, the 2nd quartile, and
the median.
Degrees of Freedom (df) - With interval (or ratio) data, degrees of freedom refer to the number
of scores free to vary after certain restrictions have been placed on the data. With six scores and
a restriction that the sum of these scores must equal a specified value, then five of these scores
are free to take on any value whereas the sixth score is fixed (not free to vary). In inferential
statistics, the larger the size of the sample, the larger the number of degrees of freedom. With
nominal data, degrees of freedom depend, not on the size of the sample, but on the number of
categories in which the observations are allocated. Degrees of freedom are here based on the
number of frequency values free to vary after the sum of the frequencies from all of the cells has
been fixed.
Dependent Variable - In any antecedent-consequent relationship, the consequent variable is
called the dependent variable. The dependent variable is a measure of the output side of the
ANGPAM21-106 10 NOVEMBER 2011                                                                     71


input-output relationship. In experimental research, the dependent variable is the effect half of
the cause-and-effect relationship, whereas in correlational research the dependent variable is the
measure being predicted. In the social sciences, the dependent variable is typically a response
measure.
Descriptive Statistics - Techniques for describing and summarizing data in abbreviated,
symbolic form; shorthand symbols for describing large amounts of data.
Deviation Score (x) - The difference between a single score and the mean of the distribution. It
is found by subtracting the mean, X , from the score X . The deviation score ( X  X ) is
symbolized as x . Thus, x  X  X .
Distribution - The arrangement of measured scores in order of magnitude. Listing scores in
distribution form allows the researcher to notice general trends more readily than with an
unordered set of raw scores. A frequency distribution is a listing of each score achieved,
together with the number of individuals receiving that score. When graphing frequency
distributions, one usually indicates the scores on the horizontal axis (abscissa) and the frequency
of occurrence on the vertical axis (ordinate)
Double-Blind Study - A method used by researchers to eliminate experimental error. In a
double-blind study neither the individual conducting the study nor the subjects are aware of
which group is the experimental group and which the control. This prevents any unconscious
bias on the part of the experimenter, or any contaminating motivational sets on the part of the
subjects.
Exclusion Area - The extreme areas under the normal curve. Because of the curve's symmetry,
two z scores that are equidistant from the mean exclude the extreme areas at both the top and
bottom of the curve.
Experimental Design - Techniques used in experimental research for creating equivalent groups
of subjects. There are three basic experimental designs: .
(1) after-only—when subjects are randomly assigned to control and experimental groups and the
dependent variable is measured only after the introduction of the independent variable.
(2) before-after—where a group of subjects is used as its own control, and the dependent
variable is measured both before and after the introduction of the independent variable .
(3) matched-group—when subjects are matched or equated, person for person, on some relevant
variable.
Experimental Research - Research conducted using the experimental method, where an
independent variable is manipulated (stimulus) in order to bring about a change in the dependent
variable (response). Using this method the researcher is allowed to make cause-and-effect
inferences. Experimental research requires careful controls in order to establish the pure effects
of the independent variable. Equivalent groups of subjects are formed, then exposed to different
stimulus conditions, and then measured to see if differences can be observed.
Factorial ANOVA - As opposed to a one-way ANOVA, the factorial ANOVA allows for the
analysis of data when more than one independent variable is involved. Results can be analyzed
on the basis of the effects of each independent variable or on the basis of the possible interaction
among the independent variables. Data to be analyzed should be in at least interval form.
Frequency Polygon - A graphic display of data where single points are plotted above the
measures of performance. The height where the point is placed indicates the frequency of
occurrence. The points are connected by a series of straight lines.
 72                                                                 ANGPAM21-106 10 NOVEMBER 2011


Friedman ANOVA by Ranks ( x r ) - A test of the hypothesis of difference on ordinal data when
                                                  2


the sample groups have either been matched or a single sample has been measured repeatedly.
The Friedman ANOVA is the ordinal counterpart of the correlated F.
           12
xr2                (  R12   R22   R32 )  3 N ( k  1) .
        Nk ( k  1)
Gambler's Fallacy - An erroneous assumption that independent events are somehow related. If
a coin is flipped ten times and comes up heads each of those times, the gambler's fallacy predicts
that it is virtually certain for the coin to come up tails on the next flip. Since each coin flip is
independent of the preceding one, the probability remains the same (.50) for each and every coin
flip, regardless of what has happened in the past. The gambler remembers the past, but the coin
does not.
Halo Effect - A research error arising from the fact that people who are viewed positively on one
trait are often also thought to have many other positive traits. Advertisers depend on this effect
when they use famous personalities to endorse various products—anyone who can throw
touchdown passes must be an expert in evaluating razor blades. Researchers must guard against
the halo effect, as it will contaminate the independent variable.
Hawthorne Effect - A major research error due to response differences resulting not from the
action of the independent variable, but from the flattery or attention paid to the subjects by the
experimenter. Typically, the potential for this error is inherent in any study using the before-
after experimental design without an adequate control group. Any research, for example, when
subjects are measured, then subjected to some form of training, then measured again, should be
viewed with suspicion unless an appropriate control group is used—that is, an equivalent group
that is measured, then not subjected to the training, and then measured again. Only then can the
researcher be reasonably confident that the response differences are due to the pure effects of the
independent variable.
Histogram (bar graph) - A graphic representation of data in which a series of rectangles (bars)
are drawn above the measure of performance. The height of each bar indicates the frequency of
occurrence.
Homogeneity of Variance - An assumption of both the t and F ratios, which demands that the
variability within each of the sample groups being compared should be fairly similar.
Homoscedasticity - The fact that the standard deviations of the Y scores along the regression
line should be fairly equal. Otherwise the standard error of estimate is not a valid index of
accuracy.
Inclusion Area - The middle most area of the normal curve, included between two z scores
equidistant from the mean. Because of the symmetry of normal curve, the middle most area
includes, in equal proportions, the area immediately to the left of the mean and the area
immediately to the right of the mean.
Independent Variable - In any antecedent-consequent relationship, the antecedent variable is
called the independent variable. Independent variables may be manipulated or assigned. A
manipulated independent variable occurs when the researcher deliberately alters the
environmental conditions to which subjects are being subjected. An assigned independent
variable occurs when the researcher categorizes subjects on the basis of some preexisting trait.
Whether the independent variable is manipulated or assigned, determines whether the research is
experimental (manipulated independent variable). In experimental research, the independent
variable is the causal half of the cause-and-effect relationship. In correlational research, the
independent variable is the measure from which the prediction will be made.
ANGPAM21-106 10 NOVEMBER 2011                                                                  73


Inductive Fallacy - An error in logic resulting from over generalizing on the basis of two few
observations. The inductive fallacy occurs when one assumes that all members of a class have a
certain characteristic because one member of that class has it. It would be fallacious to assume
that all Mongolians are liars on the basis of having met one Mongolian who was a liar.
Inferential (predictive) Statistics - Techniques for using the measurements taken on samples to
predict the characteristics of the population—the use of descriptive statistics for inferring
parameters.
Interdecile Range - The scores that include the middle most 80% of a distribution, or the
difference between the first and ninth deciles.
Interquartile Range - Those scores that include the middle most 50% of a distribution, or the
difference between the 1st and 3rd quartiles.
Interval Data - Data (measurements) in which values are assigned such that both the order of
the numbers and the intervals between numbers are known. Thus, interval data not only provide
information regarding greater-than or less-than status, but also information as to how much
greater or less than.
Kruskal-Wallis H Test - A test of the hypothesis of difference on ordinal data among at least
three independently selected random samples. The H test is the ordinal counterpart of the one-
way ANOVA.
                  R1  R2  R3
                    2      2      2
         12
H             (                  )  3( N  1) .
     N ( N  1) n1      n1     n1
Kurtosis (ku) - The state or degree of the curvature of a unimodal frequency distribution.
Kurtosis refers to the peakedness or flatness of the curve.
Leptokurtic Distribution - A unimodal frequency distribution in which the curve is relatively
peaked—most of the scores occur in the middle of the distribution—with very few scores
occurring in the tails. A leptokurtic distribution yields a relatively small standard deviation.
Longitudinal Research - A type of post-facto research in which subjects are measured
repeatedly throughout their lives in order to obtain data on possible trends in growth and
development. Terman's* massive study of growth trends among intellectually gifted children is
an example of this research technique. The study, begun in the early 1920s, is still in progress
today and is still providing science with new data. Longitudinal research requires great patience
on the part of the investigator, but the obtained data is considered to be more valid than that
obtained using the cross-sectional approach.
* L.M. Terman, Genetic Studies of Genius (Stanford, California: Stanford University Press,
1925, 1926, 1930, 1947, 1959)
Mann-Whitney U Test - A test on ordinal data of the hypothesis of the difference between two
independently selected random samples. The U test is the ordinal counterpart of the independent
t test.
               ( n1 )( n2 )
          U
zU                 2          .
         n1n2 ( n1  n2  1)
                  12
McNemar Test - Technique developed by the statistician Quinn McNemar that uses chi square
for the analysis of nominal data from correlated samples.
             2
       ad
x2              .
       ad
 74                                                     ANGPAM21-106 10 NOVEMBER 2011


Mean ( X ) - A measure of central tendency specifying the arithmetic average. Scores are added
and then divided by the number of cases.
      X
 X        .
        N
The mean is best used when the distribution of scores is balanced and unimodal. In a normal
distribution, the mean coincides with the median and the mode. When the entire population of
scores is used, the mean is designated by the Greek letter µ (mu)
Measurement - A method of quantifying observations by assigning numbers to them on the
basis of specific rules. The rules chosen determine which scale of measurement is being used:
nominal, ordinal, interval, or ratio.
Median (Mdn) - A measure of central tendency that specifies the middle most score in an
ordered set of scores. The median always represents the 50th percentile. It is the most valid
measure of central tendency whenever the distribution is skewed.
Mesokurtic - A unimodal frequency distribution whose curve is normal. (See Normal Curve)
Mode (Mo) - A measure of central tendency that specifies the most frequently occurring score in
a distribution of scores. When there are two most-common points, the distribution is said to be
bimodal.
Multiple R - A single numerical value that quantifies the correlation among three or more
variables. The equation for a three-variable multiple R is as follows:
            ry2,1  ry2,2  2 ry ,1ry ,2 r1,2
R y1,2                                        .
                      1  r12,2

Multiple Regression - Technique using the multiple R for making predictions of one variable
given measures on two or more others. It requires the calculation of the intercept (a) and also at
least two slopes (b1 and b2). For the three-variable situation, the multiple regression equation is
as follows:
YMpred  b1 X1  b2 X 2  a .
Nominal Data - Data (measurements) in which numbers are used to label discrete, mutually
exclusive categories; nose counting data, which focuses on the frequency of occurrence within
independent categories.
Nonparametrics - Statistical tests that neither predict the population parameter, µ, or make any
assumptions regarding the normality of the underlying population distribution. These tests may
be run on ordinal or nominal data, and typically have less power than do the parametric tests.
Normal Curve - A frequency distribution curve resulting when scores are plotted on the
horizontal axis (X) and frequency of occurrence is plotted on the vertical axis (Y). The normal
curve is a theoretical curve shaped like a bell and fulfilling the following conditions:
(1) most of the scores cluster around the center, and as we move away from the center in either
direction there are fewer and fewer scores.
(2) the scores fall into a symmetrical shape—each half of the curve is a mirror image of the
other.
(3) the mean, median, and mode all fall at precisely the same point, the center; and.
(4) there are constant area characteristics regarding the standard deviation.
Null Hypothesis - The assumption that the results are simply due to chance. When testing the
hypothesis of difference, the null hypothesis states that no real differences exist in the population
ANGPAM21-106 10 NOVEMBER 2011                                                                       75


from which the samples are drawn. When testing the hypothesis of association, the null
hypothesis states that the correlation in the population is equal to zero (does not exist)
Odds - The chances against a specific event occurring. When the odds are 5 to 1, for example, it
means that the event will not occur five times for each single time that it will occur.
Ordinal Data - Rank-ordered data, that is, derived only from the order of the numbers, not the
differences between them. Ordinal measures provide information regarding greater-than or less-
than status, but not how much greater or less.
Ordinate - The vertical or Y axis in the coordinate system. On a frequency distribution, the
ordinate indicates the frequency of occurrence.
Parameter - Any measure obtained by having measured the entire population. Parameters are,
therefore, unusually inferred rather than directly measured.
Partial Correlation - Correlation technique that allows for the ruling out of the possible effects
of one or more variables on the relationship among the remaining variables. In the three-variable
situation, the partial correlation rules out the influence of the third variable on the correlation
between the remaining two variables. The equation for separating out out the influence of the
third variable is as follows:
               ry ,1  ry ,2 r1,2
ry ,12                                .
             (1  ry2,2 )(1  r12,2 )

Pearson r - Statistical technique introduced by Karl Pearson for showing the degree of
relationship between two variables. Also called the product-moment correlation coefficient, it is
used to test the hypothesis of association, that is, whether or not there is a relationship between
two sets of measurements. The Pearson r can be calculated as follows:
   XY
         ( X )(Y )
r N                .
       Sx S y

Computed correlation values range from +1.00 (perfect positive correlation) through zero to -
1.00 (perfect negative correlation). The farther the Pearson r is from zero, whether in a positive
or negative direction, the stronger is the relationship between the two variables. The Pearson r
can be used for making better-than-chance predictions, but can not be used for isolating causal
factors.
Percentiles (or centiles) - The percentage of cases falling below a given score. Thus, if an
individual scores at the 95th percentile, that individual has exceeded 95 percent of all persons
taking that particular test. If test scores are normally distributed, and if the standard deviation of
the distribution is known, percentile scores can easily be converted to the resulting z scores.
Percentile Rank - The value that indicates a given percentile. A point at the 75th percentile is
said to have a percentile rank of 75.
Phi Coefficient - A test of correlation on nominal data when the number of independent cells is
exactly 4 (that is, a 2 X 2 chi square analysis)
            x2
             .
            N
Platykurtic Distribution - A unimodal frequency distribution in which the curve is relatively
flat. Large numbers of scores appear in both tails of the distribution. A platykurtic distribution
of scores yields a relatively large standard deviation.
 76                                                     ANGPAM21-106 10 NOVEMBER 2011


Point of Intercept (a) - In a scatter plot, the point of intercept is the location where the
regression line crosses the ordinate, or the value of Y when X is equal to zero. In the regression
equation, Y = bX + a, the intercept is symbolized by the a term.
Population - The entire number of persons, things, or events (observations) having at least one
trait in common. Populations may be limited (finite) or unlimited (infinite)
Post-Facto Research - A type of research that, while not allowing for cause-and-effect
conclusions, does allow the researcher to make better-than-chance predictions. In such research,
subjects are measured on one response dimension and these measurements are compared with
different response measures. Responses are compared with responses, as in comparing the SAT
scores with grade-point averages for a group of students. Since the experimenter does not treat
the subjects differently (there is no manipulation of an independent variable), cause-and-effect
conclusions may not be drawn from post-facto data.
Power (1 - ß) - A measure of the sensitivity of a statistical test. The more powerful a test is, the
less is the likelihood of committing the beta error (accepting the null hypothesis when it should
have been rejected). The higher a test's power, the higher is the probability of a small difference
or a small correlation being found to be significant.
Probability (P) - The statement as to the number of times a specific event, s, can occur out of
the total possible number of events, t.
         s
 P .
         t
Probability should be expressed in decimal form. Thus, a probability of 1/20 is written as .05.
Quartiles - Divisions of a distribution representing quarters; the 1st quartile representing the
25th percentile, the 2nd quartile the 50th percentile (or median), and the 3rd quartile the 75th
percentile.
Quota Sampling - Selecting a sample that directly reflects the population characteristics. If it is
known that 45% of the population is composed of males, and if it is assumed that gender is a
relevant research variable, then the sample must contain 45% of male subjects.
Random Sample - Sample selected in such a way that every element or individual in the entire
population has an equal chance of being chosen. When samples are selected randomly, then
sampling error should also be random and the samples representative of the population.
Range (R) - A measure of variability that describes the entire width of the distribution. The
range is the difference between the two most extreme scores in a distribution, and is, thus, equal
to the highest value minus the lowest value.
Ratio Data - Data (measurements) that provide information regarding the order of numbers, the
difference between numbers, and also an absolute zero point. It permits comparisons, such as A
being three times B, or one-half of B.
Regression line - The single straight line that lies closest to all of the points in a scatter plot.
The regression line can be used for making correlational predictions when three important pieces
of information are known:
(1) how much the scatter points deviate from the line.
(2) the slope of the line.
(3) the point of intercept.
Y. . bX. .a.

              .
ANGPAM21-106 10 NOVEMBER 2011                                                                      77


A sample that reflects the characteristics of the entire population. Random sampling is assumed
to result in representative samples.
Sample - A group of any number of observations selected from a population, as long as it is less
than the total population.
Sampling Distributions - Distributions made up of measures taken on successive random
samples. Such measures are called statistics, and when all samples in an entire population are
measured, the resulting sampling distributions are expected to be normal. (See Central Limits
Theorem.) Two important sampling distributions are the distribution of means and the
distribution of differences.
Sampling Error - The expected difference between the mean of the sample and the mean of the
population ( X   ). Under conditions of random sampling, the probability of obtaining a sample
mean greater than the population mean is identical to the probability of obtaining a sample mean
less than the population mean (P = .50)
Scatter Plot - A graphic format in which each point represents a pair of scores, the score on X as
well as the score on Y. The array of points in a scatter plot typically forms an elliptical shape (a
result of the central tendency usually present in both the X and Y distributions)
Secular Trend Analysis - A method using the regression technique to predict trends across time.
Historical data are used for predicting future results, based on the assumption that the past trend
will continue.
Significance - A statistical term used to indicate that the results of a study are not simply a
matter of chance. Researchers talk about significant differences and significant correlations, the
assumption being that chance has been ruled out (on a probability basis) as the explanation of
these phenomena.
Skewed Distribution - An unbalanced distribution in which there are a few extreme scores in
one direction. In a skewed distribution, the best measure of central tendency is the median.
The Spearman rs - Correlation coefficient devised by Charles E. Spearman for use with rank-
ordered (ordinal) data. Sometimes called the Spearman rs (rho), the coefficient is found as
follows:
             6 d 2
rs  1 
           N ( N 2  1) .

Standard Deviation (S) - A measure of variability that indicates how far all scores in a
distribution vary from the mean. The standard deviation has a constant relationship with the area
under the normal curve (see Normal Curve) The sample standard deviation is calculated with
the following equation:
       X2    2
S         X
        N       .
The estimated standard deviation of the population is calculated with the following equation:
                  ( X )2
       X 
         2

s                   N      .
                N 1
When the standard deviation is calculated on the basis of all scores in the entire population, it is
designated as  (Greek letter sigma)
 78                                                                           ANGPAM21-106 10 NOVEMBER 2011


Standard Error of Difference ( s X                             ) - An estimate of the standard deviation of the entire
                                                       1 X2

distribution of differences between pairs of, successively and randomly selected samples means.
This estimate can be made on the basis of the information contained in just two samples.
sX             s 2X  s 2X .
     1  X2         1      2


When sample selections are independent of each other, the correlation term ( 2r1,2 s X s X ) is equal       1   2

to zero and is, therefore, not used. The equation for the standard error of difference for
independent samples is, thus, as follows:

sX             s 2X  s 2X  2r1,2 s X s X
     1 X2           1     2               1   2   .
Standard Error of Estimate (SEest) - A technique for establishing the accuracy of a predicted
Y value obtained by using the regression equation. The higher the correlation between X and Y,
the lower is the resulting value of the standard error of estimate and the more accurate is the
predicted Y value. When r = 0, the standard error of estimate is equal to the standard deviation
of the Y distribution.
Standard Error of the Mean ( s X ) - An estimate of the standard deviation of the entire
distribution of random sample means successively selected from a single population until that
population has been exhausted. This estimate can be made on the basis of information contained
in a single sample, that is, the standard deviation of the sample and the size of the sample.
          S
sX           .
        N 1
Standard Error of Multiple Estimate - A technique for assessing the accuracy of a prediction
that has been generated from the multiple regression equation. For the three-variable situation,
the standard error of multiple estimate is as follows:

SE M est  S y 1  Ry21,2
                                  .
Statistic - Any measure that is obtained from a sample as opposed to the entire population. The
range (or the standard deviation or the mean) of a set of sample scores is a statistic.
Sum of Squares (SS) - An important concept for ANOVA; the sum of squares equals the sum of
the squared deviations of all scores around the mean.
                               ( X )2
SS   x 2   X 2                    .
                                 N
When the sum of squares is divided by its appropriate degrees of freedom, the resulting value is
called the mean square, or variance.
t Ratio - Statistical test used to establish whether or not a significant (non-chance) difference
exists between two sample means. It is the ratio of the difference between two sample means to
an estimate of the standard deviation of the distribution of differences.
     X1  X 2
t                .
      sX 1 X 2
ANGPAM21-106 10 NOVEMBER 2011                                                                    79


T Score - A converted standard score such that the mean equals 50 and the standard deviation
equals 10. T scores, thus, range from a low of 20 to a high of 80.
Tukey's HSD (honestly significant difference) - A technique developed by J. W. Tukey are
establishing whether or not the differences among various sample means are significant. The test
is performed after an ANOVA only when the F ratio is significant.
Unimodal Distribution - A distribution of scores in which only one mode (most frequently
occurring score) is present.
Variability Measures - Measures that give information regarding individual differences, or how
persons or events vary in their measured scores. The three most important measures of
variability are the range, the standard deviation, and the variance (which is the standard deviation
squared)
Variable - Anything that varies and can be measured. In experimental research, the two most
important variables to be identified are the independent variable and the dependent variable. The
independent variable is a stimulus, is actively manipulated by the experimenter, and is the causal
half of the cause-and-effect relationship. The dependent variable is a measure of the subject's
response and is the effect half of the cause-and-effect relationship.
Variance - A measure of variability that indicates how far all of the scores in a distribution vary
from the mean. Variance is equal to the square of the standard deviation.
Wilcoxon T Test - A test on ordinal data of the hypothesis of difference between two sample
groups when the selections are correlated (as in the matched-group design). The Wilcoxon T is
the ordinal counterpart of the correlated t.
Yates Correction - A correction factor applied to a 2 X 2 chi square analysis (or anytime df = 1)
whenever any of the expected frequencies are less than 10. The difference between ƒo and ƒe is
reduced by .50 resulting in a slightly lower chi square value.
z Score (standard score) - A number that is the result of the translation of a raw score into units
of standard deviation. The z score specifies how far above or below the mean a given score is in
the standard deviation units. Any score above the mean converts to a positive z score, while
scores below the mean convert to negative z scores. The z score is also referred to as a standard
score. The mean of the z score distribution is equal to zero.
     XX
z           .
        S


Definition of Common Terms.
The following information is provided as standard tools for tracking and displaying information
for various MAJCOM weapons systems. The listed calculation methods indicated reflect the
metrics that will be used when preparing the Monthly Summary, other reports and requests for
special studies. Categories of indicators are sequenced alphabetically by key words. Metrics
used by the unit will be dependent upon the weapon system and gaining MAJCOM.
Abort, Rate Total: The sum of air abort rate plus the ground abort rate. Retrieved by: IMDS -
Screen 362; G081 - Manually tracked by MOC/Plans and Scheduling
                          Air Aborts + Ground Aborts
                                                               X 100
                     Total Sorties Flown + Ground Aborts.
    80                                                  ANGPAM21-106 10 NOVEMBER 2011



.
Addition: Any aircraft/sortie added to the flying schedule that was not printed on the the weekly
schedule will be recorded against the agency that requested the additonal aircraft or sortie.
Functional Check Flight (FCF) and Operational Check Flight (OCF) whose primary purpose is to
perform maintenance checks are not additions. Record FCF and OCF sorties as flown as
scheduled. FCF/OCF sorties and sorties originating off-station without home-unit support (e.g.
maintenance repair/response teams – MRTs) will be considered ―scheduled as flown‖ without
recording deviations. NOTE: Addition of multiple sorties (i.e. FOL) will be counted as
multiple sorties added. Retrieved by: IMDS -Screen 362; GDSS - Manually tracked by
MOC/Plans and Scheduling.
Addition Rate:
                                  Number of additions
                                                               X 100
                             Number of sorties scheduled


.
Air Abort (AA or A/A): An aircraft or sortie that cannot complete its primary or alternate
mission for any reason. Air aborts are considered a sortie flown when reporting total sorties
flown. Air aborts will be coded to the agency or condition that caused the aborted mission. An
air abort will not be recorded when malfunctions occur during the ―Before Takeoff Checklist‖
portion of helicopter sorties. Note: Effective mission decisions will be made by Operations. A
non-effective mission does not necessarily mean an air abort occurred. For example: a sortie
where all planned mission tasks were completed but the mission was non-effective due to
uncompleted training events, wold not be coded as an air abort. An air abort also counts as a
code-three break. Retrieved by: IMDS -Screen 362; G081 - Manually tracked by MOC/Plans
and Scheduling.
Air Abort Rate: The total number of air aborts per sorties flown. The rate only includes air
aborts caused by maintenance or operations. Its purpose is to reflect the percentage of
unsuccessful missions once the aircraft is airborne.
                                  Number of Air Aborts
                                                           X 100
                                      Sorties Flown


.
Aircraft Availability: The degree to which an item is in an operable and commitable state at
the start of a mission when the mission is called for at an unknown (random) time. Availability
is dependent on reliability, maintainability, and logistics supportability. Percentage of a fleet not
in a Depot possessed status or NMC aircraft (that are unit possessed). NOTE: The metric may
be created at the Mission Design (MD)/ MDS level or may be grouped by fleet (e.g., Aggregate,
Bombers, Fighters) to determine ―Aircraft Availability‖
ANGPAM21-106 10 NOVEMBER 2011                                                                    81


                                     MC hours*PAI
                                                           X 100
                                           TAI


Aircraft Average Fleet Time: The average number of remaining flying hours until the next
periodic or phase inspection. This is the prime leading logistics indicator that identifies the
unit‘s ability to sustain future OPNS tempo and inspection flow requirements. Retrieved by:
IMDS - Using the “TDI” (screen 400) on the last day of the month and based on 200-hr phase
average, use the average time divided by the inspection interval; G081 - Program 8005. For
each possessed acft count how many days until the next scheduled ISO. Total all of the days and
divide by the possessed acft.This is the Avg fleet time. To get the Pct of fleet time:


IMDS: Example:       103.75 / 200 = .51875 * 100 = 51.9% Desired = 50% (± 5%)
G081 Example: Avg fleet time / ISO Interval Insp X 100.


Desired = 50% (± 5%)


Aircraft Configuration Management (ACM): Provides the capability to determine the actual
versus approved configuration of an aircraft. The intent of the configuration management
subsystem is to ensure that selected serially controlled and/or time change items (TCI) are
properly loaded to the applicable database and to track compliance with TCTOs. P&S Tracked.
Aircraft Possessed Hours: Total number of clock hours accumulated for a specified period for
all of the possessed aircraft for a unit. Retrieved by: IMDS – Screen 460; G081 – Program 9025.
Attrition: Losses based on historical data. Attrition sorties are added to the flying schedule and
unit‘s sortie contract to allow for projected losses due to maintenance, weather, operational, and
other calculated losses. They reflect unit seasonal/historic variations. For illustration purposes,
when computing attrition for Jan 04, use historical data for Jan 03, Jan 02, Jan 01, Jan 00, Jan 99,
and so forth. Use as much historical data as required to ensure that seasonal variations are
considered. Factors used to compute attrition will be Maintenance, Operations, Supply, Higher
Headquarters, Weather and Other. NOTE: For the computation, ―Other‖ do NOT include sorties
canceled due to completion of the monthly/annual flying program. When computing attrition use
the total sorties lost for each particular category. Multiply the Attrition Factor by the Sorties
required and add the result to the schedule. Attrition is calculated by dividing the
categorized/total cancellations by the total number of scheduled sorties.


                   Total Monthly             Total Cancellations
                                                                           X 100
                     Attrition
                                             Total Sched Sorties

                                         Maintenance cancellations
               Maintenance Attrition                                       X 100
                                             Total Sched Sorties
    82                                                    ANGPAM21-106 10 NOVEMBER 2011


                                           Operations Cancellations
                  Operations Attrition                                        X 100
                                               Total Sched Sorties

                                           Other/HHQ Cancellations
                 Other/HHQ Attrition                                          X 100
                                               Total Sched Sorties

                                            Weather Cancellations
                   Weather Attrition                                          X 100
                                               Total Sched Sorties


.
Attrition computation example:


Maintenance       =    30cx/1000schd sorties         =     .03
Operations        =    20cx/1000schd sorties         =     .02
Supply            =    05cx/1000schd sorties         =     .005
HHQ               =    02cx/1000schd sorties         =     .002
Weather           =    50cx/1000schd sorties         =     .05
Sympathy          =    10cx/1000schd sorties         =     .01
Air Traffic       =    02cx/1000schd sorties         =     .002
Other             =    05cx/1000schd sorties         =     .005
Total             =    124cx/1000 schd sorties       =    .124 attrition factor


             Sorties Required =     1000    Attrition Factor      =    .124
           1- Attrition Factor
                       1-.124 =     .876


    (Total sorties to schedule) =   1141.552            1142


.
Attrition (Aircraft): Aircraft required to replace primary aircraft inventory losses. Do not
confuse this term with computing attrition rates (e.g. Weather) for developing the flying
schedule.
Average Hangar Queens: An aircraft that has not flown for 30 consecutive days. An average
computed by dividing the total Hangar Queen days accrued in the reporting period by the
ANGPAM21-106 10 NOVEMBER 2011                                                               83


inclusive number of days in the reporting period, e.g., 20 Hangar Queen days divided by 30 days
in the reporting period equals .67 average Hangar Queens.
Average Mission Length: The flying time for a mission from beginning to completion.

                           Total Flying Time
                                                        X 100
                             Total Missions


.
Average Possessed Aircraft: Average number of aircraft possessed for specified period of
time. Retrieved by: IMDS Screen 460; G081 Program 9025.

                 Total Possessed Hours (Month To Date)
                                                                     X 100
             24 x # Of Days (Month To Date) x # of aircraft


.
Average Sortie Duration (ASD): Average flying time of a sortie.
                             Hours Flown
                                                        X 100
                             Sorties Flown


.
Awaiting Maintenance (AWM) Rate. The total number of acft in AWM status, divided by
possessed aircraft X 100. (See Delayed/Deferred Discrepancies for more details)
Awaiting Parts (AWP) Rate. The total number of aircraft in AWP status, divided by possessed
aircraft X 100. (See Delayed/Deferred Discrepancies for more details)
Backup Aircraft Inventory (BAI). Aircraft above the primary mission inventory to permit
scheduled and unscheduled maintenance, modifications, inspections and repair without reduction
of acft available for operational missions.
Base Repair Capability: Capability of unit maintenance complex to repair the components
with existing experience and equipment.
                          Action taken codes (A/F/G/K/L/Z)
                                                                          X 100
              Sum of action taken codes (A/F/G/K/L/Z/1/2/3/4/5/6/7/8/9)


.
Base Self-Sufficiency.    Capability of unit‘s maintenance complex to repair authorized
components.
                         Action taken codes (A/F/G/K/L/Z)                 X 100
    84                                                ANGPAM21-106 10 NOVEMBER 2011


              Sum of action taken codes (A/F/G/K/L/Z/2/3/4/5/6/7/8/9)


.
Cancellation: Any sortie that was scheduled in the weekly flying schedule that did not take off
within 2 hours of its published time. NOTE: scheduling of multiple sorties (i.e. FOL) if
canceled will be counted as multiple sorties canceled. Retrieved by: IMDS -Screen 362; GDSS2
- Manually tracked by MOC/Plans and Scheduling.
                                # of Cancellations
                                                                  X 100
                         Scheduled Sorties or Departures


.
Cannibalization: The authorized removal of a specific assembly, subassembly or part from one
weapon system, support system or equipment end item for installation on another end item to
satisfy an existing supply requisition and to meet priority mission requirements with an
obligation to replace the removed item. Weapon system, support systems, or equipment end
items include aircraft, missiles, drones, Unmanned Aerial Vehicles (UAVs), uninstalled engines,
uninstalled engine modules, aircrew and/or launch crew training devices, C-E equipment, AGE,
TMDE, automatic test equipment, serviceable uninstalled ECM pods, and guns. Retrieved by:
IMDS -Screen 105 “AT” = “T”, “U” (Run for each MDS and then the workcenter that owns the
engine) ; G081 – Program 67051.
 Cannibalization Man-hours: Using data from IMDS/G81 determine the total monthly man-
hours expended for the month. Action taken ―T‖ and ―U‖ used to calculate the total man-hours
expended. Retrieved by: IMDS -Screen 104, detail; G081 – Program 67126 or web focus report
“cann report previous month”
Cannibalizations Man-hours per Average Possessed Aircraft: Average number of
cannibalization removals (action taken ―T‖ and ―U‖) per average possessed acft, this rate
includes any aircraft-to-aircraft or engine-to-aircraft cannibalization actions. The number
represents the average man-hours expended per possessed acft.
                    Total number of cann man-hours (T and U)
                                                                   X 100
                            Average possessed aircraft


.
Cannibalization Rate.     This rate includes any aircraft-to-aircraft or engine-to-aircraft
cannibalization actions. The rate represents the amount of cannibalizations per 100 sorties
flown.
            (# Of Acft-To-Acft Canns) + (# Of Engine-To-Acft Canns)
                                                                           X 100
                                Total Sorties Flown
ANGPAM21-106 10 NOVEMBER 2011                                                                 85


.
Chargeable Aircraft (CHRG). The number of aircraft against which units should build their
flying programs. The two most common situations are units in conversion and aircraft short due
to PDM/Depot/Modification requirements. Chargeable aircraft will be calculated for applicable
units, by month.
Code Three Break: System malfunction occurring in-flight that renders aircraft NMC after
landing. An air abort for maintenance problems is a code-three break and should be included.
Retrieved by: IMDS -Screen 185; G81 – Program 67076 or 67117 (manually sort all Red X
write-ups that occurred with a When Discovered Code A, B, C or D).      .
Code Three Break Rate. This is the percentage of acft that land with an overall acft landing
status of Code 3 divided by the total sorties flown X-100. (IMDS will provide rate for you on
screen 185.)
                 # of Sorties With Code-3 Landing Status (Breaks)
                                                                       X 100
                                Total Sorties Flown


.
Commitment Rate: The percent of aircraft that is scheduled/designated for headquarters (DOD,
HAF, MAJCOM, NAF) tasked missions, spares, alerts, local missions, ground trainers, and static
displays. This is a snapshot of the information at that time. Run once a week and average it for
the month. Retrieved by: IMDS -Screen 360; G081 – Program 67057
                     Cumulative Number of Committed Aircraft
                                                                    X 100
                     Cumulative Number of Possessed Aircraft


.
Contractor Logistics Support (CLS): Non-government managed logistics processes such as
supply and/or maintenance performed by civilian contractors.
Contractor Operated and Maintained Base Supply (COMBS): Non-government managed
supply operation/warehouse supported by civilian contractors.
Cumulative (CUM): The CY (Calendar Year) or FY (Fiscal Year) cumulative performance of
an indicator.
 Data Integrity Rate: The percentage of records found correct in the JDD/MDD subsystem
during the Data Integrity Team (DIT) review. Compute the error rate and subtract from 100 to
establish the DIT Rate. For Data Integrity rate computations, a record is one Detail Data Record
(DDR)/Work Event Separator (WES). When a DDR/WES contains more than one
documentation error, the error rate will reflect one error for the entire DDR/WES. Report the
uncorrected error rate.


                      Error Rate::        # of Errors Found         X 100
    86                                                 ANGPAM21-106 10 NOVEMBER 2011


                                        # of Documents Checked


                            Data Integrity Rate:    100 – Error Rate


.
Delayed/Deferred Discrepancy. Malfunctions or discrepancies not creating NMC or PMC
status that are not immediately corrected and are usually transferred from the 781A to the 781K.
Discrepancies are considered deferred when: a) they are discovered and the decision is made to
defer them, b) discrepancies are scheduled with a start date greater than 5 calender days after the
discovery date, or c) discrepancies are awaiting parts with a valid off base requisition (AFI 21-
101_ANG Supp)
Delayed/Deferred Discrepancies (Average) Rate: Each week, analysts will take a "snap shot",
for each reportable MDS, of the total number of deferred discrepancies for both maintenance and
supply (Awaiting Maintenance, AWM and Awaiting Parts AWP). The "snapshot" will include a
total of all 781K write-ups. Retrieved by: IMDS -Screen 539; G081 – Program 67150. Then,
the following formula will apply: .
Maintenance Deferred Discrepancies (AWM):
                 Total AWM (Snapshot) Discrepancies
                                                                            X 100
                     Average Possessed Aircraft


      .
Parts Deferred Discrepancies (AWP):
                        Total AWP (snapshot) discrepancies
                                                                                    X 100
                            Average Aircraft Possessed


      .
Delayed/Deferred Discrepancies For Maintenance and Parts, Total Monthly: A figure
derived by the sum of no less than four weekly snapshots during month. The following formula
will use the weekly rates to form a cumulative monthly rate.
Monthly AWM Rate:
          AWM Rate(WK1) + AWM Rate(WK2) + AWM Rate(WK3) + AWM Rate(WK4)
                                                                                             X 100
                          # Of Weekly Rates (In This Sample: 4)


      .
Monthly AWP Rate:
ANGPAM21-106 10 NOVEMBER 2011                                                                  87


         AWP Rate(WK1) + AWP Rate(WK2) + AWP Rate(WK3) + AWP Rate(WK4)                         X 100
                           # Of Weekly Rates (In This Sample: 4)


.
Delayed/Deferred Discrepancy Rate, Monthly Total: The final average monthly deferred
discrepancy rate reported and will be the total of the monthly AWM and AWP rates.
Total Monthly AWM + Total Monthly AWP.
Using four samples:.


Example     #AWMs       #AWPs             #Acft      AWM     AWP      DDS Rate
Week 1           49        100            50         1.0     2.0      3.0
Week 2           55        110            55         1.0     2.0      3.0
Week 3           25        135            53         0.5     2.5      3.0
Week 4      30          105               50         0.6     2.1      2.7


.
AWM - 1.0 + 1.0 +0.5 + 0.6 / 4 = 0.8.
AWP – 3.0 + 3.0 +3.0 +2.7 / 4 = 2.9.
Total Delayed Discrepancy = Avg AWM + Avg AWP (0.8 + 2.9 = 3.7)


Departure: A takeoff.
Departure (Delivery) Reliability, Logistics: Measures the ability of logistics to ensure aircraft
depart within 14 minutes of the scheduled departure time.


                       # of Departures – # of Logistics Delays
                                                                   X 100
                                   # of Departures

.
Deviations: Changes from the printed weekly flying schedule identified as either an
Addition/Deletion, or Early/Late takeoff. Retrieved by: IMDS -Screen 362; G081 - Manually
GDSS2 tracked by MOC/Plans and Scheduling
    Chargeable: Deviations that the unit has control over. (personnel, pilots, maint, parts,
    scheduling, etc.)
    Non-Chargeable: Deviations that are not within the unit‘s control. (weather, FAA,
    88                                                     ANGPAM21-106 10 NOVEMBER 2011


      tower/traffic delays, clearance, HHQ, etc.)
      Additions: Any sortie/Home Station Launch (for the heavies) flown that was not on the
      published weekly schedule. (See deviation reporting procedures below.) FCFs are not
      considered ADDS, and are "flown as scheduled.".
      Cancellations: Any sortie scheduled in the weekly flying schedule that did not take off
      within 2 hours of its published time. NOTE: Scheduling of multiple sorties (i.e.: FOL) if
      canceled will be counted as multiple sorties canceled.
      Deletions: Removal of a scheduled launch/sortie for any reason. NOTE: scheduling of
      multiple sorties (ie: FOL) if deleted will be counted as multiple sorties canceled.
      Early Takeoff: Any sortie on the weekly flying schedule that takes off (ACC – more than
      30 minutes; AMC – more than 20 minutes) before its scheduled takeoff time.
      Late Takeoff: Any sortie on the weekly flying schedule that takes off (ACC – more than 15
      minutes; AMC – more than 14 minutes) after its scheduled takeoff time.
      Multiple Deviations: Multiple Deviations against a single line entry will not count toward
      FSE except for (a) additions that ground abort or (b) additions that cancel, and (c) added
      aircraft/sorties that take-off late.
      Higher Headquarters: Deviations resulting from guidance outside unit/wing control.
      Guidance may include alert tasking, HQ-directed quiet hours, airspace/route/range
      denials/closures, depot schedule changes, DV traveler requirements or other external,
      customer driven changes. Higher headquarters asking, or changing tasking, which are
      received too late for inclusion into the flying schedule.
      Sympathy: When under the command of a flight leader/instructor pilot is canceled or is late
      in taking off due to the cancellation or delay of one or more of the acft in the flight. Only the
      cause acft will be counted as chargeable, with the remainder of the flight being recorded as
      sympathy (non-chargeable). Record these non-chargeable deviations as other.
      Weather: Launches that are early/late or cancelled/added due to weather conditions at
      home station or destination.


.
Direct Labor: Productive maintenance tasks (such as troubleshooting, bench checking,
servicing, inspecting, towing, etc) documented as either on-equipment or off-equipment work.
Dropped Object Rate: Rate of dropped objects per 100 departures or sorties
                           Number of dropped object incidents
                                                                      X 100
                                Sorties or departures

.
Engine Test Cell Reject Rate: Percent of total engines tested cells that were rejected.
                                  Total engines rejected
                                                                      X 100
                                   Total engines tested
ANGPAM21-106 10 NOVEMBER 2011                                                                     89



.
Engine Unscheduled Removal: An engine removed due to malfunctioning of the engine caused
by breakdown, deterioration, metal fatigue, wear, etc. Retrieved by: IMDS -Screen 108; G081 –
Program 67015.
Fix Rate: This is the percent of all broken aircraft (landing status code 3) that are returned to
flyable status within a specified time period (i.e., 4, 8, or 12 or 24 duty hours). For fighters, an
aircraft break and a 4-hour fix interval is equivalent to a surge sortie opportunity lost in wartime.
This interval is used for fighter aircraft (except T-38A, F-4F, and F-117A)
Note: Insert number of hours where ―X‖ is.
            # Code 3 Breaks Fixed Within “X” Duty Hours After Landing
                                                                                 X 100
                                 Total Code 3 Breaks

.
Flying Hours Allocated: Determined by ANGRC/XOOH for specific unit (Ops) requirements.
Flying Scheduling Effectiveness Rate: The FSE rate is the percentage of scheduled sorties a
unit successfully launches on time. Its purpose is to determine how efficiently the flying
schedule was executed. Indicates unit turmoil caused by flying schedule deviations.
                Adjusted Sorties Scheduled – Chargeable Deviations
                                                                            X 100
                             Adjusted Sorties Scheduled

                                       Sorties Scheduled – Total Deviations
          MAF formula:                                                              X 100
                                                 Sorties Scheduled

                                              Maintenance Deviations
        Maintenance Rate                                                            X 100
                                              Total Sorties Scheduled

                                               Operations Deviations
         Operations Rate                                                            X 100
                                              Total Sorties Scheduled

                                                 Supply Deviations
           Supply Rate                                                              X 100
                                              Total Sorties Scheduled

                                         Higher Headquarters Deviations
            HHQ Rate                                                                X 100
                                              Total Sorties Scheduled

                                                Weather Deviations
             WX Rate                                                                X 100
                                              Total Sorties Scheduled
    90                                                ANGPAM21-106 10 NOVEMBER 2011



                                              Sympathy Deviations
            Sym Rate                                                             X 100
                                             Total Sorties Scheduled

                                                Other Deviations
            Other Rate                                                           X 100
                                             Total Sorties Scheduled

.
Full Mission Capable (FMC): The aircraft is capable of doing all assigned missions.
Functional Check Flight (FCF): The flight of an aircraft, in accordance with the applicable -6
manual, to verify the airworthy condition of the aircraft.
Functional Check Flight (FCF) Release Rate: The FCF release rate is the percentage of
aircraft that successfully complete an FCF versus the total number of FCFs attempted. Its
purpose is to monitor the quality of maintenance performed following the repair of critical
components or systems that require an FCF prior to resuming normal flying.


                                              Total FCFs released
         FCF Release Rate                                                        X 100
                                             Total FCFs attempted

.
Ground Abort: An event that prohibits the aircraft from becoming airborne, after aircrew
arrival. Do not include sympathy ground aborts in the total number of ground aborts. Its
purpose is an early warning indicator for quality of maintenance in regards to preflight and basic
post flight (BPO) maint.
Ground Abort Rate                              # of Ground Aborts                X 100
                                      # of Ground Aborts + Sorties Flown

Gun Fire Out Rate                             # of Successful Bursts             X 100
                                                    # of Attempts

.
Hangar Queen: A Hangar Queen is an aircraft that has not flown for at least 30 consecutive
days in their possessed status, or not flown within 10 days after being gained from depot
possession or alert status. (AFI 21-101_ANG Sup)
Hangar Queen (Average) Rate: An average computed by dividing the total Hangar Queen
days accrued in the reporting period by the inclusive number of days in reporting period, e.g. ,
20 Hangar Queen days divided by 30 days in reporting period equals .67 average Hangar
Queens.
ANGPAM21-106 10 NOVEMBER 2011                                                                 91


                                     Total Acft Days in all HQ Categories       X 100
                                           Days in Reporting Period


.
Hourly Utilization (UTE) Rate. The average hours flown per possessed aircraft per month.
                                                 Hours Flown                    X 100
                                          Average Possessed Aircraft


.
Hours Possessed. The total number of hours of possessed aircraft. Retrieved by: IMDS -Screen
359; G081 – Program 9025.
Indirect Labor: Unproductive man-hour expenditures (i.e.         leave, comp time, alert duty,
training, etc)
In Flight Emergency (IFE): An airborne aircraft that encounters a situation or emergency that
results in an IFE being declared by the aircrew.
Issue Effectiveness Rate. The percentage of requirements immediately satisfied by Base
Supply, used to measure how well customers are supported. It is used as a macro index for
effectiveness from all supply sources.
                               Total line items issued
               Total line items issued + total line items backordered      X 100


.
Labor-Hours (Man-hours), Documented: Total direct labor hours documented by maintenance
personnel for a specific MDS and SRD within that MDS. Includes hours documented to aircraft
engine and excludes transient maintenance labor-hours (type maintenance ―Y‖). Exclude:
PMEL, AGE, SE and Transient Maintenance Hours.
Logistics: This term, when used in reference to aircraft maintenance, will consider anything
concerning both maintenance and supply.
Logistics Reliability: Logistics reliability is a measure of the systems ability to operate under
the defined operational and support concepts using specified logistics resources (for example,
spares and manpower). Logistics reliability recognizes the effect of all occurrences that place a
demand on the logistics support system even when mission capability is unaffected.
Maintainability: A measure of the time or maintenance resource needed to keep an item
operating or restore it to operational (or serviceable, in the case of certain munitions) status.
Maintainability may be expressed as the time to do maintenance (for example, maintenance
downtime per sortie), as a usage rate of manpower resources (for example, maintenance man
hours per flying hour). As the total required manpower (for example, Maintenance Personnel per
    92                                               ANGPAM21-106 10 NOVEMBER 2011


Operational Unit), or as the time to restore a system to operational status (for example, Mean
Downtime). (AFI 99-101/AFI 21-118)
Maintenance Scheduling Effectiveness (MSE) Rate: The MSE rate is the percentage of
scheduled aircraft maintenance events that were started on or prior to the date printed in the
weekly maintenance schedule. Its purpose is to measure the success of a unit in executing its
planned maintenance events. Point values are established in AFI 21-101_ANG Sup.
                            Maintenance points earned
                            Maintenance points possible     X 100


Mean Time Between Critical Failures (MTBCF): MTBCF is the mean time between any
critical failure that will cause the system to be incapable of completing its designed
function/mission (does not include degrades). MTBCF can also be defined as ―The average time
between failures of essential system functions.‖ Normally the minimum MTBCF required will
be established at a .96 probability of mission success excluding enemy action but including
tactics to avoid enemy actions for the total aircraft system.
Mean Time Between Maintenance Action (MTBMA): MTBMA is the mean time between
any maintenance action, scheduled or unscheduled (example: calibration, cleaning, adjustment,
servicing, repair, removal, etc.).
         MTBMA is also categorized into types of actions when:
         MTBMA-Type 1 identifies inherent failures (item fails due to its own internal failure
         pattern).
         MTBMA-Type 2 identifies induced failures (item fails but not due to its own internal
         failure).
         MTBMA-Type 6 identifies maintenance resources were expended due to policy,
         location, or calibration and no defects existed at the time of maintenance.


.
Mean Time Between Maintenance (MTBM), AKA Mean Time Between Failure (MTBF).
A measure of how well a particular item/system performs or can be expected to perform.
Analysis of past data can be an indication of how well an item/system can be expected to
perform. Items/systems can be tracked by moving averages to show possible trends in
performance. A negative (lessening) MTBM would indicate poorer item/system reliability.
For formulas and further guidance refer to T.O. 00-20-2.
Mission Completion Success Probability (MCSP). MCSP is the probability that a system will
complete a specified mission, given that the system was initially capable of performing that
mission. MCSP is a measure of system reliability as it affects the mission, but excludes factors
such as probability of kill (enemy action), circular error probability and other measures of
capability. These excluded factors are considered during operations planning which determines
specific ground threats, air threats, and target hardness. In order to determine MCSP a MTBCF
must be established.
ANGPAM21-106 10 NOVEMBER 2011                                                                93


Mission Capable (MC). A system‘s ability to perform at least one of its assigned peacetime or
wartime missions. If no wartime mission is assigned, the system must be capable of performing
any one assigned peacetime mission. Retrieved by: IMDS Screen 460; G081 Program 9025.
Mission Capable (MC) Rate: The percent of possessed time an aircraft/system is partially or
fully mission capable.
                            FMC Hours + PMC Hours
                               Possessed Hours              X 100


.
Mission Design Series (MDS): Designation of PAI or POAI aircraft. (i.e. F-016A)
Mission Reliability: Mission reliability is a measure of the ability of a system to complete its
planned mission or function. (DODI 5000-2)
Monthly Flying Hour/Sorties Scheduled: The combined weekly schedules as refined at the
daily planning/scheduling meeting and accepted by Ops/Maint (including FOL sorties scheduled)
Monthly Flying Hours/Sorties Flown:            By MDS as directed and obtained through
IMDS/G081/REMIS and verified with Ops.
Not Mission Capable (NMC): The aircraft cannot do any assigned missions. Retrieved by:
IMDS Screen 460; G081 Program 9025.
                  NMCB Hours + NMCM Hours + NMCS Hours
                                                                      X 100
                              Possessed Hours

Not Mission Capable Both Maintenance and Supply (NMCB): The aircraft cannot do any
assigned missions because of maintenance and supply. The aircraft cannot fly (restricted from
use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Both Maintenance and Supply Airworthy (NMCBA): The aircraft
cannot do any assigned missions because of maintenance and supply. The aircraft can fly (not
restricted from use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Both Maintenance and Supply Scheduled (NMCBS): The aircraft
cannot do any assigned missions because of supply and scheduled maintenance. The aircraft
cannot fly (restricted from use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Both Maintenance and Supply Scheduled Airworthy (NMCBSA):
The aircraft cannot do any assigned missions because of supply and scheduled maintenance. The
aircraft can fly (not restricted from use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Both Maintenance and Supply Unscheduled (NMCBU): The aircraft
cannot do any assigned missions because of supply and unscheduled maintenance. The aircraft
cannot fly (restricted from use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Both Maintenance and Supply Unscheduled Airworthy (NMCBUA):
The aircraft cannot do any assigned missions because of supply and unscheduled maintenance.
 94                                                   ANGPAM21-106 10 NOVEMBER 2011


The aircraft can fly (not restricted from use). Retrieved by: IMDS Screen 460; G081 Program
9025.
Not Mission Capable Maintenance (NMCM): The aircraft cannot do any assigned missions
because of maintenance. The aircraft cannot fly (restricted from use). Retrieved by: IMDS
Screen 460; G081 Program 9025.
Not Mission Capable Maintenance Airworthy (NMCMA): The aircraft cannot do any
assigned missions because of maintenance. The aircraft can fly (not restricted from use).
Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Maintenance Scheduled (NMCMS): The aircraft cannot do any
assigned missions because of scheduled maintenance. The aircraft cannot fly (restricted from
use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Maintenance Scheduled Airworthy (NMCMSA): The aircraft cannot
do any assigned missions because of unscheduled maintenance. The aircraft can fly (not
restricted from use) Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Maintenance Unscheduled (NMCMU): The aircraft cannot do any
assigned missions because of unscheduled maintenance. The aircraft cannot fly (restricted from
use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Maintenance Unscheduled Airworthy (NMCMUA): The aircraft
cannot do any assigned missions because of unscheduled maintenance. The aircraft can fly (not
restricted from use). Retrieved by: IMDS Screen 460; G081 Program 9025.
Not Mission Capable Supply (NMCS): The aircraft cannot do any assigned missions because
of supply. The aircraft cannot fly (restricted from use). Retrieved by: IMDS Screen 460; G081
Program 9025.
Not Mission Capable Supply Airworthy (NMCSA): The aircraft cannot do any assigned
missions because of supply. The aircraft can fly (not restricted from use). Retrieved by: IMDS
Screen 460; G081 Program 9025.
Operational Check Flight (OCF): The first flight of an aircraft that has had extended
downtime or extensive maintenance which does not require an FCF. 1-1-3001.
Overall Base Repair Cycle Processing Time—The total time in days for an unserviceable asset
to be repaired on base or sent NRTS to another repair agency. It starts when the replacement
part is issued by the flightline parts store (or Base Supply) and ends when the asset is returned
serviceable to the part store‘s shelf or is sent NRTS to another repair agency. Overall Repair
Cycle Processing Time and narratives are obtained from the Supply DIFM Monitor.
Partial Mission Capable (PMC): Material condition of an aircraft or training device indicating
it can perform at least one, but not all, assigned missions. Retrieved by: IMDS Screen 460; G081
Program 9025.
Partial Mission Capable Both (PMCB): The aircraft can do at least one, but not all of its
assigned missions because of maintenance and supply at the same time. The formula for PMCB
rate is PMCB hours/Possessed hours. Retrieved by: IMDS Screen 460; G081 Program 9025.
Partial Mission Capable Maintenance (PMCM): Material condition of an aircraft or training
device indicating it can perform at least one, but not all, assigned missions because of
ANGPAM21-106 10 NOVEMBER 2011                                                                  95


maintenance requirements existing on the inoperable subsystem(s). The formula for PMCM rate
is PMCM hours/Possessed hours. Retrieved by: IMDS Screen 460; G081 Program 9025.
Partial Mission Capable Supply (PMCS): Material condition of an aircraft or training device
indicating it can perform at least one, but not all, assigned missions because maintenance
required to clear the discrepancy cannot continue due to a supply shortage. The formula for
PMCS rate is PMCS hours/Possessed hours. Retrieved by: IMDS Screen 460; G081 Program
9025.
Percent of Assigned Aircraft Within 30 Percent of No Remaining Time: Provide the percent
of assigned aircraft within 30 percent of running out of inspection time. For example: A-10
Minor/Major Phase cycle is 400 Hours. 30 percent of 400 hours is 120 hours. Therefore, it is
expected that 30 percent of assigned aircraft will fall within 120 hours of total phase time.
 Possessed Hours, Aircraft: Total number of clock hours accumulated for a specified period for
all of the possessed aircraft for a unit.
Primary Aircraft Authorized (PAA): Aircraft authorized for performance of the unit‘s
mission (e.g. Combat, Combat Support, Training, Test and Evaluation, etc…). The PAA forms
the basis for the allocation of operating resources to include manpower, support equipment, and
flying hour funds. The operating command determines the PAA required to meet their assigned
missions.
Primary Aircraft Inventory (PAI): Aircraft assigned to meet primary aircraft authorization
(includes pdai, pmai, posi, and ptai)
Primary Development/Test Aircraft Inventory (PDAI): Aircraft assigned primarily for the
test of the aircraft or its components for purposes of research, development, test and evaluation,
operational test and evaluation, or support from testing programs.
Primary Mission Aircraft Inventory (PMAI): Aircraft assigned to a unit for performance of
its wartime mission.
Primary Training Aircraft Inventory (PTAI): Aircraft required primarily for technical and
specialized training of crew personnel or leading to aircrew qualification.
Primary Other Aircraft Inventory (POAI):            Aircraft required for special missions not
elsewhere classified.
Quarterly Flying Hours Flown: A total of the individual months flying in the quarter.
Reconstitution Reserve: Aircraft stored or on the ramp which are planned for return to the
operating forces in the event of mobilization, replacement, or reconstitution.
Recur Rate: A recurring discrepancy on an aircraft occurs on the second through fourth sortie
or attempted sortie after corrective action has been taken and the system or subsystem is used
and indicates the same malfunction.
                        Total # of Recurring Discrepancies
                                                                           X 100
               Total Number of Pilot Reported Discrepancies (PRDs)

Repair Cycle Processing Time: The average time in days for an unserviceable asset to be
repaired on base or sent NRTS to another repair agency. It starts when the replacement part is
 96                                                  ANGPAM21-106 10 NOVEMBER 2011


issued by the flight line parts store (or Base Supply) and ends when the asset is returned
serviceable to the part store's shelf or is sent NRTS to another repair agency.
Repair Cycle Time by Segments (Buckets of Time): .


              Pre =       The time a serviceable part is issued from Supply until the broken
                          part is received by the backshop for repair.
              Repair =    The time a part remains in the shop until repaired, minus time
                          spent awaiting parts (AWP).
              Post =      The time it takes for the repaired part to be turned back into
                          Supply.


                                 Total Number of Days in Pre-Maintenance
Pre-Maintenance Time      =
                                      Total Number of Items Repaired


                               Total Number of Days in Repair – AWP days
Repair Time               =
                                      Total Number of Items Repaired


                                Total Number of Days in Post-Maintenance
Post-Maintenance Time     =
                                      Total Number of Items Repaired


Total Cyle Time           =    Pre-Maintenance Days + Repair Days + Post- Maintenance Days


Repeat Rate: A repeat write-up is one which occurs on the next sortie or attempted sortie after
corrective action has been taken and the system or subsystem is used and indicates the same
malfunction.
                                Total # of Repeats
                                                       X 100
                                      PRDs

Repeat / Recur Rate: Sum of repeats and recurs divided by total number of PRDs.


                                Repeats + Recurs
                                                       X 100
                                      PRDs
ANGPAM21-106 10 NOVEMBER 2011                                                                97


Scheduled Sortie: An aircraft scheduled for flight by tail number on the weekly flying schedule
and confirmed on the daily flying schedule. Functional Check Flights (FCF) and Operational
Check Flights (OCF) are excluded.
Sortie: The takeoff and subsequent full stop landing of a single aircraft as pre-briefed.
Sortie Attempts: Includes sorties flown and ground aborts.
Sortie Utilization (UTE) Rate: The average sorties flown per possessed aircraft per month.
                                    Sorties Flown
                                                                X 100
                            Number of Possessed Aircraft

Spare Engine Rate: The average number of serviceable spare engines by TMSM for the unit.
This data is the average of at least 4 weekly snapshots provided to HHQ by the SRAN Engine
Managers. This rate is computed against the fleet WRE for each TMSM.
        Snapshot WK 1 + Snapshot WK2 + Snapshot WK 3 + Snapshot WK 4
                                                                                     X 100
                       #Weekly Snapshots [In this sample (4)]


Stock Record Account Number (SRAN): The ―FJ‖ account at each base that engines are
assigned to.
Supply Issue Effectiveness Rate:
                                  Line Items Issued
                                                                         X 100
                    Line Items Issued + Line Items Back ordered

Time Distribution of Time (Next Inspection): A display of aircraft inspections by tail number
showing the amount of time remaining to next inspection. Retrieved by: IMDS Screen 400;
G081 Program 8005.
Time Percent of Assigned Aircraft Within 30 percent of Remaining Time To Inspection:
Provide the percent of assigned aircraft within 30% of running out of flying time. For example:
A-10 Minor/Major Phase cycle is 400 Hours. 30% of 400 hours is 120 hours. Therefore, it is
expected that 30% of assigned aircraft will fall within 120 hours of total phase time.
Total Active Inventory (TAI): Aircraft assigned to operating forces for mission, training, test,
or maintenance functions (includes primary aircraft inventory, backup aircraft inventory,
attrition, and reconstitution reserve)
                                           PAI + BAI.


Total Abort Rate—The total number of aborts (air and ground) per sortie attempts. The number
of air and ground aborts should match those used for the Air and Ground Abort Rates.
                             Air Aborts + Ground Aborts
                                                                 X 100
                           Ground Aborts + Sorties Flown
    98                                                 ANGPAM21-106 10 NOVEMBER 2011



Total Not Mission Capable Airworthy (TNMCA): NMCBA, NMCMA, NMCMSA,
NMCBUA, NMCBSA, NMCMUA, and NMCSA added together equal TNMCA.
 Total Not Mission Capable Maintenance (TNMCM) Rate: It is the average percentage of
possessed aircraft (calculated monthly/annually) that are unable to meet primary assigned
missions for maintenance reasons (excluding aircraft in ―B-Type‖ possession identifier code
status). The TNMCM is the amount of time aircraft are in NMCM plus Not Mission Capable
Both (NMCB) status.
                           NMCM Hours + NMCB Hours
                                                               X 100
                                  Possessed Hours

Total Not Mission Capable Supply (TNMCS) Rate: The TNMCS rate is the time aircraft are
in NMCS plus NMCB status. TNMCS is based on the number of airframes out for mission
capable (MICAP) parts that prevent the airframes from performing their mission (NMCS is not
the number of parts that are MICAP).
                           NMCS Hours + NMCB Hours
                                                               X 100
                                  Possessed Hours

.
Total Partial Mission Capable Maintenance (TPMCM): PMCM and PMCB added together
equal TPMCM. The aircraft can do at least one, but not all, assigned missions because of
maintenance.
                            PMCM Hours + PMCB Hours
                                                               X 100
                                   Possessed Hours

Total Partial Mission Capable Supply (TPMCS): PMCS and PMCB added together equal
TPMCS. The aircraft can do at least one, but not all, assigned missions because of supply.
                            PMCS Hours + PMCB Hours
                                                               X 100
                                  Possessed Hours

Total Sorties Scheduled: Home base, deployed, or otherwise scheduled + Off Base flown.
Two Level Maintenance (2LM) Repair Cycle Processing Time: The average time in days for
an unserviceable 2LM asset on base to be repaired or sent NRTS to another repair agency. It
starts when the replacement part is issued by the flightline parts store (or Base Supply) and ends
when the asset is returned serviceable to the part store‘s shelf or is sent NRTS to another repair
agency.
Type, Model, Series, and Modification (TMSM): The standard nomenclature for engines
according to MIL-STD-879.
ANGPAM21-106 10 NOVEMBER 2011                                                              99


UTE Rate: The average number of sorties or hours flown per authorized/chargeable
aircraft/system per month. See the Hourly and Sortie UTE Rate Definitions.
War Reserve/Readiness Engines (WRE): The quantity of serviceable engines required to
support an operational units increased activity and/or problems caused by delay in maintenance
deployment, extension of transportation pipelines, or operational peculiarities during war.
Weapons Release Reliability Rate:
                             # of Successful Releases
                                                         X 100
                                  # of Attempts

Yearly UTE Goal: The annual sortie or hourly UTE rate target for a unit.
 100                                                     ANGPAM21-106 10 NOVEMBER 2011


                                            Attachment 2
                              SYSTEM RELIABILITY AND CAPABILITY.

A2.1. Reliability: Reliability is the probability that an item will perform its intended function
for a specific interval under stated conditions. (DODI 5000-2) Another definition is: The
probability that an item will perform a required function under specified conditions for a
specified period of time or at a given point in time. Also, expressed as the average time an item
will perform a specified function without failure. (AFM 11-1)
A2.2. System Capability: The number of flying hours/sorties expected (based on past
occurrences) between flying hours or maintenance for a particular system/component which
renders it unserviceable (broken). Degraded performance does not detract from system
capability.
A2.3. System Reliability: The percent of time a system was used with no discrepancies
occurring. System performance that was degraded or unsatisfactory (all discrepancies) is used to
compute system reliability. Normally expressed as the number of times a system was code 1
versus the number of times used.
      A2.3.1. The next step is to look at some ways to assess reliability. First let's review some
      basic formulas that lead us to overall system performance evaluation and work our way up to
      some more complicated processes.
A2.4. Combined System Performance: To assess combined system performance you could
use the following formula:
  1
                1       .
M     T           M   i


      A2.4.1. This formula allows you to combine system or component MTBMAs in series to
      obtain aircraft or system overall performance. End results are easily ranked to identify good
      or bad performing aircraft, systems, or subsystems. Where Mi = the individual system or
      component MTBMA and MT = the total MTBMA for all individual systems or components
      in series. Simply stated this formula provides the reciprocal of the sum of the reciprocals. In
      addition, you can substitute MTBF, MTBCF, or other indicators in place of MTBMA.
A2.5. Assessing Combined Performance Of Components In Series.
      A2.5.1. To assess the total MTBMA for components in series use the following formula: Mi
      = N X MT      or;
      A2.5.2. When Mi is the MTBMA of each component in series, N is the total number of
      components in series and MT is the total MTBMA for all the individual components in series.
      NOTE: Formula assumes that the individual component MTBMAs are the same. In
      addition, all components must be operable for success.
A2.6. Assessing Combined Performance Of Components In Parallel: .
      A2.6.1. To assess the total MTBMA for components in parallel use the following formula:
      MT = Mi (1+ 1/2 + 1/3 + ... 1/N) OR;
ANGPAM21-106 10 NOVEMBER 2011                                                                 101


        M
            1



        M
            2




       M
           N




        M
            T

    A2.6.2. When MT is the total MTBMA, all the individual components in parallel, Mi is the
    MTBMA each component in parallel, and N is the total number of components in parallel.
    NOTE: Formula assumes that the individual component MTBMAs are the same and only
    one out of all components are required to function properly.
A2.7. Assessing Combined Performance Of Dual Components Or Systems: .
    A2.7.1. To assess the total MTBMA for dual system or a dual component string use the
    following formula:   M M M                   MM        1   2
                                                                          .
                           T       1        2
                                                    M M  1           2

    A2.7.2. When M(1,2) is the MTBMA of the individual component string or dual system and
    MT is the combined total MTBMA.
A2.8. Determining System/Subsystem Reliability: To determine a required subsystem
reliability requirement with other known subsystem reliabilities and a known overall system
reliability requirement use the following formula:
                               Required System(RT)
Required Subsystem (RS)=
                               R1      R2           R…
ASSUME THE FOLLOWING RELIABILITIES:                                       R1       = .95
                                                                          R2       = .99
                                                                          R3       = .98
                                                                          R4       = .90
                                                                          RT       = .829
.
If the total system (RT) is required to be .90, what would the reliability of R4 have to be
improved to?
                   RT                               .90
Req R4=                        =                                              =.976 (RS)
                R1 R2    R3            (.95) (.99) (.98)
    102                                             ANGPAM21-106 10 NOVEMBER 2011


.
NOTE: The reliabilities for each R can be established at the component, subsystem, or system
level to determine the reliability at the subsystem, system, and weapon system level.
A2.9. Mixed Model System (Combined) Reliability:
     A2.9.1. To determine combined system reliability with systems in series and a system in
     parallel use the following formula:

Figure A2.1. Formula for a combined system reliability with systems in series and a system
in parallel.




      A2.9.2. Another example of mixed model reliability computation follows.   This model
      assumes independence and active redundancy.
ANGPAM21-106 10 NOVEMBER 2011                                                                 103


Figure A2.2. Formula for mixed model reliability computation. This model assumes
independence and active redundancy.




NOTE: Components with the same number (i.e., R1 or R3) are identical and consequently have
the same reliability.
A2.9.3. The first step is to break the system into subsystems as shown by the circles in the above
diagram. A Roman numeral assigned to the R area identifies the subsystems. For example, the
subsystem string that includes the parallel components R1 and serial component R2 is identified
as subsystem RI. The parallel components R3 are shown as subsystem RII and the combination of
components of R1, R2, and R3 are contained in the subsystem designation RIII where:

Figure A2.3. Subsystem RI.
 104                                               ANGPAM21-106 10 NOVEMBER 2011


Figure A2.4. Subsystem RII.




Figure A2.5. Subsystem RIII.




   A2.9.4. The methods for determining system reliability as demonstrated above are only part
   of the weapon system evaluation process. They represent the state of heath for the period
   included in the data collection. They do not provide an indication of good or bad
   performance. In order to make decisions regarding performance, management needs to know
   what the requirements are. One method for determining the requirements for system
   reliability is by using the following formula:
                          (req)MTBCF=        Mission Duration
ANGPAM21-106 10 NOVEMBER 2011                                                                  105


                                                  -LN(probablity)
    A2.9.5. When the mission duration is the planned mission duration that must be achieved for
    mission success, the -LN is the negative natural logarithm. The probability is the probability
    needed to successfully complete the mission. For example, the combat air patrol (CAP)
    mission for an F-15 may be planned for an eight-hour mission. Systems must be operational
    that allow the aircraft to launch, navigate, communicate, air refuel, and ultimately engage and
    destroy targets that are deemed hostile. Using this basic mission profile we would see that
    the minimum required MTBCF for the aircraft would be 196 hours if we planned for a .96
    probability of meeting the mission objectives. Or;
                                            8 hours
                      (req)MTBCF=                      =196 hours
                                            -LN(.96)

    A2.9.6. If the combined system reliability for essential systems does not meet or exceed the
    196 hours, then management has a problem with system performance that must be
    investigated in further detail.
    A2.9.7. The following table contains pre-computed MTBCF requirements for a given
    probability and mission duration using the required MTBCF formula.
A2.10. Given Probability System Will Work For Mission Duration.

Table A2.1. Required MTBCF.
Probability
Mission.       .96        .95         .90        .85        .80        .75
Duration


12 hours       294        234         114        74         54         42
10 hours       245        195          95        62         45         35
8 hours        196        156          76        49         36         28
7 hours        171        136          66        43         31         24
6 hours        147        117          57        37         27         21
5 hours        122         97          47        31         22         17
4 hours         98         78          38        25         18         14
3 hours         73         58          28        18         13         10
2 hours         49         39          19        12         9           7
1 hour          24         19          9         6          4           3

A2.11. Evaluation Processes.
106                                                 ANGPAM21-106 10 NOVEMBER 2011


  A2.11.1. Now that we have some of the fundamentals of reliability assessment and how to
  determine basic reliability requirements, let's look at a mission scenario and break down an
  evaluation process.
 A2.11.2. First, define the mission. Our missions vary by weapon system and even within the
 weapon system. Multiple role aircraft require multiple mission scenarios and evaluations to
 determine abilities to meet the mission. Even dedicated mission aircraft (i.e. air-to-air
 combat only) have peacetime and wartime mission scenarios that impact differently upon the
 logistics systems supporting them. A good starting point is to review and understand the
 Minimum Essential Subsystem List (MESL) for your aircraft. It will provide a general
 starting point for you to develop specific operational scenarios and what is required to be
 operating.
 A2.11.3. Next, by using maintenance data collection (MDC) system failure rates, MDC can
 help to identify malfunctions or the impact of a failed component on system performance.
 The automated debriefing systems provide the best information to determine system
 performance. Specifically, use debriefing system capability data to develop MTBCF
 numbers used to assess mission success probabilities. Capability results should complement
 the MESL docks and provide sortie related data to support sortie related predictions.
  A2.11.4. Develop MTBCFs for each system needed to perform satisfactorily for the scenario
  you have developed. If you hadn‘t noticed, you need to be talking to the pilots to build the
  scenarios. As with any other statistical sample, the larger the data base, the more accurate
  the results are, and the greater the confidence in the results. Use as much data as possible,
  but be sensitive to past changes (TCTOs) that would skew the results. Eliminate bad data.
 A2.11.5. Finally, compare the scenario requirements with current performance. If there are
 shortfalls your task is just beginning. If not, management has a product that provides some
 sense of satisfaction in knowing that they can expect good results. Use the same information
 to help in determining where to place emphasis for improvements. Let‘s say the MTBCF for
 engines is 210 hours, and the MTBCF for landing gear is 100 hours. Initially, you might say
 that the landing gear system is a bigger problem than the engines. But, if the required
 MTBCF for landing gear is only 25 hours, it is in fact out producing requirements, while the
 engine system could be greatly under performing if its required MTBCF was 400 hours. For
 example:
ANGPAM21-106 10 NOVEMBER 2011              107


Figure A2.6. Combat Air Patrol Scenario.
 108                                                  ANGPAM21-106 10 NOVEMBER 2011


                                         Attachment 3
                    HELPFUL HINTS FOR DATA INVESTIGATION

A3.1. Building Narratives for Out of Standard Indicators:
   A3.1.1. Monthly Report: As a starting point.
       A3.1.1.1. What are the major contributing systems for downtime?
       A3.1.1.2. What are the major contributing systems for man-hour consumption?
       A3.1.1.3. What are the common write-ups within the major contributing systems?
       A3.1.1.4. Are there aircraft with multiple write-ups in the major contributing systems or
       different systems?
       A3.1.1.5. Do PRDs indicate a recent trend in system write-ups for major contributors?
       A3.1.1.6. Do PRDs indicate a recent trend in write-ups for a particular tail number
       within major contributing systems?.
       A3.1.1.7. What type of corrective actions were taken?
       A3.1.1.8. Do similar discrepancies still reappear?
       A3.1.1.9. Could cannibalizations have been a factor?
       A3.1.1.10. Did the aircraft/system cause problems with other maintenance indicators?
       A3.1.1.11. Is MICAP information available on aircraft with high supply down times?
       A3.1.1.12. How has the aircraft/system performed since the last incident? (PRD
       listing/IMDS Screen 174)
       A3.1.1.13. Are there any previously stated historical facts that apply that should be
       mentioned? (previous weeklies, studies, etc.)
       A3.1.1.14. Check repeat/recurs in an effort to identify actual component failures versus
       maintenance procedural, training, or skill level problems. This will involve contacting
       the shop responsible for the repair.
       A3.1.1.15. Are there any systems trends?
       A3.1.1.16. Are there technical data limitations?
       A3.1.1.17. Is there a lack of proper tools problem?
       A3.1.1.18. Add any other information you believe is pertinent to answer the particular
       maintenance indicator that you‘re writing about. Answer the questions, what is the
       problem, what is the unit doing to resolve the problem or what does the headquarters staff
       need to know to resolve the problem.
A3.2. Investigative Hints: Breaks, aborts, break rates, AWM, AWP, Phase/ISO Flows, Fleet
Time, and Canns are only a few examples.
A3.3. Technical Information: Always check with Deficiency Analysis if you have technical
questions. It‘s a good idea to review their summaries also. Sometimes they‘ve already gathered
ANGPAM21-106 10 NOVEMBER 2011                                                                  109


historical/current information on the same aircraft/systems you‘re looking at. You should also
make it a point to talk to shops concerning problem aircraft and components.
A3.4. Fix rates, write-ups exceeding the 4/8/12 hour requirement: You may be able to get
the average time it takes to troubleshoot and repair some items from Deficiency Analysts or the
shop responsible for repair.
A3.5. Supply: The MICAP section of Supply should have information on aircraft and
components with high supply times.
A3.6. Trends: There are various types of trends you should investigate. Detailed analysis will
depend on how much time is available to you and the type of data you‘re researching.
   A3.6.1. Are other units with like aircraft having similar problems?
   A3.6.2. Ask yourself; are the failures seasonal (more failures in hot/cold temperatures)?
   A3.6.3. Are the components failing environmentally sensitive (i.e., when the component
   goes from one temperature extreme to another, corroding, etc.)?
   A3.6.4. Could the failures be operating time related?
   A3.6.5. Do corrective actions point toward lack of training; or possibly the performing of
   workarounds due to lack of parts or tech data? Note. After reports and spreadsheets are
   updated is a good time to start looking at historical data for possible trends in current data.
A3.7. Break Rates:
   A3.7.1. Verify documentation of Code 3 aircraft.
   A3.7.2. Was debriefing done correctly?
   A3.7.3. Are landing times correct?
   A3.7.4. Are the proper break systems used?
   A3.7.5. Verify accuracy of aircraft status.
   A3.7.6. Are the start and stop times of NMC condition correct?
   A3.7.7. Does the work unit code match the Code 3 system on the debrief form?
   A3.7.8. Isolate the problem system.
   A3.7.9. If the verification of the documentation of Code 3 Aircraft and verification of
   aircraft status is done, what systems stand out?
   A3.7.10. How do these systems compare with past history?
   A3.7.11. Identify components within suspect systems.
   A3.7.12. Does the documentation seem reasonable?
   A3.7.13. Does the start and stop times for maintenance actions agree with the aircraft status
   times?
   A3.7.14. Follow the maintenance trail for problem items through the back shop.
   A3.7.15. Can you confirm the failure?
 110                                                  ANGPAM21-106 10 NOVEMBER 2011


   A3.7.16. Can you identify common repair actions in the shop?
   A3.7.17. Check for Repeat/Recur/CND actions.
   A3.7.18. Were they good fixes?
   A3.7.19. Is there a chance that some Code 3 actions are self-inflicted?
A3.8. Abort Rates:
   A3.8.1. Verify documentation of aborting aircraft.
   A3.8.2. Was debriefing done correctly?
   A3.8.3. Does the data correlate with the daily flying schedule?
   A3.8.4. Were the proper systems entered?
   A3.8.5. Are the proper ―When Discovered‖ codes used?
   A3.8.6. Are the proper abort cause codes used?
   A3.8.7. Isolate the problem.
   A3.8.8. What systems stand out?
   A3.8.9. How do these systems compare with past history?
   A3.8.10. Identify components within suspect systems.
   A3.8.11. Does the MIS documentation seem reasonable?
   A3.8.12. Do the start and stop times for maintenance actions agree with the aircraft status
   times?
   A3.8.13. Follow the maintenance trail for problem items through the back shop.
   A3.8.14. Can you confirm the failure?
   A3.8.15. Can you identify common repair actions in the shop?
   A3.8.16. Check for Repeat/Recur/CND actions.
   A3.8.17. Were they good fixes?
   A3.9.1. CANN Rates:
   A3.9.2. If Cannibalization logs are maintained, do they match what is documented in MIS?
   A3.9.3. Are there any obvious gaps in the cannibalization log (missing or incomplete data)?
   A3.9.4. Are there notes of cannibalization actions initiated but canceled?
   A3.9.5. Does it appear parts have been sitting around?
   A3.9.6. What‘s the average amount of time between a cannibalization action and when the
   part was received?
   A3.9.7. Isolate the problem.
   A3.9.8. What systems stand out?
   A3.9.9. How do these systems compare with past history?
ANGPAM21-106 10 NOVEMBER 2011                                                             111


  A3.9.10. Identify components within the suspect system.
  A3.9.11. Does the MIS documentation seem reasonable or is everything coded to the next
  higher assembly or subsystem level?
  A3.9.12. Try to associate the cannibalization action to a previous event or failure.
  A3.9.13. Can you confirm the failure?
  A3.9.14. Can you identify common repair actions in the shop?
  A3.9.15. Was the cannibalization action faster than removing/repairing/replacing the item?
  A3.9.16. Check for Repeat/Recur/CND actions.
  A3.9.17. Was the cannibalization action a good fix?
  A3.9.18. Did the cannibalization action only provide a partial fix?
  A3.9.19. Take your list of problem or suspect subsystems and components to Quality
  Assurance and the Technical Representatives.
  A3.9.20. Have the problems been previously identified?
  A3.9.21. Have there been QDRs, SRs, etc. submitted?
  A3.9.22. Are there pending MOD programs?
  A3.9.23. Identify Problem Aircraft.
  A3.9.24. Correlate your findings with other areas.
  A3.9.25. Is there a common thread with:
  A3.9.26. Causes for NMC/PMC conditions.
  A3.9.27. Overall system/component failures on your fleet.
  A3.9.28. Problem items in Base Self Sufficiency Program.
  A3.9.29. Overall Pilot Reported Discrepancies.
  A3.9.30. Air and Ground Abort causes.
  A3.9.31. Overall Repeat/Recur/CND problems/rates.
  A3.9.32. Are you tracking sufficient samples to get an accurate picture?
  A3.9.33. If updates to IMDS are backlogged or if MIS has been down, did you give plenty
  of time for data to be updated before taking your sample(s)?
  A3.9.34. Did you compare your sample or monthly average with past history?
  A3.9.35. Is there a large change in overall rates?
  A3.9.36. Is there a large change in overall rates from one month to another or from one
  sample to another?
  A3.9.37. Can problem aircraft be identified as a ―CANN Bird‖ or in an inspection or MOD?
  A3.9.38. Could the problem be tied to non-availability of MIS?
  A3.9.39. Could the problem be tied to a documentation problem?
112                                                  ANGPAM21-106 10 NOVEMBER 2011


  A3.9.40. If possible compare your data with other units with like aircraft and like missions.
  A3.9.41. Are your numbers comparable?
  A3.9.42. Are there similar trends?
  A3.9.43. Are there common system/component problems?
  A3.9.44. Document your findings and report them appropriately.
  A3.9.45. Key or important minimal items.
ANGPAM21-106 10 NOVEMBER 2011                                                                     113


                                          Attachment 4
                         MAINTENANCE ANALYSIS DATA REQUEST

Table A4.1. Maintenance Analysis Request.

Requester:                         Base/Office:                           Symbol:



Phone:                      Date of Request:                         Date Required:



Time Frame and Reason For Request:




Data Required:



SYS/WUC

Sorties Flown      Hours Flown

Hours:       Possessed     MC      TNMCM          TNMCS        PMCM                      PMCS
             PMCB
Rates:           Cannibalization      Break       4 Hr Fix     8 Hr Fix      12 Hr Fix    Abort
Flying Scheduling Effectiveness                   Utilization Rate
Other:




Additional Comments:
 114              ANGPAM21-106 10 NOVEMBER 2011




Analyst:         Date Completed:


Analyst Hours:   Provided To:
Remarks:
ANGPAM21-106 10 NOVEMBER 2011                                                                                        115


                                                       Attachment 5
                              SAMPLE ATTRITION SPREADSHEET

A5.1. Create one of these tables for each year of data available. The more data you use the more
accurate your attrition will be.

Table A5.1. Yearly Attrition Spreadsheet.
FY                   Oct    Nov        Dec       Jan     Feb    Mar        Apr    May     Jun     Jul    Aug    Sep
                     07     07         07        08      08     08         08     08      08      08     08     08

Sorties Sched        48     38         57        54      46     66         58     45      61      65     55     5

Maint canx                                               3      2          0              2       2

Ops canx                                                 3                 1      4       4       1

Supply canx

hhq / other Cnx

Wea canx             1                 3         1       3                 2      1       8       1

                            38         57        54      46     66         58     45      61      65     55     5

A5.2. Create a spreadsheet adding each category by each month. (eg., Sum all October Sorties
scheduled into one figure). This is a sample of just one quarter. You need one for each quarter.
Total all the numbers for a yearly total.

Table A5.2. Quarterly Attrition Spreadsheet.
As of: April 2006           130 AW / ATTRITION FACTORS FY 98-FY 06                                    C-130H
           Sorties       Maintenance             Hhq / Other               Weather             Operations       Monthly
                                                                                                                 Attr.
Month      Sched     Canx        Factor      Canx        Factor      Canx        Factor   Canx         Factor
                                                                                                                Factor


OCT        412       7           .02         0           0.00        14          .03      11           .03          0.08


NOV        356       9           .03         0           0.00         18         .05      9            .03          0.10


DEC        375       7           .02         0           0.00         19         .05      9            .02          0.09
116                                ANGPAM21-106 10 NOVEMBER 2011


QTR   1143   23   .02   0   0.00   51   .04   29    .03     0.09
ANGPAM21-106 10 NOVEMBER 2011                                                                 117


                                         Attachment 6
                   EMERGENCY AIRCRAFT INCIDENT CHECKLISTS

Table A6.1. Emergency Aircraft Incident Checklist (May be modified to meet unit
requirements – this is only a sample).
                                                 IMDS and G0-81.
                                   Emergency Aircraft Incident Checklist.
Check When
 Completed         (May be modified to meet unit requirements – this is only a sample)
              1.    After notification from the Impoundment Official IIAW AFI 21-101 para
                    11.5.5) to lock out an Aircraft due to an incident, process the following steps
                    from the ADS Remote:
              2.    Place IMDS in FUD Mode (IMDS Screen 891/G081 Screen 901)
              3.    Run a QMH (IMDS Screen 105/No screen for G081)
                    a.   Process the screen with these inputs:
                         Equip ID – AXXXX (XXXX=Aircraft Tail Number)
                         Check – On and In-Shop Work
                             Performing Work Center.
                             01-09 WUC.
                             TCTO Data.
                             Events Narrative.
                             Detailed Data Record Narrative
                         Start Date – Enter 2 years from today‘s date.
                         Stop Date – Enter today‘s Julian Date
              4.    Run SHD (IMDS Screen 510/G081 Screen 9013, 9035, 9037)
                    a.   Process the screen with the following inputs
                         Specified Item and Installed Items.
                         Stop Date – Enter today‘s Julian date.
                         Equip ID – AXXXX(XXXX=Aircraft Tail Number)
              5.    Run a TRE (IMDS screen 429) (G081 screen 9005/9014)
                    a.   Process the screen with shown inputs
                         Check - Display WUC/LCN Summary.
                         Check - Suppress Open Events (Open JDD Data)
                         Check - Display Events in History (Completed JDD Data)
118                                                  ANGPAM21-106 10 NOVEMBER 2011


                                                IMDS and G0-81.
                                  Emergency Aircraft Incident Checklist.
Check When
 Completed        (May be modified to meet unit requirements – this is only a sample)
                        Equip ID – AXXXX (XXXX = tail number)
             6.    Run a ARC (IMDS screen 418) (G081 screen 9032D/67028)
                   a.   Process the screen with shown inputs
                        Equip ID – AXXXX(XXXX= Tail Number)
             7.    Run a PRA (IMDS screen 396) (G081 no screen found)
                   a.   Process the screen with shown inputs
                        Type Equipment – Equip ID.
                        Equipment Request – AXXXX(XXXX= Tail Number)
             8.    Run a DDL (IMDS screen 181) (G081 no screen found)
                   a.   Process the screen with shown inputs
                        Type of Report - Detail.
                        Start Date – 90 days from today‘s date.
                        Stop Date – Today‘s Julian date.
                        Sort by – WUC/LCN and Deviation Cause.
                        Equip ID – AXXXX (XXXX = tail number)
             9.    Run a STL (IMDS screen 533) (G081 screen 8027/8040)
                   a.   Process the screen with shown inputs: (Report - all TCTOs)
                        Report Start Date – 91001.
                        Report Stop Date – Enter today‘s Julian date.
                        Equip ID – AXXXX(XXXX= Tail Number)
             10. Run a FTR (IMDS screen 185) (G081 CODE3A)
                   a.   Process the screen with shown inputs
                        Start Date – 90 days from today‘s date.
                        Stop Date – Today‘s Julian date.
                        Select - Equip Id , Equip ID – AXXXX (XXXX = tail number)
             11. Run a PRD (IMDS screen 179) (G081 screen 67117PRD)
                   a.   Process the screen with shown inputs
                        Type Report – Detail.
                        Start Date – 90 days from today‘s date.
ANGPAM21-106 10 NOVEMBER 2011                                                          119


                                             IMDS and G0-81.
                                 Emergency Aircraft Incident Checklist.
Check When
 Completed      (May be modified to meet unit requirements – this is only a sample)
                      Stop Date – Today‘s Julian date.
                      Sort by – Equip ID.
                      Equip ID – AXXXX(XXXX= Tail Number)
             12. Run Documented Maintenance Inquiry (IMDS screen 380) (G081 screen
                 8035/8044)
                 a.   Type Event – Option 6 – All Deferred Unscheduled and Scheduled
                      Events
                 b.   Equip ID – AXXXX (XXXX = tail number)
                 c.   Save output as text file
             13. Run Summarized/Detailed Status Inquiry for an Equipment ID (IMDS screen
                 460) (G081 screen 8047)
                 a.   Format – Detailed
                 b.   Equip ID – AXXXX (XXXX = tail number)
                 c.   Start Date – 90 days from today‘s date
                 d.   Period of report – 90 days
                 e.   Save output as text file
             14. Run Shop Equipment Operational Inquiry (IMDS screen 726) (G081 screen
                 8038/9025B)
                 a.   Type Transaction – Utilization
                 b.   Equip ID – AXXXX (XXXX = tail number)
                 c.   Start Date – 31 days from today‘s date
                 d.   Period of Report – 31 days
                 e.   Save output as text file
             15. Run Shop Equipment Operational Inquiry (IMDS screen 726) (G081 screen
                 8038/9025B)
                 a.   Type Transaction – History
                 b.   Equip ID – AXXXX (XXXX = tail number)
                 c.   Start Date – 31 days from today‘s date
                 d.   Period of Report – 31 days
                 e.   Save output as text file
120                                                ANGPAM21-106 10 NOVEMBER 2011


                                             IMDS and G0-81.
                                 Emergency Aircraft Incident Checklist.
Check When
 Completed      (May be modified to meet unit requirements – this is only a sample)
             16. Run Parts Tracked Inquiry (IMDS screen 810) (G081 screen 8110)
                 a.   Equip ID – AXXXX (XXXX = tail number)
                 b.   Installed on Chain – Y-List All Installed On Levels
                 c.   Save output as text file
             17. Run Debriefing Sortie Recap (IMDS screen 174) (G081 screen 9020/67034)
                 a.   Equipment ID – AXXXX (XXXX = tail number)
                 b.   Check – Detailed Report
                 c.   Check – Include JDD
                 d.   Check – Include Status
                 e.   Start Date – 7 days from today‘s date
                 f.   Stop Date – Enter today‘s Julian date
                 g.   Save output as text file
             18. Run 781 Automated Forms Discrepancies Inquiry (IMDS screen 538) (G081
                 screen 9032D)
                 a.   Discrepancy Type – All
                 b.   Equip ID – AXXXX (XXXX = tail number)
                 c.   Save output as text file
             19. Equipment Record Inquiry (IMDS screen 824) (G081 screen 67191)
                 a.   Check – Eqpp-158
                 b.   Check – Scheduled
                 c.   Start Date – 90 days from today‘s date
                 d.   Check – Unscheduled
                 e.   Save output as text file
                 f.   Repeat and Check – History
                 g.   Save output as text file
             20. Download all files to DEPCON. Verify all files are correct
             FILE FILENAME.                           .
             Maintenance History (QMH)                8FS35P.
             Significant Historical Data (SHD)        8FS70P.
ANGPAM21-106 10 NOVEMBER 2011                                                           121


                                              IMDS and G0-81.
                                 Emergency Aircraft Incident Checklist.
Check When
 Completed        (May be modified to meet unit requirements – this is only a sample)
             Transfer Record for Equipment (TRE)     8FS14P.
             Automated Records Check (ARC)           8FS34P.
             Planning Requirements (PRA)             8FS18P.
             Deviation Detail Listing (DDL)          8FS3BP.
             Serial Number Detail Listing (STL)      8FS28P.
             Code 3 Fix Time Report (FTR)            8FS2TP.
             Pilot Reported Discrepancies (PRD)      8FSISP.


             21. Convert all files to PDF
             22. Email files (PDF and text) to QA
             23. Call OKC and have them save the database to a square tape. Write down the
                 tape numbers. Record Tape Reel Numbers
             a.                                        b.
             c.                                        d.


                  If additional reports are required, contact OKC to have the above tape
                  numbers loaded to an empty ELC. Find out which ELC and contact SSG to
                  gain access.
             24. Bring the system out of FUD using the same User-Id and Terminal Id that put
                 the system in FUD (TISX1)
             25. Contact MOCC and the Impoundment Official that all steps are completed
             26. The MOCC (or the MDSA) will lock/unlock records for impoundment or
                 investigation using program 9012
             27. The MOCC will follow local Emergency Action procedures.
             28. The MDSA will run programs 8005, 8035, 8038 and any other requested data
                 from Quality Assurance. All equipment records will be gathered and
                 maintained in the security of the MOCC. The Investigation Team will also
                 have inputs and requests that may vary from incident to incident.
                                                                     Attachment 7
                                                         7401 PROCEDURAL GUIDANCE

            Table A7.1. 7401 Procedural Guidance.
Unit:                                   User Name and Unit fields are automatically populated according to logon.
Primary Analyst:
MDS:
Month/Year (mm/yyyy):


METRIC                                  METRIC DEFINED                       SOURCES FOR DATA                       REFERENCE
* = Required field                                                           G0-81                    IMDS

                                        Determined by ANG/XOOH for                                                  AFI 21-101_ANGSup Chapter
FH Allocated *                                                               PS&D / Operations
                                        specific unit requirements                                                  15 & AFI21-103 Chapter 2


                                        FH Scheduled & Sorties
                                        Scheduled: PS&D will plan a
                                        monthly flying schedule based on
                                        the following: no later than 20
                                        August of each year or within 10
FH Scheduled *                          working days after receipt of the                   PS&D / Operations
                                        ANGRC unit flying hour
                                        allocation; flying days in each
                                        month, required flying hours and
                                        estimated sorties and missions is
                                        provided to PS&D by Ops.
             ANGPAM21-106 10 NOVEMBER 2011                                                     123


                                                                                                                      Reports should not be run until
                                                                                                                      the 5th of the month to
                                                                                                                      accommodate any "late"
                                  Actual flying hours executed for   Program 67034 (Flying   TRIC AUR, Screen 362,    reporting of flying time (aircraft
FH Flown *
                                  the month.                         Hour Audit)             Option B.                cross-country).Verify with
                                                                                                                      Scheduler after flying hours
                                                                                                                      have been reconciled and
                                                                                                                      approved by Ops.

Sorties Scheduled *               See FH Scheduled
                                                                                                                      Verify with Scheduler after
                                  Actual sorties executed for the    Program 67034 (Flying   TRIC AUR, Screen 362,    flying hours have been
Sorties Flown *
                                  month.                             Hour Audit)             Option B                 reconciled and approved by
                                                                                                                      Ops.
Maint Cancels *
                                  Attrition Rates: Represent the
Ops Cancels *                     unit‘s historical seasonal/historical                      TRIC DPR, Screen 182.
                                  variations and allows for projected Provided by MOCC       Validate input against   ANGPAM 21-103
Weather Cancels *                 losses due to maintenance, Ops,                            monthly AUR
                                  weather, and other calculated losses
Other Cancels *
               124                           ANGPAM21-106 10 NOVEMBER 2011



                     Aircraft that are under the control
                     of their owning base are possessed
                     by that organization. An aircraft
                     that flies to depot for
                     maintenance/inspection or is
                     repaired by a depot team at the base
Possessed Hours *                                         Program 9025             TRIC EST, Screen 459    AFI 21-101_ANGSup
                     is temporarily possessed by depot.
                     In calculating the various aircraft
                     maintenance metrics, possession is
                     calculated in units of hours
                     normally for specific time periods
                     (e.g., monthly, annual, etc.)



                                                            Program 67117, When    TRIC AUR, Screen 362,   AFI 21-101_ANGSup Chapter
                     An aircraft or sortie that cannot
                                                            Discovered code A in   Option B Section1;      1.10, Metrics Handbook for
Air Aborts *         complete its primary or alternate
                                                            column 57; verify      verify against TRIC     Mobility Air Forces 2nd Edition,
                     mission for any reason
                                                            against abort log.     DRP, Screen 182         ANGPAM 21-103



                                                            Program 67117, When    TRIC AUR, Screen 362,   AFI 21-101_ANGSup Chapter
                     An event that prohibits the aircraft
                                                            Discovered code C in   Option B Section1;      1.10, Metrics Handbook for
Ground Aborts *      from becoming airborne, after
                                                            column 57; verify      verify against TRIC     Mobility Air Forces 2nd Edition,
                     aircrew arrival
                                                            against abort log      DRP, Screen 182         ANGPAM 21-103


                                                                                                           AFI 21-101_ANGSup Chapter
                     In most cases, a cann action takes                            TRIC QKB, Screen 104.
                                                                                                           1.10, Metrics Handbook for
                     place when base supply cannot          Focus FMDSCANN &       Use Grand total of
                                                                                                           Mobility Air Forces 2nd Edition,
Cannibalizations *   deliver the part when needed and       FENGCANN; verify       Action Taken T for
                                                                                                           ANGPAM 21-103, Maintenance
                     mission requirements demand the        against cann log       number of
                                                                                                           Metrics U.S. Air Force
                     aircraft be returned to an MC status                          cannibalizations.
                                                                                                           20Dec01
           ANGPAM21-106 10 NOVEMBER 2011                                                         125


                                  Total number of man-hours
Canns "T" & "U" Man-Hours Exp *                                               Sum all Cann T & U hours expended.
                                  expended for cann T & U actions
                                                                                               TRIC EST, Screen 460,
MC Hours *                        Total of FMC and PMC hours             Program 9025
                                                                                               Format 1
                                  Hours an aircraft is fully mission
FMC Hours *                                                                       "                       "
                                  capable

                                  Hours aircraft can do at least one,
                                  but not all of its assigned missions
PMCB Hours *                                                                      "                       "
                                  because of maintenance and supply
                                  at the same time

                                  Hours aircraft can perform at least
                                  one, but not all, assigned missions
PMCM Hours *                      because of maintenance                          "                       "
                                  requirements existing on the
                                  inoperable subsystem(s)

                                  Hours aircraft can perform at least
                                  one, but not all, assigned missions
PMCS Hours *                      because maintenance required to                 "                       "
                                  clear the discrepancy cannot
                                  continue due to a supply shortage

                                  Hours the aircraft cannot do any
                                  assigned missions because of
NMCB-U Hours *                                                                    "                       "
                                  supply and unscheduled
                                  maintenance

                                  Hours the aircraft cannot do any
NMCB-S Hours *                    assigned missions because of                    "                       "
                                  supply and scheduled maintenance
             126                                              ANGPAM21-106 10 NOVEMBER 2011


                                     Hours the aircraft cannot do any
NMCM-U Hours *                       assigned missions because of                       "                       "
                                     unscheduled maintenance

                                     Hours the aircraft cannot do any
NMCM-S Hours *                       assigned missions because of                       "                       "
                                     scheduled maintenance

                                     Hours the aircraft cannot do any
NMCS Hours *                         assigned missions because of                       "                       "
                                     supply
                                     Discrepancies reported during           Program 67117,           TRIC PRD, Screen 179,
Pilot Reported Discrepancy *
                                     debrief                                 columns 55-57 blank      Option D


                                     Aircraft or system has major
                                     discrepancies in mission essential
                                     equipment that may require
                                     extensive repair or replacement                                                          AFI 21-101_ANGSup Chapter
                                     prior to further mission assignment.    Program 67070,                                   1.10, Metrics Handbook for
                                                                                                      TRIC FTR, Screen 185,
                                     The discrepancy may not affect          column 1 option 4.                               Mobility Air Forces 2nd Edition,
Verified Landing Status Code-3's *                                                                    verify against TRIC
                                     safety-of-flight and the aircraft may   Verify against Debrief                           ANGPAM 21-103, Maintenance
                                                                                                      SSE, Screen 180
                                     be NMC flyable. When you find           Logs in MOCC.                                    Metrics U.S. Air Force
                                     an aircraft discrepancy during                                                           20Dec01
                                     flight, maintenance status starts at
                                     the time the aircraft returns to its
                                     parking spot/engine shutdown.


                                     A repeat discrepancy is when the
                                     same malfunction of a                                                                    AFI 21-101_ANGSup Chapter
                                     system/subsystem occurs on the                                                           1.10, Metrics Handbook for
Repeat Write-ups *                                                           Program 67089            TRIC PRD, Screen 179
                                     next sortie or attempted sortie after                                                    Mobility Air Forces 2nd Edition,
                                     corrective action, (this includes                                                        ANGPAM 21-103
                                     CNDs/no-defect-noted, etc).
            ANGPAM21-106 10 NOVEMBER 2011                                                            127



                                 Mean Time Between Maintenance:
                                 Also known as Mean Time
                                                                                                   TRIC QFA, (Screen
                                 Between Failure, this metric is a
                                                                                                   317). Select 1, 2, 3
                                 measure of how well a particular
                                                                                                   options for the specific
Total Failures *                 item/system performs and provides      Program 67117
                                                                                                   month. Add the number
                                 an average equipment operating
                                                                                                   of "Failures" and
                                 time before a failure occurs; a
                                                                                                   "Induced Failures".
                                 lower/negative MTBM indicates a
                                 poor reliability


                                 Work unit codes do not cover most
                                                                                                   TRIC QMH, (Screen
                                 non-repairable items; therefore,
                                                                                                   105). Select MDS, AT
                                 when items such as those are
                                                                                                   = G, MCC Code = A &
AT ="G" Failures *               repaired or replaced, this AT code   Program 67117                                           G Failures: Sum all G failures
                                                                                                   B, On-equipment, and
                                 will be used. The WUC will
                                                                                                   Report Sequence "S",
                                 identify the assembly being directly
                                                                                                   for the month.
                                 related to parts being repaired.

                                 Number of engine changes               Program 67015.             TRIC QER, Screen 108.
Unscheduled Engine Removals *    accomplished for any reason other      Verify with Engine         Verify with Engine         AFI 21-101_AMCSUP1
                                 than scheduled removal                 Manager and CEMS           Manager and CEMS
                                 The primary objective of the           From Monthly Flying        TRIC TDI, Screen 400,
                                 phased/ISO inspections is to           and Maintenance Plan       Format 2. Use hours
                                 minimize the length of time that an    ISO Inspections            remaining divided by
                                 aircraft is out of commission due to   section, sum days          cycle, multiply by 100
                                 a scheduled inspection. This           remaining (excluding
PCT Fleet Time *                 metric signifies the percentage of     PDM aircraft), divide
                                 time remaining till next ISO           this number by
                                 inspection for possessed aircraft      Average Possessed
                                 currently on base.                     aircraft, divide by days
                                                                        in inspection cycle;
                                                                        multiply by 100.
             128                                           ANGPAM21-106 10 NOVEMBER 2011


                                   Is a measurement of the amount of
                                   days from the last flight of an                                     TRIC SAE, (Screen          If more than one aircraft
                                                                          Run program 8038 for
                                   aircraft entering into ISO                                          726), for last and first   completes an ISO inspection in
                                                                          last and first flight days
                                   inspection to the first flight of an                                flight days for the        a reporting month, average the
Fly to Fly Days *                                                         for the particular
                                   aircraft after ISO inspection.                                      particular aircraft.       days by the number of aircraft.
                                                                          aircraft. Subtract the
                                   Calculations based on aircraft that                                 Subtract the last from     If no aircraft were returned to
                                                                          last from first.
                                   have completed ISO and flown                                        first.                     service leave this field blank.
                                   within the given month.

                                                                          Program 67150. Run           TRIC EVL (Screen 539)
                                                                          reports each week.           Run reports each week.     Metrics Handbook for Mobility
Total Number AWM Discrepancies *   A higher than normal AWM may
                                                                          Avg total AWM                Avg total AWM              Air Forces 2nd Edition
                                   be due to poor scheduling
                                                                          discrepancies.               discrepancies.
                                   discipline, if left unfixed after
                                   major scheduled maintenance,
                                                                          Program 67150. Run           TRIC EVL (Screen 539)
                                   could cause extended PMC or
                                                                          reports each week.           Run reports each week.     Metrics Handbook for Mobility
Total Number AWP Discrepancies *   NMC conditions.
                                                                          Avg total AWP                Avg total AWP              Air Forces 2nd Edition
                                                                          discrepancies.               discrepancies.

                                   This is the total number of sample     Program 67117, DIT           TRIC QMH, screen 105       AFI 21-101_ANGSup,
                                   documents reviewed by the Data         will identify errors on      to pull Job Data           ANGPAM 21-103
                                   Integrity Team (DIT). A ‗sample        the report and give to       Documentation (JDD)
                                   document‘ refers to a single DDR       appropriate squadron         for the DIT to review
Documents Sampled *                (Documentation Data Record),           for correction.              each month.
                                   within a WCE (WorkCenter
                                   Event), within a JCN (Job Control
                                   Number).
            ANGPAM21-106 10 NOVEMBER 2011                                                             129



                                                                                                                              Individual errors found within a
                                                                                                                              single DDR are not summed in
                                                                                                                              this statistic. A single DDR
                                                                                                                              contains multiple possible errors
                                                                                                                              (i.e. wrong ATC (Action Taken
                                      This is the total number of sample
                                                                                                                              Code), wrong WUC (Work Unit
                                      documents reviewed by the DIT
                                                                                                                              Code), missing T.O. (Technical
Documents Rejected *                  that were found to be in error. This
                                                                                                                              Order) reference, insufficient
                                      is not the total number of
                                                                                                                              Corrective Action narrative,
                                      individual errors found.
                                                                                                                              etc.). A single DDR with
                                                                                                                              multiple possible errors still
                                                                                                                              counts as 1 ‗sample document‘
                                                                                                                              (DDR) rejected and found to be
                                                                                                                              in error.

                                      Time Percent of Assigned Aircraft      Find days remaining    TRIC TDI, Screen 400.
                                      Within 30 percent of No                per aircraft from      Calculate percentage
                                      Remaining Time To Inspection:          current P&S Monthly    using formula in Metric
                                      Provide the percent of assigned        Maintenance            Defined column.
                                      aircraft within 30% of running out     Summary. Do not
Acft Below 30% Time to Inspection *   of flying time. Phase Cycle/ISO x      include data for PDM                             ANGPAM 21-103
                                      .30                                    aircraft. Calculate
                                                                             percentage using
                                                                             formula in Metric
                                                                             Defined column.

                                      This is a leading indicator that
Maintenance Points Earned *           measures success in the unit‘s
                                      ability to plan and complete
                                                                             PS&D                   PS&D                      ANGPAM 21-103
                                      inspections and periodic
Maintenance Points Possible *         maintenance on-time per the
                                      maintenance plan.
              130                                                  ANGPAM21-106 10 NOVEMBER 2011


CALCULATED METRIC                           FORMULA
FH Allocated vs. Executed                   FH Flown - FH Allocated
Avg No. of Possessed Acft                   Possessed Hours / (Flydays * 24)
Code-3 Break Rate                           (Verified Landing Status Code-3 / Sorties Flown) * 100
Data Integrity (Pct GOOD)                   (Documents Rejected / Documentd Sampled) * 100
Sorties Scheduled vs. Sorties Flown         (Sorties Scheduled / Sorties Flown) * 100
ASD (Average Sortie Duration)               FH Flown / Sorties Flown
Monthly Attrition Rate                      ((Maint Cancels + Ops Cancels + Weather Cancels + Other Cancels) / Sorties Scheduled) * 100
Hourly UTE Rate                             (FH Flown / (Possessed Hours / (Flydays * 24))) * 100
Sortie UTE Rate                             (Sorties Flown / (Possessed Hours / (Flydays * 24))) * 100
Air Abort Rate                              (Air Aborts / Sorties Flown) * 100
Ground Abort Rate                           (Ground Aborts / Sorties Flown) * 100
Cann Rate Per 100 Sorties                   (Cannibalizations / Sorties Flown) * 100
MC Rate                                     ((FMC Hours + PMCB Hours + PMCM Hours + PMCS Hours) / Possessed Hours) * 100
FMC Rate                                    (FMC Hours / Possessed Hours) * 100
TPMCM Rate                                  ((PMCB Hours + PMCM Hours) / Possessed Hours) * 100
TPMCS Rate                                  ((PMCB Hours + PMCS Hours) / Possessed Hours) * 100
TNMCM Rate                                  ((NMCB-U Hours + NMCB-S Hours + NMCM-U Hours + NMCM-S Hours) / Possessed Hours) * 100
TNMCS Rate                                  ((NMCB-U Hours + NMCB-S Hours + NMCS Hours) / Possessed Hours) * 100
Repeat Rate                                 (Repeat Write-ups / Pilot Reported Discrepancy) * 100
MTBM (Exc. "G") Failures                    ((FH Flown / (Total Failures - AT="G" Failures)) * 100
Avg No. Delayed Discrepancies Per Acft      ((Total AWM discrepancies + Total AWP discrepancies) / ((Possessed Hour / (Fly days*24)))
Maintenance Scheduling Effectiveness Rate   (Maintenance Points Earned / Maintenance Points Possible) * 100
           ANGPAM21-106 10 NOVEMBER 2011                                                                 131


QA
                                     The info for the following fields is provided to MDSA by QA. QA is required to report this quarterly. It is the units choice
AMXS MSEP Inspections Pass *
                                     to report
AMXS MSEP Inspections Fail *         monthly or fill in the data on the last month of each quarter, Dec, Mar, Jun, Sep.
AMXS MSEP Violations *
MXS/CMF MSEP Inspections Pass *
MXS/CMF MSEP Inspections Fail *
MXS/CMF MSEP Violations *
MXS/EMF MSEP Inspections Pass *
MXS/EMF MSEP Inspections Fail *
MXS/EMF MSEP Violations *
MXG/MOF/QA MSEP Inspections Pass *
MXG/MOF/QA MSEP Inspections Fail *
MXG/MOF/QA MSEP Violations *


QA Calculated Fields                 FORMULA
AMXS MSEP Score                      (((AMXS Pass / (AMXS Pass + AMXS Fail) * 100 ) - (AMXS Violations * .5)))
MXS/CMF MSEP Score                   (((MXS/CMF Pass / (MXS/CMF Pass + MXS/CMF Fail) * 100 ) - (MXS/CMF Violations * .5)))
MXS/EMF MSEP Score                   (((MXS/EMF Pass / (MXS/EMF Pass + MXS/EMF Fail) * 100 ) - (MXS/EMF Violations * .5)))
MXG/MOF/QA MSEP Score                (((MXG/MOF/QA Pass / (MXG/MOF/QA Pass + MXG/MOF/QA Fail) * 100 ) - (MXG/MOF/QA Violations * .5)))
