BY ORDER OF THE COMMANDER                      AIR FORCE NUCLEAR WEAPONS CENTER
AIR FORCE NUCLEAR WEAPONS                                      INSTRUCTION 63-111
CENTER KIRTLAND AIR FORCE BASE
                                                                           4 FEBRUARY 2011

                                                                                    Acquisition

                                                        VERIFICATION AND VALIDATION



             COMPLIANCE WITH THIS PUBLICATION IS MANDATORY

ACCESSIBILITY: ACCESSIBILITY: Publications and forms are available for downloading
               or ordering on the e-Publishing website at www.e-Publishing.af.mil.

RELEASABILITY: There are no releasability restrictions on this publication.

OPR: AFNWC/EN                                                         Certified by: AFNWC/EN
                                                                             (Dr. Yolanda King)
                                                                                      Pages: 13


This Verification & Validation (V&V) Instruction establishes policies and procedures for
conducting V&V for all products and products components produced or managed by Air Force
Nuclear Weapons Center (AFNWC). This Instruction establishes and documents process
stakeholders, roles and responsibilities of V&V activities in accordance with Air Force Materiel
Command Instruction (AFMCI) 63–1201, Implementing Operational Safety, Suitability and
Effectiveness (OSS&E) and Life Cycle Systems Engineering (LCSE). This instruction applies to
all units within all AFNWC organizations except for the 377th Air Base Wing. This instruction
does not apply to Air Force Reserve Command (AFRC) units. This instruction does not apply to
Air National Guard (ANG). Refer recommended changes and questions about this publication to
the Office of Primary Responsibility (OPR) using the Air Force (AF) Form 847,
Recommendation for Change of Publication; route AF Form 847s from the field through the
appropriate functional‘s chain of command. Ensure that all records created as a result of
processes prescribed in this publication are maintained in accordance with Air Force Manual
(AFMAN) 33–363, Management of Records, and disposed of in accordance with Air Force
Records Information Management System (AFRIMS) Records Disposition Schedule (RDS)
located at https://www.my.af.mil/afrims/afrims/afrims/rims.cfm.            This is the initial
publication of this instruction. See Attachment 1 for a Glossary of References and Supporting
Information.

1. Applicability.
   1.1. Scope. This instruction establishes guidance for demonstrating requirements
   traceability by conducting verification (the process of determining that product accurately
   meets specifications) & validation (the process of determining that product accurately meets
 2                                                          AFNWCI63-111 4 FEBRUARY 2011


     user needs) for all programs and products managed by AFNWC and subordinate
     organizations. It establishes the guidelines, policies and procedures for AFNWC personnel
     who develop, review, approve, or manage V&V for systems, subsystems, end–items, models,
     and services.
     1.2. General Applicability. This instruction applies to all products and products
     components produced or managed by AFNWC. Additional direction for all V&V activities
     may be found in Air Force Instruction (AFI) 99 – 103, Capabilities–Based Test and
     Evaluation. Further guidance may be found in the Defense Acquisition Guidebook, AFMAN
     63–119, Certification of System Readiness for Dedicated Operational Testing, and also OSD
     System Engineering Plan (SEP) Preparation Guide, Version 2.01. Legacy systems V&V
     activity may include flight tests and/or static tests for applicability and compliance.
     1.3. Technical Order (TO) Verification Applicability. TO verification will be performed
     in accordance with TO 00-5-3, Air Force Technical Order Management. This document
     does not amend, enhance, or revise TO V&V processes or requirements.
     1.4. Software, Modeling & Simulation Applicability. To the extent practical, this
     instruction also applies to software-only products which include modeling and simulation.
     This instruction applies to internally developed models and simulations used for internal
     programmatic decisions and trade studies. Internally developed or sponsored modeling and
     simulation tools shall adhere to V&V requirements specified in AFI 14–206, Modeling and
     Simulation, AFI 16–1001, Verification, Validation and Accreditation, AFI 16–1002,
     Modeling and Simulation to Support Acquisition, and Military Standard (MIL-STD) 3022,
     Documentation of Verification, Validation, and Accreditation (VV&A) for Models and
     Simulations. This instruction does not address accreditation.
     1.5. Critical Program Information (CPI) Applicability. This instruction does not relieve
     programs or products which have CPI requirements. These must V&V implemented Anti-
     Tamper methods according to CPI and Anti-Tamper guidance and requirements established
     in DoD Instruction (DODI) 5200.39, Critical Program Information (CPI) Protection Within
     the Department of Defense.
     1.6. First Article Testing. This instruction does not apply to first article testing.
     1.7. Precedence. In accordance with AFI 33-360, Publications and Forms Management,
     paragraph 2.3, statutory law, Federal, Department of Defense, Joint Staff, United States Air
     Force (USAF), Undersecretary of Air Force for Acquisitions (SAF/AQ), and Air Force
     Materiel Command guidance, instructions, and directives take precedence over this
     instruction.
2. Roles and Responsibilities.
     2.1. AFNWC Roles & Responsibilities. The AFNWC Commander (AFNWC/CC) shall
     designate a Center Test Authority. The AFNWC/CC shall designate an approval authority
     for acceptance for validation of organizationally developed modeling and simulations
     without external customers.
     2.2. Subordinate organization responsibilities. The V&V activities shall be accomplished
     by coordinating efforts between engineering, financial management, contracting, testing,
AFNWCI63-111 4 FEBRUARY 2011                                                                3


  logistics, and program management functional areas within the applicable organization or
  appropriate program office.
  2.3. Program Manager (PM). The PM shall develop, document, and execute an overall
  V&V plan. This integrated testing plan shall be documented in a Test & Evaluation Master
  Plan (TEMP), or similar appropriate program document. The PM shall establish the V&V
  plan and execute its implementation as practical.
     2.3.1. The PM shall provide adequate resources for performing the V&V process,
     developing work products, and providing the necessary services of the process as and
     when needed. Resources will include adequate funding, appropriate physical facilities,
     skilled personnel, and appropriate tools.
     2.3.2. The PM shall assign responsibility and authority for performing the V&V
     activities, developing work products, and providing the services of the process. The PM
     will assign and document responsibility using detailed job descriptions and/or project
     documents, and ensure designated personnel have documented authority to perform the
     responsibilities assigned. The PM may assign a lead or responsible engineer for each
     requirement in the Requirements Verification Matrix (RVM).
     2.3.3. The PM shall assure required training for members of the project team who both
     perform and support V&V.
  2.4. Chief Engineer (CE). The CE will monitor activities to ensure compliance with this
  instruction and the program‘s V&V plan. The CE may designate a program V&V POC for
  monitoring this process.
     2.4.1. The CE shall execute and maintain plans for performing the V&V process. The
     TEMP shall describe the activities, resources, and schedule required to accomplish this.
     2.4.2. The CE shall monitor and control the V&V process, ensuring:
         2.4.2.1. The process is performed as describe and planned,
         2.4.2.2. The process yields the determinate outcomes/products according to an
         objective evaluation of work products against the applicable standards, and
         2.4.2.3. Issues are identified and necessary corrective actions are taken.
     2.4.3. The CE shall review the activities, status and results of the V&V process with the
     PM, and higher level management as necessary, and resolve any identified problems.
     The CE shall facilitate peer review on appropriate work products. Following process
     reviews, the CE or designated program V&V POC will coordinate with the AFNWC
     V&V Process POC and provide recommendations to the PM regarding policy, process
     and resources to facilitate improvement.
  2.5. War fighter. Air Force Global Strike Command (AFGSC) and United States Air
  Forces in Europe (USAFE) are the primary users of the systems developed and supported by
  AFNWC and may be directly involved in all V&V activities. All pertinent information
  concerning V&V will be provided to AFGSC and USAFE to aid in decision–making.
  2.6. Developmental Test & Evaluation (DT&E) Organizations. DT&E organizations
  shall be designated by the system program office to test performance requirements as
  indicated in the system test plan.
 4                                                        AFNWCI63-111 4 FEBRUARY 2011


     2.7. Operational Test & Evaluation (OT&E) Organizations. OT&E organizations shall
     test overall system integration and operational performance and effectiveness as indicated in
     the TEMP or similar appropriate document.
     2.8. Center Test Authority. The roles and responsibility of the Center Test Authority is
     established in AFI 63-103, Joint Air Force-National Nuclear Security Administration (AF-
     NNSA) Nuclear Weapons Life Cycle Management, AFI 99-103, and Air Force Nuclear
     Weapons Center Instruction (AFNWCI) 99-103, Capabilities–Based Test and Evaluation.
3. Work Products.
     3.1. Inputs. The following work product inputs shall be generated prior to any AFNWC
     V&V process as appropriate for the effort being performed: System Performance
     Specification (SPS), RVM, TEMP (or equivalent), Functional Configuration Audit (FCA),
     FCA Completion Letter, System Test Plan, Flight Test Plans (as appropriate), Specialized
     Test Plans, and System Readiness for Force Deployment Evaluation (FDE) Phase II or
     OT&E Briefing. If such documents do not exist for legacy programs, the appropriate
     equivalent document shall be identified. The FCA uses test results from all levels of testing
     against the RVM. Verification criteria shall be established as component of the SPS. If SPSs
     are required, they shall include appropriate environmental requirements for early test
     planning to support seamless verification.
     3.2. Intermediate Work Products. In addition to controlled modification of the
     appropriate input documents listed in paragraph 3.1, the following work product inputs shall
     be generated during any AFNWC V&V process as appropriate for the effort being
     performed. If such documents do not exist for legacy programs, the appropriate equivalent
     document shall be identified. Although programs may vary, the following list is in
     approximate chronological order of development:
        3.2.1. SPS.
        3.2.2. Work products documenting resulting from peer reviews, such as Findings and
        Action Items.
        3.2.3. Verification strategy plan.
     3.3. Outputs. Unless otherwise specified by the accreditation authority, the V&V effort
     shall produce a V&V report which formally documents V&V activities, their results, and
     recommendations. This document, maintained by the PM, shall be used to support current
     and future accreditation decisions, feasibility assessments, and future enhancements. The
     report, at a minimum, will:
        3.3.1. Specify configuration, including hardware and software identification or version
        numbers.
        3.3.2. Identify key V&V planning, technical review, and implementation participants or
        organizations, and their V&V responsibilities.
        3.3.3. Describe V&V methodologies, implementations, and their results.
        3.3.4. Describe verification, validation, and certification activities performed on input
        data sets used in V&V activities.
AFNWCI63-111 4 FEBRUARY 2011                                                                  5


       3.3.5. Identify V&V criteria, such as Measures of Effectiveness / Measures of
       Performance.
       3.3.6. Describe any strengths, weaknesses, or limitations identified as a result of the
       V&V activity, with recommended remedial actions, as appropriate.
4. Processes.
   4.1. Verification. The purpose of verification is to ensure that work products meet their
   specified requirements. The purpose of validation is to demonstrate that a product or product
   component fulfills its intended use when placed in its intended environment. The general
   verification process shown in Figure 4.1, is further defined in the following paragraphs.
   Peer or appropriate working group reviews should occur between appropriate processes.

Figure 4.1. Verification Processes with Typical Products.
6                                                         AFNWCI63-111 4 FEBRUARY 2011


    4.2. System Performance Verification Planning. Performance requirements shall be
    documented in a performance specification. During V&V planning, all performance
    specification requirements shall be reviewed to ensure that they are clearly defined,
    measurable, and testable. During the development of the performance specification, the
    verification of each requirement shall be determined and documented. An overall
    verification strategy and plan, including the integrated testing approach shall be established.
    The verification strategy shall address the technical approach to product verification and the
    specific approaches that will be used to verify that work products meet their specified
    requirements. As part of this strategy, verification criteria and methods shall be established
    for each requirement.
    4.3. Determination and Documentation of Verification Methods. Prior to testing, the
    environment and resources needed to support verification shall be established and
    maintained. The test plan shall identify the facilities and support required. The responsible
    test authority shall ensure that any impacts to the external environment, e.g., pollutant
    emissions, noise, electromagnetic radiation, etc., are within allowable National
    Environmental Policy Act limits and required certifications are obtained. Verification
    methods shall be limited to the following: Analysis, Demonstration, Inspection, and Test.
    Methods of verification for all requirements shall be documented in the RVM, contained in
    the SPS.
    4.4. Develop System Test Plan. Once the SPS has been established, the RVM shall be used
    as the primary input to generate the System Test Plan. For each requirement verification
    activity, the System Test Plan shall contain the following information, with software-specific
    verification information appropriately tailored for compliance with Appendix B of MIL-STD
    3022:
       4.4.1. Requirement as stated in SPS
       4.4.2. Verification Method
       4.4.3. Responsible Engineer
       4.4.4. Scheduled Requirement Completion Date
       4.4.5. Actual Requirement Completion Date
       4.4.6. Verification Passed Field
       4.4.7. Requirement Completion History
       4.4.8. Comments, as appropriate
    4.5. Develop Specialized Test Documentation. Some requirements may warrant further
    test planning documentation. Software testing may require different test planning
    documentation. The PM shall document specialized test setups, equipment, facilities,
    manning, and/or schedule in a test plan. This specialized test plan may be a separate
    document. Examples of supplemental test include integration tests, chamber tests,
    electromagnetic interference / electromagnetic compatibility tests, software integration tests,
    software compatibility tests, and flight tests. Low risk requirements that do not require FCA
    shall be noted as ―No FCA required‖ in the RVM.
AFNWCI63-111 4 FEBRUARY 2011                                                                7


  4.6. Develop Verification Procedures. Verification procedures and criteria shall be
  established and documented.
     4.6.1. Work products to be verified, verification methods to be used, and any associated
     constraints, shall be identified. Document verification methods shall be adapted and
     revised to remain concurrent with product and product–component requirements and
     designs.
     4.6.2. The results of all verification activities shall be analyzed, documented, and
     managed as an element of the project data management effort. Analysis shall be
     conducted in order to identify trends, potential risks, and opportunities for process
     improvement. Each requirement shall have documented performance verifications via a
     ―Performance Assessment Report.‖ The performance assessment report is the method of
     communication and documentation for the process, and shall contain the following:
         4.6.2.1. Requirement Name
         4.6.2.2. Requirement as stated in the SPS
         4.6.2.3. Verification Method(s)
         4.6.2.4. Verification Notes for All Verification Method(s)
         4.6.2.5. System Configuration During Test (Hardware/Software/Firmware)
         4.6.2.6. Sufficient Data to Substantiate Performance Assessment Claim
         4.6.2.7. Conclusion
     4.6.3. The analysis and documentation of the results of verification activities shall be
     contained for each requirement in a specific performance assessment report.
  4.7. Reporting. The CE or Lead Engineer shall maintain the FCA Completion Letter and
  the Readiness for FDE or OT&E briefing as part of official program documentation. This
  documentation shall be maintained until the program is removed from inventory. Programs
  authorized to proceed without ‗FCA Completion Letter‘ and ‗Readiness‘ documents shall
  maintain appropriate equivalent documentation.
  4.8. Validation. The PM shall establish and maintain a documented plan for validation.
  This plan shall document all the required tools, test activities, and appropriate models and
  simulations used for product validation. The general validation process shown in Figure 4.2,
  is further defined in the following paragraphs.
 8                                                      AFNWCI63-111 4 FEBRUARY 2011


Figure 4.2. Validation Processes.




       4.8.1. The PM shall develop a product validation strategy and identify work products for
       validation and ensure the validation strategy addresses the technical approach to
       demonstrate user needs are satisfied. This documented strategy shall identify work
       products for validation against the user‘s needs and to address product performance risks.
       4.8.2. The PM shall establish, coordinate, document, and maintain validation criteria,
       methods and procedures. This documentation shall define validation procedures and
AFNWCI63-111 4 FEBRUARY 2011                                                                      9


     criteria to ensure that the product will fulfill its intended use when placed in its intended
     environment. The PM shall develop and document procedures and criteria to support
     military utility assessments by the operational test activity
     4.8.3. Validation of performance specification requirements prior to test shall be
     accomplished in accordance with applicable requirements management process
     requirements. Validation of performance specification requirements once tests have
     begun shall be accomplished during test planning meetings and during actual test
     conduct.
     4.8.4. The validation portion of the TEMP or similar document defines the environment
     and resources needed to support validation and the procedures and criteria for conduct. If
     formal survivability and lethality test and evaluation are required, the strategy and plans
     will be documented in the TEMP or similar document. Finally, plan for and obtain
     appropriate certifications and accreditations required for validation activities prior to their
     relevant need date.
  4.9. Early Operational Assessment. Prior to Early Operational Assessment, the PM and
  user shall establish and maintain the environment and resources necessary to support
  validation. The PM and user shall ensure that an adequate environment to carry out
  validation activities is maintained. The PM and user shall ensure that operators, enabling
  systems, and test facilities will be available in order to conduct validation. For new systems,
  or for systems with new additional capabilities, the program shall plan for OT&E use of the
  system (on a non–interference basis with DT&E) as early as possible. The user shall provide
  early feedback from operational users in a realistic environment on system integration,
  controls and displays, and performance of the system.
  4.10. OT&E. Prior to OT&E, the OT&E Responsible Test Organization shall have an
  approved test plan. Prior to OT&E, the program TEMP shall be updated, if required.
     4.10.1. During OT&E, the system shall be evaluated against the requirements stated in
     the Operational Requirements Document (ORD) (or if an ORD is not used, the
     Capabilities Development Document or appropriate equivalent document) against a
     realistic operational environment.
     4.10.2. Validation on the selected products and product components and data collection
     shall be performed according to the approved methods, procedures, and criteria.
     4.10.3. Results of the validation activities shall be analyzed and documented. The
     documented results shall be managed as an element of the overall project data
     management effort. Analysis shall be conducted to identify trends and opportunities for
     process improvement. Issues shall be documented and appropriate corrective actions
     initiated. Corrective actions shall be tracked to closure by an established and documented
     process, communicating these corrective actions to appropriate stakeholders.
     4.10.4. Results and analysis of OT&E shall be documented in a final test report, authored
     and maintained by the OT&E Responsible Test Organization.
  4.11. Corrective Actions. Any issues resulting from validation activities shall be
  documented in the test report. They may be also documented in Deficiency Reports (DRs).
 10                                                      AFNWCI63-111 4 FEBRUARY 2011


   DRs shall be evaluated by the developing agency to assess if the DR is valid and/or if
   corrective action is needed.
   4.12. Fielding Recommendation. Following completion of FDE Phase II, the CE shall
   brief the Single Manager and PM on readiness to field the system.
        4.12.1. Once all action items from the briefing have been satisfactorily resolved, the CE
        shall sign an OSS&E Compliance Letter to the system PM stating that the system
        meets/exceeds all performance requirements, meets all interface requirements, and meets
        all safety requirements. Additional information shall be prepared and attached to this
        letter to assist the CE in preparation of the OSS&E package.
        4.12.2. The CE shall maintain the Fielding Recommendation Briefing, the OSS&E
        Compliance Letter, and the Fielding Recommendation Letter on the program network.
        This data shall be maintained until the program is removed from inventory or is
        transferred to a logistics center.

5. Prescribed and Adopted Forms.

5.1. Adopted Forms.
      AF Form 847, Recommendation for Change of Publication.




                                             EVERETT H. THOMAS
                                             Brigadier General, USAF
                                             Commander
AFNWCI63-111 4 FEBRUARY 2011                                                             11


                                     ATTACHMENT 1
         GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION

References
AFI 14–206, Modeling and Simulation, 14 January 1994
AFI 16–1001, Verification, Validation and Accreditation, 1 June 1996
AFI 16–1002, Modeling and Simulation to Support Acquisition, 1 June 2000
AFI 33-360, Publications and Forms Management, 18 May 2006
AFI 63-103, Joint Air Force-National Nuclear Security Administration (AF-NNSA) Nuclear
Weapons Life Cycle Management, 24 September 2008
AFI 99–103, Capabilities–Based Test and Evaluation, 26 February 2008
AFMAN 33–363, Management of Records, 1 March 2008
AFMAN 63–119, Certification of System Readiness for Dedicated Operational Testing, 20 June
2008
AFMCI 63–1201, Implementing Operational Safety, Suitability and Effectiveness (OSS&E) and
Life Cycle Systems Engineering (LCSE), 14 October 2009
AFNWCI 99-103, Test and Evaluation Management, 6 November 2009
Defense Acquisition Guidebook (DAG), Version 1.6, 24 July 2006, (https://dag.dau.mil)
DODI 5200.39, Critical Program Information (CPI) Protection Within the Department of
Defense, 16 July 2008
MIL-STD 3022, Documentation of Verification, Validation, and Accreditation (VV&A) for
Models and Simulations
OSD System Engineering Plan (SEP) Preparation Guide, Version 2.01, April 2008
TO 00-5-3, Air Force Technical Order Management, 1 October 2010

Acronyms and Abbreviations
AF—Air Force
AF—NNSA—Air Force–National Nuclear Security Administration
AFGSC—Air Force Global Strike Command
AFI—Air Force Instruction
AFMAN—Air Force Manual
AFMCI—Air Force Material Command Instruction
AFNWC—Air Force Nuclear Weapons Center
AFNWC/CC—Commander, Air Force Nuclear Weapons Center
AFNWC/EN—Air Force Nuclear Weapons Center Engineering Directorate
 12                                                   AFNWCI63-111 4 FEBRUARY 2011


AFNWCI—Air Force Nuclear Weapons Center Instruction
AFRC—Air Force Reserve Command
AFRIMS—Air Records Information Management System
ANG—Air National Guard
CE—Chief Engineer
CPI—Critical Program Information
EUCOM—US European Command
DR—Deficiency Report
DT&E—Developmental Test & Evaluation
FCA—Functional Configuration Audit
FDE—Force Deployment Evaluation
LCSE—Life Cycle Systems Engineering
MIL—STD—Military Standard
OPR—Office of Primary Responsibility
ORD—Operational Requirements Document
OSD—Office of the Secretary of Defense
OSS&E—Operational Safety, Suitability and Effectiveness
OT&E—Operational Test & Evaluation
PM—Program Manager
POC—Point Of Contact
RDS—Records Disposition Schedule
RVM—Requirements Verifications Matrix
SAF/AQ—Undersecretary of Air Force for Acquisitions
SEAM—Systems Engineering Assessment Model
SEP—Systems Engineering Plan
SPS—System Performance Specification
TEMP—Test & Evaluation Master Plan
TO—Technical Order
USGSC—US Global Strike Command
V&V—Verification & Validation
VV&A—Verification, Validation, & Accreditation
USAF—United States Air Force
AFNWCI63-111 4 FEBRUARY 2011                                                                          13


Term:
Accreditation—Official determination that a model or simulation is acceptable for use for a
specific purpose.
Analysis—Analysis is the process of relating existing data (by interpolation, extrapolation,
and/or interpretation) to specified requirements. Data may be derived from documented sources,
such as design manuals, specifications, or produced by current or previous testing or simulation.
Analysis includes the review of existing data from similar devices or system configurations to
verify compliance with requirements.
Demonstration—Demonstration is an exhibition, the express purpose of which is to verify the
operability of an article or system under intended service use conditions. These exhibitions are
usually not repetitive and are oriented almost exclusively towards the acquisition of qualitative
data.
Inspection—Inspection is the examination of material (equipment, documents, computer
programs, or drawings) to determine compliance with specification requirements. Inspection
precludes the need for special laboratory resources or procedures, and is generally non–
destructive.
Product—The result of research, development, test, and evaluation in terms of hardware or
software being produced (manufactured). This may or may not be the formal effort of a program
office. Products may be systems, subsystems, end–items, models, or services.
Requirements Verifications Matrix—Verification criteria established as a component of the
SPS.
Test—A test is a procedure or action by which a device is subjected to conditions, real or
simulated, that will demonstrate its true qualities, capabilities, or suitability for use in a particular
application or environment. A test requires controlled conditions and usually requires the use of
special test equipment and instruments to obtain quantitative data for analysis. Under certain
circumstances, repetitive testing may be necessary for statistical purposes. In general, given the
same test procedure and conditions, test results should be repeatable.
Validation—In General, the process of determining that product accurately meets specifications.
For Software and M&S, the rigorous and structured process of determining the extent to which
the software accurately represents the intended "real world" phenomena from the perspective of
the intended the software use.
Verification—In General, the process of determining that product accurately meets user needs.
For Software and M&S, the process of determining that the software accurately represent the
developer‘s conceptual description and specifications.
Work Product—A work product is any artifact used or produced in a specific, defined process.
It may be a form, report or document. It may be in electronic form such as a database, electronic
message.
