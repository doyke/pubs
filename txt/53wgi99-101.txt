BY ORDER OF THE COMMANDER                                    53D WING INSTRUCTION 99-101
53D WING (ACC)
                                                                                  6 MAY 2011
                                                 Incorporating Through Change 2, 3 JUNE 2013

                                                                             Test and Evaluation

                                                            53D WG TEST AND EVALUATION


             COMPLIANCE WITH THIS PUBLICATION IS MANDATORY

ACCESSIBILITY: Publications and forms are available on the e-publishing website at
               www.e-publishing.af.mil for downloading or ordering. This publication
               is also available on the 53 Wing (WG) Community of Practices (CoP)
               website.

RELEASABILITY: There are no releasability restrictions on this publication.


OPR: 53 TMG/TO                                                           Certified by: 53 WG/CC
                                                                          (Col Michael E. Gantt)
                                                                                        Pages: 67




This 53 WGI implements Air Force Policy Directive (AFPD) 99-1, Test and Evaluation Process,
Air Force Instruction (AFI) 11-260, Tactics Development Program, AFI 99-103, Capabilities-
Based Test and Evaluation, AFI 10-703, Electronic Warfare Integrated Reprogramming, AFI
10-706, Electronic Warfare (EW) Operations, AFI 10-707, Spectrum Interference Resolution
Program, AFPD 10-9, Lead Command Designation and Responsibilities for Weapon Systems,
Air Combat Command Instruction (ACCI) 99-101, ACC Test and Evaluation, Commander, Air
Combat Command (COMACC ) Plan 001 and Air Force Global Strike Command (AFGSC)/CC
Plan 001, Nuclear Weapon System Evaluation Program (NucWSEP) Combat Sledgehammer,
COMACC Plan 85, Air-to-Air Weapon System Evaluation Program, COMACC Plan 90, Air-to-
Ground Weapon System Evaluation Program, and United States Air Force Warfare Center
Instruction (USAFWCI) 99-103, USAFWC Test and Evaluation. This instruction applies to all
units within the 53 WG. It does not apply to Air Force Materiel Command (AFMC) units and
members attached to the 53 WG as part of combined test forces unless conducting operational
testing under this instruction. Ensure that all records created as a result of processes prescribed
in this publication are maintained in accordance with Air Force Manual (AFMAN) 33-363,
Management of Records, and disposed of in accordance Air Force Records Information
Management System (AFRIMS) Records Disposition Schedule (RDS) located at
https://www.my.af.mil/gcss-af61a/afrims/afrims/. Contact supporting records managers as
required. Refer recommended changes and questions about this publication to the office of
 2                                                                                                53WGI99-101 6 MAY 2011


primary responsibility using the AF Form 847, Recommendation for Change of Publication;
route AF Forms 847 from the field through channels, to 53 TMG/TO, 203 W. D Avenue, Suite
406, Eglin AFB, FL 32542-6867. Additional policies may apply and are found in AFI 63-1201,
Life Cycle Systems Engineering, AFPD 63 1/AFPD 20-1, Acquisition and Sustainment Life Cycle
Management, AFPD 90-9, Operational Risk Management, AFMAN 63-119, Certification of
System Readiness for Dedicated Operational Test and Evaluation, and AFI 63-114, Quick
Reaction Capabilities. This instruction designates the 53d Test Management Group (TMG) as
the 53 WG focal point for all test management activities, guidelines, and procedures. This
instruction provides guidance for group and squadron leadership, project managers, test team
members, support and management personnel in planning, conducting, analyzing, reporting, and
disseminating projects in the 53 WG. This instruction is not intended to be an all-inclusive
document and does not take precedence over established directives, instructions, manuals, or
standards. The 53 WG Test Team Handbook (TTH) contains more detailed guidance, including
document formats and examples. The TTH is located on the 53 WG CoP. See attachment 1 for
a glossary of references and supporting information.

SUMMARY OF CHANGES

This interim change primarily implements new guidelines, procedures, and references addressing
the assimilation of the 17th Test Squadron (AFSPC) into Air Combat Command and the 53d
Wing. It also amends all occurrences of ACC/A8T to ACC/A5T IAW an Air Combat Command
functional reorganization. All references to Operational Risk Management (ORM) have been
changed to Risk Management (RM), per AFI 90-802. Additionally, Chapter 1 now contains
descriptions and applications of Joint Urgent Operational Needs, Joint Emerging Operational
Needs, Urgent Operational Needs, and the Quick Reaction Capability process. A margin bar (|)
indicates newly revised material.


Chapter 1—VISION AND IMPLEMENTATION CONCEPT                                                                                             5
       1.1.     PURPOSE OF TEST AND EVALUATION (T&E). ............................................                                      5
       1.2.     TRADITIONAL T&E, TECHNOLOGY DEVELOPMENT, AND RAPID
                ACQUISITION. .....................................................................................................      5
       1.3.     T&E GUIDANCE. .................................................................................................         7
       1.4.     INTEGRATED TESTING. ....................................................................................                8
       1.5.     APPLICABILITY. .................................................................................................        8

Chapter 2—53 WING ROLES AND RESPONSIBILITIES                                                                                            9
       2.1.     ORGANIZATION. ................................................................................................          9
       2.2.     53 WG TESTING. .................................................................................................       10
       2.2.4.   Wing Early Involvement. .......................................................................................        11
       2.2.5.   Wing Monitored. ....................................................................................................   11
       2.3.     T&E FUNDING. ...................................................................................................       11
53WGI99-101 6 MAY 2011                                                                                                                    3


       2.4.     TEST TEAM COMPOSITION. ............................................................................                       12
       2.5.     ROLES AND RESPONSIBILITIES. ....................................................................                          13

Chapter 3—TEST ENTERPRISE                                                                                                                 19
       3.1.     INTRODUCTION. ................................................................................................            19
       3.2.     TEST AND EVALUATION MASTER PLAN (TEMP). .....................................                                             19
       3.3.     TEST RESOURCE PLAN (TRP). .........................................................................                       19
       3.4.     INTEGRATED TEST TEAM (ITT). ....................................................................                          19
       3.4.1.   53 WG/CC is normally the approval authority for ITT Charters which specify 53
                WG participation. ...................................................................................................     19
       3.4.2.   In cases where 53 WG is the operational test co-chair of an ITT, the 53 TMG/CC
                is normally designated to fulfill this responsibility. ...............................................                    19
       3.4.3.   Any 53 WG subject matter expert may be tasked to support an ITT, at the
                discretion of the project manager in coordination with the unit commander. ........                                       19
       3.5.     RESPONSIBLE TEST ORGANIZATION (RTO). ...............................................                                      19
       3.6.     OPERATIONAL TEST ORGANIZATION (OTO). .............................................                                        19
       3.7.     PARTICIPATING TEST ORGANIZATION (PTO). ...........................................                                        19
       3.8.     DIRECTOR, OPERATIONAL TEST AND EVALUATION (DOT&E). ............                                                           20
       3.9.     HQ, US AIR FORCE, DIRECTORATE OF TEST AND EVALUATION (HQ
                USAF/TE). .............................................................................................................   21
       3.10.    AIR FORCE OPERATIONAL TEST AND EVALUATION CENTER
                (AFOTEC). ............................................................................................................    21

Chapter 4—TEST PROCESS                                                                                                                    23
       4.1.     TEST PHASES. .....................................................................................................        23
       4.2.     PROJECT INITIATION. .......................................................................................               23
Figure 4.1.     ACC Process for Out-of-Cycle EPOs ....................................................................                    24
Table 4.1.      Coordination Matrix (other than 17th Test Squadron) ...........................................                           28
Table 4.2.      Coordination Matrix (17th Test Squadron) ............................................................                     31
Table 4.3.      Coordination Timeline Guidance ...........................................................................                32
Table 4.4.      Suspense Guidelines ..............................................................................................        33
       4.3.     PLANNING. ..........................................................................................................      34
       4.4.     EXECUTION. ........................................................................................................       40
       4.5.     REPORTING. ........................................................................................................       44
       4.6.     PROJECT CLOSE OUT. .......................................................................................                45
 4                                                                                                   53WGI99-101 6 MAY 2011


Chapter 5—TRAINING                                                                                                                           46
      5.1.   GENERAL. ............................................................................................................           46
Table 5.1.   Test Team Training ................................................................................................             46
      5.2.   TEST TRAINING. .................................................................................................                47
      5.3.   TRAINING COURSE MANAGEMENT. .............................................................                                       48
      5.4.   OTHER TRAINING. .............................................................................................                   48
      5.5.   TIF. ........................................................................................................................   48
      5.6.   PRESCRIBED AND ADOPTED FORMS ...........................................................                                        48

Attachment 1—GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION                                                                               49

Attachment 2—TEST INFORMATION RELEASE (GENERAL)                                                                                              62

Attachment 3—TEST INFORMATION RELEASE TO DIRECTOR, OPERATIONAL
             TEST & EVALUATION                                                                                                               65
53WGI99-101 6 MAY 2011                                                                          5


                                           Chapter 1

                      VISION AND IMPLEMENTATION CONCEPT

1.1. PURPOSE OF TEST AND EVALUATION (T&E). The overarching functions of T&E
are to mature system designs, manage risks, identify and resolve deficiencies expeditiously, and
ensure systems are operationally effective and suitable. The Air Force T&E community plans
for and conducts integrated T&E as an efficient continuum known as seamless verification in
collaboration with the requirements and acquisition communities. AFI 99-103, describes the
roles and responsibilities of the T&E community in executing this function.
1.2. TRADITIONAL         T&E,      TECHNOLOGY            DEVELOPMENT,            AND     RAPID
ACQUISITION.
   1.2.1. Developmental Test and Evaluation (DT&E). Developmental testing is conducted
   throughout the acquisition and sustainment processes to assist in engineering design and
   development and to verify critical technical parameters are achieved. Developmental testing
   supports the decision to certify systems ready for dedicated operational testing according to
   AFMAN 63-119. The 53 WG supports DT&E as requested through Air Combat Command
   (ACC)/A5/8/9.
   1.2.2. Operational Test and Evaluation (OT&E). Operational testing includes several
   categories: initial operational test and evaluation (IOT&E), qualification operational test and
   evaluation (QOT&E), follow-on test and evaluation (FOT&E), and multi-service operational
   test and evaluation (MOT&E). These tests are normally conducted by the United States Air
   Force (USAF) service operational test agency (OTA), the Air Force Operational Test and
   Evaluation Center (AFOTEC). Since AFOTEC does not possess aircraft, the 53 WG
   supports AFOTEC’s OT&E of Combat Air Force (CAF) systems when directed by
   ACC/A5/8/9. Operational testing in ACC is conducted through two test centers: the United
   States Air Force Warfare Center (USAFWC) using the 53 WG and 505th Command and
   Control Wing (CCW), and the Air National Guard and Air Force Reserve Command Test
   Center (AATC). Operational testing for ACC includes force development evaluations
   (FDE), tactics development and evaluations (TD&E), operational assessments (OA),
   operational utility evaluations (OUE), sufficiency of test reviews (SOTR), verification flight
   testing (VFT), capabilities and limitations report (C&LR), foreign materiel exploitation
   (FME), Weapons Systems Evaluation Program (WSEP), NucWSEP, simulator certification
   (SIMCERT), mission data optimization (MDO), mission planning environment (MPE), and
   performance characterization assessments (PCA). Some of these test definitions can be
   found in the Glossary.
   1.2.3. Test Support for Technology Transition Mechanisms. Technology transition
   mechanisms include advanced technology demonstrations (ATD), joint capability technology
   demonstrations (JCTD), exercises, and experiments. The 53 WG is occasionally requested
   by ACC/A5/8/9 to support these efforts.
   1.2.4. Joint Expeditionary Force Experiment (JEFX). JEFX is a Chief of Staff of the Air
   Force (CSAF)-directed series of experiments combining live, virtual and constructive air,
   space, naval, and ground forces, and process and technology innovation into a near-seamless
   joint warfighting environment. The Air Force Command and Control Integration Center
6                                                                   53WGI99-101 6 MAY 2011


    (AFC2IC) conducts JEFX and the 53 WG supports it as directed by ACC/A5/8/9 and in
    accordance with the experiment project order.
    1.2.5. Joint Test and Evaluation (JT&E) and Quick Reaction Test (QRT) Support. JT&E
    and QRT programs are sponsored by the JT&E Program Office to evaluate technical or
    operational tactics, techniques, and procedures applicable to more than one service.
    Candidate programs are nominated by the services and directed and funded by the Office of
    the Secretary of Defense (OSD). USAFWC is the USAF agent for JT&Es and QRTs.
    JT&Es and QRTs usually do not result in the acquisition of systems.
    1.2.6. Foreign Comparative Test (FCT). FCT is an OSD-sponsored T&E program
    conducted on foreign nations’ systems, equipment, and technologies to determine their
    potential to satisfy validated US operational requirements.
    1.2.7. Investigative Firings/Special Interest Profiles. The goal of these firings is to further
    define weapon system envelopes, evaluate deficiencies which surface during WSEP, or
    complete unfinished phases of DT&E/OT&E for fielded weapons or software.
    1.2.8. Simulator Certification (SIMCERT). SIMCERT is an ACC program defined in AFI
    362251, Management of Air Force Training Systems, and AFI 36-2248, Operation and
    Management of Aircrew Training Devices, to ensure training devices are maintained to their
    design configuration and provide accurate and credible aircrew training. SIMCERT is
    conducted IAW procedures coordinated with HQ ACC and defined in the SIMCERT Master
    Plan. In cases of conflict with 53 WG procedures, AFI 36-2251, AFI 36-2248 or the
    SIMCERT Master Plan will take precedence.
    1.2.9. Aircraft Monitor and Control (AMAC) Certification and Surveillance Test Program.
    The Air Force Nuclear Weapons Center, Nuclear Systems Division (AFNWC/NCS), Kirtland
    AFB, New Mexico, performs periodic certification and surveillance testing of the nation’s
    vital deterrent systems. The purpose of surveillance testing for each weapon system is to
    monitor changes in the baseline for each system that would indicate a design issue created by
    aging, aircraft modification, or a combination thereof. This testing is performed IAW AFI
    63-125, as prescribed in the test plan provided by the AFNWC/NCS.
    1.2.10. Rapid Acquisition. Combatant Command identified and Joint Staff or Service
    Component validated urgent operational needs represent the highest priority of the DoD. The
    Defense Rapid Acquisition System (DRAS) is designed to rapidly deliver urgently needed
    capability to the warfighter. The DRAS is optimized for speed and considers acquisition
    risks (cost, schedule, and performance) and the operational risk to the user if an effective
    solution is not deployed in the required timeframe identified by the requesting Commander.
    The goal for fielding an initial capability is within two to 24 months. DoD’s Warfighter
    Senior Integration Group (WSIG) may designate any candidate or validated UON or critical
    warfighter requirement for joint action and management of its resolution by the WSIG.
    Otherwise, the following standard UON processes apply:
       1.2.10.1. Urgent Operational Need (UON). UONs are capability requirements identified
       by a DoD Component as impacting an ongoing or anticipated contingency operation. If
       left unfulfilled, UONs result in capability gaps potentially resulting in loss of life or
       critical mission failure Service Components, in their own terminology, may use a
       different name for a UON. A UON normally includes a target date for initial fielding and
53WGI99-101 6 MAY 2011                                                                            7


       a description of the concept of operations (CONOPs). The CONOPs identifies any non-
       materiel options under consideration to mitigate the gap. For USAF, a Commander Air
       Force Forces (COMAFFOR) submits a UON to the lead command and the lead command
       validates the need in accordance with AFI 10-601, Attachment 3.
       1.2.10.2.1. JUONs are UONs that are identified by a Combatant Command or by DoD
       as inherently joint and impacting an ongoing contingency operation.
          1.2.10.2.2. JEONs are UONs that are identified by a Combatant Command as
          inherently joint and impacting an anticipated or pending contingency operation.
          1.2.10.2.3. In accordance with CJCSI 3470.01, a Unified Combatant Commander
          (UCC) submits a JUON/JEON to the Joint Staff. The Joint Staff validates the
          JUON/JEON and forwards to the Joint Rapid Acquisition Cell (JRAC) for
          coordination of a Defense-wide solution. Based on the potential solution and
          capability type, the JRAC will assign a Lead Service to fulfill the JUON/JEON.
          JUON/JEON solutions will be fielded in accordance with JRAC direction.
          1.2.10.2.4. The AF will leverage the JRAC for fielding guidance, special waiver
          authorities, and possible funding sources. SAF/AQX is the principal USAF member
          of the JRAC and will receive and disposition AF-assigned JUON/JEONs. Per AFI
          10-601, JUON/JEONs will be administratively processed in the same manner as
          UONs.
       1.2.10.3. Chief of Staff of the Air Force (CSAF) Requirements. AF/CC provides top-
       down direction to rapidly fulfill an urgent need. The lead command shall upload the
       documented AF/CC direction to the Information and Resources Support System (IRSS)
       and notify AF/A5RP, SAF/AQXA, and the implementing command (AFMC/A5C or
       AFSPC/A5X) for action.
   1.2.11. Quick Reaction Capability (QRC) Development and Testing Process. Per AFI 63-
   114, urgent need validation initiates the QRC process, signified by one of three triggers:
   UON, JUON/JEON, or AF/CC direction. QRC programs do not operate outside of normal
   acquisition procedures, but rather speed up the fielding of systems and capabilities to satisfy
   near-term urgent warfighting needs. QRC programs accomplish this through the use of
   cross-functional teams, tightly scoped requirements, higher risk thresholds, concurrent
   activities, delegated authorities, and a standardized process. The QRC process shall not
   exceed 180 calendar days to initial fielding, unless endorsed and approved by proper
   authority. Initial fielding is the sole responsibility of the using command (requirements) and
   the acquisition program manager (development and test). Development work prior to initial
   fielding often does not incorporate dedicated operational testing; additionally, any test results
   available prior to initial fielding are normally documented in a Capabilities and Limitations
   Report. QRC programs shall meet a Capability Transition Review (CTR) to formally decide
   the long-term disposition of fielded solutions. The CTR shall occur not later than 180
   calendar days following initial fielding. If the CTR decision is to transition the initially
   fielded capability to a traditional development and sustainment program, normal product
   development and testing processes will then pertain.
1.3. T&E GUIDANCE. AFI 99-103 tasks each major command’s (MAJCOM) designated test
organization to establish disciplined processes for planning and executing T&E activities. This
 8                                                                    53WGI99-101 6 MAY 2011


document, along with ACCI 99-101, AFSPCI 99-103, USAFWCI 99-103, and the 53 WG Test
Team Handbook, establishes the direction and processes for conducting 53 WG testing.
ACC/A5T is the ACC office responsible for testing and is responsible for ACCI 99-101.
USAFWC/A3 is the USAFWC agency for testing and is responsible for USAFWCI 99-103. The
53 WG will:
     1.3.1. To the maximum extent possible, conduct T&E in a joint environment.
     1.3.2. Use dedicated test assets (as available) in accordance with (IAW) AFPD 10-9, when
     conducting tests.
     1.3.3. Conduct T&E over the life of a system to ensure it continues to meet user
     requirements and to explore non-materiel means of satisfying deficiencies.
     1.3.4. Conduct testing in as realistic an operational environment as possible to determine
     system operational effectiveness and suitability.
     1.3.5. Require all test articles (to include support equipment, software, government-
     furnished equipment [GFE]) be as production-representative as possible. Sufficient
     quantities of test articles must be available.
     1.3.6. Conduct FDE to refine estimates made during IOT&E or QOT&E, to evaluate
     changes and verify correction of deficiencies, to assist in tactics, techniques, and procedures
     (TTP) development, and to reevaluate a system to ensure it continues to meet operational
     needs.
     1.3.7. Support IOT&E, MOT&E, QOT&E, and FOT&E; however, the 53 WG will not
     conduct these forms of operational test.
     1.3.8. Support QRC processes as directed by ACC, AFGSC, and AFSPC.
1.4. INTEGRATED TESTING. Integrated testing is a concept for test design that must be
intentionally built into the earliest program strategies, documentation, and test plans. The earlier
integrated testing strategies are developed and adopted, the greater the opportunities and benefits.
Successful integrated testing will identify system design improvements much earlier in
developmental testing, as well as reduce the number of T&E resources needed for operational
testing. The 53 WG supports the integrated testing concept, along with other adjunct tester
collaboration opportunities during early requirements definition and system development
activities. Key tenets of integrated testing are that the operational tester must ensure integrity of
the data, and must also retain data assessment independence.
1.5. APPLICABILITY. The policies and processes in this instruction will be used for all 53
WG conducted test projects regardless of acquisition category (ACAT), to include non-ACAT
programs, unless specifically excluded by this directive or by other more applicable higher
headquarters guidance or direction.
53WGI99-101 6 MAY 2011                                                                        9


                                          Chapter 2

                       53 WING ROLES AND RESPONSIBILITIES

2.1. ORGANIZATION. The 53 WG is organized into four groups, each with distinct missions.
   2.1.1. The 53d Test Management Group (TMG). The 53 TMG is the principal point of
   contact (POC) for managing wing-assigned test programs. It acts as the wing’s POC with
   headquarters (HQ) AF/TE, OSD/DOT&E, HQ ACC, HQ AFMC, HQ AFSPC, HQ AFGSC,
   AFOTEC, and other Department of Defense (DoD) and contractor test organizations for
   integrating USAF test requirements with future and ongoing wing tests and real-world
   efforts. The 53 TMG is responsible for planning, coordinating/managing resources,
   developing support plans/agreements, monitoring execution, gathering and analyzing data,
   and preparing and publishing plans, reports, fielding recommendations, and interim
   documents for the majority of the 53 WG operational testing. The 53 TMG manages and
   executes Agile Combat Support, including chemical and biological defense systems, Aircrew
   Flight Equipment Systems, and MPE operational testing and evaluation for USAF and non
   USAF agencies. The 53 TMG is the functional manager and technical expert for testing,
   modification, acquisition, sustainment, and certification of all CAF aircrew training device
   systems. The 53 TMG is responsible for training personnel in required test management
   skills. The 53 TMG is also the wing focal point for non-EW portions of the FME program.
   The 53 TMG provides communication and information systems for the 53 WG and enhances
   CAF warfighting capability by operating, protecting, and maintaining computer systems
   critical to development and dissemination of EW software.
   2.1.2. The 53d Electronic Warfare Group (EWG). The 53 EWG is the CAF’s focal point for
   operational and technical EW issues. It supports the CAF EW mission IAW AFI 10-706 and
   ACCI 10-707. The 53 EWG is the CAF’s sole provider of mission software for jamming and
   detection systems to enhance aircraft survivability. It provides 24-hour contingency response
   reprogramming of EW systems to ensure mission effectiveness in support of the electronic
   warfare integrated reprogramming (EWIR) process as outlined in AFI 10-703. Units within
   the group support operational test of new/modified EW systems, conduct mission data (MD)
   programming and optimization, and make release recommendations based on those results.
   The 53 EWG conducts exploitation tests of foreign threat systems in conjunction with the
   FME program and independently as part of their MD development role. The group also
   executes the Electronic Warfare Assessment Program, Combat Shield, under ACC/A3
   direction as outlined in ACCI 10-707.
   2.1.3. The 53d Test and Evaluation Group (TEG). The 53 TEG is responsible for managing
   the wing’s flying activities except for the USAF air-to-air (A/A) and air-to-ground (A/G)
   WSEPs. The 53 TEG members execute tests as assigned by HQ ACC and managed by the
   53 TMG. The group is responsible for managing and conducting NucWSEP (IAW
   COMACC Plan 001 and AFGSC/CC Plan 001) through the 49 Test and Evaluation Squadron
   (TES) and 72 TES.
      2.1.3.1. The 31 TES mission is to provide direct OT&E execution and analysis support
      to AFOTEC test teams for IOT&E/FOT&E of ACC weapon systems as directed by
      ACC-approved Test Resource Plans (TRP). Most 31 TES personnel are normally
 10                                                                53WGI99-101 6 MAY 2011


      assigned to AFOTEC test teams at Edwards AFB, CA; all 31 TES personnel are under
      the administrative control of the 53 TEG.
      2.1.3.2. The 53 TEG, Detachment 2 mission is to conduct high-altitude intelligence,
      surveillance, and reconnaissance testing on U-2 and RQ-4 aircraft through; this testing is
      managed by the 28 TES.
      2.1.3.3. The 17th Test Squadron (TS) mission is to provide an independent assessment of
      space systems performance. It serves as the expertise for operational testing of space
      systems, reports test results to support weapons system acquisition and fielding decisions,
      and enhances AFSPC’s warfighting capabilities through testing and evaluation of space
      forces. The 17 TS possesses detachments and operating locations responsible for
      enhancing space superiority through space situational awareness, enhancing space force
      and space control capabilities through operational testing of Integrated Tactical Warning
      and Attack Assessment systems, and testing the range modernization programs and
      associated systems for the Eastern Range at Cape Canaveral AFS.
   2.1.4. The 53d Weapons Evaluation Group (WEG). The 53 WEG conducts the USAF A/A
   WSEP, known as Combat Archer IAW COMACC Plan 85, and the USAF A/G WSEP,
   known as Combat Hammer IAW COMACC Plan 90. It also supports Weapons Instructor
   Course air-to-air formal training syllabi under COMACC Plan 92. The 53 WEG’s two E-9A
   aircraft provide range sweep, telemetry relay, and communications link support for numerous
   DoD tests. Unit personnel provide all USAF aerial target support for DoD in the Gulf
   Ranges and full scale targets for testing at White Sands Missile Range, Holloman AFB, NM.
   The group also plans, manages, and executes the USAF air-to-air weapons competition
   known as William Tell.
2.2. 53 WG TESTING. The 53 WG is annually assigned a variety of tests by ACC. These
tests are broken into five main categories.
   2.2.1. Wing Conducted. Wing conducted tests identify the 53 WG as the lead test agency
   and are managed by a 53 WG-assigned project manager (PM). The PM is responsible for
   project planning, execution, and final reporting.
   2.2.2. Wing Supported. Wing supported tests are managed or conducted by other lead test
   agencies (AATC, 505 CCW, AFMC, AFOTEC, etc.) with close involvement by an assigned
   53 WG PM. The 53 WG PM is responsible for writing a test support plan and coordinating
   53 WG support for the lead test agency as directed in the ACC electronic project order
   (EPO). The 53 TMG/CC may waive developing a formal test support plan if the lead test
   agency plan adequately delineates 53 WG execution responsibilities. A formal 53 WG-
   published test report is not required. However, PMs will provide a memorandum for record
   (MFR) summary of 53 WG participation to the 53 WG/CC.
   2.2.3. Wing Integrated. Some tests are combined DT&E and OT&E efforts, where the 53
   WG is supporting other organizations in one portion of the overall test, but is conducting a
   dedicated operational test (OA, FDE, OUE, etc) in another portion. All 53 WG units,
   especially those participating in combined test forces (CTF), should look to conduct
   integrated testing in order to reduce costs, optimize schedules, and increase overall test
   efficiency. Test planning will reflect both a support and a conducted role and responsibility.
53WGI99-101 6 MAY 2011                                                                          11


2.2.4. Wing Early Involvement. This supports 53 WG participation (usually pre-Milestone
B) in pre-OT&E acquisition planning activities (normally ACAT 3 and smaller MAJCOM-
managed programs). 53 WG participants are authorized to provide operational test advice, as
well as draft test planning inputs to the acquisition community, for programs projected to be
assigned to the 53 WG for future operational testing. Draft inputs include, but are not limited to,
operational test planning for Test Strategy and Test and Evaluation Management Plan (TEMP)
documents. 53 WG personnel are authorized to participate in the associated acquisition program
office-sponsored Integrated Test Teams (ITT). Participating 53 WG personnel will ensure they
are not setting/perceived-as-setting MAJCOM capability requirements for the system under
development.
2.2.5. Wing Monitored. A wing monitored test has no 53 WG flying or ground test support.
The wing either monitors another agency’s testing or monitors early progress or planning of a
project in anticipation of a future test. A test support plan or report is not required. In some
cases, 53 WG aircrews may fly these tests in non 53 WG aircraft to provide an operational
perspective. A test support plan is not needed for these missions.
2.3. T&E FUNDING. Funding for wing-conducted testing may come from several sources.
The primary sources are the ACC budget process which yields an initial distribution from HQ
ACC/FM to the 53 WG and ultimately to the groups and units; and AFGSC and AFSPC which
fund the 53 WG units directly supporting their operational test requirements. Other funding
agencies include, but not limited to, aircraft systems groups (SG); weapons SGs; Air Force
Research Laboratory (AFRL); other MAJCOMs; or other services. The method for these
organizations to provide funding will be coordinated with the PM and the applicable budget
analyst/resource advisor (RA). Organizations requesting 53 WG support for their testing
(AFOTEC, AFMC, AATC, 505 CCW, etc.) normally fund all the 53 WG support required for
the test except flying hours and civilian/military pay.
   2.3.1. Planning and Budgeting for T&E Activities. Each group within the 53 WG is
   responsible for budgeting resources required to accomplish test and evaluation
   responsibilities. Administrative costs to run each squadron or detachment (e.g. computers,
   office supplies, phones, et al) should be budgeted separately from specific T&E activities and
   submitted upon 53 WG's call for execution plans. Funding provided by ACC/FMA within
   ACC’s assigned operating budget account numbers (OBAN) will be distributed to the groups
   based on recommendations of the financial working group (FWG) and approval by the
   financial management board (FMB). The groups will then determine distribution to their
   individual units. Funding received from sources external to the ACC and FME budget
   process will be controlled and accounted for by the receiving 53 WG unit and reported to 53
   WG/FM. Test units are responsible for submitting test-specific funding requirements
   through the Test Management System (TMS) EPO process, a web based tool outlining the
   test description, its purpose, required resources to accomplish the test, and the scope of the
   effort for each test program by fiscal year and type of funds. After the EPO is approved,
   PMs will provide updated cost data to the unit RA or budget analyst for budgeting.
   2.3.2. A/A and A/G WSEP. These programs are ACC/A3 centrally managed and executed
   by the 53 WEG. 53 WEG will ensure 53 WG/FM has visibility on all project funding.
   2.3.3. NucWSEP. This program is AFGSC funded and executed by the 53 TEG. 53 TEG
   will ensure 53 WG/FM has visibility on all project funding.
 12                                                                 53WGI99-101 6 MAY 2011


2.3.4. FME FME budgeting and funding is a 53 EWG/53 TMG-coordinated process in
conjunction with ACC/A8Z and the Air Staff. A prioritized FME test list will be created by
53 TMG/EWG action officers, coordinated through the group commanders, and approved by the
53 WG/CC. The 53 WG FM will use the FME test priority list to distribute funds to the
respective groups.

2.3.5. SIMCERT. Funding for ACC, United States Air Forces Europe (USAFE), and Pacific
Air Forces (PACAF) SIMCERTs is provided through the 29th Training Systems Squadron
(TSS). All other SIMCERTs will be funded by the appropriate MAJCOM IAW AFI 36-2248.

2.4. TEST TEAM COMPOSITION. Each 53 WG test is conducted using a team effort led by
the PM. The test team includes a variety of expertise to include, but not limited to, rated
personnel, engineers, analysts, and maintenance support. Unit commanders must ensure
personnel assigned to a test team are current and qualified to perform the required duties for
support of test operations IAW Chapter 5 of this instruction.
   2.4.1. Project Manager (PM). The PM is the single focal point within the 53 WG for the
   test. The PM is required to be either a civilian or military government employee. The
   managing group commander may waive this requirement on a case-by-case basis. The PM
   will direct the test team and assign responsibilities to ensure all aspects of planning,
   execution, and reporting are accomplished. See paragraph 2.5.9 for additional PM
   responsibilities.
   2.4.2. Unit Project Officer (UPO).          A UPO will be designated for each test
   execution/supporting unit assigned to the test project. UPOs will be the personal
   representatives of the unit commander and have authority to coordinate with outside agencies
   regarding test details as directed by the PM. Authority to commit the unit’s resources IAW
   the EPO will be delegated to the UPO at the discretion of the unit commander.
   2.4.3. Rated Project Officer (RPO). Each test involving flying will have a RPO assigned to
   the test team. RPOs will be pilots, navigators, or weapons systems officers currently
   qualified in their aircraft. The RPO will be the team expert on aircraft systems, operations,
   and tactical employment. More than one RPO may be assigned to the test team depending on
   the number of different types of aircraft involved. RPOs may also act as the UPO at the
   discretion of the executing unit commander.
   2.4.4. Operations Analyst (OA). The OA is responsible for implementing the appropriate
   test planning methodology. This will be accomplished in every wing-conducted test by
   building efficient test matrices (plans) and analyzing results with the appropriate analysis
   tools. After collaborating with other test team members, the OA will present this analysis in
   the required report and/or briefing format at the conclusion of testing. If a separate technical
   plan or report is used, the OA will be responsible for producing and coordinating it through
   the squadron and group technical chains (normally squadron lead engineer or technical
   advisor).
   2.4.5. Test Engineer (TE)/Flight Test Engineer (FTE). The TE/FTE will ensure the technical
   adequacy of the ground or flight test being conducted. The TE/FTE is normally the subject
   matter expert in the overall system being tested, possessing the most in-depth knowledge of
   the test item, its integration with the test platform, and its connectivity with off-board
53WGI99-101 6 MAY 2011                                                                           13


  systems. The TE/FTE will assist the OA in determining the test matrix, developing the
  mission run cards, interpreting the data gathered during test missions, and recommending the
  course of action for subsequent testing. Additionally, a project test engineer usually has
  primary responsibility for ensuring that the system-under-test is instrumented sufficiently to
  meet the data collection and analysis requirements of the test.
  2.4.6. Operational Suitability Analyst (OSA). OSAs ensure operational suitability test
  objectives adequately reflect system requirements and fielding recommendations; and final
  reports reflect the test team’s fielding concerns with respect to system suitability (reliability,
  maintainability, availability and other suitability factors). OSAs assist the PM in reviewing,
  submitting and tracking deficiency reports (DRs).
  2.4.7. Mission Control Room Personnel. A control room is defined as any facility, ground
  or airborne, which provides two-way communications with the aircrew and real-time
  capability to monitor safety of flight, quality of test data, and/or flight termination system
  information. Essential control room personnel, for the purposes of this instruction, are test
  directors and test conductors, as well as designated subject matter experts whose active
  presence in the control room is essential to safety of flight and/or mission success. See 53
  WGI 99-104, Complex Test Mission Preparation and Control, and the associated 53 WG
  mission control room web-based training for additional guidance on control room operations.
     2.4.7.1. Test Director (TD). The TD is a highly experienced individual who acts as the
     test team supervisor in the control room and has emergency direct communication with
     the mission test aircrew.
     2.4.7.2. Test Conductor (TC). The TC is an experienced individual who is designated as
     the primary communicator with the test aircrew. The TC will adjudicate in-flight
     changes to the briefed mission run cards and will clear the test pilot to proceed from one
     test point to the next. In missions where a TC is not needed, all test conductor duties will
     be accomplished by the TD.
  2.4.8. Instrumentation Technician (IT). ITs are responsible for the pre-flight and post-flight
  of aircraft, data acquisition, and telemetry systems. Duties include ensuring all data
  requirements are identified and data collection systems are properly configured to
  successfully record accurate test data.
2.5. ROLES AND RESPONSIBILITIES.
  2.5.1. The 53 WG/CC is responsible for the safe, effective, and efficient conduct of all
  testing within the wing. The commander is the approving authority for all test plans and
  reports (except for tests on OSD Oversight) and delegates that approval authority to the vice
  commander or acting wing commander as necessary. The commander also delegates
  approval authority to group commanders for specific test types as indicated in Chapter 4,
  Table 1.
  2.5.2. The 53 TMG/CC will:
     2.5.2.1. Provide guidance for test planning, coordinating resources, developing support
     plans/agreements, monitoring execution, gathering and analyzing data, and preparing and
     publishing plans, reports, fielding recommendations, and interim documents for the
     majority of 53 WG operational testing.
14                                                               53WGI99-101 6 MAY 2011


     2.5.2.2. Monitor defense acquisition programs and manage HQ ACC-directed T&E.
     2.5.2.3. Support testing conducted by other agencies as directed by HQ ACC EPOs.
     2.5.2.4. Prepare an annual financial plan, including resources required, for each projected
     test.
     2.5.2.5. Conduct a concept of test review for all wing-conducted flying tests.
     2.5.2.6. Conduct a test readiness review (TRR) for all wing-conducted flying tests which
     will result in a fielding recommendation or development of TTPs.
     2.5.2.7. Develop and maintain a set of metrics on test operations.
     2.5.2.8. Manage test training for the wing.
     2.5.2.9. Initiate updates to 53 WGI 99-101, 53 WGI 99-104, 53 WGI 63-1101, and the
     53 WG Test Team Handbook.
     2.5.2.10. Ensure procedures are established for planning, conducting, and reporting test
     programs IAW AFI 99-103, AFI 63-114, AFMAN 63-119, ACCI 99-101, and
     USAFWCI 99-103.
     2.5.2.11. Administer the USAFWC Integration Collaboration Memorandum of
     Agreement.
     2.5.2.12. Approve all certifications for readiness for operational test IAW AFMAN 63-
     119.
     2.5.2.13. Coordinate on all test plans, test reports, and release/fielding recommendations
     authored by 53 WG personnel excluding NucWSEP, WSEP, MDO, and Combat Shield.
     2.5.2.14. DELETED
  2.5.3. The 53 TEG/CC will:
     2.5.3.1. Maintain qualified aircrews and mission-ready aircraft to support 53 WG testing.
     2.5.3.2. Manage the annual TEG flying hour program and reflows, re-allocations, and
     reporting as required to ACC/A3TB.
     2.5.3.3. Conduct NucWSEP IAW COMACC Plan 001 and AFGSC/CC Plan 001.
     2.5.3.4. Prepare an annual financial plan including resources required to execute
     assigned testing.
     2.5.3.5. Conduct a flight readiness review (FRR) for all tests flown by TEG aircrews and
     aircraft.
     2.5.3.6. Conduct configuration review board (CRB) for all modifications to 53 WG
     assigned aircraft.
  2.5.4. The 53 WEG/CC will:
     2.5.4.1. Conduct A/A and A/G WSEP IAW COMACC Plans 85 and 90.
     2.5.4.2. Manage the annual WEG flying hour program and reflows, re-allocations, and
     reporting as required to ACC/A3TB.
53WGI99-101 6 MAY 2011                                                                     15


     2.5.4.3. Develop and maintain a set of metrics on WSEP and drone operations.
     2.5.4.4. Prepare an annual financial plan including resources required for each projected
     evaluation and submit to ACC/A3 through 53 WG/FM.
     2.5.4.5. Plan for and allocate missiles under the Tactical Air Missile Program (TAMP)
     for the accomplishment of 53 WEG WSEPs.
     2.5.4.6. Conduct an appropriate safety review for all weapons system evaluation flying.
  2.5.5. The 53 EWG/CC will:
     2.5.5.1. Conduct mission data programming, testing, and optimization.
     2.5.5.2. Conduct EW FME testing.
     2.5.5.3. Conduct Combat Shield assessments.
     2.5.5.4. Conduct a TRR for all assigned EW tests.
     2.5.5.5. Prepare an annual financial plan including resources required for each projected
     test.
     2.5.5.6. Develop and maintain a set of metrics for PACER WARE and other test-related
     electronic warfare operations.
     2.5.5.7. Provide EW technical expertise and support for all 53 WG EW test-related
     activities.
  2.5.6. The 53 WG/SE will:
     2.5.6.1. Provide a safety representative to support each test team, when requested.
     2.5.6.2. Review and coordinate on all test plans for safety considerations and mission
     risk management (RM).
     2.5.6.3. Develop and maintain RM templates and worksheets for test operations through
     the 53 WG RM manager.
     2.5.6.4. As available, attend all wing flight readiness reviews.
  2.5.7. The 53 TMG/TO. The 53 TMG/TO is the wing focal point for all test tasking from
  and reporting to HQ ACC, except for WSEPs. The 53 TMG/TO has direct liaison authority
  with ACC/A5T and USAFWC Staff. The 53 TMG/TO will:
     2.5.7.1. Coordinate the annual ACC Test Priority List (TPL), participate in the CAF
     Tactics Review Board (TRB) and TPL integrated product team (IPT) conferences, and
     coordinate all additions and deletions to the TPL.
     2.5.7.2. Coordinate all wing test documents (EPOs, plans, reports, fielding
     recommendations, TRPs, TEMPs, etc) through other group, safety, wing, and higher
     headquarters (HHQ) offices, as applicable.
     2.5.7.3. Coordinate all SEEK EAGLE flight clearances and aircraft modifications for all
     wing aircraft and non-wing aircraft supporting 53 WG testing except WEG-owned E-9
     and drone aircraft IAW 53 WGI 63-1101.
16                                                                53WGI99-101 6 MAY 2011


     2.5.7.4. Manage the wing’s conventional munitions requests and allocations. Submit
     wing munitions requirements to HQ ACC/A3TW per AFI 11-212 direction.
     2.5.7.5. Manage the wing’s A/A target requests and kill authorizations.
     2.5.7.6. Manage the wing’s telemetry instrumentation kit requests and allocations in
     support of test and evaluations IAW AFI 99-120.
     2.5.7.7. Develop and manage the wing aircraft instrumentation plan.
     2.5.7.8. Maintain and forward any published test plans and interim/final reports to the
     Defense Technical Information Center (DTIC) in accordance with the AFRIMS under
     Table and Rule T-99-01 R02.00.
     2.5.7.9. Ensure all published technical documents (i.e., test plans, final reports) are
     assigned a document number by scientific and technical information (STINFO)
     procedures according to the guidance in AFI 61-201, The Local Scientific and Technical
     Information Process, and AFI 61 202, USAF Technical Publications Program.
     2.5.7.10. Coordinate operational support for testing as required through ACC’s CAF
     Aviation Scheduling System.
     2.5.7.11. Manage the development and sustainment of the TMS.
     2.5.7.12. Post appropriate 53 WG test documents on the 53 WG Test Plans and Reports
     on the Secret Internet Protocol Router Network (SIPRNET) website after approval.
     2.5.7.13. Organize weekly consolidated test video teleconferencing and supply read-
     aheads to participants at least 24 hours prior.
     2.5.7.14. Act as the coordinating authority for 53 WG non-standard Global Positioning
     System (GPS) cryptographic key handling concept of operations (CONOPs) requirements
     IAW National Security Telecommunications and Information Systems Security
     Instruction (NSTISSI) 3006, Operational Security Doctrine for the NAVSTAR GPS User
     Segment.
     2.5.7.15. Maintain and update 53 WGI 99-101 and 53 WGI 63-1101.
  2.5.8. The 53 TMG/TR. The 53 TMG/TR is the wing focal point for test training and the
  TTH. The 53 TMG/TR will:
     2.5.8.1. Develop 53 WG test training course curriculum and conduct annual curriculum
     reviews.
  2.5.8.2. Publish an annual test training schedule and provide for course registration via the
  53 TMG/TR CoP.
     2.5.8.3. Conduct 53 WG test team training IAW Chapter 5 of this instruction.
     2.5.8.4. Track unit and individual test training completion and provide quarterly status of
     test training briefings.
     2.5.8.5. Maintain and update the 53 WG Test Team Handbook.
     2.5.8.6. Maintain the Test Information File (TIF) IAW Chapter 5 of this instruction.
53WGI99-101 6 MAY 2011                                                                      17


     2.5.8.7. Maintain a data base of all wing personnel in acquisition professional
     development program (APDP) coded positions and document APDP training status of
     each individual.
     2.5.8.8. Periodically organize Hanger Flys as directed by 53 TMG/CC.
  2.5.9. Squadron/Detachment Commanders will:
     2.5.9.1. Appoint test team members as appropriate for each wing-conducted, wing-
     supported, wing-integrated, or wing-monitored test project.
     2.5.9.2. Ensure all personnel supporting test operations are trained and qualified.
     2.5.9.3. Establish a process to document unit leadership review and approval of all
     mission run cards and test procedures.
     2.5.9.4. Prepare an annual financial plan including resources required for each projected
     test.
     2.5.9.5. Monitor the progress of assigned tests.
     2.5.9.6. Ensure timely dissemination of TIF information to test team members and
     establish a signoff method to ensure individual test team members have read the TIF.
  2.5.10. Project Manager (PM). A PM will be assigned as the 53 WG’s single POC for each
  project assigned to the wing. The PM will:
     2.5.10.1. Develop an EPO, preplan activities for subsequent ACC-managed or supported
     T&E, and ensure (in coordination with TMG/TO) the EPO is updated with significant
     information.
     2.5.10.2. Assemble a test team and prepare a test team assignment memorandum to be
     signed by all commanders providing support to the test team. The PM will designate a
     team member as the alternate PM in the memorandum.
     2.5.10.3. Confirm all test team members are adequately trained to execute their specific
     roles in assigned test missions.
     2.5.10.4. Establish an electronic records management (ERM) case folder for the project
     and maintain it IAW paragraph 4.2.3 of this document.
     2.5.10.5. Ensure the adequacy and completeness of test planning including compliance
     with test objectives. Ensure a comprehensive mission RM analysis is prepared and the
     summary results are included in the test plan.
     2.5.10.6. Monitor the activities of other commands or agencies involved in a particular
     acquisition effort for assigned projects.
     2.5.10.7. Identify and coordinate resources required for the test project. Obtain
     coordination of information contained within the test plans with other commands,
     services, or agencies for facilities, ranges, aircraft, personnel, logistics, engineering,
     funding, or information support.
     2.5.10.8. Ensure test data handling, processing, distributing, and archiving is IAW test
     requirements, applicable security policies, security classification guides, and GPS non-
     standard cryptographic key handling CONOPs.
18                                                                53WGI99-101 6 MAY 2011


     2.5.10.9. If not accomplished by assigned squadron munitions manager, submit
     munitions allocation requests to the wing munitions manager for inclusion into the
     forecast for planned tests. Report munitions expenditures to the munitions manager for
     quarterly and annual reports.
     2.5.10.10. Ensure Concept of Test Briefings (COTB), Combined Readiness Reviews
     (CRR), and Test Support Plan (TSP) FRRs, as applicable, are prepared and presented to
     the appropriate group commander(s). Prepare an email summary of the CRR and TSP
     FRR briefings and follow-up action to group commanders (for transmittal to 53 WG/CC).
     2.5.10.11. Ensure an environmental impact analysis is completed and approved before
     any decision to start testing as required. Where needed, submit AF Form 813 to
     appropriate environmental office for analysis.
     2.5.10.12. Develop and publish project test plans or test support plans, as required.
     2.5.10.13. Identify critical operational issues/objectives and ensure test is executed to
     answer those issues/objectives.
     2.5.10.14. Ensure TMS is current and accurate for all assigned tests.
     2.5.10.15. Prepare final reports and assist the ACC project officer in updating test
     information, as required.
     2.5.10.16. Prepare fielding/release recommendations, when required, within 30 days
     after the last test event and initiate coordination process.
     2.5.10.17. Submit TTPs to the 561st Joint Tactics Squadron (561 JTS) within 30 days
     after the last test event as applicable.
     2.5.10.18. Ensure a road show brief is prepared within 30 days of the last test event for
     any test which results in a fielding recommendation or provides TTPs for operational use.
     2.5.10.19. Participate in high performance teams (HPT) as directed when new
     capabilities documents are being developed.
     2.5.10.20. Participate in ITT; participate in test integrated product teams (TIPT) or test
     plan working groups (TPWG), as required.
     2.5.10.21. Participate in Joint Reliability and Maintainability Evaluation Teams
     (JRMET) to assist in collecting, analysis, verification, and categorization of reliability,
     maintainability, and availability data.
     2.5.10.22. For testing requiring 53 WG flying, other than TD&Es, ensure that the
     appropriate developmental test hazard/test safety risk management (i.e. risk management
     boards, test hazard analysis sheets, safety review boards, etc.) has been accomplished by
     the product development MAJCOM (normally AFMC).
53WGI99-101 6 MAY 2011                                                                        19


                                           Chapter 3

                                     TEST ENTERPRISE

3.1. INTRODUCTION. The 53 WG supports test efforts across the spectrum of a system’s life
cycle. The 53 WG units support developmental efforts of AFMC systems groups and the AFRL;
operational testing with AFGSC, AFSPC, AFOTEC, AATC, the 505 CCW, and other
MAJCOMs; and other services’ OT&E efforts. This chapter addresses requirements for
providing support to outside test agencies.
3.2. TEST AND EVALUATION MASTER PLAN (TEMP). The TEMP integrates
requirements, acquisition, T&E, and sustainment strategies, along with all T&E schedules,
funding, and resources, into an efficient continuum of integrated testing. The acquisition
program manager updates the TEMP before each major milestone of the acquisition program.
HQ ACC may task 53 WG for inputs to the TEMP.
3.3. TEST RESOURCE PLAN (TRP). The TRP is a document identifying the resources and
timelines required to support an AFOTEC or other agency-conducted test. All projects which
support AFOTEC-managed tests require a TRP. HQ ACC/A5T is the focal point for the ACC
TRP coordination process. HQ ACC/A5TT will task the appropriate staff agency and 53 WG to
coordinate and concur on the resource requirements cited in the TRP. Coordinated inputs will be
integrated into a formal concur or non-concur memorandum back to AFOTEC by HQ ACC/A5T.
3.4. INTEGRATED TEST TEAM (ITT). The ITT is a cross functional team of empowered
representatives from multiple disciplines and organizations and co-chaired by operational testers
and the acquisition program manager. The ITT is responsible for developing the T&E strategy
and TEMP, assisting the acquisition community with T&E matters, and guiding the development
of integrated test plans.
3.4.1. 53 WG/CC is normally the approval authority for ITT Charters which specify 53 WG
participation.
3.4.2. In cases where 53 WG is the operational test co-chair of an ITT, the 53 TMG/CC is
normally designated to fulfill this responsibility.
3.4.3. Any 53 WG subject matter expert may be tasked to support an ITT, at the discretion of the
project manager in coordination with the unit commander.
3.5. RESPONSIBLE TEST ORGANIZATION (RTO). The lead government developmental
test organization is designated the RTO. The RTO is qualified to conduct and oversee DT&E.
Neither AFOTEC nor MAJCOM operational testers may be the RTO.
3.6. OPERATIONAL TEST ORGANIZATION (OTO). The lead operational test
organization can be AFOTEC, a MAJCOM operational test organization, or another service
operational test organization.
3.7. PARTICIPATING TEST ORGANIZATION (PTO). The 53 WG can be designated as
the PTO to support a lead operational test organization or to support an RTO. PTO support
normally includes providing specific T&E data and/or resources for a T&E program or activity.
PTOs will:
   3.7.1. Participate in ITTs and TIPT as soon as they are formed and as required.
 20                                                                53WGI99-101 6 MAY 2011


   3.7.2. Assist other test organizations as described in program documentation and integrated
   test plans.
   3.7.3. Ensure T&E training is provided for PTO personnel involved in T&E activities.
3.8. DIRECTOR, OPERATIONAL TEST AND EVALUATION (DOT&E). OSD/DOT&E
maintains a list of major programs having congressional interest. Project managers owning
53 WG test projects of potential DOT&E interest should not submit any documents to or prepare
any briefings for DOT&E until they have had ACC/A5T confirm DOT&E has an interest in
receiving these specific reporting and briefing products. Any test documents or briefings going
to OSD will be coordinated through the 53 WG, ACC/A5/8/9 or AFGSC/A3 or USAFWC/CC,
and HQ USAF/TE, per tables 4.1 and 4.2. The following guidelines apply for 53 WG test
projects on OSD Oversight where DOT&E has confirmed its interest in receiving specific
products:
   3.8.1. Operational Test Concept Briefings. DOT&E may require a test concept briefing up
   to 180 days before the start of dedicated operational tests for programs on OSD OT&E
   Oversight. HQ USAF/TEP will arrange for corporate Air Force-level reviews of test concept
   briefings. A pre-brief to HQ USAF/TE and Air Staff agencies is normally required before
   going to brief DOT&E.
   3.8.2. Operational Test Plans and Test Plan Briefings. An operational test plan is due to
   DOT&E a minimum of 60 days before test start. DOT&E may request – or the operational
   test organization may elect to present – a briefing to accompany the final test plan. Test
   projects on OSD Oversight may not start active testing until DOT&E approves the adequacy
   of the test plan in writing. HQ USAF/TEP will assist with the review, coordination, and
   submission of this information.
   3.8.3. OSD Involvement. Once ACC/A5T confirms the interest of OSD in a test, direct
   communication with OSD action officers within the ITT is authorized to determine
   OSD/DOT&E involvement, testing, and reporting requirements. Where OSD action officers
   decline participation in test planning, or the T&E forum is unable to make a determination as
   to OSD involvement or requirements, ACC test agencies should elevate their concerns
   through ACC/A5T for adjudication and resolution.
   3.8.4. Operational Test Reporting Requirements.
       3.8.4.1. Significant Test Event Reports in OSD Oversight Projects. The PM will submit
       reports via appropriate coordination channels, to ACC/A5/8/9 within 24 hours of any
       significant test event as specified in the approved test plan. These reports will be routed
       to DOT&E through HQ USAF/TE.
       3.8.4.2. Final Reports. All final reports are due to DOT&E not later than 30 days prior
       to the decision review event being supported. These reports must strike the proper
       balance between system capabilities versus limitations while taking into account how
       well the system performed mission essential tasks. As applicable, a production or
       fielding recommendation should be documented within final OUE or FDE reports. All
       Category I deficiency reports (DR) and the top 10 Category II DRs will be listed. Final
       report briefings will be provided to HQ USAF staff and OSD as requested. HQ
       USAF/TEP will assist with the review, coordination, and submission of this information.
53WGI99-101 6 MAY 2011                                                                      21


       3.8.4.3. Interim Summary Reports. The PM will provide an interim summary report to
       OSD when a final report cannot be ready in time to support the associated acquisition
       decision review. A formal briefing may also be required. HQ USAF/TEP will assist
       with the review, coordination, and submission of this information.
   3.8.5. EW Programs. All EW programs on OSD Oversight are required to report progress
   annually in implementing the DoD T&E Process for EW Systems according to Public Law
   (P.L.) 103-160 §220(a). Test organizations for these programs will provide T&E information
   to HQ USAF/TEP as required in AFI 99-103.
   3.8.6. DOT&E Access to MAJCOM Test Information. When assisting DOT&E to fulfill its
   supervisory role for OSD Oversight programs, operational testers should be aware that Public
   Law allows DOT&E access to all operational test data and records within the DoD. Any
   delay in delivering test data to DOT&E must be based on practical limitations and not on
   concern over how the data might reflect on the program. See attachment 3 for additional
   guidance.
3.9. HQ, US AIR FORCE, DIRECTORATE OF TEST AND EVALUATION (HQ
USAF/TE). This organization functions as the chief T&E advisor to Air Force senior leadership.
It is responsible for establishing Air Force T&E policy, determining the adequacy of T&E
resources required to support weapons system development, and resolving T&E issues. HQ
USAF/TE responsibilities include:
   3.9.1. Acting as the final Air Force T&E review authority and signatory for TEMPs.
   3.9.2. Responding to and mediating T&E issues between HQ USAF principals, MAJCOMs,
   Air Force testers, other services, OSD, and Congress.
   3.9.3. Reviewing and/or preparing T&E information for release to OSD, and assuring timely
   availability of T&E results to decision makers. All 53 WG test plans, test reports, and
   briefings submitted to DOT&E for programs on OSD Oversight must be submitted through
   HQ USAF/TEP.
   3.9.4. Overseeing the Air Force T&E infrastructure by ensuring adequate resources to
   support system acquisition activities.
   3.9.5. Co-chairing the Air Staff Foreign Materiel Program Committee which provides
   Foreign Materiel Program management oversight and funding.
   3.9.6. Providing advice on ITT charter development and membership requirements. Review
   ITT charters for programs on OSD Oversight.
   3.9.7. Authorizing office for Air Force drone presentation and kills.
3.10. AIR FORCE OPERATIONAL TEST AND EVALUATION CENTER
(AFOTEC). AFOTEC plans and conducts OT&E for all ACAT I and II programs and those on
OSD OT&E Oversight, as required by Title 10. Refer to AFI 99-103, paragraph 4.6, to
determine the Operational Test Organization. ITTs and MAJCOMs will afford AFOTEC the
opportunity to review all other projects and programs to determine if an AFOTEC-conducted
OT&E is warranted. AFOTEC responsibilities include:
   3.10.1. Expeditiously determine its level of involvement for technology projects and
   acquisition programs, based on criteria specified in AFI 99-103. ACC operational testers
22                                                              53WGI99-101 6 MAY 2011


  may solicit AFOTEC involvement in projects not meeting the AFI 99-103 involvement
  threshold via a request through ACC/A5T.
  3.10.2. Acting as the operational test ITT co-chair. In cases of AFOTEC non involvement,
  MAJCOM operational testers must assume this responsibility.
  3.10.3. Helping prepare T&E strategies and integrated test plans and preparing the OT&E
  portions of the TEMP. The 53 WG units may be asked to contribute applicable FDE portions
  of the overall operational test plan for programs on OSD Oversight.
  3.10.4. Determining the quantity of test articles required for OT&E in consultation with the
  MAJCOM and the systems groups.
  3.10.5. Participating in certification of readiness for dedicated OT&E according to AFMAN
  63119. In cases of AFOTEC non-involvement, MAJCOM operational testers must assume
  this responsibility (see paragraph 4.3.12).
53WGI99-101 6 MAY 2011                                                                      23


                                          Chapter 4

                                      TEST PROCESS

4.1. TEST PHASES. The test phases for most projects assigned to the 53 WG are Project
Initiation, Planning, Execution, Reporting, and Close Out.
4.2. PROJECT INITIATION. The ACC TPL contains all assigned projects for the current
fiscal year, prioritized by the ACC staff, and approved by ACC/A5/8/9. ACC begins drafting the
next fiscal year’s TPL in February as part of ACC’s preparation of the “in-cycle” process. Test
projects are submitted to ACC/A5TT during its annual call-for-tests by the ACC staff, AFMC,
AFOTEC, AFGSC, AFSPC, other services, and other agencies requiring ACC testing or test
support. ACC compiles the list of tests and submits it to the ACC test agencies for validation.
EPOs are then drafted by the test agency and submitted to ACC to meet the required suspense
date for TPL coordination and approval. Projects not identified during this “in-cycle” process
are handled individually during the year as “out-of-cycle” tests. ACC/A5T sends a message to
the responsible test agency describing the test and requesting the agency prepare an EPO. The
process is outlined in figure 4.1. 17 TS will work directly with AFSPC/A3TW to ensure all
applicable space operational test projects are included on the ACC TPL.
 24                                                               53WGI99-101 6 MAY 2011


Figure 4.1. ACC Process for Out-of-Cycle EPOs




                                                                                .
   4.2.1. Electronic Project Order (EPO). An ACC-approved EPO is required for every 53 WG
   project; this is ACC’s authority to execute a test. The PM drafts an EPO and submits it for
   coordination and approval as described in tables 4.1 and 4.2. Beyond conducting research
   and attending meetings, PMs will not expend ACC resources without an approved EPO.
   4.2.2. End-of-Year Rollovers. Assigned projects which continue or are deferred into the
   next fiscal year will be automatically “rolled over” to the new TPL, retaining the original
   project order number. The 53 TMG/TO will add roll-over projects to the new TPL using the
   test milestones dates in TMS extending into the new fiscal year.
   4.2.3. Electronics Records Management (ERM) Case Folder. The PM is required to
   establish and maintain a separate case folder for each assigned project. The case folder will
   hold, at the minimum, the test team letter, the EPO, test plan, fielding recommendation,
   interim/final reports, analysis results, program introduction document/statement of
   capabilities (PID/SOC) documentation, environmental/safety approvals, and any
   amendments to these documents.
53WGI99-101 6 MAY 2011                                                                       25


  4.2.4. Test Risk Management. It is incumbent to reduce both technical and safety test risk
  through meticulous test team planning and judicious leadership oversight/reviews.
     4.2.4.1. Technical Risk. Drawing incorrect conclusions from a test can lead to either
     fielding a faulty system or failing to field a capable system. All 53 WG-conducted tests
     should be designed with statistical power and confidence, exploring as broad a spectrum
     of the battlespace as feasible. The principles of statistical design of experiments (DOE)
     are explicitly endorsed as a means to these ends. Regardless of the test design
     methodology used, project managers should describe how they will achieve desired levels
     of statistical confidence and power in their test programs.
     4.2.4.2. Operational Test Execution Risk. Hazards unique to operational test conduct
     will be identified. Mission RM is the test team’s primary tool for assessing, clarifying,
     and classifying test mission risk. Pre-existing hazards associated with the weapon system
     safety shall be considered in the mission risk analysis. Test mishap accountability will be
     clearly documented IAW AFI 91-204, Safety Investigations and Reports.
     4.2.4.3. Test Safety Risk. 53 WG aircraft and/or aircrew support to developmental test;
     or to conduct operational test which was not preceded by governmental developmental
     testing may involve test safety hazards to aircraft and/or aircrew not normally expected in
     the conduct of routine operational test. Developmental test safety risks are usually
     catalogued and assessed via an AFMC center test hazard analysis process (often called a
     risk management board [RMB]), which in turn, normally supports an AFMC operations
     group-level safety review (often called a safety review board [SRB]). AFMC test safety
     documentation (often called test hazard analysis sheets [THA]) should be thoroughly
     reviewed and understood by the 53 WG test support team prior to executing the requested
     flying support. Project teams should be especially vigilant when tasked to support testing
     for systems that have neither undergone developmental testing nor been assigned a USAF
     RTO.
         4.2.4.3.1. Any test support adjudicated by an AFMC test safety process to be
         elevated risk (Medium or High Risk) will not be agreed to, planned for, or executed
         by 53 WG project teams without the specific approval of the 53 TMG and 53 TEG
         commanders.
         4.2.4.3.2. Any test support adjudicated by an AFMC test safety process to be
         Medium or High Risk (aka “elevated risk”) will normally require a temporary
         possession transfer of the involved ACC flying asset to AFMC, as well as the
         assignment of qualified developmental test personnel to the appropriate aircraft
         aircrew positions (to include pilot in command, as applicable) for all elevated risk
         missions.
         4.2.4.3.3. HQ AFSPC may require an independent safety review of space operational
         test projects. 17 TS is authorized to work directly with HQ AFSPC/A3TW to
         determine and satisfy these safety review requirements.
         4.2.4.3.4. PMs will not proceed with test planning in cases where no governmental
         developmental testing has been accomplished and no USAF RTO has been assigned.
         PMs should contact 53 TMG staff for guidance on how to proceed.
     4.2.4.4. Technical Adequacy, Credibility, Operational Sufficiency.
26                                                                53WGI99-101 6 MAY 2011


        4.2.4.4.1. Technical adequacy addresses the relevance of the technical information
        produced by the test in relation to the purpose of the test. A test is technically
        adequate if the evaluation of test data provides the acquisition customer and the
        user/warfighter with decision-quality information (e.g., informs decisions to accept,
        acquire, produce, field, employ, etc.).
        4.2.4.4.2. Technical credibility addresses the depth of the technical information
        produced by the test. A technically credible test provides the acquisition customer
        and the warfighter with an indication of decision risk. Decision risk should be
        addressed by characterizing weapon system capabilities with the likelihood of an
        event occurring and the consequences of the event’s occurrence.
        4.2.4.4.3. Operational Sufficiency. Operational sufficiency addresses the breadth of
        the technical information produced by the test in relation to operations of new or
        modified capabilities within the context of representative employment and support
        concepts. The evaluation is considered operationally sufficient if it provides the
        acquisition customer and warfighter with results drawn from test events executed
        across sufficient operational conditions to identify the capabilities and limitations
        associated with employment and sustainment.
     4.2.4.5. Planning and Reviews. Test mission risk is further addressed through the use of
     detailed test plans and comprehensive leadership reviews. All T&E will be conducted
     with an approved plan. Test plans will be prepared so as to meet the defined objectives
     and those agreed to by the customer and organization conducting the test. Before
     conducting the test, each test plan will be subjected to technical and safety reviews in
     accordance with applicable Air Force and MAJCOM directives, as well as this
     instruction.
        4.2.4.5.1. TRR. A TRR is conducted before starting ground or flight testing and will
        assure a) system maturity is at the level required by the test, b) all technical
        preparations (including instrumentation) for initiating a test are adequately completed,
        and c) known system anomalies have not compromised successfully meeting the
        goals of the test.
        4.2.4.5.2. FRR. An FRR is conducted before flight testing and will assure a) the
        mission risk of executing the planned flight profiles is understood, manageable, and
        acceptable, b) successful execution of the plan will yield the test information desired,
        and c) known system anomalies will not jeopardize aircraft or aircrews.
        4.2.4.5.3. HQ AFSPC may require an independent test planning review and
        assessment of space operational test projects. 17 TS is authorized to work directly
        with HQ AFSPC/A3TW to determine and satisfy these test review requirements.
     4.2.4.6. Standard 53 WG Test Project Milestones.
        4.2.4.6.1. 53 WG-conducted operational flight testing. Standard milestones for 53
        WG-conducted operational tests involving flying include a comprehensive ORM
        analysis; as well as a test plan, test report, and fielding recommendation, all signed by
        the wing commander. Tests involving flying also normally include a COTB and CRR
        presentation to the applicable 53 WG management and execution group
53WGI99-101 6 MAY 2011                                                                          27


         commander(s), usually at a weekly (Thurs, 1300 Central Time) consolidated test brief
         video teleconference.
         4.2.4.6.2. 53 WG-supported flight testing. Standard milestones for 53 WG-supported
         tests involving flying include a comprehensive mission RM analysis; as well as a test
         support plan approved by the wing commander and a test support MFR signed by the
         unit commander. Support tests involving flying also normally include an FRR
         presentation to the applicable 53 WG execution group commander, usually at a
         weekly (Thurs, 1300 Central Time) consolidated test brief video teleconference.
         4.2.4.6.3. 53 WG ground-only testing. Standard milestones for tests that do not
         involve 53 WG flying include a comprehensive mission RM analysis; as well as a test
         plan, test report, and fielding/release recommendation signed by the applicable test
         management group commander (normally, 53 TMG or EWG commanders). All
         technical and safety reviews are delegated to the test management unit commander
         (i.e. squadron or detachment) and are not normally briefed at the Thursday group
         Consolidated Test Reviews. Unit commanders retain the right to up-echelon review
         and/or approval authority to group or wing leadership, for any ground test they
         consider of special interest or risk.
         4.2.4.6.4. DELETED
         4.2.4.6.5. AFSPC may require additional, independent reviews of space operational
         test projects. 17 TS is authorized to work directly with HQ AFSPC/A3TW to
         determine and satisfy these additional review requirements.
         4.2.4.6.6. Other supported testing. Some 53 WG test support involves assisting data
         collection on flying assets not assigned to or routinely employed by the 53 WG.
         Questions about appropriate 53 WG review/approval levels for flying tests that do not
         involve 53 WG aircraft or aircrews should be sent to the 53 TMG staff for
         adjudication.
  4.2.5. Document Coordination Process. The majority of test documents are handled
  similarly. However, there are a variety of different plans and reports which require different
  signature levels and coordination trails. As a basic guideline, most test/test support plans will
  be approved at the wing level if 53 WG efforts involve flying or space testing; otherwise
  non-flying, non-space test plans and all non-space test reports will be signed by the 53
  TMC/CC. Exceptions: DOT&E Oversight, WEG engineering and WSEP, mission data
  optimization, and Verification Flight Tests (VFTs). Document coordination should be
  handled as defined in table 4.1 and 4.2 below and processed IAW the timelines in table 4.3 to
  meet the desired suspense in table 4.4.
     4.2.5.1. Verification Flight Test plans will be approved by the 53 TMG/CC.
     4.2.5.2. AFSPC may require independent reviews and coordination for space operational
     test projects. 17 TS is authorized to work directly with HQ AFSPC/A3TW to determine
     and satisfy these additional review requirements.
     4.2.5.3. Managing units will initiate the document and unit commanders will ensure all
     test documents are coordinated with supporting/executing units prior to submitting the
 28                                                           53WGI99-101 6 MAY 2011


      document for group commander coordination. 53 TEG/CC has delegated test document
      coordination authority to its executing unit commanders.

Table 4.1. Coordination Matrix (other than 17th Test Squadron)
                  Execution
                      /     TMG     Execution/                           ACC/A5/8/9/A
                   Support    /      Support     WG/   WG/       AWC/       3 or
Document            Unit     CC       Group      SE    CC        CC/A5   AFGSC/A3/A5
EPO                   C       I         I        N/A    I          I         A
Concept of Test
Brief (COTB)
                     C        C          I       N/A     I         I          C1
for OSD
Oversight
Test Plan
                     C        C         I2        C      A        N/A        N/A
(fly)
Verification
Flight Test          C        A          I        C      I        N/A        N/A
(VFT) Test Plan
Test Plan
                     C        A          I        C      I        N/A        N/A
(ground)
Test Plan for
                     C        C          I        C      C         I          S1
OSD Oversight
SIMCERT Test
                     C        A        N/A       N/A     I        N/A        N/A
Plan
Test Support
Plan (TSP)           C        C         I2        C      A        N/A        N/A
(fly)
TSP
                     C         I        A         C      I         I         N/A
(NucWSEP)
TSP (ground)         C        A          I        C      I        N/A        N/A
TSP Waiver           C        A          I       N/A     I        N/A        N/A
Pause Test Msg       C        I          I       N/A    N/A       N/A        N/A
Stop Test Msg
                     C        A          I       N/A     I         I          I
(non-Oversight)
Stop Test Msg
                     C        C          I       N/A     A         I          C1
OSD Oversight
VFT Memo For
                     C        A          I       N/A     I        N/A        N/A
Record (MFR)
Test Report          C        A         I2       N/A     I        N/A        N/A
Test Report for
                     C        C         I2       N/A     C         I          A
OSD Oversight
SIMCERT
                     C        A         C        N/A     I        N/A        N/A
Report
Test Support
                     C        C          I       N/A     I        N/A        N/A
MFR
53WGI99-101 6 MAY 2011                                                                               29


                      Execution
                          /     TMG            Execution/                                ACC/A5/8/9/A
                       Support    /             Support        WG/       WG/     AWC/       3 or
Document                Unit     CC              Group         SE        CC      CC/A5   AFGSC/A3/A5
Fielding
Recommendatio              C            C            I2         N/A          S    A         Action
n
Release
Recommendatio            N/A           A           N/A          N/A          I    N/A        N/A
n
Sufficiency of
Test Review                C           A             I          N/A          I    N/A        N/A
(SOTR)
Capabilities and
Limitations                C           A             I          N/A          I    N/A        N/A
Report (C&LR)
Combat Shield
Quarterly and              C            I            C          N/A          A    N/A         I
Annual Reports
NucWSEP
Quick look                 C            I            A          N/A          I    N/A         I
Report
NucWSEP Final
                           C            I            C          N/A          A    N/A        N/A
Report
A/A & A/G
WSEP
                           C          N/A            A          N/A          I    N/A        N/A
Deployment
Report
A/A & A/G
WSEP, and
                           C            I            C          N/A          S    A           I
NucWSEP
Annual Report
Test Resource
                           C           A             I          N/A          I     I        Action
Plan (TRP)
Test and
Evaluation
Master Plan                C            C            I          N/A          C     I          S1
(TEMP) for
OSD Approval
TEMP for AF
                           C            C            I          N/A          A     I         N/A
PEO Approval
Integrated Test
Team (ITT)                 C            C            I          N/A          S    N/A        N/A
Charter
A-Approve (final approval authority; signature may or may not be required)
S-Sign (signature required, but is not the final approval authority)
 30                                                                           53WGI99-101 6 MAY 2011


                      Execution
                          /     TMG            Execution/                                     ACC/A5/8/9/A
                       Support    /             Support        WG/      WG/       AWC/           3 or
Document                Unit     CC              Group         SE       CC        CC/A5       AFGSC/A3/A5
C-Coordination (coordinates on the document; no signature required)
I-Information (no action required)
N/A-Not applicable to that office
1
  Coordination continues thru AF/TE to OSD
2
  Documents involving EW systems that are coordinated with the EW supporting units will also be coordinated with
the EWG/CC
53WGI99-101 6 MAY 2011                                                               31


Table 4.2. Coordination Matrix (17th Test Squadron)
                  SPC     TMG/    TEG/     SPC/       WG/   AWC/     ACC/     SPC/
   Document       Staff    CC      CC       SE        CC     CC      A5/8/9   HHQ1
EPO                C        I       I      N/A         I      I        A       N/A
Test Asset
Support
                   C        C       I      N/A        N/A    N/A      N/A      N/A
Request
(TASR)
Concept of Test
Brief (COTB)
                   T        C       I      N/A         I     C2       N/A      N/A
for OSD
Oversight
Operational
Asset Use
                   C        C       C      N/A         I      I        I        A
Request
(OAUR)
Counterspace
Activity
Approval           C        C       C      N/A         C      C        I       A3
Package
(CAAP)
Test Plan          T        C       I       C         A      N/A      N/A      N/A
Test Plan for
                   T        C       I       C          C      S2       I        I
OSD Oversight
Test Readiness
Review Board       C        I       I      N/A        N/A    N/A      N/A      N/A
(TRRB)
Start Test Msg      I       I       I      N/A        N/A    N/A      N/A      N/A
Pause Test Msg      I       I       I      N/A         I     N/A      N/A      N/A
Stop Test Msg
                   T        A       I      N/A         I      I        I       N/A
(non-Oversight)
Stop Test Msg
                   C        C       I      N/A        A       I       C2        I
OSD Oversight
                                                            Action
Crescent Edge      C        I       I      N/A         I      4        I       A3
Test Complete
                    I       I       I      N/A        N/A    N/A      N/A      N/A
Msg
Test Report        T        C       I      N/A        A      N/A      N/A      N/A
Test Report for
                   T        C       I      N/A         C     A2        I       N/A
OSD Oversight
Fielding
Recommendati       T        C       I      N/A         S      A        I      Action
on
Final Results
                   C        I       I      N/A        N/A    N/A      N/A      N/A
Briefing (FRB)
 32                                                                     53WGI99-101 6 MAY 2011


                      SPC      TMG/      TEG/       SPC/       WG/      AWC/       ACC/         SPC/
    Document          Staff     CC        CC         SE        CC        CC        A5/8/9       HHQ1
 Sufficiency of
 Test Review            T         A         I       N/A          I          I         I         Action
 (SOTR)
 Capabilities
 and Limitations        T         A         I       N/A          I          I         I         Action
 Report (C&LR)
 Test Resource
                        T         A         I       N/A        N/A        N/A         C           A
 Plan (TRP)
 Test and
 Evaluation
 Master Plan            T         C         I       N/A          C         A2         I           I
 (TEMP) for
 OSD Approval
 TEMP for AF
                        T         C         I       N/A          S        N/A        N/A         N/A
 PEO Approval
 Integrated Test
 Team (ITT)             T         C         I       N/A          S        N/A        N/A         N/A
 Charter
 A-Approve (final approval authority; signature may or may not be required)
 S-Sign (signature required, but is not the final approval authority)
 C-Coordination (coordinates on the document; no signature required)
 T-To be determined by AFSPC/A3T and 17 TS (internal AFSPC process not required by ACC/53 WG)
 I-Information (no action required)
 N/A-Not applicable to that office
 1
   As dictated by AFSPC directives or AFSPC/A3
 2
   Coordination continues thru AF/TE to OSD
 3
   Coordination continues through Air Staff and DoD Staff, as required
 4
   AWC responsible for informing COMACC and ACC/A5/8/9

Table 4.3. Coordination Timeline Guidance
                              30 Calendar Days after PM submits to Div Chief/Flt Commander to
Managing Unit
                              submit to Group
                              5 Work Days for Execution Squadron or Detachment/10 Work Days
Exec/Supt Unit
                              for Support Squadron (worked concurrently with above)
Test Management Group 10 Work Days
WG/SE                         1 Work Day (if required)
Outside Organizations         10 Work Days (if required)
These are goals set to expedite document processing and to provide managing units a guideline on
when documents need to be submitted to 53 TMG/TO to begin the upper-level review process.
 53WGI99-101 6 MAY 2011                                                                       33



 Table 4.4. Suspense Guidelines
Document                              Time Required              Comments

                                                                 EPOs must be approved by ACC
Electronic Project Orders (EPO)       As directed
                                                                 prior to start of test
Concept of Test Brief (COTB) for      Up to 180 Days Prior to
OSD Oversight                         Test (as determined by Must be at DOT&E
                                      DOT&E)
SIMCERT 30-day Notification           30 Days Prior to Test
                                                                 Signed by 53 TMG/CC
Message                               Start
                                      15 Days Prior to Test
Test Plan (fly or space)                                         Must be in WG/CC’s office
                                      Start
Verification Flight Test (VFT) Test   15 Days Prior to Test
                                                                 Must be in TMG/CC’s office
Plan                                  Start
                                      5 Days Prior to Test
Test Plan (ground)                                               Signed by Group/CC
                                      Start
                                      60 Days Prior to Test
Test Plan for OSD Oversight                                      Must be at DOT&E
                                      Start
Test Plan (WEG engineering test       5 Days Prior to Test
                                                                 Signed by WEG/CC
flights and confidence flights)       Start
                                      30 Days Prior to Test
SIMCERT Test Plan                                                Signed by TMG/CC
                                      Start
                                      15 Days Prior to Test
Test Support Plan (fly)                                          Must be in WG/CC’s office
                                      Start
                                      5 Days Prior to Test
Test Support Plan (ground)                                       Signed by TMG/CC
                                      Start
                                      5 Days Prior to Test
Test Support Plan (NucWSEP)                                      Signed by TEG/CC
                                      Start
                                      5 Days Prior to Test
Test Support Plan Waiver                                         Signed by TMG/CC
                                      Start
                                      30 Days Prior to the
Test Report for OSD Oversight         Supported Acquisition      Must be at DOT&E
                                      Decision Review
                                      150 Days after last test
Test Report (space)                                              Submitted to TMG/CC
                                      event
                                      150 Days after last test
Test Report                                                      Submitted to Group/CC
                                      event
                                      30 days after last test
Sufficiency of Test Review                                       Submitted to TMG/CC
                                      event
  34                                                                 53WGI99-101 6 MAY 2011


Document                              Time Required              Comments
                                      150 Days after last test
SIMCERT Report                                                   Signed by TMG/CC
                                      event
                                      As required by
VFT MFR                                                          Signed by TMG/CC
                                      supported organization
                                      30 Days after last test
End of Test Support MFR                                          Submitted to WG/CC
                                      event
                                      30 days after last test
Fielding Recommendation                                          Submitted to TMG//CC
                                      event
                                      30 days after last test
Road show briefing/TTP bulletin                                  Ready for delivery/distribution
                                      event
                                      30 days after last test
Release Recommendation                                           Signed by applicable Group/CC
                                      event
Combat Shield Quarterly and
                                      As required
Annual Reports
                                      2 Days after test team     Signed by TEG/CC and sent to
NucWSEP Quick look Report
                                      returns from mission       AFGSC/A3
                                      150 Days after Last
NucWSEP Final Report                                             Must be in WG/CC’s office
                                      Test Event
A/A & A/G WSEP Deployment             IAW COMACC Plans
                                                                 Signed by WEG/CC
Report                                85/90
A/A & A/G WSEP, and NucWSEP
                                      As soon as practical
Annual Report

NOTE: All days are calendar days

 4.3. PLANNING. Thorough planning is the key to a successful test project. While plans are
 not expected to be perfect, they must provide enough detail to allow the test team and leadership
 to schedule and fund resources, coordinate resources provided by other units, and safely manage
 the project. Note: AFSPC may require additional, independent reviews of space operational test
 projects. 17 TS is authorized to work directly with HQ AFSPC/A3TW to determine and satisfy
 these additional review requirements. At a minimum, PMs are required to update TMS with
 comments and milestone updates on a bi weekly basis during the Planning Phase. The following
 section provides some of the major areas the PM must consider and comply with during this
 phase.
    4.3.1. Test Team Assignment. One of the first steps a PM must accomplish is assembling
    the test team. Each test management commander will determine the process to assign unit
    members to the test team. For test team members outside the unit, the PM will prepare a test
    team assignment memorandum to be signed by all commanders providing support. The PM
    will designate a team member as the alternate PM in the memorandum. Following approval,
    the PM will file the memorandum in the project case folder.
    4.3.2. Research. The PM must conduct pre-test research for prior testing, system design
    capabilities, system performance requirements, operational performance requirements, and
53WGI99-101 6 MAY 2011                                                                    35


  evaluation criteria which may be referenced in the TEMP, the program management directive
  (PMD), the initial capabilities document (ICD), the capability development document
  (CDD), and the capability production document (CPD). Urgent operational need (UON)
  requirements can be found in AFI 10-601, Attachment 3A, Warfighter Urgent Operational
  Needs. The PM must research previous DRs, open discrepancies, watch items, or software
  problem reports as necessary.
  4.3.3. Test Integrated Product Teams (TIPT). The PM should use a TIPT to plan, organize
  staff, provide guidance for, and manage the test team. TIPT members are normally experts in
  specialized areas; many attendees at the TIPT meeting will represent other organizations
  having specific requirements for test support (e.g., range scheduling, safety, resource
  management, execution squadron scheduling, modifications, munitions, etc.). TIPTs are
  normally chartered by the acquisition program’s ITT.
  4.3.4. EPO Amendments. Typically, EPO amendments are required for changes in aircraft
  MDS, significant changes in scope, funding, or the test item. PMs should coordinate with 53
  TMG/TO to determine if an EPO amendment is necessary. EPO amendments are processed
  IAW the Test Team Handbook.
  4.3.5. Test Design and Analysis Methodology. If PMs decide to use a test methodology
  different from DOE, the specifics and justification will be documented in the project test
  plan.
  4.3.6. Concept of Test Briefing (COTB). The PM will provide a COTB to the managing and
  executing group commanders, or their representatives, before starting test plan development
  for all wing-conducted and wing-integrated tests. PMs will use 53 WG-approved COTB
  briefing sample formats. If there are fewer than 60 days between formal tasking of the test
  and the required test start date, the CRR will fulfill the requirement for a COTB unless
  otherwise requested by the group commanders. OSD may request a COTB for programs on
  OSD Oversight.
  4.3.7. Test Plan/Test Support Plan. The assigned PM will produce a test plan for all wing
  conducted/integrated and verification flight test projects, or a test support plan for wing
  supported test projects – including support under a blanket EPO – for the coordination and
  approval authorities listed in tables 4.1 and 4.2. Following approval, the 53 TMG/TO will
  post the approved test plan or test support plan on the 53 WG Test Plans and Reports
  SIPRNET website. PMs are responsible for disseminating the plan to appropriate ACC or
  other interested agencies. Dissemination may entail only notification of the posting of the
  plan on the SIPRNET and providing the information needed to access the plan.
     4.3.7.1. The 53 WG test management group commander may waive the requirement for
     a test support plan. A request for waiver should be submitted by the test management
     unit commander through 53 TMG/TO. This request should contain the rationale for the
     waiver, as well as a copy of the supported organization’s test plan, if applicable.
     4.3.7.2. Sample Formats. PMs will use the 53 WG-approved sample formats found in
     the Test Team Handbook “Templates” section. The Team Handbook is accessed by
     using the 53 TMG/TR Sharepoint site.
     4.3.7.3. Milestones.
36                                                                   53WGI99-101 6 MAY 2011


        4.3.7.3.1. The PM will submit the test plan/test support plan for coordination and
        approval with sufficient lead time to ensure the document is at the wing commander’s
        office NLT 15 calendar days before the scheduled test start (75 calendar days for
        projects on OSD Oversight) for flying tests and at the applicable management group
        commander’s office 5 days prior to test start for ground tests.
        4.3.7.3.2. For flying tests, the test plan/test support plan should normally be reviewed
        by the 53 WG/CC before conducting the TRR and /or the FRR briefing. If the wing
        commander review cannot be accomplished beforehand, then, as a minimum, the test
        plan/test support plan will be reviewed by the test management group commander
        before conducting the briefing.
     4.3.7.4. 1 Familiarization Sorties. Familiarization sorties can be flown anytime after the
     program office issues a military flight release permitting the system-under-test to be
     flown by OT. This program office authorization is normally informed by appropriate DT
     with associated written documentation (often called a Phase I release) advocating for
     release of the test item to OT. The military flight release is often restricted to specific tail
     numbers or units so as to preclude use by operational wings. Because these sorties are
     not a part of an approved DT, DT Support, or OT test plan; no formal 53 WG approval is
     required to fly familiarization sorties. Likewise, these sorties should not be counted as
     fulfilling test sortie requirements documented in approved test or test support plans.
     4.3.7.5. Range Coordination. Each range has specific requirements to support testing.
     The PM should contact the range(s) to be used and determine the requirements for test
     support. Most ranges require the PM to submit a PID to the range plans and programs
     office early in the planning process. Most range offices will accept draft test plans along
     with the PID to get a head start on the scheduling process. The range should respond
     with a SOC indicating the requested support can be provided and a rough order of
     magnitude (ROM) cost estimate. The PM must then make provisions with the range to
     fund expenses associated with the project. PMs will work these arrangements through
     the unit’s RA to execute mutually acceptable funding procedures. The PM will ensure
     the range’s final range safety approval (RSA) is available at the CRR.
     4.3.7.6. Environmental Requirements. The PM will coordinate with the range
     environmental office to complete any required environmental assessments. Some ranges
     require environmental assessments for every project while others may use existing
     environmental assessments covering the planned test activities. Long lead times may be
     necessary and early engagement by the PM is critical.
     4.3.7.7. Instrumentation Requirements and Considerations. PMs will ensure all
     instrumentation requirements for aircraft, weapons, and control room setups are identified
     early in the planning process to 53 TMG/TO. The PM must ensure data formatting is
     compatible with all sources and users including manufacturing contractors required for
     data analysis (Lockheed Martin, Boeing, Raytheon, etc.).
     4.3.7.8. Modifications and SEEK EAGLE Requests (SER). The PM will ensure all 53
     WG aircraft modifications/de modifications and SERs are submitted to 53 TMG/TO IAW
     53 WGI 63-1101.
53WGI99-101 6 MAY 2011                                                                            37


     4.3.7.9. Flight Clearances. The PM will ensure the responsible platform systems group
     has issued a flight clearance, in writing, for all aircraft modifications and configurations
     (hardware, software, stores, etc.) under test prior to flight.
     4.3.7.10. Munitions Planning. To ensure munitions required for the test are available,
     the PM will coordinate all munitions and expendable countermeasures with the wing
     munitions manager within the 53 TMG/TO (through their unit’s munitions manager, if
     applicable). Significant lead time may be required for munitions authorization, delivery
     or transfer, and build up. The PM will also ensure munitions build-up and configuration
     is coordinated with the host base.
     4.3.7.11. A/A Target Requests and Kill Authorizations. The PM will ensure appropriate
     target requests are submitted IAW Tactical Air Missile Program (TAMP) procedures to
     53 TMG/TO for either full-scale or sub-scale target drones. The PM will ensure the
     appropriate kill authorization is submitted and certified well in advance of the testing
     event.
     4.3.7.12. Telemetry Instrumentation Kits (TIK) and Flight Termination Systems (FTS).
     The PM will determine the need for and, if necessary, ensure procurement of TIK and/or
     FTS for the respective A/A or A/G test weapons through 53 TMG/TO. The PM will
     ensure TIK compatibility with the weapon, mission profile, and range telemetry
     downlinks. The PM will ensure the TIK and/or FTS costs are included in the EPO cost
     estimates.
     4.3.7.13. Test Plan/Test Support Plan Briefings. Normally, test plan/test support plan
     briefings are not required unless specifically requested by the PM or leadership. An
     exception to this policy may be when a test is on OSD Oversight (see paragraph 3.8.2)
  4.3.8. DELETED
  4.3.9. Test Plan/Test Support Plan Amendment. PM’s must ensure the 53 TMG/TO is
  notified, in writing, of all test plan amendments.
     4.3.9.1. An amendment not changing the scope of an approved test plan may be
     approved by the unit commander having test management responsibility for the project.
     Amendments requiring a change in scope must be approved by the original test plan
     approval authority.
     4.3.9.2. A change in scope amendment is defined as a change which significantly alters
     the size of the test; exceeds the previously approved test envelope; deletes or adds
     significant ground or flight test preparation events (modeling and simulation, captive
     carry flights, dress rehearsals, etc); deletes or adds significant test resources; significantly
     alters the mission profiles, changes the planned mission test/training range(s) to a
     different, geographically-separated range; introduces any new test hazards; modifies the
     previously approved test objectives; or changes the operational test agency (53 WG to
     AFOTEC, etc). Note: Any change to an approved OSD Oversight test plan is considered
     a change-in-scope.
     4.3.9.3. Taking into consideration the test management unit commander’s
     recommendation as to whether a test plan amendment changes its scope, the 53 TMG/TO
     will either initiate additional coordination/approval action on the amendment or keep the
38                                                                 53WGI99-101 6 MAY 2011


     unit-approved amendment on file. The coordination/approval process for change in
     scope test plan amendments will mirror those accomplished for the original test plan.
     4.3.9.4. PMs will use 53 WG-approved test plan amendment sample formats. Following
     test plan amendment approval, the 53 TMG/TO will post the approved document on the
     53 WG Test Plans and Reports SIPRNET website.
  4.3.10. Test Plan/Test Support Plan Approval Expiration. Authorization to execute testing
  under an approved 53 WG test/test support plan is automatically rescinded three years after
  the approval authority signature date. If testing has not been completed, the test/test support
  plan must be re-coordinated according to paragraph 4.3.7 of this instruction.
     4.3.10.1. Approval of test plan/test support plan amendments by the original 53 WG test
     plan/test support plan approval authority (i.e. “out of scope” amendments) also suffices as
     re-coordination for the original test/test support plan and any previously approved
     amendments, and will expire three years after the most recent approval authority
     signature date. Amendment packages submitted for approval should include all previous
     test plans/test support plans and amendments.
     4.3.10.2. 53 WG test plans approved under OSD Oversight are not subject to the three-
     year re-coordination requirements of this paragraph.
  4.3.11. Deficiency Review Procedures.
     4.3.11.1. Watch Items (WIT). WITs are anomalies observed and documented by
     developmental test organizations as described by TO 00 35D-54 (or OT&E organizations
     per paragraph 4.4.1.1). WITs are contained in a central database which can be
     maintained by either the government or the prime contractor. Some contractors may call
     their anomalies something other than WITs (e.g., Test Problem Reports, Alerts, CPRs,
     etc). During the planning process, it is very important for operational testers to review all
     WITs since they provide insight into the overall health of the test item. For test items
     which will be in lengthy developmental or integrated testing, PMs must establish a means
     to access the WIT database (periodic hard copies are acceptable). PMs must work
     closely with their AFMC counterparts to ensure all appropriate-severity WITs are
     converted to DRs IAW T.O. 00 35D 54 before starting dedicated OT&E. A DR review
     board will periodically review, validate, and prioritize all open DRs. DRs should be
     rank-ordered, and the most critical worked first or as agreed by the user(s), operational
     testers, and the RTO.
     4.3.11.2. Procedures for Handling Open Deficiency Reports. Open DRs, as well as
     capabilities deferred past the start of dedicated OT&E, must be reviewed and prioritized
     by a DR review board and an impact analysis performed. DRs having a high likelihood
     of precluding successful conduct of dedicated OT&E, should not be allowed to remain
     unresolved. Category I DRs must be fixed and closure verified according to an agreed
     upon plan. Category II DRs must be fixed and closure verified, or suitable workarounds
     provided. For DRs which cannot be resolved before starting dedicated OT&E, a plan
     must exist for testing deferred capabilities and fixes after dedicated OT&E is completed.
     A summary of these actions should accompany the Certification of System Readiness for
     Operational Testing.
53WGI99-101 6 MAY 2011                                                                       39


  4.3.12. Certification of System Readiness for Operational Testing. Per AFMAN 63-119,
  AFI 99103, and ACCI 99-101, all systems should undergo some form of review and
  certification of readiness, in writing, before starting OT&E. To be certified ready for OT&E,
  the system must be mature and demonstrate stabilized performance in an operationally
  relevant environment and all necessary test support must be available as planned. The
  system must have a high likelihood of a successful OT&E. Identified shortfalls will be
  remedied before testing starts or negotiated work-around solutions will be developed.
  Certification correspondence coordination/approval levels will be appropriate to the ACAT
  of the project being certified.
     4.3.12.1. If an ACC test organization is responsible for conducting the OT&E, it will
     perform the same certification functions as the OTA would have performed in accordance
     with AFMAN 63-119. ACC operational test organizations will participate in this
     certification process for FDEs and/or OUEs when full rate production and/or fielding
     decisions are planned.
     4.3.12.2. For 53 WG tests, the appropriate test management group commander will
     acknowledge, agree-to, caveat, or reject the acquisition authority’s certification
     recommendation, in writing, before starting OT&E. This certification document
     exchange provides the OT&E management group commander the opportunity to review
     the certification official’s assessment as well as address any unresolved issues.
  4.3.13. Pre-Test Group Readiness Reviews. [Note: AFSPC may require additional,
  independent safety reviews of space operational test projects beyond those listed below. 17
  TS is authorized to work directly with HQ AFSPC/A3TW to determine and satisfy these
  additional review requirements.]
  4.3.13.1. Combined Readiness Reviews. CRR is a decision briefing that combines a TRR
  and an FRR.
     4.3.13.1.1. PMs will use 53 WG-approved CRR briefing templates found in the 53 WG
     Test Team Handbook, located at the 53 TMG/TR Sharepoint site.
         4.3.13.1.2. The PM, with the assistance of the executing UPO/RPO, will normally
         present the CRR to the responsible test management and execution group
         commander(s), or their designated representative(s), after the test plan is reviewed by
         the approving authority and two weeks before the first test event (ground or flight).
         Normally, the PM will present the TRR portion of the briefing and the UPO/RPO will
         present the FRR portion of the briefing. A separate TRR and FRR may be
         accomplished if ground testing starts significantly earlier than flight testing.
         4.3.13.1.3. During the course of a test, if unexplained test results or anomalies occur
         which could have a detrimental effects on test safety, the PM must consider prior
         CRR approval-to-test to be rescinded and confer with the FRR approval authority for
         additional direction. PMs should also consider reconvening a CRR after any
         unplanned, extended break in test activity (test item fixes, test item recertification,
         etc).
         4.3.13.1.4. VFTs do not require a CRR. Project Managers are authorized to secure
         FRR approval for VFTs via any process satisfactory to the flying execution group
 40                                                                   53WGI99-101 6 MAY 2011


           commander. VFTs are not expected to be formally briefed at the weekly (Thurs,
           1300 Central Time) Consolidated Test Brief video teleconference.
           4.3.13.1.5. A CRR fulfills the test mission risk review requirements mandated in
           ACCI 99-101.
       4.3.13.2. Flight Readiness Reviews. Stand-alone FRRs are decision briefings normally
       used when test project technical reviews are not required (e.g. test support).
           4.3.13.2.1. Test Support project managers should utilize the TSP FRR briefing
           template in the Test Team Handbook in lieu of the CRR template.
           4.3.13.2.2. An FRR fulfills the test mission risk review requirements mandated in
           ACCI 99-101.
           4.3.13.2.3. The 53 WG is not required to conduct a mission RM or FRR on behalf of
           non-53 WG flying units that are actively participating in 53 WG-led testing. Instead,
           the 53 WG test management unit commander should send a briefing or signed MFR
           to each tasked operational unit’s Operations Group commander or his designated
           representative, describing the system-under-test and outlining execution expectations
           for the tasked operational unit(s). This briefing or MFR requirement does not apply
           to operational participants that are acting solely as adversary air or in a similar
           supporting role.
       4.3.13.3. DELETED
       4.3.13.4. DELETED
       4.3.13.5. DELETED
       4.3.13.6. DELETED
4.4. EXECUTION. The PM will manage the test team to ensure test events are executed IAW
the plan and the test item configurations are correct. Testing will not begin without an approved
EPO, a signed test/test support plan, and an approved test safety review (RM). Test team
operations analysts, suitability analysts, and data collection personnel must play lead roles in test
monitoring to resolve issues and to ensure correct data are being collected. Analysts will analyze
test data throughout the Execution Phase using DOE principles of test, pause, and analysis to the
maximum extent possible. They will determine if additional or different testing is required. The
PM will begin drafting any required test reports during the Execution Phase and update TMS
with comments and milestone updates on a weekly basis as a minimum.
   4.4.1. Documenting Test Item Deficiencies.
       4.4.1.1. Watch Items (WIT). During extended integrated testing, particularly where
       there is significant overlap between developmental and operational test, 53 WG testers
       have the latitude to enter deficiencies as WITs (instead of DRs) in the interest of getting
       rapid turnaround fixes (particularly OFP software) to the test item before it goes into
       dedicated OT&E. When using the WIT system to enter potential deficiencies, all testers
       should include as much information as possible. If instrumentation was used to collect
       data, the data should be preserved for the contractor’s use in defining and remedying the
       problem. Unconfirmed anomalies may be tracked during testing as WITs until the WIT
       is closed or until it is reported to the appropriate system program office in DR format.
53WGI99-101 6 MAY 2011                                                                        41


     4.4.1.2. Deficiency Reporting. All 53 WG testers should use the Joint Deficiency
     Reporting System (JDRS), and not an alternate deficiency tracking database, to report
     confirmed anomalies once dedicated OT&E begins. When a reportable condition exists,
     a DR will be submitted via JDRS IAW TO 0035D54 definitions and procedures.
     AFMPS DRs will use the mission planning database as directed by the mission planning
     TEMP. A project team member who encounters a situation warranting a DR will
     document the conditions. The PM will originate/review the submission to ensure it is
     properly categorized, valid, accurate, and complete. If the DR condition is discovered at
     a deployed location and the PM is unavailable, the detachment commander will act as the
     DR originator. The maintenance quality control organization at the deployed location
     will screen and release the DR when category time constraints require immediate action.
     The releaser will be instructed to include the support wing, if applicable, as an
     information addressee.
  4.4.2. Decertification and Recertification. If systems fail to perform adequately, and
  continuing dedicated OT&E is not in the best interests of the government, the appropriate
  group commander should declare a project “stop test” and recommend decertification to the
  acquisition authority/certifying official. When this recommendation is approved, the
  certifying official must issue a decertification message. Before the system resumes OT&E,
  the certifying official must again certify the system is ready via formal correspondence after
  appropriate corrective actions have been taken.
  4.4.3. Alternative to Decertification. For test concerns of a less serious or temporary nature
  (non flight safety related), the managing OT&E unit may declare a “pause test” to allow time
  to study and remediate these concerns. Managing OT&E unit commanders may resume
  testing when assured the concern was explained or the test item performance discrepancy
  was resolved. Any anomaly resulting in a concern for flight safety will immediately be
  elevated to “stop test” with an associated recommendation to decertify the test item or
  configuration. A series of “pauses” may indicate more serious problems requiring system
  decertification.
  4.4.4. Enhancement Reporting. When a condition exists that is not a deficiency, but
  identifies a recommended change to improve the system’s operational effectiveness or
  suitability, it should be submitted as an enhancement IAW TO 00-35D-54. Enhancements
  are not absolutely required for successful mission accomplishment. System DRs should not
  be categorized as enhancements simply because they are “out-of-scope” of the original
  contractor design.
  4.4.5. Interim Reports. As required, the PM will produce interim reports during the
  Execution Phase (usually at the end of test phases or periodically for long-duration projects).
  Formats for interim reports will adhere to 53 WG final report formats. The title will be
  adjusted to indicate the document is an interim report. To avoid multiple documents for a
  single project, the PM will consolidate any interim report information into the project’s final
  report.
  4.4.6. Fielding Recommendations. A fielding recommendation is normally delivered to
  ACC/A5/8/9, AFGSC/A5, or AFSPC/A3 shortly after the execution phase is complete, with
  the goal being submittal to the 53 TMG/CC no later than 30 days after the last test event.
  However, the PM will not initiate a fielding recommendation (positive or negative) until test
42                                                                53WGI99-101 6 MAY 2011


  results and the associated analysis are sufficient to support the recommendation. Fielding
  recommendations will be signed by the wing commander and endorsed by the USAFWC
  commander.
     4.4.6.1. A restricted fielding recommendation will be issued if the planned operational
     testing is complete and there are known constraints or deficiencies to using the system.
     The system may be restricted to certain employment regimes, restricted to training use
     only, employed only by certain aircraft types, employed only by selected aircrews, or
     have other restrictions as deemed appropriate until corrective actions are taken. These
     specific restrictions should be enumerated in the restricted fielding recommendation
     document. An amended fielding recommendation will be issued if additional testing
     confirms the constraints on the system no longer apply.
     4.4.6.2. An interim fielding recommendation will be issued if a fielding recommendation
     is requested before planned operational testing is fully completed. An interim fielding
     recommendation is normally used to expedite a system’s deployment to the field (UON)
     or to meet an acquisition or initial operational capability (IOC) milestone. The interim
     fielding recommendation should address all areas where adequate operational testing was
     not accomplished. As in a restricted fielding recommendation, specific restrictions to
     operations should be enumerated. A final fielding recommendation will be issued when
     all required testing is completed.
  4.4.7. Release Recommendations. Software intensive ground tests (e.g. mission planning)
  require the PM to produce a release recommendation to ACC staff for action. However, the
  PM will not initiate a release recommendation (positive or negative) until test results are
  sufficient to support the recommendation. Release recommendations will be signed by the
  53 TMG/CC.
  4.4.8. 3 Verification Flight Testing (VFT). To smartly use limited resources and streamline
  tasking processes, routine integration verification flight(s) using ACC and/or AFGSC aircraft
  may be requested to support the fielding decision of an aircraft subsystem or store. The
  purpose of VFT is to provide post-DT&E integration verification information without having
  to conduct formal operational test. In some cases, the 53 WG Sufficiency of Test Review
  (SOTR) PM (see para 4.4.8.1) may require several integration flights beyond developmental
  test to conclude that sufficient testing has been accomplished on the test item in question. In
  other cases, one or more aircraft System Groups (SG) may require several integration flights
  beyond developmental test prior to certifying the developmental product in question on their
  specific platform(s). Requests for authorization to conduct VFTs should be made by the
  applicable aircraft systems group and/or ACC program functional; or by the 53 WG SOTR
  PM (via 53 TMG/CC) to ACC/A5T or AFGSC/A3/5. VFT requests should be mission
  design series (MDS)-specific. Scope of VFT should involve no more than five integration
  sorties per request. VFT emphasis is on platform subsystem/store integration and should
  contain an absolute minimum number of stores free-flight events. VFTs will not be used in
  cases where no developmental flight testing has been accomplished.
     4.4.8.1. Sufficiency of Test Review (SOTR). In certain cases where there are minor
     hardware / software system changes and operational testing is not deemed as warranted
     by ACC, AFGSC, or AFSPC, the applicable MAJCOM functional will initiate a request
     to ACC/A5T to conduct a SOTR. If ACC/A5T approves the request, 53 WG will be
53WGI99-101 6 MAY 2011                                                                        43


     tasked to conduct a SOTR in lieu of operational testing. The assigned 53 WG PM will
     then research all relevant development, ground test, and flight test activity to determine
     whether the risk to field the system-in-question without additional testing is acceptable.
     SOTRs should recommend whether or not additional testing (DT&E and/or OT&E) is
     advisable prior to fielding. SOTR assessments will be released to ACC/A5T,
     AFGSC/A5B, or AFSPC/A3T by 53 TMG/CC.
     4.4.8.2. Capabilities and Limitations Report (C&LR). With an intent by ACC, AFGSC,
     or AFSPC to release hardware and/or software before completing planned formal testing
     (DT&E and/or OT&E), ACC/A5T may task the 53 WG to perform a C&LR of the
     developmental item in question. After C&LR tasking, the assigned 53 WG PM will
     research all relevant development, ground test, and flight test activity. This information
     will be provided to ACC, AFGSC, or AFSPC and will include all known system-under-
     test shortcomings, as well as the scope of planned testing which has not yet been
     accomplished. C&LRs should make no recommendation as to the advisability of
     releasing this item to the field – i.e. ACC, AFGSC or AFSPC assumes all fielding
     decision risk. C&LRs will be released to ACC/A5T, AFGSC/A5B, or AFSPC/A3T by
     53 TMG/CC.
  4.4.8.3. Verification Flight Testing (VFT). To smartly use limited resources and streamline
  tasking processes, routine integration verification flight(s) using ACC and/or AFGSC aircraft
  may be requested to support the fielding decision of an aircraft subsystem or store. The
  purpose of VFT is to provide post-DT&E integration verification information without having
  to conduct formal operational test. In some cases, the 53 WG Sufficiency of Test Review
  (SOTR) PM (see para 4.4.8.1) may require several integration flights beyond developmental
  test to conclude that sufficient testing has been accomplished on the test item in question. In
  other cases, one or more aircraft System Groups (SG) may require several integration flights
  beyond developmental test prior to certifying the developmental product in question on their
  specific platform(s). Requests for authorization to conduct VFTs should be made by the
  applicable aircraft systems group and/or ACC program functional; or by the 53 WG SOTR
  PM (via 53 TMG/CC) to ACC/A8T or AFGSC/A3/5. VFT requests should be mission
  design series (MDS)-specific. Scope of VFT should involve no more than five integration
  sorties per request. VFT emphasis is on platform subsystem/store integration and should
  contain an absolute minimum number of stores free-flight events.
  4.4.9. Classifying Test Project Information. All information associated with a test shall be
  classified in accordance with DoD 5200.1-R, Information Security Program, January 1997,
  and the appropriate program Security Classification Guides. For collateral classified test
  projects, information, media, and equipment shall be marked as specified in the Guidelines
  for Controlled Access Program Coordination Office (CAPCO) Markings, HQ USAF/A7SI.
  All unclassified information generated by a test will be treated as For Official Use Only as a
  minimum.
  4.4.10. Protection of Classified Test Project Materials. Physical security requirements
  applying to test project materials will be addressed as instructed in AFI 31-101, Air Force
  Installation Security Program.
  4.4.11. Disseminating 53 WG-generated Test Information. See attachments 2 and 3 for
  detailed test information and data release guidance.
44                                                                  53WGI99-101 6 MAY 2011


4.5. REPORTING.
  4.5.1. Final reports are required for wing conducted and integrated tests. The Reporting
  Phase starts after the last test event, which is normally defined as the last data collection
  event. PMs are required to update TMS with comments and milestone updates on a bi-
  weekly basis during the Reporting Phase. Note: HQ AFSPC may require additional space
  operational test project reports, variations on standard 53 WG test report formats, or require
  additional, independent reviews of draft space test reports. 17 TS is authorized to work
  directly with HQ AFSPC/A3TW to determine and satisfy these additional requirements.
  4.5.2. PMs will use the 53 WG-approved final report sample formats found in the Test Team
  Handbook. Deviations from these formats will be addressed by the PM in the final report’s
  coordination package. The PM will submit the final report package for coordination and
  approval with sufficient lead time to ensure the report is at the approval authority’s office not
  later than (NLT) 150 calendar days after the last data collection event. Final reports on OSD
  oversight must be to the wing commander 40 calendar days prior to the acquisition decision
  review being supported to meet AFI 99-103 requirements. PMs are responsible for
  disseminating reports to the appropriate ACC or other interested agencies. Dissemination
  may only entail providing notification-of-posting on the 53 WG Test Plans and Reports
  SIPRNET website along with the information needed to access the report.
  4.5.3. VFT results will be documented in an MFR. Sample format can be found in the Test
  Team Handbook. Approval resides with the appropriate test management group commander.
  The PM will submit the MFR results package for coordination and approval with sufficient
  time to meet the supported office’s need date.
  4.5.4. Wing-supported tests may require a final report or data package as agreed to by the 53
  WG PM. At a minimum, the PM will provide an MFR to the wing commander, signed by the
  unit commander, detailing the support provided. In the MFR, PMs need not include specific
  test results or judgments, but should limit the report to general information regarding
  timeline, resources expended, and unit responsible for providing the final report.
  4.5.5. Following report/MFR approval, the 53 TMG/TO will post the signed document on
  the 53 WG Test Plans and Reports SIPRNET website.
  4.5.6. Exemption from Requirement to Accomplish a Test Report. In cases where a unit
  commander concludes accomplishing a final report essentially duplicates information already
  formally documented in a companion release or fielding recommendation, the commander is
  encouraged to submit, with rationale, a “Request for Exemption from Accomplishing a Final
  Report” to the 53 TMG/CC for exemption approval. Expectation is these requests will
  normally involve ground test projects under the approval authority of the TMG commander
  (e.g. Mission Planning Environment testing, Agile Combat Support, Aircrew Training
  Systems, etc.). If test approval authority rests with the 53 WG/CC, the exemption request
  should reflect 53 TMG/CC coordination and 53 WG/CC approval.
  4.5.7. Test Report Briefings. Normally, test report briefings are not required unless
  specifically requested by the PM or leadership. Two exceptions to this may be when the test
  project is on OSD Oversight (see paragraph 3.8); and for space operational test projects
  where a Final Results Board (FRB) is used to inform space stakeholders.
53WGI99-101 6 MAY 2011                                                                     45


4.6. PROJECT CLOSE OUT. After the project is completed (final report or MFR report
signed), the PM will make final updates in TMS and advise the 53 TMG/TO to place the project
in “Complete” status.
   4.6.1. Road Shows/Field Training. PMs will budget for and include test results road shows
   in the EPO and project test plan. Road shows may be expanded to provide initial training to
   operational personnel on new software, equipment, and TTPs.
   4.6.2. Case Folder Disposition. Following a project’s close out, the PM will review the
   ERM based project case folder and delete non-essential correspondence and documentation.
   Data analysis results will remain in the ERM case folder along with a copy of the EPO,
   test/test support plan with all amendments, and all reports. Maintain documentation IAW
   guidance found in AFRIMS Table T99-02, Rule 2.03 (accessed through the AF Portal).
 46                                                                   53WGI99-101 6 MAY 2011


                                           Chapter 5

                                          TRAINING

5.1. GENERAL. Unit commanders are responsible for ensuring test team personnel are fully
trained before assuming unsupervised duties in their test specialty. Table 5.1 lists the mandatory
minimum requirements. Unit commanders may add to these requirements as necessary. Course
requirements may be waived based on an individual’s experience. The unit will send a waiver
request (e-mail memorandum) with justification to the 53 TMG/TR for applicable coordination.
The response from 53 TMG/TR will be filed in individual’s training record. Due to unique
WSEP training requirements, the WEG/CC is delegated responsibility for developing WEG team
training requirements to ensure that personnel are fully trained before assuming unsupervised
duties. This does not release responsibility of the WEG/CC for meeting the intent of these
guidelines.

Table 5.1. Test Team Training


                PM      UPO      OA      OSA     TE/FTE AIRCREW             TD/TC     SIMCERT
TTT              X        X       X        X         X            X            X           X
PMT              X                                                                         X
OS               X                         X
Intro to         X                         X                                               X
DOE
DOE for                   X                                       X
Aircrew
DOE                               X                  X
Foundations
DOE I & II                        X                  X
DOE III                           R                  R
MCRT             R        R       R                  R                         X
X—Mandatory           R—Recommended
Note 1: AFOTEC-equivalent courses can meet training requirements for 31 TES test personnel.
Note 2: Aircrew Training Device (ATD) PMs are required to take ATD suitability training in
lieu of wing suitability training.
Note 3: DOE II is not mandatory for EWG engineers, but is highly encouraged.
Note 4: 53 WEG and 53 TMG/Det1 – 49 TES are authorized to use local training to fulfill
MCRT requirements.
Note 5: 17 TS may have additional test team training requirements not reflected in this table.
53WGI99-101 6 MAY 2011                                                                           47


5.2. TEST TRAINING. The 53 TMG is responsible for developing and managing the
following wing-mandated test training courses:
  5.2.1. Test Team Training (TTT). This one-day course is intended primarily for new 53 WG
  personnel. It describes the relationship of the 53 WG to the T&E community, where
  individual team members fit in the testing/evaluation process, and how they support a 53 WG
  PM in planning, executing, and reporting tests/evaluations.
  5.2.2. Project Manager Training (PMT). The focus of this four-day course is managing
  wing-conducted and supported tests/evaluations.           The PMT course is designed to
  immediately follow the TTT course where PMs/SIMCERT directors receive baseline
  information related to operational testing. This course goes into greater detail of the test and
  evaluation process and provides specific tools for effective management.
  5.2.3. Operational Suitability (OS). This 2 -day course prepares PMs and OSAs to test and
  evaluate suitability of systems under test. The course provides detailed operational testing
  instruction of the 14 suitability areas including the major areas of reliability, maintainability,
  and availability.
  5.2.4. Design of Experiments (DOE) for Aircrew. This four-hour course is intended for
  aircrew and UPOs. It describes the process and principles of test design from an
  aircrew/execution perspective to promote effective test team interaction and understanding.
  5.2.5. Introduction to Design of Experiments. This two-day course is designed for PMs,
  OSAs, and SIMCERT directors. It provides an overview of the DOE process to understand
  factors affecting test results. This course explores the importance of efficient test and
  evaluation design to ensure maximum confidence in results with minimum resource
  expenditure.
  5.2.6. Design of Experiments Foundations. This one-week course provides a statistics
  review to prepare OAs and TEs for the applied DOE courses. Practical concepts of basic
  statistics are covered including the problem of experimental variation, graphical exploratory
  data analysis (histograms, box-plots), confidence intervals, types I and II error, and
  hypothesis testing.
  5.2.7. Design of Experiments I. This one-week applied statistics course provides OAs and
  TE/FTEs the fundamentals of experimental design -- constructing factorial test matrices to
  account for day-to-day environmental changes with emphasis on statistical modeling and
  factorial design.
  5.2.8. Design of Experiments II. This one-week applied statistics course extends the
  concepts learned in DOE I into the full complement of test matrix alternatives to handle
  many variables over multiple testing/evaluation periods (missions, days, and sessions).
  5.2.9. Design of Experiments III. This optional one-week course in advanced topics in DOE
  covers methods seasoned practitioners require to handle test and evaluation aberrations. The
  course equips the OA and TE with tools for attacking more challenging problems where
  either the variable composition or underlying response surface is complex, or violations of
  standard assumptions and principles occur.
  5.2.10. Mission Control Room Training (MCRT). A mission control room provides two-
  way communication with the aircrew and real-time telemetry capability to monitor safety of
 48                                                              53WGI99-101 6 MAY 2011


   flight and/or test data. This 30-minute web-based course located on the 53 WG CoP
   provides a foundation of mission control room procedures and communication, and fulfills
   the Test Control and Conduct Training academic requirement of the 53 WGI 99-104 test
   director/conductor checkout program.
5.3. TRAINING COURSE MANAGEMENT. Course registration, schedules, status of
training reports, instructions, and DOE material are located on the 53 WG CoP under “WG
Applications”, “Test Operations”, and “Training Course Management.”
5.4. OTHER TRAINING. Other training courses/resources are available to test and evaluation
team personnel through the Defense Acquisition University and Air Armament Academy.
5.5. TIF.
   5.5.1. The TIF is a formal avenue for disseminating test-related communication from wing
   or group leadership to the squadrons and detachments. Its purpose is to provide operational
   and mission direction, guidance, policy changes, or emphasis to wing units and individual
   test team members, while maintaining an accessible electronic library of current and
   rescinded information.
   5.5.2. The 53 TMG/TR will establish and maintain an electronic TIF library. The library,
   located on the 53 TMG/TR Sharepoint site, is divided into two read files: current and
   rescinded. Current TIFs are annotated with an assigned control number, subject, OPR,
   purpose, description, approving authority, and distribution/expiration dates. Rescinded TIFs
   are removed from the current read file, annotated with the date rescinded, and placed in the
   rescinded read file.
5.6. PRESCRIBED AND ADOPTED FORMS
   5.6.1. Adopted Forms. AF Form 847, Recommendation for Change of Publication. AF
   Form 813, Request for Environmental Impact Analysis




                                           MICHAEL E. GANTT, Colonel, USAF
                                           Commander
53WGI99-101 6 MAY 2011                                                                 49


                                          Attachment 1
         GLOSSARY OF REFERENCES AND SUPPORTING INFORMATION

References
P.L. 103-160 §220(a)
DoD 5200.1-R, Information Security Program , January 1997
AFPD 10-9, Lead Command Designation and Responsibilities for Weapon Systems,
AFPD 20-1/AFPD 63-1, Acquisition and Sustainment Life Cycle Management
AFPD 90-9, Operational Risk Management
AFPD 99-1, Test and Evaluation Process
AFI 10-601, Operational Capability Requirements Development
AFI 10-703, Electronic Warfare Integrated Reprogramming
AFI 10-706, Electronic Warfare (EW) Operations
AFI 10-707, Spectrum Interference Resolution Program
AFI 11-260, Tactics Development Program
AFI 31-101, Air Force Installation Security Program
AFI 36-2248, Operation and Management of Aircrew Training Devices
AFI 362251, Management of Air Force Training Systems
AFI 61-201, The Local Scientific and Technical Information Process,
AFI 61 202, USAF Technical Publications Program.
AFI 63-114, Quick Reaction Capabilities
AFI 63-1201, Life Cycle Systems Engineering
AFI 91-204, Safety Investigations and Reports
AFI 99-103, Capabilities-Based Test and Evaluation
AFMAN 33-363, Management of Records
AFMAN 63-119, Certification of System Readiness for Dedicated Operational Test and
Evaluation
ACCI 99-101, ACC Test and Evaluation
USAFWCI 99-103, USAFWC Test and Evaluation.
53 WGI 63-1101, Modification and Flight Clearance Management
53 WGI 99-104, Complex Test Mission Preparation and Control
NSTISSI) 3006, Operational Security Doctrine for the NAVSTAR GPS User Segment.
T.O. 00 35D 54WA-1, USAF Deficiency Reporting, Investigation, and Resolution System.
 50                                                          53WGI99-101 6 MAY 2011


T.O. 00-5-1-WA-1 Air Force Technical Order System
T.O. 00-5-3-WA-1 Air Force Technical Manual Acquisition Procedures
Air Force Global Strike Command (AFGSC)/CC Plan 001
Nuclear Weapon System Evaluation Program (NucWSEP) Combat Sledgehammer, COMACC
Plan 001
Air-to-Air Weapon System Evaluation Program, COMACC Plan 85
Air-to-Ground Weapon System Evaluation Program, COMACC Plan 90
53 WG Test Team Handbook.

Abbreviations and Acronyms
A/A—air-to-air
A/G—air-to ground
AATC—Air National Guard and Air Force Reserve Command Test Center
ACAT—acquisition category
ACC—Air Combat Command
ACCI—Air Combat Command Instruction
AFC2IC—Air Force Command and Control Integration Center
AFGSC—Air Force Global Strike Command
AFI—Air Force Instruction
AFMAN—Air Force Manual
AFMC—Air Force Materiel Command
AFOTEC—Air Force Operational Test and Evaluation Center
AFKN—Air Force Knowledge Now
AFPD—Air Force Policy Directive
AFRIMS—Air Force Records Information Management System
AFRL—Air Force Research Laboratory
ATD—advanced technology demonstration
ATD—aircrew training device
ATEC—Army Test and Evaluation Command
C&LR—Capabilities and Limitations Report
CAF—combat Air Force
CAPCO—Controlled Access Program Coordination Office
CCW—Command and Control Wing
53WGI99-101 6 MAY 2011                              51


CDD—capability development document
COI—critical operational issue
COMACC—Commander, Air Combat Command
CONOPs—– Concept of Operations
COTB—Concept of Test Briefing
CPD—Capability Production Document
CoP—Community of Practices
COTS—commercial-off-the-shelf
CPR—contractor performance reports
CRR—combined readiness review
CSAF—Chief of Staff of the Air Force
CTF—combined test force
DAA—Designated Approval Authority
Det— - Detachment
DoD—Department of Defense
DOE—Design of Experiments
DOT&E—Director of Operational Test and Evaluation
DR—deficiency report
DT&E—developmental test and evaluation
DTIC—Defense Technical Information Center
EPO—electronic project order
ERM—electronic records management
EW—electronic warfare
EWG—Electronic Warfare Group
EWIR—electronic warfare integrated reprogramming
FCT—foreign comparative test
FDE—force development evaluation
FMB—financial management board
FME—foreign materiel exploitation
FOT&E—follow-on test and evaluation
FOIA—Freedom of Information Act
FRR—flight readiness review
 52                                                           53WGI99-101 6 MAY 2011


FTE—flight test engineer
FTS—flight termination systems
FWG—financial working group
GFE—government furnished equipment
GPS—Global Positioning System
HPT—high performance team
HHQ—higher headquarters
HQ—headquarters
ICD—initial capabilities document
IOC—initial operational capability
IOT&E—initial operational test and evaluation
IPT—Integrated Product Team
ITT—Integrated Test Team
JCTD—Joint Capability Technology Demonstration
JDRS—Joint Deficiency Reporting System
JEFX—Joint Expeditionary Force Experiment
JRMET—Joint Reliability and Maintainability Evaluation Team
JT&E—Joint Test and Evaluation
JTS—Joint Tactics Squadron
JTTP—joint tactics, techniques, and procedures
MAJCOM—Major Command
MCOTEA—Marine Corps Operational Test and Evaluation Agency
MCRT—Mission Control Room Training
MD—mission data
MDAP—major defense acquisition program
MDO—mission data optimization
MDS—mission design series
MFR—memorandum for record
MOT&E—Multi-Service Operational Test and Evaluation
MPE—Mission Planning Environment
NDI—non-developmental item
NSTISSI—National Security Telecommunications and Information Systems Security Instruction
53WGI99-101 6 MAY 2011                                53


NucWSEP—Nuclear Weapon System Evaluation Program
OA—operations analyst
OA—operational assessments
OBAN—operating budget account number
OFP—operational flight program
OPTEVFOR—operational test and evaluation force
ORM—operational risk management
OS—operational suitability
OSA—operational suitability analyst
OSD—Office of the Secretary of Defense
OT&E—operational test and evaluation
OTA—operational test agency
OTO—operational test organization
OUE—operational utility evaluation
PA—public affairs
PACAF—Pacific Air Forces
PCA—performance characterization assessment
PID—program introduction document
PM—project manager
PMD—Program Management Directive
PMT—project manager training
POC—point of contact
PTO—participating test organization
QOT&E—qualification operational test and evaluation
QRT—quick reaction test
RA—resource advisor
RDS—records disposition schedule
ROM—rough order of magnitude
RPO—rated project officer
RSA—range safety approval
RTO—responsible test organization
SER—SEEK EAGLE request
 54                                               53WGI99-101 6 MAY 2011


SG—systems groups
SIMCERT—Simulator Certification
SIPRNET—Secret Internet Protocol Router Network
SOC—statement of capabilities
SOTR—sufficiency of operational test Review
STINFO—scientific and technical information
T&E—test and evaluation
TAMP—Tactical Air Missile Program
TC—test conductor
TD—test director
TD&E—tactics development and evaluation
TE—test engineer
TEG—Test and Evaluation Group
TEMP—Test and Evaluation Master Plan
TES—Test and Evaluation Squadron
TIF—Test Information File
TIK—telemetry instrumentation kit
TIPT—Test Integrated Product Team
TMG—Test Management Group
TMS—Test Management System
TP—test plan
TPL—test priority list
TRB—Tactics Review Board
TRP—Test Resource Plan
TRR—test readiness review
TSP—test support plan
TSS—Training Systems Squadron
TTH—Test Team Handbook
TTP—tactics, techniques, and procedures
TTT—test team training
UON—Urgent Operational Need
UPAR—unit public affairs representative
53WGI99-101 6 MAY 2011                                                                         55


UPO—unit project officer
USAF—United States Air Force
USAFE—United States Air Forces Europe
USAFWC—United States Air Force Warfare Center
USAFWCI—United States Air Force Warfare Center Instruction
VFT—Verification Flight Testing
WEG—Weapons Evaluation Group
WIT—watch items
WG—Wing
WSEP—Weapon System Evaluation Program

Terms
Acquisition Category (ACAT) Acquisition— categories determine the level of review,
decision authority, and applicable T&E policies and procedures. They facilitate decentralized
decision making and execution, and compliance with statutorily imposed requirements. See
DoDI 5000.02, enclosure 3 for details.
Advanced Technology Demonstrations (ATD)— These are advanced development efforts
used to meet the needs of employment concepts and capability requirements through “proof of
principle” demonstrations in operationally realistic environments.
Capabilities and Limitations Report (C&LR)— Warfighter operational needs may require
rapid and/or early fielding of new capabilities before operational testing is completed. The C&L
Report provides the most current operational test perspective on developmental system capabili-
ties and limitations based on testing done to date. C&L Reports will be based on existing,
verifiable T&E data (contractor, developmental, and operational) derived from all available
system development, ground, and flight test activities. A C&L Report does not obviate the
requirement for dedicated OT&E.
Critical Operational Issue (COI)— 1. Operational effectiveness and operational suitability
issues (not parameters, objectives, or thresholds) that must be examined during operational
testing to determine the system’s capability to perform its mission. 2. A key question that must
be examined in operational test and evaluation to determine the system’s capability to perform its
mission. Testers normally phrase a COI as a question to be answered in evaluating a system's
operational effectiveness or suitability.
Dedicated Operational Testing— Operational test and evaluation conducted independently
from contractors, developers, and operators and used to support production or fielding decisions.
Deficiency Report (DR)— The report used to identify, document, and track system deficiency
or enhancement data while a system is in advanced development, operational test, or operational
transition.
1. Category I DRs are those which could cause death, severe injury, severe occupational illness,
major loss or damage, or directly restrict combat or operational readiness if left uncorrected.
 56                                                                53WGI99-101 6 MAY 2011


2. Category II DRs are those which do not meet the criteria of a Cat I DR. They are attributable
to errors in workmanship, nonconformance to specifications, drawing standards, or other
technical requirements; or identify a problem for potential improvement or enhancement.
3. Enhancements are a type of Category II DR which identifies conditions that complement, but
are not absolutely required for successful mission accomplishment. The recommended
condition, if incorporated, will improve a system’s operational effectiveness or suitability.
Developmental Test and Evaluation (DT&E)— Test and evaluation conducted to evaluate
design approaches, validate analytical models, quantify contract technical performance and
manufacturing quality, measure progress in system engineering design and development,
minimize design risks, predict integrated system operational performance (effectiveness and
suitability) in the intended environment, and identify system problems (or deficiencies) to allow
for early and timely resolution. DT&E includes contractor testing and is conducted over the life
of the system to support acquisition and sustainment efforts.
Follow—on Operational Test and Evaluations (FOT&E) - The continuation of IOT&E or
QOT&E activities past the full-rate production decision. FOT&E answers specific questions
about unresolved COIs or completes areas not finished during the IOT&E or QOT&E. It ensures
the initial system acquisition process is complete.
Force Development Evaluation (FDE)— The operational test and evaluation of fielded,
operational systems during the sustainment portion of the system life cycle after acceptance for
operational use. The focus is on maintaining or upgrading operational systems after the initial
acquisition process is complete. An FDE also supports acquisition of MAJCOM-managed
systems.
Foreign Comparative Test (FCT)— A T&E program centrally managed by OSD which
provides funding for U.S. T&E of selected equipment items and technologies developed by
allied or friendly countries when such items or technologies are identified as having good
potential to satisfy valid DoD requirements.
Foreign Materiel Exploitation (FME)— FME projects are used to examine weapon systems
used by foreign countries and testing is generally focused on determining capabilities and
countermeasures.
High Performance Team (HPT)— An AF/A5RD facilitated team used to develop capabilities-
based requirements documents. An HPT consists of a lead (normally the sponsor), core team
(ideally 7 - 11 members, consisting of SMEs from the Air Force, government agencies, and other
Services as required) and support team members. The HPT accelerates the documentation
process and increases the potential for a quality document. Its overarching objective is to
capture, articulate, and document the operator’s requirements in minimum time, while achieving
stakeholder buy-in. The HPT leverages the expertise of all stakeholders by inviting them to
participate in the development of the document.
Initial Operational Test and Evaluation (IOT&E)— See Operational Test and Evaluation.
Integrated Test Team (ITT)— A cross-functional team of empowered representatives from
multiple disciplines and organizations and co-chaired by operational testers and the program
manager. The ITT is responsible for developing the T&E strategy and TEMP, assisting the
53WGI99-101 6 MAY 2011                                                                         57


acquisition community with T&E matters, and guiding the development of integrated test plans.
There is one ITT for each acquisition program.
Integrated Testing— Any combination of two or more types of testing used to achieve greater
test efficiency, reduced cost, and schedule savings without compromising the objectives and
needs of the participating test organizations.
Joint Capability Technology Demonstration (JCTD)— A demonstration of the military utility
of a significant new technology and an assessment to clearly establish operational utility and
system integrity
Joint Test and Evaluation (JT&E)— An OSD-sponsored T&E program conducted among
more than one military service to provide T&E information on combat operations issues and
concepts for the purpose of writing and publishing Joint Tactics, Techniques, and Procedures
(JTTP). JT&E does not support system acquisition.
Logistics Supportability— The degree to which the planned logistics support allows the system
to meet its availability and wartime usage requirements. Planned logistics support includes the
following: test, measurement, and diagnostic equipment; spare and repair parts; technical data;
support facilities; transportation requirements; training; manpower; and software.
Maintainability— The capability of an item to be retained in or restored to a specified condition
when maintenance is performed by personnel having specified skill levels, using prescribed
procedures and routines, at each prescribed level of maintenance and repair.
Measurable— Having qualitative or quantitative attributes (e.g., dimensions, velocity,
capabilities) that can be ascertained and compared to known standards. (See Testable.)
Measure of Effectiveness (MOE)— A qualitative or quantitative measure of a system’s
performance or a characteristic that indicates the degree to which it performs the task or meets a
requirement under specified conditions. MOEs should be established to measure the system’s
capability to produce or accomplish the desired result.
Measure of Performance (MOP)— A quantitative measure of a system’s capability to
accomplish a task, typically in the area of physical performance (e.g., range, velocity,
throughput, payload)
Measure of Suitability (MOS)— A qualitative or quantitative measure of a system’s readiness
to be placed and sustained satisfactorily in the field, with primary areas of interest being
reliability, availability, and maintainability.
Mission Data Optimization (MDO)— MDO is the 53 EWG formal approach for developing
“missionized” system software for EW systems. MDO is a special type of test which is
governed by AFI 10-706 and Air Combat Command Instruction (ACCI) 10-707, instead of AFI
99-103. MDOs will begin with a performance characterization assessment (PCA) of the threat to
be countered. These PCAs are scientific assessments to determine and quantify the capabilities
and vulnerabilities of the threat system for the purpose of jammer technique and/or emitter
identification development. PCAs are completed using generic EW techniques generation, and
do not test any current fielded EW system for techniques generation. Results from the PCA will
help the system engineers determine potential vulnerabilities of the threat that can be exploited
by the EW system's capabilities. System engineers enter an MDO with a trial set of
jamming/receiving techniques expected to provide better aircraft survivability than currently
 58                                                                   53WGI99-101 6 MAY 2011


fielded software. The trial set can be expanded or modified as results are determined. Results of
MDOs can lead to the formal release of new or better system software. MDOs usually are
executed as ground mount testing followed by flight testing to verify the ground test
performance.
Multi—Service Operational Test and Evaluation (MOT&E) - OT&E conducted by two or more
Service OTAs for systems acquired by more than one Service. MOT&E is conducted according
to the T&E directives of the lead OTA, or as agreed in a memorandum of agreement between the
participants.
Objective— An operationally significant increment above the threshold. An objective value
may be the same as the threshold when an operationally significant increment above the
threshold is not significant or useful.
Operational Assessment (OA)— An analysis of potential operational effectiveness and
suitability made by an independent operational test activity, with operator support as required, on
other than production systems. The focus of an operational assessment is on significant trends
noted in development efforts, programmatic voids, areas of risk, adequacy of requirements, and
the ability of the program to support adequate operational testing. Operational assessments may
be made at any time using technology demonstrators, prototypes, mockups, engineering
development models, or simulations, but will not substitute for the dedicated OT&E necessary
to support full production decisions.
Operational Effectiveness— Measure of the overall ability to accomplish a mission when used
by representative personnel in the environment planned or expected for operational employment
of the system considering organization, doctrine, tactics, supportability, survivability,
vulnerability and threat.
Operational Suitability— The degree to which a system can be placed and sustained
satisfactorily in field use with consideration given to availability, compatibility, transportability,
interoperability, reliability, wartime usage rates, maintainability, safety, human factors,
habitability, manpower, logistics, supportability, logistics supportability, natural environmental
effects and impacts, documentation, and training requirements.
Operational Test and Evaluation (OT&E)— 1. The field test, under realistic combat
conditions, of any item of (or key component of) weapons, equipment, or munitions for the
purpose of determining the effectiveness and suitability of the weapons, equipment, or munitions
for use in combat by typical military users; and the evaluation of the results of such test. (Title 10
§139(a)(2)) 2. Testing and evaluation conducted in as realistic an operational environment as
possible to estimate the prospective system's operational effectiveness and operational suitability.
In addition, OT&E provides information on organization, personnel requirements, doctrine, and
tactics. It may also provide data to support or verify material in operating instructions,
publications, and handbooks.
Operational Test Organization (OTO)— The operational test organization that has the
responsibility to plan, execute, and report on a test. There may be other operational test
organizations from within the USAF or other services that support the test, or may conduct
specific phases of the test.
Operational Utility Evaluation (OUE)— OUEs are a highly streamlined, tailored OT&E
activity designed to obtain a quick-look assessment of military capabilities and limitations.
53WGI99-101 6 MAY 2011                                                                          59


OUEs are specifically limited in time and scope and will not afford the same rigor as an IOT&E.
OUEs will only be used when an IOT&E, QOT&E, or FOT&E cannot be tailored to meet
unusual test program needs. OUEs cannot be used to replace IOT&E, QOT&E, or FOT&E.
AFOTEC or the MAJCOM OTA conducts OUEs.
Operational Test Agency (OTA)— An independent agency reporting directly to the Service
Chief that plans and conducts operational tests, reports results, and provides evaluations of
effectiveness and suitability on new systems. NOTE. Each Service has one designated OTA:
The Air Force has the Air Force Operational Test and Evaluation Center (AFOTEC). The Navy
has the Operational Test and Evaluation Force (OPTEVFOR). The Army has the Army Test and
Evaluation Command (ATEC). The Marine Corps has the Marine Corps Operational Test and
Evaluation Agency (MCOTEA).
Operator— Refers to the operating command which is the primary command operating a
system, subsystem, or item of equipment. Generally applies to those operational commands or
organizations designated by Headquarters, US Air Force to conduct or participate in operations
or operational testing, interchangeable with the term “using command” or “user.” In other
forums the term “warfighter” or “customer” is often used.
Oversight— Senior executive-level monitoring and review of programs to ensure compliance
with policy and attainment of broad program goals.
Oversight Program— A program on the OSD T&E Oversight List for DT&E, LFT&E, and/or
OT&E. The list includes all ACAT I (MDAP) programs, ACAT II (major system) programs,
and any other programs selected for OSD T&E Oversight. These programs require additional
documentation and have additional review, reporting, and approval requirements.
Participating Test Organization (PTO)— Any test organization required to support a lead test
organization by providing specific T&E data or resources for a T&E program or activity.
Performance Characterization Assessments (PCA)— PCAs are scientific assessments to
determine and quantify the capabilities and vulnerabilities of the threat system for the purpose of
jammer technique development and/or emitter identification development.
Qualification Operational Test and Evaluation (QOT&E)— A tailored type of IOT&E
performed on systems for which there is little to no RDT&E-funded development effort.
Commercial-off-the-shelf (COTS), non-developmental items (NDI), and government furnished
equipment (GFE) are tested in this manner.
Reliability— The capability of a system and its parts to perform its mission without failure,
degradation, or demand on the support system.
Responsible Test Organization (RTO)— The lead government developmental test organization
on the ITT that is qualified to conduct and responsible for overseeing DT&E
Seamless Verification— A concept for structuring T&E to more effectively support the
requirements and acquisition processes so new capabilities are brought to operators more
quickly. Seamless verification promotes using integrated testing procedures coupled with tester
collaboration in early requirements definition and system development activities. It shifts T&E
away from the traditional “pass-fail” model to one of providing continuous feedback and
objective evaluations of system capabilities and limitations throughout system development.
 60                                                                 53WGI99-101 6 MAY 2011


Simulator Certification (SIMCERT)— SIMCERT ensures that Air Force prime mission
system simulators/services and their components support accurate and credible training for
allocated tasks, missions, and events including DMO activity, through verification and validation
of training system hardware and software performance.
Sufficiency of Operational Test Report— For some programs of limited scope and complexity,
system development testing or integrated developmental and operational test events may provide
adequate operational test data to support MAJCOM fielding decisions. In these situations, the
lowest appropriate level of required MAJCOM operational testing may consist of a review of
existing data rather than a separate, dedicated operational test event. The SOTR may only be
used to inform MAJCOM or user system fielding decisions. It may not be used as the sole source
of operational test information for any type of acquisition milestone or production decisions. The
SOTR may not be used for acquisition milestone decisions associated with OSD OT&E
Oversight programs unless approved by DOT&E.
Survivability— The capability of a system and crew to avoid or withstand a man-made hostile
environment without suffering an abortive impairment of its ability to accomplish its designated
mission. Survivability consists of susceptibility, vulnerability, and recoverability.
Sustainment— 1. The provision of personnel, logistic, and other support required to maintain
and prolong operations or combat until successful accomplishment or revision of the mission or
of the national objective. 2. The Service's ability to maintain operations once forces are
engaged. 3. Activities that sustain systems during the operations and support phases of the
system life cycle. Such activities include any investigative test and evaluation that extends the
useful military life of systems, or expands the current performance envelope or capabilities of
fielded systems. Sustainment activities also include T&E for modifications and upgrade
programs, and may disclose system or product deficiencies and enhancements that make further
acquisitions necessary.
Tactics Development and Evaluation (TD&E)— TD&E is a tailored type of FDE specifically
designed to further exploit doctrine, system capabilities, tactics, techniques, and procedures
during the sustainment portion of the system life cycle. TD&Es normally identify non-materiel
solutions to tactical problems or evaluate better ways to use new or existing systems.
Testable— The attribute of being measurable with available test instrumentation and resources.
NOTE. Testability is a broader concept indicating whether T&E infrastructure capabilities are
available and capable of measuring the parameter. The difference between testable and
measurable may indicate a test limitation. Some requirements may be measurable but not testable
due to T&E infrastructure shortfalls, insufficient funding, safety, or statutory or regulatory
prohibitions.
Test and Evaluation (T&E)— The act of generating empirical data during the research,
development or sustainment of systems, and the creation of information through analysis that is
useful to technical personnel and decision makers for reducing design and acquisition risks. The
process by which systems are measured against requirements and specifications, and the results
analyzed so as to gauge progress and provide feedback.
Test and Evaluation Master Plan (TEMP)— Documents the overall structure and objectives
of the T&E program. It provides a framework within which to generate detailed T&E plans and
it documents schedule and resource implications associated with the T&E program. The TEMP
53WGI99-101 6 MAY 2011                                                                           61


identifies the necessary developmental, operational, and live-fire test activities. It relates
program schedule, test management strategy and structure, and required resources to: COIs;
critical technical parameters; objectives and thresholds documented in the requirements
document; and milestone decision points.
Test and Evaluation Organization— Any organization whose designated mission includes test
and evaluation.
Test and Evaluation Strategy— The overarching integrated T&E plan for the entire acquisition
program that describes how operational capability requirements will be tested and evaluated in
support of the acquisition strategy. Developed prior to Milestone A, the T&E strategy addresses
modeling and simulation, risk and risk mitigation, development of support equipment, and
identifies how system concepts will be evaluated against mission requirements, among other
things. The T&E strategy is a precursor to the test and evaluation master plan.
Test Event— Any flight or ground event designed to collect data for the purpose of determining
effectiveness, suitability, or TTPs in a formal test environment. All test events should be defined
in the test plan.
Test Integrated Product Team (TIPT)— Any temporary group consisting of testers and other
experts who are focused on a specific test issue or problem. There may be multiple TIPTs for
each acquisition program.
Test Limitation— Any condition that hampers but does not preclude adequate test and/or
evaluation of a critical technical parameter, operational requirement, or critical operational issue
during a T&E program.
Test Team— A group of testers and other experts who carry out integrated testing according to a
specific test plan. NOTE. A combined test force (CTF) is one way to organize a test team for
integrated testing.
Threshold— A minimum acceptable operational value below which the utility of the system
becomes questionable.
Waiver— A decision not to conduct OT&E required by statute or policy.
 62                                                                  53WGI99-101 6 MAY 2011


                                          Attachment 2
                       TEST INFORMATION RELEASE (GENERAL)

A2.1. Test Support Information/Data. For 53 WG tests supporting outside agencies (AFOTEC,
AFMC, etc.), test results and data should only be released to the test director of the primary test
agency. The 53 WG personnel will not make value judgments, written or oral, or discuss test
data, findings, conclusions, or results outside the test team without specific written approval from
that test agency. In cases where data collected is used by both the supported organization and the
53 WG to fulfill individual test plan requirements, test results and data release by the 53 WG
should be pre-coordinated with the supported agency.
A2.2. Release of 53 WG-conducted test information to entities outside the DoD. Generally,
53 WG personnel do not have the authority to release information to any organization outside the
DoD (see routine test data exception in paragraph A1.3.3.1). To secure release of 53 WG/CC
approved test reports, the requesting non-DoD organization must submit a written request to the
DTIC (DTIC Form 55). For all other 53 WG test products not normally archived at DTIC (e.g.
fielding recommendations, draft reports, preliminary assessments) or for final test reports not yet
archived at DTIC, requests for release should be submitted directly to ACC/A5T (or ACC/A3T
in the case of a TD&E); AFGSC/A5B (or AFGSC/A3T in the case of a TD&E) for tests
sponsored by AFGSC; or AFSPC/A3T for tests sponsored by AFSPC.
A2.3. Release of 53 WG-conducted test information to entities within the DoD. For 53 WG
conducted tests, no 53 WG personnel will make value judgments, written or oral, or discuss
findings, conclusions, or results outside of ACC, AFGSC, or AFSPC (as applicable) without
approval from the 53 WG/CC (or the cognizant 53d group commander in cases of mission
planning or electronic warfare mission data optimization testing). The PM should ensure 53 WG
leadership is aware of any potentially unfavorable test results before discussing or distributing to
ACC/AFGSC/AFSPC staff or other government organizations. Note: See attachment 2 for
additional guidance on releasing information to DOT&E.
   A2.3.1. Fielding/release recommendations and reports. A signed fielding or release
   recommendation constitutes 53 WG/CC consent to discuss the contents of that fielding or
   release recommendation within the DoD. A signed final report constitutes 53 WG/CC
   consent to release that report and all associated test data within the DoD.
   A2.3.2. Preliminary assessments/findings will not be presented or released to DoD
   organizations outside ACC/AFGSC/AFSPC without the written approval of the 53 WG/CC
   (or the appropriate 53d group commander for mission planning or electronic warfare mission
   data optimization testing). Note: The 29 TSS simulator certification teams are authorized to
   out brief preliminary results to the commander (or his designated representative) of the
   inspected organization (within or outside of ACC) upon completion of SIMCERT, without
   specific approval from 53 WG/CC.
   A2.3.3. Test Data.
       A2.3.3.1. Routine Test Data Requests. Routine test data is information collected via data
       or video recording systems onboard or off board (via telemetry) the weapons system
       under test. Any routine data collected during the course of a weapons system platform
       test required by the prime contractor to remedy deficiencies and/or validate system
53WGI99-101 6 MAY 2011                                                                         63


       performance may be delivered to the acquisition systems group and/or system prime
       contractor at the discretion of the test PM. It is desirable, but not mandatory, to deliver
       this data to the prime contractor via the responsible systems group. Care must be taken to
       not deliver the subsystem proprietary information of another contractor (e.g. munitions)
       to the weapons system platform prime contractor.
       A2.3.3.2. Special Test Data Requests. Special test data is data collected external to the
       weapons system platform under test (i.e. munitions telemetry, range weapons impact
       scores, weapons impact camera video, etc). This data is often proprietary information
       owned by a contractor other than the platform prime contractor. Release of special test
       data always requires the approval of the 53 WG/CC (or the cognizant 53 WG group
       commander for mission planning or electronic warfare mission data optimization testing)
       and should only be released to a DoD agency, normally the acquisition systems group
       having programmatic cognizance over the specific item under test (e.g., Joint Direct
       Attack Munition (JDAM) data would be released only to the JDAM acquisition systems
       group). The appropriate DoD agency (to include ACC/AFGSC/AFSPC staff) has the
       authority, in turn, to release this data to the test item contractor(s) without additional
       approval from the 53 WG/CC. Note: In cases where sensitive and/or controversial test
       findings/data/assessments may have a major negative impact on an acquisition program,
       53 WG personnel should not transmit or discuss any information outside of the 53 WG
       (including with ACC/AFGSC/AFSPC staff) without first discussing ramifications of test
       results with and securing release approval from the 53 WG/CC.
       A2.3.3.3. Tactics, Techniques, and Procedures (TTPs). After ensuring coordination with
       and consensus of all 53 WG execution unit commanders using the weapons system in
       question (e.g. 85 TES and 422 TES commanders for F-16 TTP), the originating TTP unit
       commander is authorized to release TTPs to the CAF via the most expedient means
       available. Direct dissemination of TTPs to joint organizations and/or other service
       components must be approved by the 53 WG/CC prior to transmittal. Target Location
       Error (TLE) findings from 53 WG operational testing are considered TTP for test
       information release purposes.
       A2.3.3.4. Tactics and system performance “road shows” will be provided to field units as
       directed by the test plan.
A2.4. Release of Test Information to Foreign Nationals. HAF/CVAII, through the foreign
disclosure office, must review all requests and approve release of all technical information prior
to providing to foreign nationals, governments, or agencies.
A2.5. Freedom of Information Act (FOIA). The 53 WG/CCEA, DSN 872-0053 or (850) 882
0053, will process written FOIA requests as specified in DoD 5400.7.
Note: In accordance with DoD Directive 5230.25, Withholding of Unclassified Technical Data
from Public Disclosure, test plans and test reports are exempt from release under FOIA.
A2.6. Public Release of Information. One of the most important tools to showcase the 53 WG is
an aggressive and proactive Public Affairs (PA) program. Wing-generated information that has
the potential for public release includes ongoing test activity and results, as well as associated
conference/symposium test results briefings. 53 WG CC/CV will approve release of any media
items involving 53 WG test technical information. 96 ABW/PA will provide a security and
 64                                                                53WGI99-101 6 MAY 2011


policy review of text and imagery proposed for public release, and will act as the primary
releasing authority. In collaboration with 96 ABW/PA, the 53 WG command section will
determine if HHQs (USAFWC, MAJCOM, Air Staff, etc.) also need to review any potentially
controversial media item prior to public release.
   A2.6.1. For technical text and imagery generated using source data from a single 53 WG
   unit’s test project(s), the applicable unit technical advisor will review the product for
   technical and capabilities public releasability. For technical text and imagery generated
   using source data from more than a single unit’s test project(s), the applicable group
   technical advisor will review the product for technical and capabilities public releasability.
   A2.6.2. 96 TW/PA will provide a security and policy review of the text and imagery
   proposed for public release, and will act as primary public release authority for the product.
   A2.6.3. 53 WG CC/CV will be advised by applicable units of any technical media items
   approved by 96 TW/PA for public release prior to public presentation. In collaboration with
   96 ABW/PA, the 53 WG command section will determine if HHQ(s) – USAFWC,
   MAJCOM, Air Staff, etc. – also need to review any potentially controversial media items
   prior to public release.
53WGI99-101 6 MAY 2011                                                                          65


                                         Attachment 3
     TEST INFORMATION RELEASE TO DIRECTOR, OPERATIONAL TEST &
                           EVALUATION

A3.1. The 53 WG continues to play a key role in assessing several programs on the OSD Test
and Evaluation Oversight list.
A3.2. As required by Public Law, Title 10, DOT&E formulates independent assessments of new
combat capability for major defense acquisition programs (MDAP) or other special interest
programs. To make its assessments, DOT&E relies heavily on AFOTEC’s and ACC’s
operational test data and results. There are several items to keep in mind when assisting DOT&E
to fulfill its mandate.
   A3.2.1. AFI 99-103 explains the Air Force process for delivering formal operational test
   documents (Concept of Test Briefings, Test Plans, Test Reports, etc.) to DOT&E.
   A3.2.2. Regarding test data and information, Public Law grants DOT&E access to all test
   data and records within DoD to facilitate fulfilling its charter of oversight of operational
   testing. Any delay in presenting operational test data to DOT&E must be based on practical
   limitations and not on concern over how that data might reflect on the particular program.
   A3.2.3. Before delivering test data and information (particularly preliminary aircrew or test
   team assessments) to any outside agency, including DOT&E, it must be reviewed and
   validated. Passing inaccurate information could result in flawed conclusions, faulty
   assertions and recommendations, and ultimately, misinformed leadership decisions. Review
   and release of effectiveness data for ACC/AFGSC/AFSPC projects on OSD Oversight is
   normally accomplished by the 53 TMG/CC. Review and release of MD for ACC/AFGSC
   projects on OSD Oversight is normally accomplished by the 53 EWG/CC. Review and
   release of suitability data for projects on OSD Oversight is normally accomplished by the
   cognizant program office in ACC/A5/8/9, AFGSC/A5, or AFSPC/A3. Government
   generated WITs that have not been adjudicated and selected for inclusion in the USAF JDRS
   are not considered reviewed and validated information.
   A3.2.4. All information shall be presented to DOT&E in an objective manner. AF/TE,
   AFOTEC, ACC, AFGSC, AFSPC and the affected acquisition program leadership should be
   appraised of all test information releases and should be made aware ahead of time, the
   specifics of any release that could potentially result in an unfavorable DOT&E assessment.
A3.3. The following guidance applies to all 53 WG members queried directly by DOT&E to
deliver program/test information or data products:
   A3.3.1. Queried 53 WG members should refer the DOT&E requestor to the managing group
   commander.
   A3.3.2. For 53 WG operational testing on OSD Oversight, 53 WG intent is to provide data
   products in the form of complete “mission packages.” These packages will normally include
   applicable third party data and analysis (frequently referred to as “PTO data”), validated pilot
   questionnaires, and validated engineer/analyst notes. Decision authority as to the specific
   composition of mission packages, as well as the delivery medium, normally resides with the
   53 TMG/CC. In the case of mission data products, this authority normally resides with the
 66                                                                 53WGI99-101 6 MAY 2011


   53 EWG/CC. All 53 WG products provided to DOT&E will also be made available to
   AFOTEC, as requested.
   A3.3.3. Depending on the nature of the test, these mission packages may also include
   cockpit video as well as other unique electronic presentation products generated by the 53
   WG. Due to the size of these files, the unique application software often required to view
   them, and/or wing manpower limitations; 53 WG policy is to provide DOT&E
   representatives access to these storage intensive products on-site at the applicable 53 WG
   location (see paragraph A3.4 for guidance).
   A3.3.4. Since third party data and/or analysis will potentially not be delivered to the 53 WG
   on a schedule which permits adequate time for DOT&E assessment, the 53 TMG/CC or the
   53 EWG/CC, as applicable, will consider DOT&E requests for delivery of specific subsets of
   mission package products on a case-by-case basis. For the reasons mentioned in paragraph
   A3.3.3, 53 WG policy is to normally provide DOT&E representatives access to partial
   mission packages, on-site at the applicable 53 WG location (see paragraph A3.4 for
   guidance), in lieu of transmitting partial mission packages.
   A3.3.5. When an IOT&E/FOT&E is in progress, all government-generated items for USAF
   Deficiency Report consideration, as well as all suitability data collected are controlled and
   adjudicated by AFOTEC. DOT&E requests to review IOT&E/FOT&E generated deficiency
   and suitability data should be referred to the applicable AFOTEC detachment. At the
   completion of IOT&E/FOT&E, control of suitability data will normally revert to the 53 WG.
A3.4. The following guidance applies to DOT&E on-site visits to 53 WG locations.
   A3.4.1. For specific projects on Oversight, DOT&E representatives are permitted supervised
   access to all applicable 53 WG facilities to which they are security-cleared. Government
   DOT&E representative visits will be hosted by a 53 WG O-4/GS-13 or higher, as
   appropriate. DOT&E contractor (IDA) representative visits will be hosted by an appropriate
   level 53 WG government technical expert.
   A3.4.2. Visit hosts will honor all DOT&E requests to see Oversight program
   information/data generated by 53 WG testing, with the following exception: deficiency and
   suitability data generated by IOT&E/FOT&E are controlled and adjudicated by AFOTEC.
   DOT&E requests to review IOT&E/FOT&E-generated deficiency and suitability data should
   be referred to the applicable AFOTEC detachment.
   A3.4.3. DOT&E visitors shall be advised that all working papers/records, to include WITs,
   post-mission pilot questionnaires, and engineers’/analysts’ notes, are preliminary in nature,
   and are not considered valid until officially released by the 53 WG. DOT&E visitors must
   also be reminded that preliminary information is potentially inaccurate and could lead to the
   drawing of faulty conclusions with respect to assessment of operational effectiveness and/or
   suitability of the test item. Confirmation that this verbal caveat has been delivered to each
   visiting DOT&E representative should be annotated in the host’s visit summary.
   A3.4.4. All 53 WG DOT&E visit hosts will e-mail a “visit summary” to the appropriate 53
   WG project manager within 24 hours of completion of the DOT&E representative’s visit.
   This visit summary should include the name(s) of visitor(s), name(s) of host(s), dates of visit,
   topics covered, data/information reviewed, and items the DOT&E representative(s) took
   particular interest in. As a minimum, all visit summaries should also be forwarded to the
53WGI99-101 6 MAY 2011                                                            67


  cognizant test management unit commander, as well as the 53 TMG/CC or 53 EWG/CC (as
  applicable), and the appropriate AFOTEC detachment commander.
